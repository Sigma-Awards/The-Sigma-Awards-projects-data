Country,Publishing organisations,Organisation size,Results,Category,Project title,Publication date,Tags,Technologies tools used,A short description of the project,What was the impact of the project,What tools techniques technologies did you use and how did you use them,What was the hardest part of this project What should the jury know to better understand what you did and why it should be selected ,What can other journalists learn from this project,Project link 1,Project link 2,Project link 3,Project link 4,Project link 5,Project link 6,Project link 7,Who made this project,Short biography/ies,,,
Malaysia,Malaysiakini,Big,Participant,Best visualization (small and large newsrooms),The 2020 Malaysian federal budget,10/11/19,"Explainer,Quiz/game,Infographics,Chart,Economy","Personalisation,Json,CSV"," The Budget 2020 microsite consists of three components; an infographic, a quiz and a poll.    The goal was to convey massive information of the Malaysian federal budget in a simple, comprehensible, timely and personalised manner.   The infographic gives an overview of the budget by visualising the big numbers with different charts along with explanations.   The quiz is to help Malaysians understand how they are impacted by the latest federal budget with personalised information by answering some questions.   The poll component allowed readers to share how they felt about key announcements in the budget.    "," The Budget 2020 microsite was received well by the public. In the first two days after it was launched, the microsite received more than 92 thousand unique page views with an average time of 3 minutes and 57 seconds. Within a week, the microsite received 168 thousand unique visitors in total.   The budget quiz poll also served as a springboard for public discussion of the 2020 federal budget, with readers debating the pros and cons of each new policy when we shared the poll results on social media. There were more than 231 thousand people participated in the poll.   It was the second time for Malaysiakini to build a microsite for the budget and we started to see a trend for it as other Malaysian newsrooms, such as <a href=""https://sites.thestar.com.my/budget2020/visual-story.aspx"">The Star</a> and Nanyang Siang Pau [<a href=""https://www.enanyang.my/?p=1300318"">1</a>][<a href=""https://www.enanyang.my/?p=1301026"">2</a>][<a href=""https://www.enanyang.my/news/20191012/%E3%80%90%E4%BA%92%E5%8A%A8%E6%B8%B8%E6%88%8F%E3%80%91%E8%B4%A2%E6%A1%88%E5%85%B3%E6%88%91%E4%BD%95%E4%BA%8B/"">3</a>] to produce similar works. In other words, it has created an impact in the Malaysian news industry to produce more data and visual-driven interacitve journalism work.  "," We used HTML, CSS and JavaScript, React, Google Firebase, amCharts and D3 to create the microsite.   For the infographic, we used data visualisation JavaScript library amCharts and D3 to render the charts and tables.   Generating charts and tables with JavaScript libraries, without relying on any third-party tool, was particularly useful for a multi-language news portal like Malaysiakini. Once the JavaScript files are properly written, we only need to modify the data once and the outcome will be shown for three languages.   We wrote a dedicated JavaScript script for the quiz. We store the data (the questions and answers) in a separate JSON file. Each object in the JSON file contains a question, the choices, the answers, and most importantly, an id.   The script allows us to show a question for our readers based on the previous answers to the previous question. It was done by identifying the id of the questions. The purpose of it is to make the answers to be more personalised.   For example, if a reader answer that she is a woman in a question, the next questions will be some policies or announcements from the budget that related to her gender.   The poll was created with React library to make sure that it has a faster loading speed, especially when drawing the polling result from Google Firebase. Using Google Firebase also prevent us from dealing with the issue of spam votes.    "," The hardest part of this project is to launch the project in the interest of time. The public’s attention span on the budget announcement could be very short, so our aim was to publish it within the same day when the budget was announced before our readers losing their interest in it.   It was quite challenging because the budget was announced by the Malaysian finance minister in the parliament at 4pm. His speech took about 2 hours to finish, which left us about 6 hours before 12 midnight to process the speech that contains more than 10,000 words, extract the salient points, write them in a personalised quiz and poll format, then test and retest the quiz and poll, and create charts for the infographic, then translated all of them from English to Malay and Chinese version.   Luckily, we have done much of the work for the infographic a day before as most of the data is obtained from the economic outlook report that was released two days before. However, it was not the same for the quiz and poll because this could only be done after the speech was delivered in the Parliament.   Furthermore, with the experience of producing similar works for last year’s budget, we had better planning and workflow to avoid any unnecessary mistakes which happened in 2019.   Having said that, we still suffer from a technical issue as our server crashed at midnight, just minutes before the quiz and poll were launched. As a result, the release of the poll and quiz were delayed for three hours.   Another challenge is the ability in explaining the big numbers in the budget in a simple, comprehensible, timely and personalised manner. To achieve that, we decided to use simple charts to present the data.    "," Data journalism and visualisation often take time, but for this project, we managed to come out with three different products within a short period of time.    Thus, we think that the key aspect that others can learn from us is how to produce a data-driven project with the mentality of producing breaking news.   We cannot stress enough that project management plays an important role to ensure that it was a success. We had planned and prepared for the execution two months ahead.   Most importantly, we hope to inspire more newsrooms, especially the newsrooms in Malaysia, to flourish by working on more data-driven visual storytelling.    ",https://pages.malaysiakini.com/budget2020/en/,https://pages.malaysiakini.com/budget2020/en/infographic/,https://pages.malaysiakini.com/budget2020/en/quiz/,https://pages.malaysiakini.com/budget2020/en/poll/,,,,"Lee Long Hui, Sean Ho, Nigel Aw, Koh Aun Qi, Aidila Razak, Syariman Badrulzaman, Wahyudi Mohd Yunus, Thiaga Raj Servai, Ng Xiang Yi"," Lee Long Hui is an assistant editor of Malaysiakini. He is also the project leader and programmer for this budget project. Long Hui has been a journalist for 10 years. Since 2018, he has led the Kini News Lab, a unit within the newsroom which aims to use innovation and technology to do better journalism. Under his leadership since 2018, the Kini News Lab has produced two dozen projects and set a benchmark for the Malaysian journalism industry.   Sean Ho is the Senior Products Manager and web designer for the project. Sean primarily works in web design and product management at Malaysiakini, and had previously volunteered to work on various editorial projects. As a result of several successful editorial projects, he is now seconded to the newsroom support the Kini News Lab.   Syariman Badrulzaman is the graphic designer of Malaysiakini and also one of the members of the Kini News Lab.   Thiaga Raj Web Developer. Raj contributed to the creation of the poll with React and Google Firebase.   Nigel Aw is an assistant editor of Malaysiakini. In 2015, his work won an award for editorial excellence in investigative reporting from the Hong Kong-based Society of Publishers in Asia (SOPA).   Aidila Razak is the special report editor of Malaysiakini. She has 10 years experience as a journalist, covering Malaysian current affairs. She now works on investigative, in-depth and data features. She was a shortlisted Society of Publishers Asia award and was named Online Journalist of the Year by the peer-reviewed National Press Club award.   Wahyudi Yunus is the assistant editor of Malaysiakini Malay desk. He contributed to the translation work and narrative of the infographic.   Ng Xiang Yi is a journalist of Malaysiakini's Chinese desk. She joined Malaysiakini in 2018 and has been covering current affairs of Malaysia. Xiang Yi contributed to the translation work of this project.   Koh Aun Qi was a sub-editor of Malaysiakini. She was also one of the founding members of the Kini News Lab. During her stint with Malaysiakini, Aun Qi had been actively involved in the projects conducted by the news lab. Aun Qi was also one of the project leaders to the 2019 budget microsite special project.    ",,,
China,Caixin Media,Big,Participant,Best data-driven reporting (small and large newsrooms),"We Interviewed 384 College Students about Their Relationships with Supervisors, They said...",26/01/19,"Investigation,Long-form,Database,Infographics,Politics,Culture,Crime,Human rights","Animation,D3.js,Canvas,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,CSV"," The project starts with an animation of student suicide cases, which were caused by pressure and insults from their superiors. Then it led to the analysis of real responds on students’ interactions with their superiors beyond academical levels, and the effects caused by the interactions on students based on the answers we collected by questionnaires. The project demonstrated how the UK government react to students’ complains toward their superiors, and pointed out what steps should be taken in China regarding student-superior relationships. "," The questionnaires about student-superior relationship caused active discuss among college teachers when the questions were first handed out in colleges. For many teachers, it was the first time that they take serious consideration on how it might affect student’s emotional and phycological condition when the students were asked to do things they were not supposed to do——help making superior’s presentation PowerPoints, or help taking care of superior’s family members.   The project has been massively read and forwarded after published. Many readers are still filling our questionnaire at the bottom of this project to share their stories and opinions months after it was published. All in all, it is a very influential and long-lasting project. "," The opening animation was made by mental canvas by our illustrator.   We created the questionnaire by an online tool called Wenjuanxing, the questionnaire was posted and collected online.   Project was developed by d3.js. "," Data gathering was definitely the hardest point. Since there was no existing data about student-supervisor relationship, we developed our own questionnaire to collect data. We’ve collected 384 valid feedbacks, which reached the minimum sample size for research purpose. The lack of data openness in China makes data collection the most difficult part.   It is important to know that it took 9 months to complete this project. We did not just make it fast to get the project done, we waited for a long time in order to get a reasonable sample size. It’s a project that we determined to do to help the student from bad relationships with their superiors.  "," People can learn that lacking of existing data should not be the reason to abandon an important project. When there’s no existing data, we can collect data on our own, even if it could be time consuming.   It is our first time using mental canvas to make animation, it is also the first time a Chinese data journalism team using mental canvas to make animation. It is always good to learn that a data journalism team should never stopped trying new tools and new ideas.  ",http://datanews.caixin.com/mobile/interactive/2019/gaoxiao/,http://datanews.caixin.com/mobile/interactive/2019/gaoxiao/img/story_1.mp4,,,,,,"Miao Ding, Jinghua Li, Jiaxin Liu", journalist: Miao Ding   illustrator/designer: Jinghua Li   designer/developer: Jiaxin Liu ,,,
France,Les Jours,Small,Shortlist,Innovation (small and large newsrooms),« When data meets politics »,14/01/19,"Investigation,Explainer,Database,Open data,OSINT,Infographics,Chart,Politics","Scraping,JQuery,Json,Google Sheets,CSV,Python","« When data meets politics » is a serie of eight data-driven stories based on different databases created by Wedodata from day 1 of Macron's presidency in France. Parliamentary debates, vote on laws, Twitter messages, official diaries… All information concerning French elected representatives and the government is recorded. In 2019, two episodes have been published on LesJours.fr, a French online-only news website (six in 2018). They are devoted to the exploration of French political life through the prism of Wikipedia: one on the reputation of French elected representatives and the other on the publishing war of Wikipedians on these political"," Since 2016, LesJours.fr has published 130 journalistic series on various topics : migrants, environnement, corruption, health, economic and political scandals... But this independant media - awarded by the Albert Londres prize, the French equivalent of the Pulitzer prize, in 2017 – hadn’t published before a data investigation and therefore it is a new ground for them and their subscribers.  The audience of this “data serie” was one of the best among their 130 series,  surely because it mixed the following 4 ingredients: the subject of politics ; new editorial angles thanks to the data ; “stories articles”, an innovative format inspired by Snapchat or Instagram and created for the occasion, ideal to read on mobile ; and visual articles offering datavisualizations, photos and GIFs.     The format of story proposed by a media directly on its site  and not by forcing the reader to go on an external platform was very noticed by the French press which is looking for audience’s engagement.     On an international level,  this project has been selected as a finalist for the 2019 “Online Journalism Awards” on the category“Excellence and Innovation in Visual Digital Storytelling, Small Newsroom”.   https://awards.journalists.org/entries/politics-through-the-data/ ","  Most of the data sources  that we query regularly are compiled by Python scripts that we have improved over time. In concrete terms, we use the following techniques: http request, page consultation automation with machine-controlled browsers, API queries when they exist.   This data is stored in distinct CSV files organized by a meta-database.  Analysis, data mining, and angles seach for the stories were done through Tableau .    To build the story format, we created a semi-WYSIWG editor  that allows non-developers of the team to work on their own. We use common web technologies: Javascript and PHP mainly. This editor produce a public version of the story which is easy to navigate, optimized and responsive, ready to be upload on a server. "," The hardest part of every journalistic investigation is to find the unexpected. What we learned by looking at French politics only through data is that it allowed us to  get out of the ""elements of language"" written by political communicators  and to provide concrete benchmarks to readers. This was the demonstration that one could interest readers in French politics other than through controversy and “snowclones”.   But for that, you have to look for data that is more distant from conventional open data. Of course Twitter, but also Wikipedia for these articles published in January 2019 which allowed us to show that the content of  each page of French elected representativesis the place of a war of influence  : we scrap all modifications and corrections made each day on MPs Wikipedia page to see if the pages were “vandalized” and by whomthanks to the totally open archives of the online encyclopedia : <a href=""http://screenstory.wedodata.fr/lesjours/index.php?story=wikipedia2-english#1/no"">http://screenstory.wedodata.fr/lesjours/index.php?story=wikipedia2-english#1/no</a>   Another challenge was to  find the right way of writing a “story article” . It's a whole other way of telling a story, because the texts must be short, dynamic, suitable for reading on a mobile with graphics. We tested a lot of ways before finding a solution that mixed writing on Google slides and oralizing texts as if we were preparing texts for the radio. "," This project shows that data is a raw material that adapts to many web formats, that you have to be attentive to usage to see how journalism can seize practices from social networks to inform and that you don't always need millions to innovate. ",https://docs.google.com/presentation/d/1611m86sdWF526u7-5wA2cfs9amQM6VSmEoqmY-E9Vlk/edit?usp=sharing,http://screenstory.wedodata.fr/lesjours/index.php?story=wikipedia2-english#1/no,,,,,,"Karen Bastien, Nicolas Boeuf, François Prosper, Victor Schmitt, Brice Terdjman, Clément Thorez, Anthony Veyssière"," Wedodata is a datajournalism and datavisualization studio, created 8 years ago at the very beginning of datadriven journalism in France. Its role: to find and tell stories starting from structured databases (encrypted or textual). This information graphics can be expressed as well on paper or on the Internet as in video or photo. Wedodata supports media, local communities, institutions ... in this new path of storytelling and pedagogy.   WeDoData consists of a hard core of seven people who master all the key skills of a datvisualization process, from understanding the data to its staging and interactivity. Specializing in media projects (Arte, France Télévisions, INA, Les Echos, Liberation, Les Jours…), the agency has made this experience available to the general public at the service of large public and research entities in their approach to open data (OECD, Ile-de-France Region, INPI, Ministry of Research, Paris School of Economics, Global Carbon project…)... Two distinctions at the ""Datajournalism awards"" crowned its eight years of projects, in 2012 and 2018. ",,,
India,The Indian Express,Big,Participant,Best visualization (small and large newsrooms),Four part series on Maharashtra Anti Terrorism Squad's deradicalisation programme: ‘I realised Allah didn't want me to sacrificemy life to make him happy',08/01/19,"Investigation,Explainer,Long-form,News application,Crime,Employment,Human rights,Terrorism",Microsoft Excel," I am enrolling my four part series on Maharashtra de-radicalisation programme which was published in The Indian Express newspaper from August 1- 4 in 2019. In the series, we talk about how Maharashtra Anti-Terrorism Squad has managed to trace, counsel and convinced 120 people who were radicalised by Islamic State against terrorism in Maharashtra. We have looked into different aspects of the counselling sessions that the radicals underwent and Anti-terrorism Squad also helped them in setting their career by providing them skills training under several government schemes to ensure that the youths aren’t misled again by the IS recruiters. "," Several other states in India started writing to the Anti-Terrorism Squad to take details on the de-radicalisation programme, so that they could built their own.  ", I majorly used recorders in my mobile phone to record my coversation with these de-radicalised youths.    Besides that I did not have to use much technology as I had a field job majorly. ," Initially the officials from the Anti-Terrorism Squad remained tight lipped about the project as they were worried that the identity of the youths may come out, but once they were assured that none of the youths shall be identified, they started divulging details on their programme. But the next and the toughest task was to get the de-radicalised youth and their family member to talk. Many youths and their family members knocked their doors on my face, but later with the help of my sources and the common people we (me and the de-radicalised youth) knew, I managed to get some of them to sit on the same table and we discussed. Still many people were scared as these youths have their whole career ahead, I managed to convince five of them and talk with them at lengths.      However as I was looking for granular details, they did not divulge much information at first meeting. I had to make several visits to different places in Maharashtra, meet them multiple times and after almost seven months of hard work, I managed to get the accurate information following which these four parts were published.     "," Socially it sends a very strong message to the readers, after reading this four part series, people learned that God doesn’t want you to kill someone to make them happy. The 120 youths realised after they underwent the de-radicalisation programme. I spoke with many people who said that they have realised that the terrorist organisation are just using them to keep their own agendas alive and they won’t ever fall for their claims.   While for the journalist, who wants to tell stories through data, I would want to share that this de-radicalisation programme was earlier discussed as a one day story but later after conversation with my superiors, I started chasing this as a four-part series. As nobody that includes the de-radicalised youths and the officers talked about the de-radicalisation project and as many times I was insulted and turned away by these youths and their family members, I started losing hope and gave up many times. Because I thought nobody will talk about the counselling session, they gave or underwent. But somehow, I kept working hard and I kept chasing different de-radicalised youths and eventually, I managed to find a way out to get them talking.       ",https://indianexpress.com/article/india/maharashtra-ats-deradicalisation-programme-muslim-youth-is-recruit-allah-didnt-want-me-to-sacrifice-my-life-5868100/,https://indianexpress.com/article/india/maharashtra-atss-deradicalisation-programme-families-help-police-pull-young-men-back-from-brink-5871121/ ,https://indianexpress.com/article/india/maharashtra-police-deradicalisation-project-core-curriculum-step-by-step-5873952/ ,https://indianexpress.com/article/india/after-maharashtra-police-deradicalisation-safety-net-skills-training-bank-loans-5876293/ ,,,,"Sagar Rajput, Nirupama Subramanian and Zeeshan Shaikh."," I work as a crime journalist in Mumbai. I have previously worked with the Mid-day for three and a half years which is a tabloid newspaper in Mumbai and I have also worked with Hindustan Times for one and a half year. Its been two years three months since I joined The Indian Express.    Besides covering crime incidents in Mumbai and Maharashtra, I also like to play and watch football. Due to my facination towards the game, I wanted to be a sports journalist but as things started going well for me in the crime beat, I started covering and eventually became a crime journalist.     ",,,
Germany,ZEIT ONLINE,Big,Participant,Best news application,Way to warm here (Viel zu warm hier),12/10/19,"Open data,News application,Chart,Environment","D3.js,QGIS,Python","During the 25th UN Climate Change Conference in Madrid, we showed the readers of ZEIT ONLINE climate change right on their doorstep. Using fine grid data from the German Weather Service, we were able to calculate for the first time how the temperature in every community in Germany has changed over the past 137 years. For the presentation we use so-called ""Warming Stripes"", heat strips inspired by the British scientist Ed Hawkins. They show how warm or how cold a year was compared to the internationally established reference period 1961-1990. The article is in German but can be seen translated"," We could show that nine of the ten hottest years were after the turn of the millennium. We were also able to calculate that Germany has warmed up by an annual average of about 1.5 degrees Celsius since records began in 1881. If the global climate had warmed up that much, we would already have missed one of the goals of the Paris Climate Convention. Finally, we show the readers in which climate zone their home town is located and how strong the warming was exactly there.   Our readers shared the heat strips from their community in the social media. We offered a special function for this. In addition, the Federal Environment Ministry took up our presentation and published a corresponding graphic. The article was also taken up in presentations such as at the Chaos Computer Congress. "," To batch download, analyze and convert the data we used a lot of command line tools such as GDAL and rasterstats. To verify our analysis we also used QGIS and Zonal Statistics. In the frontend, the project was implemented with React and D3. "," Dealing with scientific data at this large scale we talked to as many experts as possible in order to get everything perfectly right. In order to anticipate critics, in addition to the detailed methodology, we have made all calculations, scripts and data available on Github (https://github.com/ZeitOnline/waermestreifen-gemeinden-scripts) so that the results can be reproduced. ", Maximum transparency is key to anticipate critics. In case of critical questions about our article we could always refer to the reproducible scripts we published. ,https://www.zeit.de/wissen/umwelt/2019-12/klimawandel-globale-erwaermung-warming-stripes-wohnort,,,,,,,"Paul Blickle, Elena Erdmann, Flavio Gortana, Maria Mast, Julian Stahnke, Julius Troeger"," Paul Blickle (Information Designer), Elena Erdmann (Data Scientist), Flavio Gortana (Freelance Interaction Designer, Maria Mast (Science Editor), Julian Stahnke (Interaction Designer), Julius Troeger (Head of Visual Journalism) ",,,
China,Caixin Media,Big,Participant,Best data-driven reporting (small and large newsrooms),Brown Fields,08/02/19,"Investigation,Explainer,Long-form,Database,Open data,Infographics,Environment,Health","D3.js,QGIS,Canvas,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,CSV"," Brown fields are lands which were polluted by industries and would be used for othere purposes. For a long time, China's brown field data is secretary for people. This project is the first report which visuliazed the data for the public. Based on open files and government information, Nanjing University and the NGO Greenpeace collected brown field data from provincial capitals and municipal cities. 174 brown fields are visualized to tell where they are, how they were polluted, and what they will be transferred to. "," It helps readers know more about the fields they are living in. Residential areas, malls, hospitals and schools, these should have been the safest places. But a lot of brown fileds were transferred into these purposes without enough environmental repair. Some tragedies happended years after these buildings came into use.   Local governments have been ordered to start open brown field data to the public. But information is scattered in their websites, difficult to find and understand. This project is an easy way to check important information about these fields. It's not only a project with analysis, but also a tool for tracking open data. "," We used excel to analyze and restructure data, Adobe Illustrator and Photoshop for design, and html/css/js(d3.js) for web page and interactive data visualization. "," Making professional enverinmental knowledges and data easy to understand for normal readers is the hardest part. We used different colors to represent four mainly harmful elements. By scrolling, readers can easily get how harmful they are from previous tragedy cases. We also rearranged plots by cities and industries to show which city open the most brown field data and what industry has the most ones. "," The final part is a data query tool. Readers can locate their living position on the map and find whether there are some brown fields around them, and how they would be repaired. This can improve readers' rights to be informed about their living enveriments. ",http://datanews.caixin.com/interactive/2019/brownfield/,,,,,,,"Jiaxin Liu, Xu Gao, Bo Zhao"," Jiaxin Liu (Reporter, Developer, Designer), Xu Gao(Illustrator, Designer), Bo Zhao(Intern Developer)   Jiaxin Liu is a data visualization developer in the Caixin VisLab. Her main job is to edit code for data visualization and front-end web page. Also, she analyzes data, writes reports and design.   Xu Gao is the Graphic Executive Director in Caixin Media. He doesn't only design charts for data visualization projects, but also helped other departments reports through plots, and design important charts for the Caixin Weekly. ",,,
China,Caixin Media,Big,Participant,Best visualization (small and large newsrooms),E-Cigarettes Mist,22/02/19,"Explainer,Long-form,Database,Infographics,Map,Environment,Lifestyle,Health","D3.js,Canvas,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,CSV"," E-cigarettes were not classified in China; the supervision is still in a blank or controversy region. The attitudes of different countries toward e-cigarettes are not the same. It is based on the Global Tobacco Control database from 2017 to 2018, we visualized the attitudes and regulations of different countries around the world.   We have sorted out the acquisition information of the global tobacco giants since the birth of e-cigarettes, using timelines to show the center of tobacco market is moving to e-cigarettes. "," The project makes e-cigarettes receive enough public attention. The issue of false advertising, supervision ‘vacuum’ and the gateway effect of teenagers are widely noticed. This project has sparked heated debate on Weibo（ The largest social network platform in China） about the effect of quitting smoking, whether it should be banned by government and the new round of investment boom.   After this project, more details and issues of e-cigarettes were reported from other media organizations. China usually marks World Consumer Rights Day on March 15 with a string of media exposures. This year e-cigarettes were exposed on the annual ""3.15"" show by China Central Television (CCTV). We published it again the next day and the effect is significant.   The project has prosperous vitality. Cases of severe vaping-related pulmonary disease and a number of deaths have been reported. We update this project continuously as we get more news.   A small questionnaire was set at the beginning and end of the project, which can test the readers’ attitudes toward using e-cigarettes in public, teen vaping and advertising looked like kid-friendly food products. We can measure the impact of this project on readers by calculating how many people have changed their attitude after reading.   • the proportion of people who thinks E-cigarettes are available to guests at the restaurant decreased by 13%;    • the proportion of people who thinks E-cigarettes are acceptable to teenagers decreased by 5%;   • the proportion of people who accept e-cigarettes with labeling or advertising looked like kid-friendly food products, such as cartoon images, decreased by 3%. "," HTML, CSS and JavaScript.   In grid map, we uses JS to reconstructure the map data. All the interactive map was made by D3.js.   Other illustrations and infographics were made by Adobe Illustrator. "," This project made more media and governments pay more attention to e-cigarettes.Plenty of investigative news has put Chinese e-cigarettes under supervision.   Citing health concerns for minors, a Chinese regulator and the state tobacco monopoly jointly urged manufacturers and sellers to shut down websites related to the marketing and sale of e-cigarettes, in what could amount to an effective ban. "," We set a small questionnaire at the beginning and end of the project to test readers' attitudes toward using e-cigarettes in public, teen vaping and advertising looked like kid-friendly food products. We can measure the impact of this project on readers by calculating how many people have changed their attitudes after reading.   This project used the grid maps, which represent each country with a congruent square. It has removed the artificial visible weighting that larger geographic areas lend to a choropleth map. We present how it changes form a normal world map to a grid map, which will lower the difficulties in recoganizing country loactions and be very helpful to understand. It is also difficult for general readers to read the grid map without any reference. So next the main oceans were marked on the map . ",http://datanews.caixin.com/mobile/interactive/2019/ecig/,,,,,,,"Zhe Wang, Xingji Lu, Jinghua Li, Jiaxin Liu"," Journalist: Zhe Wang, Xingji Lu   Designer: Jinghua Li   Developer: Jiaxin Liu ",,,
Singapore,Reuters,Big,Participant,Best data-driven reporting (small and large newsrooms),How India-Pakistan tensions have disrupted air travel,26/04/19,"Investigation,Explainer,Infographics,Map","QGIS,Creative Suite"," This project visualises the empty skies and costly detours made by international airlines following an air strike in late February by the Indian military in northern Pakistan. The disruption added flight time for passengers and fuel costs for airlines.    The piece used flight tracking data to compare routes to previous days, visualised air traffic in the region, and gave granular information on delays. ", The piece revealed exactly what was happening in the skies above Pakistan and elsewhere in the region whereas other reports were referencing basic developments or headline figures of delays. The piece accompanied Reuters coverage of the ongoing story and was shared widely on social media. ," Coordinates of flight paths were plotted and sorted locally before being mapped in QGIS, then exported and taken into Illustrator to style. A range of maps were made, including an orthographic projection showing the northern hemisphere and every plane in the sky at a specific time. Dozens of flights were also analysed along with their departure and arrival times.    The page was made by placing a series of vector graphics on an HTML page and styled using CSS. An svg animation at the top of the page draws the reader in to the article. "," There was a very large amount of analysing, sorting, and prioritising flights. Choosing the right data to structure the narrative around was a challenge. We had access to literally thousands of flights and each flight was made up of hundreds of coordinates. ", Granular information can be visualised in a simple and clean way in order for readers to understand the narrative and key takeaways. Choosing to leave much of the art direction very subtle and basic allowed the flight paths and data to stand out. ,https://graphics.reuters.com/INDIA-KASHMIR-AIRLINES/010091M92G7/index.html,,,,,,,"Simon Scarr, Marco Hernandez"," The Reuters graphics desk publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Singapore,Reuters,Big,Participant,Best visualization (small and large newsrooms),The Brexit rift splitting the Conservative Party,18/07/19,Explainer,R," British Prime Minister Theresa May’s failure to secure the backing of parliament for her Brexit deal led her to resign the leadership of the Conservative Party, triggering a leadership contest. Her successor will inherit a fractured party and a House of Commons just as divided along the Brexit faultline.       With Reuters’ own analysis of the voting patterns of all 650 MPs, this project visualises the factions in the House of Commons that emerged while May tried to push through her Brexit divorce detail. ", The project was showcased on reuters.com and used throughout Reuters stories to add context and understanding of the divided parliament. ," The heart and strength of this piece was the original data analysis achieved and visualised in R. From there, the ternary visualisations were finessed in Adobe Illustrator  ", The most difficult part of the project was finding a clear and effective visualisation of voting patterns that explained the untraditional shifting support of British parliamentarians. ," R can be used to not only to analysis and report out stories, it can also be used to complete the main bulk of visualisation. ",https://graphics.reuters.com/BRITAIN-EU-LEADER/010092Q33KW/index.html,,,,,,,"Basil Simon, Léa Desrayaud"," The Reuters graphics desk publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Singapore,Reuters,Big,Participant,Best visualization (small and large newsrooms),India is running out of water,25/09/19,"Explainer,Infographics,Chart","D3.js,QGIS", India has consistently faced a water problem in recent years and the situation has only become worse. This project aimed to show the extent of the water shortage in the Indian subcontinent. , The visualisations showed how water especially in northern India has been depleted and an impending crisis is looming. The story was picked up on social media and pointed towards the gravity of India’s water shortage problem. ," QGIS and D3 were used to build these visualisations. Using the AQUASTAT database, the team was able to geographically locate increase and decreases in the water tables around the subcontinent. ", Combining the variables of population and water stress to harmoniously make that connection. This was crucial in order for teh reader to see the actual impact of the water stress. ," We felt that water is a very hard subject to visualise, so trying multiple ideas before arriving at our final visualisation was critical. ",https://graphics.reuters.com/INDIA-ENVIRONMENT-WATER/0100B2C41FD/index.html,,,,,,,"Gurman Bhatia, Manas Sharma"," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Singapore,Reuters,Big,Participant,Best visualization (small and large newsrooms),How Islamic State lost Syria,15/03/19,"Explainer,Infographics,Chart,Map",R," For the better part of the last decade, Syria has only seen conflict. As war ravages the country, control of the region has ebbed and flowed between ISIS and its proxies.        But with time forces have pushed the Islamic State to a corner of the country. The project illustrates how they’ve been forced to slowly retreat over time. "," The project was widely shared on social media since it illustrated the retreat of ISIS. Also, it was able to clarify the extent of ISIS’ control of the area.  ", The piece was primarily made using D3 (Data-Driven Documents) library to show the scale of each power blocks control in Syria. Used in conjunction with a scroll trigger which transformed the visualisation as the user scrolled down.        ," Although the visualisations smooth transitions between each other, it took a while to perfect this. Also, the formatting of the data so that it worked well in this particular visualisation took us a while to achieve "," Conflict can be reported in many ways. Although we’re sometimes restricted to ground reporting in war zones, we can also demonstrate a way to report the story through digital sources. ",https://graphics.reuters.com/MIDEAST-CRISIS-ISLAMIC%20STATE/0100913M1H0/index.html,,,,,,,Gurman Bhatia," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Canada,Radio-Canada,Big,Shortlist,Best visualization (small and large newsrooms),"Self-driving cars: Who to save, who to sacrifice?",25/04/19,"Explainer,Long-form,Quiz/game,News application,Illustration,Infographics,Business","Animation,D3.js,Canvas,Json,Adobe,Creative Suite,CSV,Node.js"," CBC/Radio-Canada created this newsgame to explain some of the ethical ramifications of letting algorithmic drivers share the road with human drivers and the moral dilemmas facing autonomous vehicle manufacturers.    To keep the subject (which involves death) from being too morbid, we made the aesthetic choice to use a whimsical pixelated art style.    The project alternates between explanatory text and a visual ethics quiz designed to keep readers engaged and make them more receptive to the concerns of experts and other stakeholders.    The project was published in French and English. "," The project received praise from visual journalists across the industry, including from the New York Times, Bloomberg and The Guardian, as well as multiple researchers studying autonomous vehicles.    The project was also successful in getting several Canadian governmental officials' positions on the proliferation of self-driving cars on the record.    The presentation and interactive elements used were successful in convincing readers to spend extra time with the article. Our read times on this project were far greater than with traditional articles, and this project was one of the top articles read that month.    The project was also featured on several Radio-Canada radio and television programs, extending its reach. We believe the project was successful in its goal of starting a conversation around self-driving car ethics in Canada. "," The project used React/Redux to render the project and to coordinate events on multiple stacked layers: canvas, svg, and html. Static rendering of markup was used to decrease perceived load time.    The text of the project was retreived from a headless CMS in order to support multiple languages (French and English) seamlessly.    Rails and roads were drawn with mathematical equations (and needed to be redrawn and recalculated on text reflows) and CSS sprites were used to animate characters and vehicles.    Responses to the visual poll were sent to or retrieved from a backend microservice, which served as a relay to a mysql database.    We created a physically accurate braking simulation using the official mathematical formulas provided by Transport Canada. We used the data from the Moral Machine project to create a novel visualisation. We also used duelling video interviews to provide opposing views on vehicle testing, and illustrations to explain different autonomy levels.    A particular challenge was creating responsive pixel art that maintained an exaggerated pixel size (4x4, or 8x8 on retina screens) despite changing screen sizes. In all our visualisations we sought to maintain that pixel size. Even the world map maintains these pixel sizes even when the map is smaller, which means we needed to draw a pixellated map on-the-fly at the requested width.    Finally, we worked hard to ensure accessibility, label buttons and visualisations according to accessibility standards, and to ensure that the entire project could be operated by keyboard/without a pointing device. The project received a perfect Lighthouse accessibility score.          "," The hardest part of this project was choosing scenarios that made sense with the accompanying article sections. This was especially challenging, because we did not know beforehand how the public would vote in the visual poll. If we incorrectly predicted the public's response, it was possible that the vote counts would contradict parts of the text.    Maintaining the same pixel-art pixel proportions for a responsive project presented several technical challenges which our team overcame.    Writing the article itself was a challenge, since a traditional article format is not made to be broken into segments. With large interruptions between sections, each block had to contribute to the larger narrative, and yet be capable of standing alone.    Finally, the reporting was a challenge since governments and companies were hesitant to discuss the potential downsides of the technologies they are actively pursuing -- especially when those downsides are abstract and not tied to a particular accident or event. "," Others can learn the power of a good presentation to drive engagement. This project was based on data that had been available for months, but the quality of the presentation and the use of a visual poll to drive people down the page, the clever visualisations and delightful drawings, all served to produce a hit for our newsroom.    As Professor Iyad Rahwan, director of the Center for Humans & Machine said on Twitter: ""[It is] quite possibly the most interactive and detailed article covering the Moral Machine to date.""    The lesson is that you don't need to be first if you can do it better. ",https://ici.radio-canada.ca/info/2019/voitures-autonomes-dilemme-tramway/index-en.html,,,,,,,"Marc Lajoie, Francis Lamontagne, Sophie Leclerc, Éric Larouche, Mélanie Meloche-Holubowski, Santiago Salcido, Kazi Stastna"," Marc Lajoie is an international award-winning data and interactive journalist who has worked for the Wall Street Journal, the South China Morning Post, and CBC/Radio-Canada. He led a talented team of designers including Francis Lamontagne, Santiago Salcido and Sophie Leclerc, whose whimsical illustrations brought the project to life. The project was edited by Éric Larouche and Mélanie Meloche-Holubowski. The English translation of the text was edited by Kazi Stastna. ",,,
United Kingdom,BBC England Online,Big,Participant,Best data-driven reporting (small and large newsrooms),English councils warned about 'exhausting' reserve cash,29/05/19,"Investigation,Explainer,Long-form,Open data,Chart,Politics","Microsoft Excel,CSV,R"," When Northamptonshire County Council declared itself nearly bankrupt last February, it sent shockwaves through local government finance.  The East Midlands council had balanced its books by draining its reserves.  Were other councils also getting dangerously close to running out of money? The BBC England data unit used public data to scrutinise the accounts of local authorities and discover which other councils might be in danger. It revealed eleven councils would run out of reserves within four years if their recent levels of spending continued. "," The story led the BBC news website and led to reaction and debate within the local government sector about the importance of cash reserves as well as the pressures councils are under because they do not know what their funding position will be from year to year. It also highlighted weaknesses in government data, namely the way that “earmarked” and “unallocated” reserves can mean different things to different councils.    The piece helped others to scrutinise the accounts of local authorities paid for and relied upon by millions of people, explaining the methodology used by the Chartered Institute of Public Finance and Accountancy (CIPFA), one of the most trusted expert bodies on the matter in clear terms and ensuring that the views of councils about the analysis and its reliability were represented fully.   It was followed up by local newspapers all over the country as well as local government sector publications.    Councillor Richard Watts from the Local Government Association, explained: ""Some councils are facing a choice between using reserves to try and plug funding gaps or further cutting back local services in order to balance the books.   ""This is unsustainable and does nothing to address the systemic underfunding that they face. Ongoing funding gaps are simply too big to be plugged by reserves.""   Professor Tony Travers of the London School of Economics said: ""Emergencies can happen and sudden increases in spending can be needed. Councils need to be resilient or they end up in the position Northamptonshire was in. However, they don't know their funding position from one year to the next."" "," We used R to download, clean and analyse years of government data on council reserves and spending and created an R notebook detailing the process of analysing council accounts to match CIPFA's measure for resilience.   Work was then done in Excel to calculate how many years councils could continue to spend their reserves, based on the changes over the previous three financial years.   R and the BBC's bbplot package were used to provide a simple bar chart showing percentage change in reserves at the councils highlighted. "," Despite councils themselves being the source of the government data, and the data being classed as national statistics, some local authorities dispute the figures and wanted us to use their own audited accounts instead.   We decided to continue with the government data on the basis that every council had been treated the same and the data had been compiled and published by the same team, rather than different auditors.   Yet we had to ensure we had been fair to all concerned and so a large part of the story was given over to local authorities for them to explain their figures.   For example, some argued that we should not be using their ""earmarked"" reserves (money set aside for specific projects"" as part of the analysis. We did so because Cipfa had done so and because while the money may have been allocated, it was still available to call upon and use should it be needed. Crucially, it had not yet been spent.   Above all, what this project showed was that while many representative bodies and analysts have used this data on many occasions to make claims about council finances, it is very rare for anyone to actually name the individual councils that they may be referring to. And once anyone does start to highlight individual examples, everyone has something they want to say about their own individual circumstances. "," Aside from understanding some of the terminology around English local government finance, this project was also an example of downloading, cleaning and analysing public data.   As well as the story itself, which focussed on reserves, the analysis we did in the research phase involved subsetting data and using regex to find specific spending types that we wanted to look at, such as social care.   We did the same when we then settled on looking at cash reserves.   It was also an exercise in data cleaning, for example dealing with columns imported as characters rather than numeric data.   Above all, however, it is an example of ensuring fairness, of seeking expert advice and of making sure that weaknesses and doubt are given appropriate weight. ",https://www.bbc.co.uk/news/uk-england-48280272,https://github.com/BBC-Data-Unit/council-reserves,,,,,,Daniel Wainwright and Paul Bradshaw,"  Paul Bradshaw  works as a consulting data journalist with the <a href=""https://github.com/bbc-data-unit"">BBC England Data Unit</a>, and also runs the <a href=""http://www.bcu.ac.uk/media/courses/data-journalism-ma-2018-19"">MA in Data Journalism</a> and the <a href=""http://www.bcu.ac.uk/courses/multiplatform-and-mobile-journalism-ma-2018-19"">MA Multiplatform and Mobile Journalism</a> at Birmingham City University. A journalist, writer and trainer, he has worked with news organisations including The Guardian, Telegraph, Mirror, Der Tagesspiegel and The Bureau of Investigative Journalism, and his awards include the CNN MultiChoice Award for an investigation into people trafficking in football. He publishes the <a href=""https://onlinejournalismblog.com/"">Online Journalism Blog</a>, is the co-founder of the award-winning investigative journalism network HelpMeInvestigate.com, and has been listed on both <a href=""http://Journalism.co.uk"">Journalism.co.uk</a>'s list of leading innovators in media, and the US Poynter Institute's list of the 35 most influential people in social media.   His books include <a href=""https://leanpub.com/scrapingforjournalists"">Scraping for Journalists</a>, <a href=""https://leanpub.com/spreadsheetstories/"">Finding Stories in Spreadsheets</a>, the <a href=""https://leanpub.com/DataJournalismHeist/"">Data Journalism Heist</a>, <a href=""http://leanpub.com/snapchatforjournalists/"">Snapchat for Journalists</a>, the <a href=""https://www.routledge.com/The-Online-Journalism-Handbook-Skills-to-survive-and-thrive-in-the-digital/Bradshaw/p/book/9781138791565"">Online Journalism Handbook</a> and most recently <a href=""https://www.routledge.com/Mobile-First-Journalism-Producing-News-for-Social-and-Interactive-Media/Hill-Bradshaw/p/book/9781138289314"">Mobile-First Journalism</a> with Steve Hill. He can be found on Twitter <a href=""https://twitter.com/paulbradshaw"">@paulbradshaw</a>.    Daniel Wainwright  leads the BBC England Data Unit, having been a founder member of the team in 2015. He specialises in original data-led projects that are often shared across BBC regional and network news. He began his career at the Express & Star, which at the time was Britain's biggest selling regional newspaper, rising to the role of political editor and was named 2014 columnist of the year at the Midlands Media Awards. His work at the BBC has seen him host podcasts on data journalism for the BBC Academy and give a keynote speech for the European Forum for Geography and Statistics Conference in 2019.   He is on Twitter <a href=""https://twitter.com/danwainwright"">@danwainwright</a>.     ",,,
United Kingdom,Sky News,Big,Participant,Best data-driven reporting (small and large newsrooms),The state of the vaccination – Why 7000 people die needlessly every day,29/08/19,"Investigation,Long-form,Health","Adobe,Microsoft Excel,CSV,R,RStudio","This was a data-driven investigation into one of the defining health issues of the age, and – as we showed – is resulting in the needless daily deaths of 7,000 people. Embracing a data-led perspective and working with a large dataset with millions of entries allowed us to take a wider perspective and see the big picture and complexity of the problem. The piece put into context many of those media stories –e.g. measles outbreaks- as well as demonstrate the scale of the ongoing need for vaccination and examine to what extent misleading information and the anti-vax movement had and"," The finished piece was rapturously received by organisations that aim to protect people from disease – Save The Children and UNICEF among them.    Save The Children said: “thank you for the superb, superb data piece on vaccination. In particular the focus on the toll from pneumonia, an issue we’ve been campaigning on for the last few years – often, it feels, in a vacuum! So a huge boost to team morale to see vaccination and pneumonia in the spotlight, and delivered so well.”    Unicef, who tweeted out the piece to their 7.9 million followers, said: “Thank you so much for these fantastic pieces, they are excellent!”    The piece was accompanied by a second story where we collected case studies which humanised the analysis.     Up to ten million unique users visit Sky News website and app every week, and this investigation received far higher for engagement than average.     The story was also broadcast on Sky News channel, breaking the traditional process of being first on TV and then online.    It has also an impact in the data community about our use of the data visualisation tool, Flourish, and its scroll down template. Sky News was the first media in using this quality from Flourish and, afterwards, several data team from different countries got interested in replicating it. "," The data for this story came from the Global Burden of Disease, an international research program coordinated by the Institute for Health and Metrics and Evaluation (IHME) at the University of Washington, and the WHO-UNICEF Joint Report.     The raw dataset we started with contained almost 8 million of observations, so we used the programming language R to explore it.     With R Studio we were able to filter the specific data needed for our analysis and we applied statistical models to find patterns and make calculations. We used a wide range of methods from simple statistics as aggregations, percentages, rates or percentages changes to correlations and logarithmic interpolations.    Some of the R libraries used were Tidyverse, reshape2, readxl, ggplot2, plotly, and gghighlight. The last three helped to produce visualisations (some of them interactive) on the R Notebook where the analysis was done. This notebook was later shared with the experts and the reporter to explain the findings through text and graphics.    Despite using R to visualise during the analysis process, we used the data visualisation tool Flourish for the published story.     Flourish had a by then hidden functionality to create scroll-down visualisations that no one else in the industry had used it before. Installing its SDK we could transform a Flourish story which moves horizontally to a vertical interactive experience.     We displayed the visualisation in portrait mode by calculating the width and height of the browser viewport and applying this to a dedicated embed developed utilizing JavaScript on the front end to interact and utilize Flourish SDK    The performance of the chart rendering was optimised by adopting a 'lazy loading' technique where loading of visualisations is deferred shortly before they are to be displayed to the user.  "," This project involved months of work, for which was essential getting the specific data. There hadn’t been story without it as we were not aiming another piece based on single cases. But finding this data was one of the hardest tasks.     Not many organisations compiled this type of information and institutions as WHO warned us about not using their data due to methodology issues.     There were also concerns with the list of vaccine-preventable diseases. Although WHO has an established list, some experts differ, and this was considered in our analysis. But nor the Global Burden of Diseases or any other database found had information about all diseases.     Reaching leading institutions in the field, we were able to find a robust database, and here started the second hardest part: understanding the data. The technical terms and some issues with the data –e.g. population sizes for some diseases in some age groups were not big enough in all countries – made it necessary to work closely with specialists. Fortunately, we kept a fluent and constant relationship with experts.    This project also involved a good command of statistics, not only to carry out the analysis but to facilitate the communication with researchers and implement their indications. Proper knowledge of the tools and techniques used by experts and statisticians like R simplify the revision process. The IHME offered to review our analysis and we shared it in the R Notebook.    As for the visualisation process, we innovate in the storytelling, creating a scroll down visualisation never used at Sky News. This made an impact internally and built bridges between departments that have been very useful in later projects. Although we used an external tool, we used it like nobody did it before and our example was followed by other data teams in different countries. ","<ul>    Even if the source of the data is trustworthy, it would be advisable to contact them if your analysis slightly differs from what they explain in the methodology. The first dataset I found was on World Health Organisation website but just after speaking to them I realised it was not suitable for our analysis.        Approaching data sources can save time to the reporter, as some of them can be interesting voices to interview and include in the story.         Collaborating with experts makes your analysis more rigorous, but researcher and journalists understand time differently. It is also recommendable agreeing with them on a formula to express academically correct how the analysis was developed (e.g. in the methodology) while keeping the story clear and engaging for the audience.        Be updated with tools and techniques and explore them further to understand the whole potential a tool has to make the most of it.     </ul> <ul>    Identify colleagues with different skills useful in a data project, try to engage them to your project and build bridges with their departments that can be reused in the future. The collaboration between the designer, the developer and the data journalist was key to produce the main visualisation.      </ul>",https://news.sky.com/story/why-7-000-people-die-needlessly-every-day-11770982,https://news.sky.com/story/dead-blind-comatose-the-victims-of-not-vaccinating-11773487,https://news.sky.com/story/pneumonia-disease-that-kills-most-children-worldwide-on-the-rise-in-uk-11859928,,,,,"Carmen Aguilar García, Philip Whiteside, Alessandra Rizzo, Ed Hollinghurst, Katarina Duvnjak, Matthew Simpson."," Carmen Aguilar is the data reporter at Sky News. She was responsible for gathering and cleaning the data, producing the analysis in which the story was based and creating the visualisations. Carmen has more than 10 years of experience in media from Spain, Chile and the UK.     Philip Whiteside is the international news reporter for the Mobile Team at Sky News. He was in charge of interviewing experts and writing the final story. Before joining Sky News in 2013, Philip worked as a roving foreign correspondent, covering wars in Afghanistan, Israel/Gaza and Georgia, revolutions in Egypt and Tunisia and disasters in Haiti, Japan and Pakistan.    Ed Hollinghurst has more than 10 years of Front End engineering experience. He worked with the Data Journalism team to develop the vaccination visualisation concept as well as liaising with the Flourish team on the technical aspects of the implementation.    Katarina Duvnjak is a Sky News designer, and she was responsible for creating the brand look of the piece. She developed the colour palette and image treatment. Katarina has been a member of the design team at Sky for four years.     Matthew Simpson is the digital art director at Sky News. He was in charge of providing direction for the creative and ensuring it aligns with Sky News brand. Matthew has 15 years of experience and has been with Sky for over eight years.    Alessandra Rizzo is a chief sub-editor of the Mobile Team at Sky News. Alessandra helped shape the story by giving it structure, breaking it into sections and editing the copy and the graphics. She has worked as a correspondent for The Associated Press in Rome and as a senior sub-editor for Reuters in London before joining Sky News.   ",,,
United Kingdom,Sky News,Big,Shortlist,Innovation (small and large newsrooms),Editorial dashboard to monitor ads spending on social media,11/04/19,"Investigation,News application,Fact-checking,Elections,Politics","Scraping,Json,R,RStudio,Node.js"," The dashboard was part of a wider Sky News project: Under the Radar, which monitored the impact political actors were having on social media during the 2019 general election campaign.    It was an interactive tool which combined and visualised aggregated data in a single place. It tracked periodical publication of ads on Facebook, Google and Snapchat by the main UK parties and political operators.    It measured daily spending on these social platforms, the performance of ads published and demographic information about the audience reached. This allowed us to understand how these political actors were using social media to target voters. "," We wanted to reveal how parties were fighting the election on social media, but the relevant information was not easily accessible for non-data journalists.    The dashboard collected, analysed and presented that data in a single tool, making the information easily available for non-data reporters who could use it whenever and wherever they needed it.    Using this tool, Sky News published several stories about political ads spending on social media, parties’ strategies on these platforms, and characteristics of the audience reached.    It gave us the first hint about political ads on Facebook disappearing days before the election, in what we later confirmed with researchers to be a ""catastrophic"" loss of transparency and accountability.    While creating the tool, we explored the different methods each social media platform used to publish their data. This gave us a better understanding of their transparency policies and their problems, and we could verify obstacles in their mechanisms to access the information.    Academics valued this knowledge and we have been later interviewed by researchers about our experience working with social media libraries and their data.    The fact that we created the tool before the general election was called involved better management of the resources during the actual campaign. The dashboard automated the process of gathering data, analysing it and visualising it. That freed the data journalist up from repeating these tasks during the campaign, when other data work needed to be done.  "," Data was gathered using the Facebook Ads Library Report, Facebook Ads Library API, Snap Political Ads Library and Google Political Ads Transparency Report.   Facebook Report offers manually downloadable files ranging in daily, weekly, monthly and three-month detail. Files update at random times and are not archived, so it had to be downloaded on the day. As Facebook prevents basic web scraping, nor does it provide downloadable URL endpoints, we developed on-premises cloud-based back end service with Node.js, which periodically monitored the library and mimicked user’s action of clicking on the download button.   Additional data was collected by Facebook Ad Library API, whose access is granted upon request and app review by Facebook.   Snapchat publishes yearly files updated daily at random times. As with Facebook, we automated the process and developed an on-premises cloud-based service to monitor and download files daily.   Google data was easily accessible through Google Cloud Public Datasets which are updated weekly.   Data collected from each social media platform was normalised and stored in separated Google Cloud BigQuery Datasets. On-premises cloud-based back end services were developed with Node.js utilizing Google Cloud Platform SDK to interact with Google BigQuery datasets.   Using the bigrquery package from the programming language R we accessed the datasets on Google Cloud. Further cleaning was needed in RStudio, especially to normalise names, and we used Tidyverse package to analyse the data and plotly and datatable libraries to create interactive visualisations and searchable tables. That would allow the journalists to easily identify trends, but also search for specific information.   Using the Shiny library from R we created an interactive application deployed in shinyapps.io. We granted access to the App to journalists and editors involved in the Under the Radar project, who could easily access to the updated dashboard from a browser. "," The compilation and standardisation of the data was complex, as the mechanisms differ from platform to platform and there is no harmonic criteria in the structure of the information.   Facebook posed bigger challenges. Its daily reports are only available manually by downloading a csv file which disappears every 24 hours. To track information for months, we needed to automate the process.   These files only provide a subset of the information we were looking for, so we had to complement it with data collected through Facebook API to understand more details of the ads, like status, impressions, distribution and audience reached.   Access to the API is not public and is being granted upon request and app review by Facebook. Requests to the API are heavily rate-limited in comparison to the number of requests required to collect all relevant data, which at times proved problematic.   Although data from the Facebook Report and the Facebook API were about the same topic and actors, both sets of information were not comparable due to the way Facebook discloses the data.   The process in Snapchat and Google was easier, but both of them offer bulk global data and none of them had a specific tag for the UK elections 2019 which would allow us to easily create a filter on the fields we required.   Snapchat changed the format of its data without prior notice, forcing us to adjust the code, but also giving us a sign to close monitor the data even after building the dashboard, as platforms could make changes which would affect our results.   Names and variables also differ in each platform, which involved cleaning to standardise the data. Due to the criteria and timing in which each platform disclose the information, we had to create different tabs in the dashboard –one per platform. "," It is required a high integration between the data journalists’ team and the developers’ one, for which it is advisable both teams to understand the other’s job, and the resources and skills they have.    Agreeing on formats and structure of the data in the gathering stage proved to simplify the process later during the analysis phase and the creation of the App.     Google Cloud and bigrquery made the data analysis faster and avoided the data journalist to upload big datasets to RStudio which could have slowed down the process.    Although we always kept in mind the potential stories we could find using this tool, investing in the input part of the project made it easier publishing several stories using information from the dashboard during the campaign process.    It is recommended to include a “Get the data” button in the dashboard if your company use specific tools to visualise, as well as training journalists about how to use the dashboard.  ",https://skynews.shinyapps.io/Political_Ads_App/,https://news.sky.com/story/researchers-fear-catastrophe-as-political-ads-disappear-from-facebook-library-11882988,https://news.sky.com/story/general-election-tories-quadruple-spending-on-facebook-ads-11879524,https://news.sky.com/story/general-election-how-much-are-parties-spending-on-adverts-11860617,https://news.sky.com/story/general-election-jo-swinson-has-a-facebook-problem-11855013,,,"Carmen Aguilar García, Przemyslaw Pluta, Peter Diapre"," Przemyslaw Pluta is an award-winning creative technology leader with more than 10 years' experience in innovation & product development across broadcast and digital platforms. In his current role as the Head of Platform Solutions at Sky, he is responsible for defining, developing and implementing solutions that streamline and simplify complex problems. He led the development team and worked with Editorial and Data Journalism team to deliver integrated services, infrastructure and data strategy for Under the Radar project.    Carmen Aguilar is an award-winning journalist with more than 10 years of experience in several media from Spain, Chile and the UK. She is the data journalist at Sky News working for all platforms in the company. In Under the Radar, she analysed and visualised the data, and she created the App to offer non-data journalists an editorial dashboard to easily find the information. She was also involved in the analysis and visualisation of the stories published online and broadcast, and she worked with both the editorial and development team, making the connection between the two departments.    Peter Diapre is an Assistant Editor at Sky News. Peter is in charge of news video for Sky News’ digital platforms (web, app and social). He worked at the BBC before joining Sky as a TV producer in 2000 and has covered multiple UK and US elections for the channel. He spent five years in Westminster as a package producer and programme editor for Adam Boulton. He studied US politics and history at university. In Under the Radar, he was the manager of the project, providing resources for each team and overlooking the correct development of each part and the efficient communication in every stage of the process.  ",,,
Malaysia,The Star Media Group Malaysia,Big,Participant,Best data-driven reporting (small and large newsrooms),Malaysia's flood hotspots and rising sea levels,12/02/19,"Investigation,Database,Open data,Infographics,Video,Map,Environment","Animation,Scraping,Json,Adobe,Microsoft Excel","On Dec 2, 2019, The Star published a special report made up of two stories, namely: 1. A data-story on flood hotspots in Kuala Lumpur and its surrounding districts (collectively referred to a Flash floods are a longstanding problem for the Malaysian city of Kuala Lumpur and its surrounding areas, collectively is known as the Klang Valley. The floods, which often occur after sudden heavy downpours are due to poorly planned development. The aim of the project was to show readers which areas in the Klang Valley are most flood prone and highlight the importance of prioritising flood mitigation in"," Data-driven content with interactive visuals is very new for The Star, and the special report is one of our earliest attempts at it.   We received a good response from our readers in terms of pageviews as well as social media comments. This particular project gave us the motivation to continue learning and scale up our data and visual stories.         ","  Flood hotspot story:    We analysed four years of flood data recorded by Malaysia's Department of Irrigation and Drainage.   Data analysis was done on Microsoft Excel, then mapped and visualised using Flourish.   We also embedded a Google Earth Timelapse image of the Klang Valley to show the rapid development in the area over the years.   A video story was also created by our video or the flood hotspot story, with video effects done using Adobe.    Sea-level rise story:    We put together the story, which comprises of interviews with affected padi farmers, photos and videos using Shorthand.   An interactive graphic in the story was done using Genial.ly. We used the free versions of Shorthand and Genial.ly after learning doing some research on which tools we could use, and then learning how to use these tools on out own.  "," For the flood hotspot story, data scraping and cleanup was done by a two person team and it was the hardest part of the project. The data comprised of flood incident reports by Malaysia's Department of Irrigation and Drainage.   The data is in pdf tables, so we had to scrape using Tabula. We then had to clean up the data, then geocode all the addresses and locations in order to map it out.   We scraped, cleaned up and analysed four years of data not just for the Klang Valley but for all of Malaysia.   This is because we wanted to do another our story which looked at flood prone areas in the east coast of Malaysia.   However, since that story on flood prone areas in the east coast of Malaysia (<a href=""https://www.thestar.com.my/news/nation/2020/01/02/34-deaths-and-rm153mil-in-losses"">https://www.thestar.com.my/news/nation/2020/01/02/34-deaths-and-rm153mil-in-losses</a>) was only published on January 2, 2020, we are not able to submit it.   As such our entry only comprises of two stories from our special report, both of which were published on Dec 2, 2019, which is before the deadline for entries.   We hope our special report will be considered because it is part of our effort to raise awareness on the importance of the environment and climate change, topics which news organisations in Malaysia do not report on enough. "," I am a beginner  in data journalism. I learnt data journalism on my own, mostly online other than attending a four-day journalism training conducted by Malaysiakini.   It was quite scary to promise my bosses that I could learn data journalism, create data-driven content and also help my colleagues with it - part of my pitch for our news organisation can adopt data journalism.   I made mistakes but I am learning from them. So, if the question is ""what can others learn from this project,"" then my answer is that if I can do this much armed only with a deep interest in data journalism and a refusal to give up, I believe that all other journalists who are considering learning data journalism and applying it can do as well, and even better! ",https://www.thestar.com.my/news/2019/12/02/these-are-the-flood-hotspots-in-the-klang-valley,https://thestar.shorthandstories.com/feature/the-sea-also-rises/index.html,https://www.thestar.com.my/news/nation/2020/01/02/34-deaths-and-rm153mil-in-losses,,,,,"Razak Ahmad, Sim Leoi Leoi, Diyana Pfordten, Imran Hilmy, Hemananthani Sivanandam, Amir Aizat Kamaruzaman, Chan Boon Kai","  Klang Valley flood hotspot story:     Razak Ahmad (News Editor) : Did the flood data analysis for the story, and wrote the Klang Valley flood story.    Diyana Pfordten (Journalist) : Did the data analysis and visualisation.    Amir Aizat Kamaruzaman (Video producer):  Produced a video story.    Sea-level rise story:     Sim Leoi Leoi (News Editor):  Led a team of journalists who worked on the sea-level rise story.    Imran Hilmy, Hemananthani Sivanandam (Journalist):  Conducted the reporting for the story.    Chan Boon Kai (Photographer):  Took photos and videos.    ",,,
Bangladesh,The Daily Star,Big,Participant,Best data-driven reporting (small and large newsrooms),Dhaka: Hazard City,29/03/19,"Investigation,Open data,News application,Illustration,Infographics,Map,Corruption","Microsoft Excel,PostgreSQL,OpenStreetMap"," Dhaka's growth is almost entirely unplanned urbanisation. It is very common to find chemical warehouses in the same building as residences, and huge skyscrapers without fire exits. As a result, Dhaka saw two of its most horrific fires in 2019 that left over 100 dead. We collected the fire service department's internal survey of how vulnerable the city's buildings are to fire hazards, and disclosed it to the public in an accessible format so that they can choose wisely which school to send their children to, or find out whether a chemical warehouse can blow up their entire neighbourhood.  "," Our project had two components: a map where people can zoom into their neighbourhoods to find structures vulnerable to fire, and a correspondent newsreport. The map acts as a timeless guide and is still referenced by our audience. As a result of the discourse generated by our project, fire safety became a priority among the general public. The government cracked down on corruption within the agency that issues building permits, filing cases against 23 people. Meanwhile, our audience reached out to us with questions on the building code, and in subsequent reports, we included visualisations and explainers for building safety.  "," When a chemical warehouse in an old part of the city blew up a whole neighbourhood on February 20, 2019, the fire service department leaked a survey they did of over 2500 buildings in the city. While this is not an exhaustive list by any means, it was still the first time we had any information on what neighbourhoods, or what structures are dangerous. The data was in Bangla and in a .pdf format. Bangla OCR is still in a developmental stage, meaning we all just had to sit down and do basic data entry. The next step was to geocode the addresses, which was a problem because far too many of these addresses did not exist on Google maps. We had to overcome that problem by manually finding latitudes and longitudes of different addresses. Because of the amount of manual work involved, the project took around a month.   "," We all worked on the project as a side-gig in addition to our respective day-jobs at the newspaper. Datajournalism is still in a very nascent stage within the newsroom, and every bit of effort put into this project represents off-the-clock hours. The only reason this project even materialised is because a group of young reporters believe in the power of datajournalism and how effective it is in conveying important information to the public. The developer who helped us out with the front-end of the project is actually from the office IT team, who has never done datajournalism in his life. A graphic designer who constructs visualisations for print, contributed with the design. If this project is selected, it will help pioneer datajournalism within The Daily Star, and convince our editors to invest more time and resources into it.  "," We believe that our project has the capacity to teach others in Bangladesh how to think beyond covering just breaking news. We could have simply investigated the two fires and left it at that. Everybody knows that the lack of building safety is an epidemic--but nobody has yet presented the breadth of the problem in the way we did. The government is the largest repository of data in the country, but most of the data is unusable, being preserved in formats that cannot be processed or analysed. In addition, most of government data is hidden to the public. We believe that this project can show people that we have a right to the information that the government holds, and that given the right skills, we can process and present them on platforms to make an impression on the public.  ",http://campaign.thedailystar.net/city-on-fire/,https://www.thedailystar.net/frontpage/dhaka-fire-not-just-hugely-risky-1721887,,,,,,"Zyma Islam, Shaer Reaz, Nasir Uddin, Nurul Ferdous"," Zyma Islam is an investigative journalist at The Daily Star in Bangladesh, with a passion for data-driven reporting. For this project she gathered the data from sources in the government (as hard-copy printouts written in Bangla) and processed the data so as to make it usable.    Shaer Reaz is a journalist, feature writer and data visualiser with The Daily Star, currently serving as the Deputy Editor of the Digital section of the paper. For this project, he took charge of the design and execution of the microsite, and created the corresponding infographics.    Nurul Ferdous is a software architect working as a developer at The Daily Star. In this project, his contribution lay in helping the team geocode obscure Bangladeshi addresses and building the map.    Nasir Uddin Mandal is a web and mobile application developer who developed the front-end of the project.  ",,,
France,"La Marseillaise, Marsactu, Médiapart, Le Ravi",Small,Participant,Best data-driven reporting (small and large newsrooms),#LaGrandeVacance (The Great Vacancy),29/10/19,"Investigation,Multiple-newsroom collaboration,Database,Open data,Infographics,Map,Politics","Scraping,Microsoft Excel,Google Sheets,OpenStreetMap,Python"," #LaGrandeVacance (The Great Vacancy) is a common investigation of multiple-newsroom based in Marseille city, France : La Marseillaise, Le Ravi, Marsactu and Médiapart about how the City of Marseille let dozens of its own buildings rot away. Using 5 424 real-estate transactions operated since 2003 by the city administration -public data recovered from real-estate recorded file- and visual checking, we have concluded that at least 68 buildings located in city-center, own by the city administration, were empty and has not been renovated at all. We also uncovered several dubious transactions involving local entrepreneurs or politicians. "," With a simultaneous publication in four Marseille media outlets on October 29, 2019, Operation #LaGrandeVacance had some media impact. Several national newspapers and radio stations relayed the operation. The municipality, plunged into silence for many months, finally had to react. At a press conference held a few days before November 5, 2019 - a year after the collapse of two buildings on Rue d'Aubagne - the mayor's office had to admit that it had a degraded real estate heritage and that the insalubrity was not only the fault of private owners.   As is often the case, the mayor, Jean-Claude Gaudin, tried to minimize the phenomenon and assure that he had done everything he could to improve the situation.   Moreover, few months after our investigation, <a href=""https://marsactu.fr/ecroue-pour-avoir-loue-un-appart-en-peril-a-des-touristes-david-b-est-un-habitue-du-genre/"">a few unscrupulous landlords who were renting slums were arrested for the first time</a> since the tragedy in the Rue d'Aubagne. "," We firtly used Google Sheets to carry out the work of capturing the administrative formalities detailing the purchases and sales of properties by the City of Marseille and its satellites. We did try to do text recognition via PDFs of the scanned documents, but the result was imprecise, so we had to enter them one by one in a Google Sheets document. However, the formalities did not include an address but cadastral data. We had to develop a script (Python / selenium web driver) to scrap the addresses corresponding to these cadastral data one by one (see how it worked on <a href=""https://www.youtube.com/watch?v=EQnQJhzk9Is&feature=emb_title"">this Youtube video</a> or in the article in link 4). This allowed us to avoid a long and tedious manual input. We also created a list of nearly 1000 buildings concerned by a dangerous structure notice or included in a public-led renovation project, sometimes using Tabula to read PDFs. We used Outwit hub to extract and list the officers behind the companies involved in the transactions, OpenRefine to clean the data (adresses, names...). Finally, we used R studio to carry out the data analysis. Once the data had been entered and checked, we used Google Street View to view the addresses of the damaged properties and, in particular, to use the function to compare photos taken in previous years. We then supplemented this work with physical site visits.  Most of our communications took place by email and then via a Slack platform created for the occasion. "," The task of manually entering the more than 5,000 administrative formalities was clearly the most challenging part of this survey. We got the paperwork in paper format, printed out, so we had to scan it and convert it to PDF. Despite this, the text recognition did not work properly. We therefore had to manually enter the data (name of the buyer, name of the seller, date of the deed of sale, cadastral reference, etc.) and fill in our Google Sheets. Each employee was responsible for entering a range of formalities (from 1 to 250, from 250 to 500, etc.). After this lengthy input, we had to check once again that the data entered was free of errors. To do this, each person was responsible for checking the entry made by a colleague.   Once the data was verified, we had to cross the data using pivot tables to isolate the only properties in the city, compare purchase and sale prices, identify sellers who had made a profit, etc. Next, we had to trace the chronology of each building to find out if the town hall was still the owner at the time of our investigation.  This work was then completed by a field survey to see the buildings, interview the neighbours to find out how long a building had been in this state, when the last works were carried out, etc. "," This work is unprecedented in the history of the Marseilles press. It is the first collaborative investigation between several local media. It could never have been carried out without the commitment of Nourredine Abouakil, a housing rights activist, who got hold of these administrative formalities and distributed them to several journalists. He then worked to bring these media together to use the data. Given the mass of data, we felt it was necessary to work together. No single media outlet was able to process the data individually.  The collapse of two buildings on Rue d'Aubagne on 5 November 2018, in which eight people died, revealed the extent of the poor housing and unhealthy conditions in Marseille's buildings. The media we represent, which sometimes compete with each other for the rest of the year, felt that the subject was too important to play individually and that dealing with such a subject required this unprecedented alliance.   We have developed a method and tool which could be replicated in cities, at least in France, where the question of housing policies related to public ownership are relevant.    PS  : sorry for any English mistakes, awkward or outright incomprehensible sentences.  Originals articles from Link 1 to 6. English translation of the articles are in a Drive folder (Link 7 below) ",https://marsactu.fr/comment-la-ville-de-marseille-a-laisse-pourrir-ses-propres-immeubles-par-dizaines/,https://www.mediapart.fr/journal/france/291019/marseille-les-68-immeubles-l-abandon-passes-entre-les-mains-de-la-ville,https://s3.amazonaws.com/external_clips/3229184/Lead___Montee_des_Accoules.pdf?1572427634,https://marsactu.fr/la-grande-vacance-les-coulisses-dune-enquete-commune/,http://u.osmfr.org/m/380025/,https://marsactu.fr/deux-familles-ont-profite-en-masse-des-ventes-immobilieres-opaques-de-la-ville-de-marseille/,https://drive.google.com/drive/folders/1lWvSy97vuJf4CIh23AltHyKdKI_eXCGR?usp=sharing,"David Coquille, Louise Fessard, Benoit Gilles, Jean-François Poupelin, Marius Rivière, Julien Vinzent"," David Coquille : Reporter, photographer, David Coquille has been a journalist at the newspaper La Marseillaise since 1996. He was awarded the ""Alexandre-Varenne 2007 First Prize of the regional and departmental daily press journalists"", for the investigation ""Le plateau d'Albion, dix ans après les missiles"" (The Albion plateau, ten years after the missiles).   Louise Fessard : Journalist at Mediapart since 2008. After following education and then police issues, she is particularly interested in minorities. She lives in Marseille, and the rest of the time in the TGV (high-speed train Paris-Marseille.   Benoit Gilles : A journalist for 20 years in Marseille, he worked successively for Le Pavé, Marseille l'Hebdo and La Marseillaise before joining Marsactu where he hold the position of editor-in-chief. He is also the founder of this local investigative newspaper. In addition, he produces cartoon reports, notably for Le Ravi.   Jean-François Poupelin : After studying political science and a little less than two years in finance, Jean-François moved to Marseille for a short training in journalism with his collection of Charlie Hebdo in his suitcases. He soon began working for Ravi, a monthly investigative and satirical magazine in the Southeast region. For all the freedom it offers, because there is investigation and because sometimes we have to laugh rather than cry about current events.  That was 14 years ago. Over the years, he takes a malicious pleasure in delving into the financial documents of the local authorities.   Marius Rivière, 27, is a freelance journalist between Paris and Marseille. He works with national media such as Libération, StreetPress, Konbini or local media such as Marsactu or La Marseillaise. He also works with Radio Télévision Suisse and produces independent podcasts. He had the crazy idea to subscribe to Sigma Awards and complete all these sections.   Julien Vinzent works for Marsactu, local pure-player, since 2010. Before #LaGrandeVacance, he lead several data driven investigations. He contributed mainly by curating and analysing the data and supervizing the visualization. He also teaches data journalism at the Journalism school of Marseille (EJCAM).     ",,,
Netherlands,"de Volkskrant, de Correspondent",Big,Participant,Best data-driven reporting (small and large newsrooms),"Recommended for you by YouTube: racism, antisemitism and misogyny – How YouTube fuels right-wing radicalization",02/08/19,"Investigation,Cross-border,Multiple-newsroom collaboration","Scraping,D3.js,Canvas,Json,R,Python"," We investigated what role YouTube plays in the real-life radicalization of viewers. In a collection of written articles, data visualization, videos and a podcast, we shed light on the most important YouTube channels of the ‘reactionary right’. We uncover their connections around themes like antisemitism, anti-feminism and white supremacy and show how this can easily lead viewers down a rabbit hole of increasingly extremist content. To sketch the full picture, we did not only analyze 600.000 videos and 120 million comments from 1500 YouTube channels, but we also tracked down a handful of the commenters that expressed increasingly extremist views. "," Unlike other research, we did not only look at the videos that the YouTube algorithm recommends, but also unearthed the underlying community of reactionary right channels that follow and feature each other. Moreover, we reconstructed the journey of users through this network using tens of millions of their comments and interviewed some of these people about the development of their views. "," To uncover the network of reactionary right channels, we compiled a list of YouTube accounts that are considered to be extremist right by anti-fascism experts, academic researchers and various media sources. Using the ‘YouTube Data Tool’ of the Digital Methods Initiative we then collected all followers, subscriptions and featured channels of these accounts. We filtered the resulting collection of channels by hand and iterated this search procedure several times. Of the final 1500 channels we collected the videos, comments and other additional information from the YouTube API, using Python scripts. We also transcribed 400.000 videos using the youtube-dl Python library. Data on the monthly number of views and subscribers of the channels were obtained from Socialblade, which also provides a measure for the influence that popular channels have.   We analyzed these data with the help of statisticians, media scientists and algorithm experts, partially through two hackathon days in september and october. For our understanding, we also simply watched hundreds of the most popular videos we surfaced. "," The hardest part was to reconstruct the journeys that viewers follow into the increasingly extremist corners of the YouTube universe. For this, it was essential to not only look at the videos themselves, but also the content of the comments. And to track down and interview some of the frequently anonymous commenters.  "," Data alone does not provide the full answer, you also need to talk to the people that are involved. ",https://www.volkskrant.nl/kijkverder/t/2019/radicalisering-youtube/,https://decorrespondent.nl/9149/aanbevolen-voor-jou-op-youtube-racisme-vrouwenhaat-en-antisemitisme/445528853-0f710148,https://www.volkskrant.nl/kijkverder/v/2019/hoe-youtube-rechtse-radicalisering-in-de-hand-werkt/,https://www.volkskrant.nl/nieuws-achtergrond/leidt-het-algoritme-van-youtube-je-naar-extreme-content~bea101e3/,https://www.volkskrant.nl/kijkverder/2019/youtubenetwerk/,https://www.volkskrant.nl/nieuws-achtergrond/zo-onderzochten-wij-radicalisering-op-youtube~bd8cebe8/?_ga=2.152566841.1018213455.1553429656-816711757.1531847512 https://github.com/CorrespondentData/YouTubeExtremism/tree/create_make_setup/DataCollection,https://www.volkskrant.nl/video/kanalen/nieuws-achtergrond~c474/series/video-s-waarin-we-iets-uitleggen~s1170/hoe-youtube-rechtse-radicalisering-in-de-hand-werkt~p66891,"Dimitri Tokmetzis, Hassan Bahara, Annieke Kranenberg, Leon de Korte, Mirjam Leunissen", Dimitri Tokmetzis and Mirjam Leunissen are data journalists   Hassan Bahara and Annieke Kranenberg are topical experts   Leon de Korte is information designer ,,,
Netherlands,de Volkskrant,Big,Participant,Best visualization (small and large newsrooms),The YouTube-stars of the reactionary right,02/08/19,"Investigation,Cross-border,Multiple-newsroom collaboration","Scraping,D3.js,Canvas,Json,R,Python"," This data visualization shows the strongly connected network of popular YouTube channels with a reactionary right ideology that follow and feature each other. We uncovered this online community during several months of research on how users are exposed to extremist content on YouTube and what role extremist vloggers and the YouTube recommendation algorithm play in the real-life radicalization of viewers.   We reveal the connections between these channels around themes like antisemitism, anti-feminism and white supremacy and show how this can easily lead viewers down a rabbit hole of increasingly extremist content, even if the recommendation algorithm itself is not biassed. "," Unlike other research, we did not only look at the videos that the YouTube algorithm recommends, but also unearthed the underlying community of reactionary right channels that follow and feature each other. Moreover, we reconstructed the journey of users through this network using tens of millions of their comments and interviewed some of these people about the development of their views. "," To uncover the network of reactionary right channels, we compiled a list of YouTube accounts that are considered to be extremist right by anti-fascism experts, academic researchers and various media sources. Using the ‘YouTube Data Tool’ of the Digital Methods Initiative we then collected all followers, subscriptions and featured channels of these accounts. We filtered the resulting collection of channels by hand and iterated this search procedure several times. Of the final 1500 channels we collected the videos, comments and other additional information from the YouTube API, using Python scripts. We also transcribed 400.000 videos using the youtube-dl Python library. Data on the monthly number of views and subscribers of the channels were obtained from Socialblade, which also provides a measure for the influence that popular channels have.   We analyzed these data with the help of statisticians, media scientists and algorithm experts, partially through two hackathon days in september and october. For our understanding, we also simply watched hundreds of the most popular videos we surfaced. ", One of the hard parts was to identify the thematic character of the different channels. This involved watching many of these YouTube videos (and transcripts) and classifying the type of content. Also tracking down and interviewing radicalized users was difficult. ," To sketch the full picture you need to consult with the data, experts and users alike. ",https://www.volkskrant.nl/kijkverder/2019/youtubenetwerk/,https://www.volkskrant.nl/kijkverder/t/2019/radicalisering-youtube/,,,,,,"Mirjam Leunissen, Dimitri Tokmetzis, Hassan Bahara, Annieke Kranenberg", Mirjam Leunissen and Dimitri Tokmetzis are data journalists   Hassan Bahara and Annieke Kranenberg are topical experts ,,,
Netherlands,"de Volkskrant, Open State Foundation, Pointer",Big,Participant,Open data,How Did Your Neighborhood Vote?,18/04/19,"Multiple-newsroom collaboration,Database,Open data,News application,Map,Elections","D3.js,Json,R,Python"," This interactive map shows the results of nationwide elections for each of the 9304 ballot boxes in The Netherlands and lets the user zoom in on political parties or localities to find specific patterns and stories. In the past, the election results were only being published at the level of entire municipalities and it took us two weeks to manually collect all of the ballot box results, in a joint effort with the Open State Foundation and Pointer.  "," Our efforts revealed the lack of nationwide standards and how hard it is to come by the detailed election results in the first place. As a result, the Dutch government now requires a standardized publication of all the ballot box results for future elections. ", We used R and python for data acquisition and analysis and mapbox and D3.js for the interactive map. , The hardest part was to gather all of the ballot box results from every single municipality in a similar format. , Faulty standard practices can change for the better if you show interest in having proper access to the data. ,https://www.volkskrant.nl/kijkverder/2019/stembureaus/index.html#/,,,,,,,"Mirjam Leunissen, Serena Frijters, Jurriaan van de Reep", The authors are data journalists ,,,
Netherlands,de Volkskrant,Big,Participant,Best news application,How healthy is your neighborhood?,24/08/19,"News application,Map,Health","D3.js,Json,R"," National Health Institutes interviewed nearly half a million people in The Netherlands about their health, from chronic illness to smoking and depression. We developed an interactive map, which shows the regional patterns in people’s health and lets the user zoom in on specific neighborhoods to find their own stories. ", Many local news media extracted stories from our map for their own publications. ," We used R for data analysis and Mapbox, deck.GL and d3.js for the interactive map. ", The hardest part was to make the design and performance of the interactive map suitable for both desktop and mobile. ," With an accessible visualization, including well-chosen filters, people will uncover many more stories than you can possibly find on your own. ",https://www.volkskrant.nl/kijkverder/2019/hoe-gezond-is-jouw-wijk-gewicht-depressie-alcohol-roken/#/,,,,,,,"Mirjam Leunissen, Serena Frijters", Mirjam Leunissen and Serena Frijters are data journalists ,,,
Mongolia,"https://ikon.mn/n/1kd7, https://www.facebook.com/MYOHP/posts/2339558936315747, https://www.facebook.com/watch/?v=415473815937403,",Small,Participant,Open data,Shadow of the pension system in Mongolia,25/04/19,"Solutions journalism,Multiple-newsroom collaboration,Open data,Infographics,Video,Economy,Employment","Animation,Microsoft Excel,Google Sheets","Shadow of the Pension system in Mongolia Our story represent a loophole of current Pension system in Mongolia. there are number of reason making the loophole bigger and bigger: 1. Total Social insurance fee, for a person with average salary, only cover pension money up to 4-5 years. 2. life expectancy getting longer by the time. According to Nasional Statistic office Mongolia, Mongolians reached retirement age, women could life could live 25 years, and man 17 years. 3. A ratio, total pensioner to toral worker who pay social insurance, would be duble from 2019 to 2051. 4. Annual loss of"," Here is the main reason of Pension system's loss    1. Total Social insurance fee, for a person with average salary, only cover pension money up to 4-5 years.   2. life expectancy getting longer by the time. According to Nasional Statistic office Mongolia,   Mongolians reached retirement age, women could life could live 25 years, and man 17 years.    3. A ratio,  total pensioner to toral worker who pay social insurance, would be duble from 2019 to 2051.    What our story propose is pension system reform, make it multi tier system, adding Base pension to current one.   here is the impacr of the story:    - Mongolian parliament organised working group with 5 parliament members, to research for pension system reform in Jan 2020..    - the forum ""Pension system reform "" held in Ulaanbaatar, lst december. "," - Open data. Our team used to open data, from 1212.mn NSO Mongolia, and National Social Insurange Agency, and etc ..    - Doctorial Dissertation of L.Oyuntsetseg, ""Pension system refrom, Introducing Base pension system""    <a href=""http://bs.num.edu.mn/%D0%B4%D0%BE%D0%BA%D1%82%D0%BE%D1%80-%D0%BB-%D0%BE%D1%8E%D1%83%D0%BD%D1%86%D1%8D%D1%86%D1%8D%D0%B3/"">http://bs.num.edu.mn/%D0%B4%D0%BE%D0%BA%D1%82%D0%BE%D1%80-%D0%BB-%D0%BE%D1%8E%D1%83%D0%BD%D1%86%D1%8D%D1%86%D1%8D%D0%B3/</a>   - Microsoft excel,  used to calculate future projection, example, number of pensioners and social insurance payers, inflation rate, discount rate, income and cost of Pension system,    - According to our calculation :  Today's 1MNT equal to 8 MNT of 2051,    - Each number represented in our story, have certain calculation.      "," Our main role, Amgalan, born 1986, male, work as technician with average salary.   At first we asked ourselves:    - When he retire, How much much he could get as pension,    - How many pensioners in Mongolia, in 2051 vs today   - How many people pay social insurance in 2051 vs today   - Which kind of problem Mongolian pension system will face in 2051.    - What is the solution for that.   then do economic calculations on google doc, sheet, slides, to find result.    After having result, we also asked for economics and statistical teachers from NUM, whether our result feasible or not?    then we entered story animation task.          "," By making the story, We learned how to do future projection and economic calculation .    All the document, data, used in story on the internet.    <a href=""https://docs.google.com/document/d/1dJoBQw-Z9macGpnPEEsGMepwb7cqM8gh9Rd2LwZ6SnM/edit"">https://docs.google.com/document/d/1dJoBQw-Z9macGpnPEEsGMepwb7cqM8gh9Rd2LwZ6SnM/edit</a>   <a href=""https://docs.google.com/presentation/d/1X7e-OOZ8FiJOA4t-VTlLmHkOsClXxafrh0PyM2PWxko/edit#slide=id.p1"">https://docs.google.com/presentation/d/1X7e-OOZ8FiJOA4t-VTlLmHkOsClXxafrh0PyM2PWxko/edit#slide=id.p1</a>   <a href=""https://docs.google.com/spreadsheets/d/1o6HfGaCKLdgFfca_95s7M9-Ph5z0J9Q2wbk4N_5Zios/edit#gid=1353384609"">https://docs.google.com/spreadsheets/d/1o6HfGaCKLdgFfca_95s7M9-Ph5z0J9Q2wbk4N_5Zios/edit#gid=1353384609</a>   <a href=""https://docs.google.com/spreadsheets/d/1Ahi1ekimxAXj0TtygGm0fYvINr4AMjDCdEKLPB9_Prw/edit#gid=581536736"">https://docs.google.com/spreadsheets/d/1Ahi1ekimxAXj0TtygGm0fYvINr4AMjDCdEKLPB9_Prw/edit#gid=581536736</a>   and it is easy to use, understand for others.  ",https://www.youtube.com/watch?v=95yc5TY9-a4,https://docs.google.com/document/d/1dJoBQw-Z9macGpnPEEsGMepwb7cqM8gh9Rd2LwZ6SnM/edit,https://docs.google.com/presentation/d/1X7e-OOZ8FiJOA4t-VTlLmHkOsClXxafrh0PyM2PWxko/edit#slide=id.p1,https://docs.google.com/spreadsheets/d/1o6HfGaCKLdgFfca_95s7M9-Ph5z0J9Q2wbk4N_5Zios/edit#gid=92154998,https://docs.google.com/spreadsheets/d/1Ahi1ekimxAXj0TtygGm0fYvINr4AMjDCdEKLPB9_Prw/edit#gid=581536736,,,"Brainstorm team of 4 journalists from different organizations, Otgontugs Ulziisuren, from MCDJ, Davaadulam Davaajav and Batzaya Ganbat, from Mongolian National Radio, Shurentsetseg Sukhbaatar, from LIVETV Mongolia."," Brainstorm Team members:   Otgontugs Ulziisuren, data journalist of MCD   Davaadulam Davaajav, editor of Mongolian National Radio   Batzaya Ganbat, investigative journalist of Mongolian National Radio   Shurentsetseg Sukhbaatar, journalist of LIVETV Mongolia ",,,
United Kingdom,Property Week,Small,Participant,Best data-driven reporting (small and large newsrooms),The great Section 106 and CIL scandal,27/09/19,"Investigation,Database,Infographics,Business","Adobe,Creative Suite,Microsoft Excel,Google Sheets,Node.js"," Ask any developer about their latest big scheme and at some point, they will mutter darkly about the Section 106 or Community Infrastructure Levy (CIL) payments they have had to make.   The payments are meant to finance infrastructure, but there is scant evidence that’s what they are going toward. The suspicion is councils are only spending a fraction of what they take in and the rest has disappeared into a sizeable black hole.   Quite how big, no one knows. There has been no requirement for local authorities to disclose the figures and dozens haven’t. Until now. ", Our investigation found that more than £2.5bn—or 63%—of the money local authorities took in through the taxes remains unspent.   Several local councillors got in touch to ask us for the data we collected on their borough and the mismatch between what was collected in s106 and CIL development tax and what has been spent. They said they would raise the discrepancies with council leadership to argue for the money to be released for its intended purpose to build new infrastructure.   One councillor said she would use the data and discrepancy in infrastructure spending as a campaign issue as she sought to become an MP in the Dec 12 UK general election.   A developer financier spoke with Property Week and said that he is creating a new business to work to claw back some of the unspent money (and by law owed back to developers) based on our investigation. ," The starting point for this investigation was crafting and sending Freedom of Information requests to more than 300 local authorities to get annual financial information between 2013 and the end of 2018 showing how much money had been taken in and spent through the s106 and CIL development taxes. At the time this investigation began in early 2019, councils were not required by law to post this information publicly.   As these financial disclosures were received through FOI over months, the data were entered by hand into an Excel spreadsheet to create a unique data set.   Once we had examined and entered data from the financials of 61% of the largest councils throughout the UK—including all London boroughs and the Big Six cities—the data was analysed to find approximate total figures for what local authorities had collected and spent through both tax mechanisms.   To present the final figures we used a combination of Adobe Illustrator and Infogram online and in the magazine to create infographics visualising the collection and spending of these taxes over time. "," The most painstaking part of this project was organising and conducting the submission of more than 300 Freedom of Information requests and then collating the financial disclosures into a bespoke data set.   The data was entered by hand from disclosures that were given in a variety of formats. Many annual totals of s106 and CIL tax collected and spent by individual councils needed to be calculated by Property Week from voluminous data disclosures. Some financial documents came in the form of non-machine readable PDFs, Word documents, or several Excel files that were not dated correctly. Each disclosure needed to be scrutinized so that we ensured our data set was accurate.   This process took one reporter around six months to complete with some contributions and help from colleagues.   Of the more than 300 councils that were sent FOI request, 43 failed to respond, some refused to disclose their figures, and many others only provided partial data. Sometimes councils pointed to where the data was available publicly. But in many of those instances, a full data set from 2013 to the end of 2018 was spread over multiple PDF documents and needed to be collated or some data was missing. "," This project will inspire other journalists and draw their attention to the power of the Freedom of Information Act and how it can be leveraged to collect data from disparate local authorities to show a larger picture of what is happening in the UK.   It will show others how collating financial information from multiple local authorities can give the public a better picture, not only of how public money is being spent locally, but how it is being spent across the country and whether there are larger trends beyond those unique to a few localities.   This project could potentially inspire others that it is possible to conduct a large FOI project in the public interest. ",https://www.propertyweek.com/insight/the-great-section-106-and-cil-scandal/5104449.article,,,,,,,Graham Lanktree," Graham writes the Intelligence page and handles data analysis and data visualisations for Property Week. Formerly he covered U.S. politics as a staff writer for Newsweek and appeared frequently as a contributor on BBC World News speaking on breaking stories in the American political scene. His previous work has also appeared in Vice UK, The Toronto Star, The Globe and Mail, and The Bureau of Investigative Journalism. ",,,
Russia,YC Valentin Balash,Small,Participant,Best data-driven reporting (small and large newsrooms),modern slavery,29/05/19,"Documentary,Video,Lifestyle,Economy,Human rights","360,Sensor,Personalisation"," Modern slaves in Russia are people who have become dependent on consumption and turned into credit debtors to banks. Tempted by advertising, they took their first loan without understanding how they would be able to pay it off, and when it came time to repay the debts-it turned out that they did not have a job. "," Thanks to the release of this story, the state Duma considered the law on new rules for microfinance credit organizations. In particular, the borrower must have a job to get a loan. "," This video is a documentary investigation, where I studied in detail how the mechanism of seducing people with consumer loans works ", The hardest part of my job was finding people. who will voluntarily agree to tell you that they are now being harassed and their lives are being threatened. As I have already said. due to the fact that they have no ability to pay. they are forced to hide from collectors. who can beat them or rape them. And there were such cases , This project calls for the main human quality of compassion. These people can be treated differently. but no one is immune from this. And how will this world behave if fate chooses such a test and you find yourself in their body? ,https://www.youtube.com/watch?v=LOV7RGA7bLg&t=6s,,,,,,,I am the producer and author of this project," Valentin Balash is a professional journalist and television correspondent. I make documentaries as a writer and Director. write a book. Heroes of my stories, people who need help. And I believe that paying attention to their lives can help make this world a better place ",,,
India,The Hindu,Big,Participant,Best data-driven reporting (small and large newsrooms),Migration in Tamil Nadu: Case of much ado about nothing?,09/08/19,"Investigation,Fact-checking,Economy,Employment,Human rights","Scraping,Microsoft Excel,Google Sheets,CSV","Tamil Nadu, the southern most State of India, is relatively one of the most liberal and progressive in the country. However, the recent anti-immigrant shift in the world politics percolated into the State's policies and parties resulting in nationalist groups calling for ""protection"" of jobs and resources from ""outsiders"". The effect such anti-migrant stance had in other States impressed upon the need to nip such sentiments in the bud through hard data and proving that the anti-migrant fear is a concoction of divisive politics. Thats what the story achieves. Using census data we proved that, not only are migrants in"," Politically, none. However, the effect of such stories will only be known when the concerned issue gets big. As i had mentioned in the description, we chose to deal with the issue in the early stages. When we addressed the problem, only a handful of parties had issued such protectionist calls. However, the main opposition too had voiced such concerns, though in a mild way. If the issue gets big, like most of them do around elections, the hard data we published will come in the way and shed light on the ridiculousness of the claim: ""migrants are taking away jobs in Tamil Nadu"". The data shows that migrants are not taking away but creating more jobs for the locals as most of them who come to the State are not seeking employment but have come for other needs. As the data is sourced from the Census, given the rationalist origins of the parties in the State, we believe that they will see sense and give up the call. However, the effect it would have had on readers is of far more value. If such parties continue with their anti-immigrant stance, we believe that readers who are also prospective voters, will neglect them in the elections. Which we feel is a prospective impact of the story.    "," Given the limitations of Census data dissemination in India, we were not in need of advanced tools or technologies. In India, most of the data is available in PDF files. Especially, some of the important documents may even be presented as image PDFs, thereby enhancing the difficulty of culling the data out. In this specific project, the Census data was available in PDFs and also they are arranged in a haphazard fashion. The challenge was to make our way through the maze of unorganised data and not so properly defined columns and make sense of the conclusions. Most of the data was culled out using Tabula and cleaned using microsoft excel. The definitions of columns were not available. Thus sources from Indian ministries and census departments were contacted and the definitions were confirmed. Then we used Tableau to visualise the scatter plots.    "," As i said before, in India, data is not collected or maintained in a uniform manner. The recent stories in Indian media about government hiding important public data (case in point the household expenditure data and the jobs data which were hidden from public and later leaked) proves the level of influence government has over its statistics department. This makes it a challenging task to fetch data in first place. The next part is about reliability of the data. Most of the column heads in the census data which we used, did not make sense. We had to call up the concerned departments, mail them to fetch those details. Most of them would not respond given the red-tapism in India. Thus, the project, which should have taken just over a week in the Western countries, took us a month to even understand what the data was really about. Then came the problem of ensuring whether the reality on the ground coincided with the data. Whoever we spoke to on the ground felt that the data was wrong. This speaks volums about the wrong perception people have about the number of migrants in the State. The non-belief in the data and the low levels of jobs-migration it was citing led to sources distancing away from the story, saying we are projecting something which was not a reality. This was in a way the success of the hard data which we pulled out, where perceptions were broken and reality was restored. No other Indian news organisation attempted this story despite common availability of census data.      Other than data collection and making sense of the data without proper column heads and ensuring its credibility given the very low levels of jobs-migration it was suggesting, the rest of the project was a breeze.  "," That, even in mundane data sets, lies the best of the stories. Also, public perception in a politically charged environment will often be skewed and only hard data can restore balance. That, when reporting hard data which goes against public perception, experts will scowl at the data and refuse to comment on the data as it is unpopular.  ",https://drive.google.com/open?id=1yzFgCdyP6FZ5Uy3ZBw9eAnH6HsocOACr,https://www.thehindu.com/news/national/tamil-nadu/migrants-in-tamil-nadu-case-of-much-ado-about-nothing/article29364682.ece,,,,,,"Vignesh Radhakrishnan, Pon Vasanth B.A., Udhav Naig"," Vignesh Radhakrishnan is a senior data reporter with The Hindu. He handles numbers across verticals including sports, economy, business, politics, climate and world affairs.  ",,,
United States,Debtwire,Small,Participant,Best data-driven reporting (small and large newsrooms),Bitter Harvest: Debt and the Bankrupting of the American Family Farm,25/03/19,"Investigation,Explainer,Long-form,Business,Agriculture",Microsoft Excel," Debtwire Investigations’ feature on a little-known chapter of the US Bankruptcy code available only to small farmers shed light on an area of the law that’s ripe for reform. Our multimedia reporting included a proprietary dataset categorizing all Chapter 12 bankruptcy filings during 2018 that we put together when we realized that no such data existed. We produced a podcast featuring an interview with Senator Chuck Grassley, who sponsored the Family Farmer Relief Act of 2019. The story also tackled the highly personal and traumatic experiences of several individual farmers who made the difficult decision to file for Chapter 12. "," We were able to determine where the pockets of farming distress were concentrated, both geographically and by farm type. The team combed through every Chapter 12 filing on Pacer for 2018 to compile the data, and concluded that Wisconsin was home to the most Chapter 12 filings by far in 2018, with 10% of all petitions. Not surprisingly, then, by farm type, dairy topped the list with 16% of all US filings last year. Dairy shared the first place position with corn-focused operations, a segment that also came in at 16%.        This proprietary data has been cited by numerous agricultural industry publications, and lead reporter Maura Webber Sadovi received feedback from a range of industry professionals thanking her for creating the much-needed data.         The data is especially important right now, as the ongoing trade wars have had an outsized impact on the farming industry – and in particular on family farmers who have taken out debt to keep up with fast-paced and expensive technological advances. And the personal storiesunderscore the reasons that broadening Chapter 12 eligibility could be a critical lifeline that helps more farmers hang onto their farms.        A few weeks after the story was published, the US Senate finally passed the Family Farmer Relief Act, which extends Chapter 12 eligibility to a broader group of farmers. And in the ensuing months, Chapter 12 filings continue to grow on a year-over-basis. Chapter 12 family farm bankruptcy filings soared in 2019 to their highest level since 2010 as unusually brutal weather heaped more stress on an agriculture sector already reeling from the trade war with China and multiple years of low commodity prices. "," We knew that Chapter 12 family farm bankruptcies were on the rise but we were looking to dig into them and figure out what was behind the rise by looking for patterns in the type of farm, location of farm and the size of farm.        We used the Pacer search functions to access and download all Chapter 12 filings in the US in 2018. We then created our own master database of the nearly 500 filings, categorizing the data contained in the filings.        We divided up the nearly 500 filings between four reporters and pulled the information based on data from the filings. Where the data was not available in the filings, we called attorneys and farmers to confirm, entailing hundreds of calls. Ultimately we were able to confidently determine that dairy and livestock farms were showing the most signs of distress.    "," Combing manually through every Chapter 12 filing on Pacer grew more complicated the longer it went on due to the lack of standardized filing forms and descriptions, as well as inaccuracies.       As the process evolved, it became apparent that we had consistent data on farm type and location but farm sizes in terms of acreage was far less available. So after initially hoping to use the data to get a picture of what sizes are farms were filing, we shifted gears to focus more closely on location and farm type.        Ultimately, phone calls to lawyers involved in all 474 of the cases were required in order to properly classify them. One of the wrinkles we addressed is that we needed to reconcile the number of filings available via Pacer versus more static data published by the US Courts quarterly analysis.   In addition, understandably, farmers were not enthusiastic about discussing their bankrupt finances in a public forum. But Maura was able to persuade several farmers to explain their stories and their difficult choices to her on the record, after she to traveled to meet with them in person. One of the on-the-record sources told Maura at the last minute that he no longer wanted to be quoted in the story – and required an arduous persuasion process to get him back on the record!   Also, the weather posed challenges for meetings and getting photos/art. Maura planned to get photos of farmers at a weekly farm auction in Wisconsin but snowstorms to the remote rural area repeatedly prevented the trip. "," Readers of “Bitter Harvest” could learn about an important lifeline for family farms: the Chapter 12 bankruptcy. Filing for a Chapter 12 bankruptcy is an extraordinarily difficult business and personal decision but it also can enable a family to get a fresh start and avoid “losing the farm.”    Still, the finances that underpin struggling farmers’ challenges–and the specialized relief available to them under Chapter 12 of the Bankruptcy Code for farmers—are often little known or misunderstood. Even some farmers interviewed for “Bitter Harvest” report weren’t aware of Chapter 12’s potential role as a financial solution for their families’ farm-thinking instead that it signaled a farm sale.   Indeed, one bankrupt farmer who was reluctant to go on the record for fear of reprisals in his community ultimately said he felt it was important for him to talk about his bankruptcy and get the word out about the options his fellow farmers had. Readers of “Bitter Harvest” learned about the rise in the bankruptcies as well as the special benefits that are offered farmers, such as lower filing fees and the fact that, unlike in Chapter 11, the judge rather than creditors approve the plan. making it more likely that the reorganization be a roadmap to the future rather than the end of the road.  ",http://investigations.debtwire.com/bitter-harvest-debt-and-the-bankrupting-of-the-american-family-farm/,https://soundcloud.com/user-510253489/debtwire-investigations-podcast/s-jDz9l,https://www.debtwire.com/intelligence/view/prime-2962050,,,,,"Maura Webber Sadovi, Maria Chutchian, Farhin Lilywala"," Maura Webber Sadovi is assistant editor at Debtwire ABS, where she writes and edits stories on commercial real estate finance and securitized debt. Before joining Debtwire in 2015, she wrote the Deal of the Week column for The Wall Street Journal and has written for Bloomberg and the Philadelphia Inquirer. Maura holds a B.A. from Middlebury College and an M.A. in Journalism from New York University.          Maria Chutchian is the associate courts editor at  Debtwire , where she writes and edits stories on large and midmarket Chapter 11 cases, as well as the ongoing Puerto Rico debt restructuring. Before joining  Debtwire  in 2014, Maria was the senior bankruptcy reporter at Law360. Maria holds a B.S. in journalism from Emerson College.     Farhin Lilywala is a reporter at  Debtwire , where she covers high yield bonds. Before joining  Debtwire  in 2017, she freelanced as a production coordinator with the creative publishing company LeBook. Farhin started her journalism career as an editor at various student newspapers and as a researcher for Ian Urbina at the  New York Times . A native of Atlanta, she has a B.A. in journalism and anthropology from New York University. ",,,
Austria,Addendum.org,Small,Participant,Best data-driven reporting (small and large newsrooms),How has your municipality's climate changed?,08/12/19,"Explainer,Long-form,Multiple-newsroom collaboration,Infographics,Chart,Map,Environment","D3.js,QGIS,Json,CSV,R,RStudio,Node.js"," In one of the hottest summers in recent years, we showed how climate change played out on a local level. We showed rising temperatures and numbers of heat days based on both historical data and projections. The piece rewrites itself based on the users' input location. ", Local newspapers in most austrian states partnered with us and either republished the investigation or based their local reporting on our findings. ," All our data sources – both historical temperature data and projected change of indicators – were provided to us as geospatial 1x1km raster files, which we processed using a R pipeline based on Timo Grossenbacher's reproducable ddj template. We used ~2100 municipality centers retrieved from openstreetmap and wikidata to transform the raster data to municipality data, which we then transformed (using javascript) to an interactive story that changes depending on users' inputs.  We also created reports for local papers showing how their regions were affected, which they used for additional reporting. "," (Use „Wien” as an example input, this will show you data on Vienna)  By raw amount of data, this was our largest (big) data project yet. The hard parts were figuring out how to make the large amount of data legible to our users (and process-able by us), creating a story that could change dependin on the users' input, and supporting other reporters' local reporting. "," Climate Change can be interesting to both reporting partners and readers if the impacts can be shown on a local level and made legible to non-specialists. The latter was achieved by using the measure of heatdays and not showing relative differences, but for example absolute temperatures. ",https://www.addendum.org/klima/klimawandel-gemeinden/,,,,,,,"Danijel Beljan, Gerald Gartner, Markus Hametner"," The Addendum data team produces personalized, interactive journalism in Vienna, Austria. ",,,
Austria,Addendum.org,Small,Shortlist,Best data-driven reporting (small and large newsrooms),Shared Living: Mohammed gets fewer responses than Anna,17/07/19,"Investigation,Long-form,Illustration,Infographics,Video,Politics,Lifestyle","Scraping,Google Sheets,R,RStudio"," We showed that people with foreign-sounding names get fewer replies than people with Austrian-sounding names in Vienna and Graz, the largest cities in Austria. "," Aside from some public discussion, none. "," We scraped classified pages for flatshares, used Google Sheets to prevent double-contacting and manually messaged every offer using different names and personas. Hence, we created data that did not exist before.  We also published the reporting using an Instagram-like storytelling format which we developed in-house, since we considered students the target audience for the story. "," Collecting the data and ensuring that the data collection method does not influence the results.  Coordinating the (and with the) journalism students who took over large pieces of the reporting and data collecting parts of the project.  Putting faces to the data: not only collecting data, but also finding people who would talk to us about their experiences on the record. "," Collecting your own data allows you to create stories and answer questions that no one else can scoop.  Connecting faces with the data – in this case, also asking people to tell us about their experiences – put the project on a much higher level than it would have been otherwise. ",https://www.addendum.org/news/wg-suche-mohammed/,,,,,,,"Danijel Beljan, Gerald Gartner, Amelie Sztatecsny, Annabell Lutz, Elena Zeh, Jara Majerus, Julia Wendy, Maximilian Miller, Vincent Leb, Yona Lévesque"," The Addendum data team produces personalized, interactive journalism in Vienna, Austria. ",,,
Serbia,BIRN Serbia,Small,Participant,Best data-driven reporting (small and large newsrooms),Serbia - a tax haven and a shelter haven for debtors,31/05/19,"Investigation,Long-form,Database,Open data,Chart,Map,Corruption,Money-laundering","Scraping,Python","Agency called World Business Solutions buys indebted companies for profit giving their old owners a fresh start. Our investigation revealed a scheme of 10 people taking over almost 2000 troubled companies from early 2014 by May 2019. All of them combined were 53 million EUR in debt. In most cases, the state was the biggest creditor meaning companies were not paying taxes timely. According to the Serbian law, this scheme was partially legal. Limited Liability Company (LLC) is not the responsibility of the owner or director, but the legal entity, that is the company itself. Prosecution and Police are investigating","  For the organisation with our scale, the story had an impressive public impact with 26 republications, including some of the major outlets in Serbia such as Al Jazeera, Blic, Danas, N1, RTS, etc. Investigative reporting portal Insajder did a follow-up <a href=""https://insajder.net/sr/sajt/tema/15051/"">https://insajder.net/sr/sajt/tema/15051/</a>.       The video about the residential building with more than 400 companies registred had more than 58.000 views at it was shared via FB and TW (YT - 418 views:TW - 36,540 views, FB – 21, 053 views).      Data visualization sets were praised as Top 10 by GIJN for May 2019. See here <a href=""https://gijn.org/2019/06/06/gijns-data-journalism-top-10-european-election-data-via-audio-tax-fraud-parserator/"">https://gijn.org/2019/06/06/gijns-data-journalism-top-10-european-electiondata-via-audio-tax-fraud-parserator/</a> . Google analytics at <a href=""http://javno.rs/"">javno.rs</a> shows that the story had 13,513 pageviews (Unique Pageviews: 12,507). On top of that, the story was shared 12,262 times and the average time spent on page is 5 min and 37 seconds.  ","  BIRN team combined investigative reporting and data journalism to tell the story about tex heaven in the making. It all started as an accident when my colleague noticed one owner had more than 20 companies registered on his name. Most of them had blocked bank accounts.       Why would someone buy a blocked company, he wondered. After some digging, he came across another case - another owner of hundreds of blocked companies. In just a few months, he would learn from the policeman investigating these cases how this problem is ""the cancer of the Serbian economy.""     Investigative reporters obtained a list of 10 owners with the most companies through FOI request from Serbian Business Registry. Since the number of companies was changing, the data team has been tracking them daily.      The scraper BIRN team designed was pulling company ID numbers and addresses from the Public Registry. Using that list of company ID numbers, we ran another scraper through the National Bank's Single Register of Accounts Search to gather data on the amount of debt for each company. We compared the same list of companies with the Tax Administration’s List of the biggest debtors determining how much some of them owe for tax. We used Python for analysis and Flourish for visualizations.      While the data team was wrangling the sets, investigative reporters went on the field. They met previously mentioned sources from the police who revealed how ""some Belgrade lawyer"" was behind it and how the police couldn't find information about his identity. Reporters visited Jasonova 11 St, a residential building in the family neighborhood with more than 420 companies registered on its address. As they were trying to enter, the public bailiff officer was leaving the notice in the lobby failing to find the company's office door.  ","  Exposing an organized debt evasion scheme had its challenges both on the investigative and data side of the production. Data journalists required a lot of precision and nerves.       The biggest challenge was creating an initial dataset in the daily changing environment. To overcome this problem, we conducted two analyses at different points in time. During the first one in February, we developed the code and methodology. In May, a couple of days before the publishing we repeated the process to have fresh data.     New dataset was created by scraping and merging together  different entities from tree publicly accessible databases - Serbian Business Registry, National Bank database, and Tax Administration's List of the biggest debtors.      We created a list of company ID numbers by the owner we obtained through FOI. Extracting ID numbers and Addresses for each company from the Serbian Business Registry was challenging since the website uses reCAPTCHA. Data on the amount of blockage for each company was deeply buried within the National Bank database,  so the scraping process took longer. Tax Administration's List is in PDF format, which required additional cleaning.     We combined all the elements mentioned to determine the total amount of blockage, tax debt, several addresses with a couple of hundreds of companies registered on it. All our findings are available as visualizations in the link section.   ","  This project is a great example of investigative and data journalists working together on revealing different angles of a knotted story. As numbers were setting the direction, the investigative part provided context and human perspective. When data journalists calculated 420 companies registered on one address, investigative reporters visited it to learn neighbors are very concerned about this problem. We believe this project sets a good example for other newsrooms interested in covering complex topics through data and investigative reporting. Findings our data team provided served as leads for investigative reporters or as facts they faced officials with. Not only that we managed to mine the data from different sources offering our readership more information than the official institutions, but we shed light on struggles police investigation was facing regarding this case. Another great resource for other organizations interested in data journalism could be the code we produced for scrapers and analysis.  ",https://javno.rs/istrazivanja/srbija-poreski-raj-i-utociste-za-duznike,https://drive.google.com/open?id=1uR7RMx4YChKwj5va-i_2InvV-oj4TIWS,,,,,,"Slobodan Georgiev, Milorad Ivanovic,  Jelena Veljkovic, Aleksandar Djordjevic, Miodrag Markovic, Natalija Jovanovic","  Editorial:      Slobodan Georgiev is Belgrade based journalist and he serves as a Programme Coordinator in BIRN Serbia. He works as a journalist in Vreme weekly from 2001 covering organized crime and sports.      Milorad Ivanovic is an Editor in Chief at BIRN Serbia in Belgrade. His investigations have included work on human trafficking, the employment of Balkan mercenaries by British and US security firms in Iraq, and arms trafficking from Ukraine into Serbia. He is also a jury member of the annual awards given by the Independent Journalists Association of Serbia, IJAS, and the US embassy in Belgrade.         Investigative reporters:     Jelena Veljkovic have been working as a journalist since 1992, mostly covering politics, public finance, corruption, and war crimes. She worked on Radio and Television B92 as news editor, host and the author of the show Truth, Responsibility, Reconciliation, radio show Kaziprst and investigative journalist for TV show Insider. She was the winner of sveral awards for investigative journalism.      Aleksandar Djordjevic has been working as a journalist since 2009. He joined BIRN in 2012 specializing in public finance reporting. Aleksandar was awarded for best media report on monitoring of public spending in Serbia by the United Nations Development Programme.  He was also a finalist at the 2012 National Investigative Journalism Award by the Independent Association of Journalists in Serbia         Data journalism:     Miodrag Markovic is a journalist with strong programming skills in Python and JavaScript required for data analysis, wrangling and visualization, text mining, web scraping and web  development. Miodrag worked for almost two decades as a journalist, reporter, investigative journalist and editor in news magazines, daily newspapers and digital media.      Natalija Jovanovic works as a journalist for BIRN Serbia. She is interested in finding stories through data and interactively presenting them. Natalija covers social topics covering maternity leave policies, culture or environmental issues. Her work also includes bringing tech innovation to local media. She is currently doing a master’s in Social Science and Computing at the University of Belgrade.  ",,,
Belgium,https://medor.coop/,Small,Participant,Best visualization (small and large newsrooms),"The pain of Belgians (""La douleur des belges"" in french)",09/02/19,"Investigation,Open data,Map,Health","AI/Machine learning,D3.js,Json,CSV,Node.js"," Fentanil, Tramadol, Oxycontin, mean   anything to you? In 2018, there has been more then one million Belgian who took, at least once, one of these highly addictive opioid based painkiller. Médor is showing you the cartography of the Belgans in pain. This investigation is aiming at sound an alarm on a possible health crisis in Belgium.    The original article was written in French and has been translated in English for this specific submission.  "," The article provides a clean and objective way to look at the situation of opioid consumption in Belgium. The story has been driven by the data and the insights extracted from it, not so much from the journalists. We believe the greatest impact this story had on people was simply to provided awareness on an potential health issue. We also collected some testimonies from people trapped in prescription opioid painkiller treatment, this story showed them they were not alone in their pain.  "," This work was twofold:    <ul>  Cleaning, aggregation, and treatment of the data was performed in javascript using ObservableHQ notebooks. This is providing a great way to replicate and document the data treatment pipelines in full transparency.    The webpage has been developped in React (javascript framework). The visuals were fully implemented in D3.js and the scrollytelling functionality was provided by a react adaptation of scrollama.   </ul>", The hardest part of this project was to fight a priori and really based the story line on facts extracted from the data. This data was provided by the major heathcare mutuality in Belgium which had genuine concerns about the opioid consumption situation. We think our article echo these concerns amongst a larger audiance in much more engaging manner.  , Datavisualisation is pretty new in the news world. We would love to share to other news rooms (especially small ones) how we approached the construction of a story based on data and heavily relying on graphical elements to provides comprehensive insights to the readers.  ,http://medor-opioids.s3-website-eu-west-1.amazonaws.com/ (Translation in english of the original publication),https://medor.coop/nos-series/la-douleur-des-belges/opioides-datavisualisations/ (Original publication in French),,,,,,"Olivier Bailly,  Karim Douieb","  Olivier Bailly  is one of the founders of Médor (a cooperative news room focussed on investigation story in Belgium). He is amoungst the chief editors of the news paper. He has written award winning papers (prix Belfius 2013 & 2011), teaches journalism and sometime writes novels. Currently focusses (but not only) on the pharmaceutical world in Belgium.    Karim Douieb  is one of the founders of Jetpack.AI (a full stack data science company based in Brussels). He holds a Phd in computer science. He likes every aspect of a data science project: data collection, data aggregation, data mining and dataviz. With this article he made his first step in data journalism.  ",,,
United States,"USA TODAY, The Center for Public Integrity, The Arizona Republic",Big,Honorable Mention,Best data-driven reporting (small and large newsrooms),"Copy, Paste, Legislate",02/06/19,"Investigation,Multiple-newsroom collaboration,Politics","AI/Machine learning,Scraping,Microsoft Excel"," Copy, Paste, Legislate marks the first time a news organization detailed how deeply legislation enacted into law at the state level is influenced by special interests in a practice known as ""model legislation."" The series explained how model legislation was used by auto dealers to sell recalled used cars; by anti-abortion advocates to push further restrictions; by far-right groups to advocate for what some called government-sanctioned Islamophobia to moves by the Catholic Church to limit their exposure to past child abuse claims. (Published February 6, April 3, May 23, June 19, July 17 and October 2, 2019) "," People in various states called for legislation to require more transparency about the origin of bill language. Legislators found themselves compelled to defend their sponsorship of model bills.   A public-facing model legislation tracker tool launched in November 2019, allowed journalists and the public to:   --Identify recent model legislation introduced nationally   --Identify recent model legislation introduced in their state   --Perform a national search for model legislation mentioning specific keywords or topics   --Upload a document they have to instantly identify if any language in their document matches any state legislation introduced since 2010   --Look up a specific bill by number to see all other bills matching it   --Look up individual legislators and see all bills sponsored by them that contain model language   As part of the project, local newsrooms were able to identify and interview major sponsors of model legislation and identified key issues that resonated in their state. Those stories explored the reach of model legislation and its surprising impact on policies across the nation.   The combined national and local reporting revealed:   --More than 10,000 bills introduced in statehouses nationwide were almost entirely copied from bills written by special interests   --The largest block of special interest bills — more than 4,000 — were aimed at achieving conservative goals   --More than 2,100 of the bills were signed into law   --The model bills amount to the nation’s largest unreported special interest campaign, touching nearly every area of public policy   --Models were drafted with deceptive titles to disguise their true intent, including “transparency” bills that made it harder to sue corporations   --Because copycat bills have become so intertwined with the lawmaking process, the nation’s most prolific sponsor of model legislation claimed that he had no idea he had authored 72 bills originally written by outside interests "," No news organization had attempted to put a number on how many of the bills debated in statehouses are substantially copied from those pushed by special interests.   We obtained metadata on more than 1 million pieces of legislation from all 50 states for the years 2010 through 2018 from a third-party vendor, Legiscan. We also scraped bill text associated with these bills from the websites of state legislatures. In addition, we pieced together a database of 2,000 pieces of model legislation by getting data from sources, downloading data from advocacy organizations and searching for models ourselves. This was done either by identifying known models and trying to find the source or finding organizations that have pushed model bills and searching for each of the models for which they have advocated. We then compared the two data sets, which proved to be complicated. The team developed an algorithm that relied on natural language processing techniques to recognize similar words and phrases and compared each model in our database to the bills that lawmakers had introduced. These comparisons were powered by the equivalent of more than 150 computers, called virtual machines, that ran nonstop for months. Even with that computing power, we couldn't compare every model in its entirety against every bill. To cut computing time, we used keywords - guns, abortion, etc. The system only compared a model with a bill if they had at least one keyword in common.   The team then developed a matching process that led to the development of an updatable, public-facing tool that reporters and members of the public can use to identify not only past bills but future model bills as they are introduced, while the bills are still newsworthy. ","  It’s hard to overstate how resource-intensive this analysis was. This was our first foray into natural language processing. We had to compare one million bills — each several pages long, with some up to 100 pages in length — to each other. Computationally, scale bought with a lot of complexities. We had to go deep into understanding how to deploy some of the software we used at scale and solve the problems we faced along the way. We spent tens of thousands of dollars on cloud services. We had to re-run this analysis every time we made changes to our methodology — which we did often. The resulting analysis and reporting took more than six months to put together. We obtained metadata on more than 1 million pieces of legislation from all 50 states for the years 2010 through 2018 from a third-party vendor, Legiscan. We also scraped bill text associated with these bills from the websites of state legislatures.In addition, we pieced together a database of 2,000 pieces of model legislation by getting data from sources, downloading data from advocacy organizations and searching for models ourselves. This was done either by identifying known models and trying to find the source or finding organizations that have pushed model bills and searching for each of the models for which they have advocated. "," The power of collaboration.   CPI and USA TODAY/Arizona Republic built two analysis tools to identify model language, using two different approaches. USA TODAY's efforts found at least 10,000 bills almost entirely copied from model language that were introduced in legislatures nationwide over the last eight years. CPI’s tool worked to identify common language in approximately 60,000 bills nationwide to flag previously unknown model legislation.    Together the tools allowed for analysis of success from identified model bills and enabled identification of new model legislation.    The computer comparisons, along with on-the-ground reporting in more than a dozen states, revealed that copycat legislation amounts to the nation’s largest, unreported special-interest campaign. Model bills drive the agenda in states across the U.S. and influence almost every area of public policy.     ",https://www.usatoday.com/in-depth/news/investigations/2019/04/03/abortion-gun-laws-stand-your-ground-model-bills-conservatives-liberal-corporate-influence-lobbyists/3162173002/,https://publicintegrity.org/politics/state-politics/copy-paste-legislate/the-multistate-push-to-let-dealers-get-away-with-selling-you-a-defective-car,https://www.usatoday.com/in-depth/news/local/arizona/2019/06/19/abortion-laws-2019-how-heartbeat-bills-passed-ohio-missouri-more/1270870001/,https://web.model-legislation.apps.fivetwentyseven.com/,https://www.usatoday.com/in-depth/news/investigations/2019/07/17/islam-sharia-law-how-far-right-group-gets-model-bills-passed/1636199001/,https://www.usatoday.com/in-depth/news/investigations/2019/10/02/catholic-church-boy-scouts-fight-child-sex-abuse-statutes/2345778001/,https://publicintegrity.org/politics/state-politics/copy-paste-legislate/big-tobaccos-surprising-new-campaign-to-raise-the-smoking-age,This is a team entry.,"For USA Today/Arizona Republic: Natalie Allison, Chris Amico, Robert Barnes, Christian Baucom, Daniel Bice, Giacomo Bologna, Ben Botkin, David Boucher, Jon Campbell, Chris Davis, Amy DiPierro, Paul Egan, Tom Foster, Dustin Gardiner, Ronald J. Hansen, Tyler Hawkins, Greg Hilburn, Greg Holman, Joe Hong, Lisa Kaczke, John Kelly, Marisa Kwiatkowski, Keegan Kyle, Kaitlin Lange, Pamela Ren Larson, Aamer Madhani, Patrick Marley, Kelsey Mo, Dan Nowicki, Rob O’Dell, Geoff Pender, Nick Penzenstadler, Svetlana Peterlin, Agnel Philip, Justin Price, Nick Pugliese, Amy Pyle, Anne Ryman, Yvonne Wingett Sanchez, Jeff Schwaner, Chris Sikich, Michael Squires, Matt Wynn

For Center for Public Integrity: Jared Bennett, Kristian Hernandez, Sameea Kamal, Rui Kaneya, Mark Olalde, Pratheek Rebala, Peter Smith, Liz Essley Whyte",,,
United States,"USA TODAY, The Center for Public Integrity, The Arizona Republic",Big,Participant,Best news application,"Copy, Paste, Legislate",02/06/19,"Investigation,Multiple-newsroom collaboration,Politics","AI/Machine learning,Scraping,Microsoft Excel"," Copy, Paste, Legislate marks the first time a news organization detailed how deeply legislation enacted into law at the state level is influenced by special interests in a practice known as ""model legislation."" The series explained how model legislation was used by auto dealers to sell recalled used cars; by anti-abortion advocates to push further restrictions; by far-right groups to advocate for what some called government-sanctioned Islamophobia to moves by the Catholic Church to limit their exposure to past child abuse claims. (Published February 6, April 3, May 23, June 19, July 17 and October 2, 2019) "," People in various states called for legislation to require more transparency about the origin of bill language. Legislators found themselves compelled to defend their sponsorship of model bills.   A public-facing model legislation tracker tool launched in November 2019, allowed journalists and the public to:   --Identify recent model legislation introduced nationally   --Identify recent model legislation introduced in their state   --Perform a national search for model legislation mentioning specific keywords or topics   --Upload a document they have to instantly identify if any language in their document matches any state legislation introduced since 2010   --Look up a specific bill by number to see all other bills matching it   --Look up individual legislators and see all bills sponsored by them that contain model language   As part of the project, local newsrooms were able to identify and interview major sponsors of model legislation and identified key issues that resonated in their state. Those stories explored the reach of model legislation and its surprising impact on policies across the nation.   The combined national and local reporting revealed:   --More than 10,000 bills introduced in statehouses nationwide were almost entirely copied from bills written by special interests   --The largest block of special interest bills — more than 4,000 — were aimed at achieving conservative goals   --More than 2,100 of the bills were signed into law   --The model bills amount to the nation’s largest unreported special interest campaign, touching nearly every area of public policy   --Models were drafted with deceptive titles to disguise their true intent, including “transparency” bills that made it harder to sue corporations   --Because copycat bills have become so intertwined with the lawmaking process, the nation’s most prolific sponsor of model legislation claimed that he had no idea he had authored 72 bills originally written by outside interests "," No news organization had attempted to put a number on how many of the bills debated in statehouses are substantially copied from those pushed by special interests.   We obtained metadata on more than 1 million pieces of legislation from all 50 states for the years 2010 through 2018 from a third-party vendor, Legiscan. We also scraped bill text associated with these bills from the websites of state legislatures. In addition, we pieced together a database of 2,000 pieces of model legislation by getting data from sources, downloading data from advocacy organizations and searching for models ourselves. This was done either by identifying known models and trying to find the source or finding organizations that have pushed model bills and searching for each of the models for which they have advocated. We then compared the two data sets, which proved to be complicated. The team developed an algorithm that relied on natural language processing techniques to recognize similar words and phrases and compared each model in our database to the bills that lawmakers had introduced. These comparisons were powered by the equivalent of more than 150 computers, called virtual machines, that ran nonstop for months. Even with that computing power, we couldn't compare every model in its entirety against every bill. To cut computing time, we used keywords - guns, abortion, etc. The system only compared a model with a bill if they had at least one keyword in common.   The team then developed a matching process that led to the development of an updatable, public-facing tool that reporters and members of the public can use to identify not only past bills but future model bills as they are introduced, while the bills are still newsworthy. ","  It’s hard to overstate how resource-intensive this analysis was. This was our first foray into natural language processing. We had to compare one million bills — each several pages long, with some up to 100 pages in length — to each other. Computationally, scale bought with a lot of complexities. We had to go deep into understanding how to deploy some of the software we used at scale and solve the problems we faced along the way. We spent tens of thousands of dollars on cloud services. We had to re-run this analysis every time we made changes to our methodology — which we did often. The resulting analysis and reporting took more than six months to put together. We obtained metadata on more than 1 million pieces of legislation from all 50 states for the years 2010 through 2018 from a third-party vendor, Legiscan. We also scraped bill text associated with these bills from the websites of state legislatures.In addition, we pieced together a database of 2,000 pieces of model legislation by getting data from sources, downloading data from advocacy organizations and searching for models ourselves. This was done either by identifying known models and trying to find the source or finding organizations that have pushed model bills and searching for each of the models for which they have advocated. "," The power of collaboration.   CPI and USA TODAY/Arizona Republic built two analysis tools to identify model language, using two different approaches. USA TODAY's efforts found at least 10,000 bills almost entirely copied from model language that were introduced in legislatures nationwide over the last eight years. CPI’s tool worked to identify common language in approximately 60,000 bills nationwide to flag previously unknown model legislation.    Together the tools allowed for analysis of success from identified model bills and enabled identification of new model legislation.    The computer comparisons, along with on-the-ground reporting in more than a dozen states, revealed that copycat legislation amounts to the nation’s largest, unreported special-interest campaign. Model bills drive the agenda in states across the U.S. and influence almost every area of public policy.     ",https://web.model-legislation.apps.fivetwentyseven.com/,https://www.usatoday.com/in-depth/news/investigations/2019/04/03/abortion-gun-laws-stand-your-ground-model-bills-conservatives-liberal-corporate-influence-lobbyists/3162173002/,https://publicintegrity.org/politics/state-politics/copy-paste-legislate/the-multistate-push-to-let-dealers-get-away-with-selling-you-a-defective-car,https://www.usatoday.com/in-depth/news/local/arizona/2019/06/19/abortion-laws-2019-how-heartbeat-bills-passed-ohio-missouri-more/1270870001/,https://www.usatoday.com/in-depth/news/investigations/2019/07/17/islam-sharia-law-how-far-right-group-gets-model-bills-passed/1636199001/,https://www.usatoday.com/in-depth/news/investigations/2019/10/02/catholic-church-boy-scouts-fight-child-sex-abuse-statutes/2345778001/,https://publicintegrity.org/politics/state-politics/copy-paste-legislate/big-tobaccos-surprising-new-campaign-to-raise-the-smoking-age,This is a team entry.,"For USA Today/Arizona Republic: Natalie Allison, Chris Amico, Robert Barnes, Christian Baucom, Daniel Bice, Giacomo Bologna, Ben Botkin, David Boucher, Jon Campbell, Chris Davis, Amy DiPierro, Paul Egan, Tom Foster, Dustin Gardiner, Ronald J. Hansen, Tyler Hawkins, Greg Hilburn, Greg Holman, Joe Hong, Lisa Kaczke, John Kelly, Marisa Kwiatkowski, Keegan Kyle, Kaitlin Lange, Pamela Ren Larson, Aamer Madhani, Patrick Marley, Kelsey Mo, Dan Nowicki, Rob O’Dell, Geoff Pender, Nick Penzenstadler, Svetlana Peterlin, Agnel Philip, Justin Price, Nick Pugliese, Amy Pyle, Anne Ryman, Yvonne Wingett Sanchez, Jeff Schwaner, Chris Sikich, Michael Squires, Matt Wynn

For Center for Public Integrity: Jared Bennett, Kristian Hernandez, Sameea Kamal, Rui Kaneya, Mark Olalde, Pratheek Rebala, Peter Smith, Liz Essley Whyte",,,
United States,CalMatters,Small,Participant,Best data-driven reporting (small and large newsrooms),"Disaster Days: How megafires, guns and other 21st century crises are disrupting California schools",16/09/19,"Investigation,Explainer,Solutions journalism,Long-form,Database,News application,Illustration,Infographics,Chart,Video,Map,Politics,Environment,Gun violence","Scraping,QGIS,JQuery,Json,Microsoft Excel,Google Sheets,CSV,Python"," In a dark, disaster-driven version of old-fashioned snow days, California schools are sending kids home in record numbers, slashing weeks of instruction in growing swaths of the nation’s most populous state. The cause? Wildfire and other climate-fueled disasters, largely. But gun violence, the widening we gap and other modern emergencies also contribute. State law doesn’t require the lost class time to be replaced — a policy that is changing because of this series. The findings, unearthed by CalMatters education writer Ricardo Cano, 27, were buried in thousands of pieces of school closure data, which Cano analyzed and reported out. "," Armed with Cano’s data, California lawmakers have introduced legislation to create a “disaster relief summer school” program to recoup the burgeoning school time being lost to natural disasters in California. It’s difficult, Cano explains, because of the extensive local control school districts are guaranteed under the state constitution. The series also is informing debate over a statewide school bond measure, placed after the series on the March 2020 ballot, to address acute maintenance issues — and to correct inadequacies that were giving poor communities less access than rich ones to state school bond money. It is also underpinning legislation to shore up air filtration and electrical systems at schools to prevent closures for smoke pollution and public safety power shutoffs. And at a time in which a youth climate activism movement has captured the world’s attention, the reporting has galvanized a campaign by students and teachers to push education leaders and institutions to publicly acknowledge how climate change is harming the next generation in California.      "," We built an interactive app, map and several databases, including the comprehensive tool featured in the series. The main database was built using 17 years of school closure records obtained through the California Public Records Act from the California Department of Education that came in Excel format. Others included a database, compiled and built by Cano, of local school bond spending throughout California dating back to 1998; a Microsoft Access database of large-scale drops in attendance reported to the state over two decades; and real-time announcements by school districts statewide of closures for wildfire or related issues during the first three months of the 2019-2020 school year. Cano spent $79 on a subscription-based online software (<a href=""https://slack-redir.net/link?url=http%3A%2F%2FGridoc.com"">Gridoc.com</a>) that he used to join multiple datasets along the reporting and data-building process.  The source of the main data was attendance waivers California school districts submit to the state when they have to temporarily close down schools for emergencies. The information on the forms, which details key information such as which schools closed, why they closed, and how many instructional days they closed, is self-reported by districts. We primarily used Excel and Google Sheets to clean and build our main Disaster Days database. D’Agostino used PHP to write a script to reshape the state’s data and records we received so that we could begin building and analyzing emergency school closures at the school site level. D’Agostino and Al Elew used the Flask Python web framework – as well as an assortment of tools, including QGIS, JQuery and JSON – to build and house the interactive database and visualizations that readers could use to search our data. Part of the cleaning and building process also involved some webscraping.     "," To our knowledge, no news outlet prior to us had reported or analyzed this data. Climate change is a big story, and wildfires often break news, but the cumulative impact of climate-driven fires on core institutions such as schools and on kids over the long term is less well reported. CalMatters had to work on a shoestring; we are tiny. The whole project was done by one education reporter (working weekends and nights, while covering a beat), a summer data intern (who also lived at the office for months and stayed on for weeks, to finish), one developer and one line editor. The original data for the main database was riddled with typos, duplicate records and inconsistencies. Because the information was self-reported by schools over nearly two decades, and because California is massive, many unique school sites had multiple name configurations (example: Calaveras High, Calaveras High School, Calaveras HS) or had since changed school names altogether. The initial data came at the school district level (with a field that included a comma-separated list of school sites) and had no relational field we could use to join the databases we needed to build our database of school closures. D’Agostino reshaped the data using a PHP script so each row began to reflect a closure per school site. After several stages of cleaning we developed our own system of codes to create a relational field between the data we had on school closures and the 17 school years’ worth of enrollment data we assembled. After doing a direct join using Gridoc, we still had to manually pair enrollment data for more than 1,000 records. To help account for duplicates in the original state data, we paired every record with a “CDS Code,” a unique, 14-digit code designated to every California public school. "," We learned at least two lessons: Sophisticated, informative and groundbreaking reporting doesn't necessarily require a massive staff, and there's no replacement for on-the-ground, shoeleather reporting. Disaster Days stemmed from data, but it didn't stop there. As we built our database and analyzed the numbers, we found gripping, untold stories on the cumulative toll fires and disasters have taken on students, teachers and public schools. Cano and photographer Anne Wernikoff hit the road to report how schools have become stretched to the breaking point by disasters that are nothing like the quaint, traditional snow days. Thousands of California students have lost homes to fires in recent years, we found, and the annual loss of days and weeks of instruction have had significant academic costs for many schools. At a time when natural disasters worsened by climate change have affected millions of schoolchildren worldwide — including in developed nations like the U.S. — we believe our effort provides a framework for reporting and data analysis that could be replicated by other journalists to tell an increasingly important story of human impact. We also learned that there is value in creating accurate and reliable data and analysis that quantifies : The data we published spurred a state senator to propose a new policy framework that would allow schools to make up days lost to now-frequent fires and disasters. ",https://calmatters.org/projects/school-closures-california-wildfire-outage-flood-water-electricity-guns-snow-days-disaster/,https://calmatters.org/projects/california-school-closures-wildfire-middletown-paradise-disaster-days/,https://calmatters.org/projects/california-wildfires-school-closures-disaster-days-power-outages-blackouts-local-control/,https://calmatters.org/projects/california-school-bond-2020-ballot-small-rural-disaster-days/,https://disasterdays.calmatters.org/california-school-closures,https://www.youtube.com/watch?v=htnVDjgZA4o&feature=emb_logo,https://calmatters.org/projects/summer-school-wildfire-blackout-climate-disasters-california-bill/,"By Ricardo Cano. Database by Ricardo Cano, Mohamed Al Elew, John Osborn D'Agostino. Video/Images by Anne Wernikoff, Randall Benton. Edited by Shawn Hubler"," Ricardo Cano covers California education for CalMatters. Cano joined CalMatters in September 2018 from The Arizona Republic and azcentral.com, where he spent three years as the education reporter. Cano has been a finalist for three awards, a Livingston Award for Young Journalists honoring outstanding achievement by professionals under the age of 35, an Education Writers Association award for data journalism and two consecutive Arizona Press Club awards for public service journalism.   Mohamed Al Elew is a news developer and former Editor-in-Chief of The Triton, an independent non-profit student newspaper at UC San Diego. He was the summer data intern at CalMatters when he worked with Cano and Osborn D'Agostino on ""Disaster Days.""   John Osborn D’Agostino is an award-winning web developer, data journalist and game designer. He's worked with CalMatters, The Hechinger Report, and EdSource. He graduated from U.C. Berkeley's Graduate School of Journalism.   Anne Wernikoff is a photographer and multimedia journalist. She's been a multimedia producer at The Hill, an assistant photo editor at National Geographic and a photography intern at KQED in San Francisco. She recently graduated from the UC Berkeley Graduate School of Journalism where she focused on New Media storytelling.   Randall Benton is a prizewinning freelance photographer in Northern California, and former staff photographer at The Sacramento Bee.   Shawn Hubler has been a deputy editor at CalMatters since 2018. Before that, she served for four years on the editorial board of The Sacramento Bee, and spent 18 years as a staff writer and columnist at The Los Angeles Times, where she shared three staff Pulitzer Prizes. She has a PEN Center West Award for literary journalism, and many other awards. ",,,
India,"https://www.indiaspend.com/, https://www.nytimes.com/, https://in.reuters.com/, https://foreignpolicy.com/, https://scroll.in/, https://undark.org/,",Small,Participant,Best news application,Land Conflict Watch,30/10/19,"Investigation,Explainer,Solutions journalism,Long-form,Breaking news,Cross-border,Multiple-newsroom collaboration,Database,Open data,News application,Fact-checking,Map,Politics,Environment,Corruption,Business,Culture,Women,Agriculture,Economy,Human rights","JQuery,Json,OpenStreetMap,Node.js"," Land Conflict Watch (LCW) is the only comprehensive data-journalism project that digitally maps, collects and publishes data about ongoing land conflicts in India. India’s ambitious agenda for economic growth sets up contests between the industry, the State, the political class and the citizens over the use and ownership of land. LCW has a network of researchers and journalists spread across India, who collect quantitative and qualitative data linked to each conflict. The interactive database helps users including journalists uncover new stories or trends about the impact of land conflicts on the environment, economy, and people.      "," Land Conflict Watch’s website that has collected over 700 conflicts so far allows any user including journalists to find their own narratives based on their questions related to land and how it intersects with politics, economy, environment, gender, caste, public spaces, indigenous communities, health and education. The website has several filters that allows the user to broaden or narrow down their query so that they get what they are looking for and are also able to analyse the data. Journalists across the world have used this database to write stories of corruption, greed, political avarice, oppression, injustice as well as narratives of protests, movements and social resilience.    A data search on the website about the conflict cases in which communitys' consent was not taken led to an investigative story about how the Indian government and industries forcefully acquired forest land without the mandatory consent of indigenous communities. The story led to a question being raised in the Indian parliament and inquiry was ordered into the matter. Our data has informed debates at critical points such as when the Indian Supreme ordered to evict millions of indigenous people who didn't have land rights papers to live in the forests where they lived. Our data pointed to cases where the government wrongfully denied land rights papers to communities, which journalists used to write stories.    Civil society organisations such as Housing and Land Rights Network use our data in their reports about forced evictions.    The stories that used our data have appeared in international and national media outlets such as  The New York Times ,  Foreign Policy , Reuters, Bloomberg-Quint, IndiaSpend,  Business Standard ,  Hindustan Times , Scroll.in, Newsclick.in, and the Wire. Each story has helped broaden society's understanding of land politics.                 "," <a href=""http://landconflictwatch.org/"" target=""_blank"">landconflictwatch.org</a> leverages data visualizations to present the data in new ways to help users discover insights. The website is built with Webflow CMS and hosted on the Amazon Web Services. The home page show all conflicts mapped across the country, built using Mapbox.   LCW uses Airtable as its database. At rest, the data is encrypted using AES-256. In transit, the data is protected using 256-bit TLS encryption. The database is hosted and backed-up on the AWS hosting infrastructure.   The annual report is presented as a dashboard built with Google Charts to power the real-time charts. "," Although we started collecting data in late 2016 which was put up on a basic website, the user friendly website was launched in October 2019. The most challenging part was to define and segregate various data points about the complex phenomenon that land conflicts are. The goal was to go as deeper as possible in each data point and connect them to macro level land questions so that the users can find what they are looking for and can also analyse it on the website. For example, if a user is looking for mining conflicts in eastern India, she can find their geolocation, when did they begin, which kind of mining is involved, how much land, what kind of land, how many people, what type of community, which government department, which private company, what laws are involved or were violated, whether or not the case is in court, which kind of human rights violations have happened, and a short description of the conflict. Codifying and standardising these intersecting data points made the tech side of things challenging.   Land conflicts have become endemic in land scarce India, and have major implications for the social, political and economic stability of the country. Despite these high stakes the discourse on land conflicts was completely uninformed by data and its analysis, which then hindered workable solutions to resolve those conflicts. Land Conflict Watch identified and filled that gap. But that also meant collecting our own data and evidence. That was also a challenging task given the issues of access because the government authorities in various parts of India do not want the truth to come out. "," The depth and the vastness of the evidence-backed data present on Land Conflict Watch website makes it a unique journalism enterprise that fills a major gap in the understanding of why land conflicts happen, how they happen, what do they mean for the wellbeing of a country, and how they can be resolved.   Land Conflict Watch serves as an example and a resource of how to create a data based journalistic product about a complex social problem. A database that is constantly informing the discourse around that social problem. Others can also create similar databases or build on this database to contribute in informing a social debate that helps people understand why the world around them works the way it works. ",https://www.landconflictwatch.org/,https://www.indiaspend.com/government-industries-nationwide-dodge-law-take-over-forest-land-without-consent-of-tribal-communities/,https://www.indiaspend.com/as-supreme-court-stays-eviction-of-9-5-million-forest-dwellers-heres-how-states-illegally-rejected-land-claims/,https://www.indiaspend.com/dalit-battles-for-promised-lands-rage-across-india/,https://undark.org/2020/01/06/india-indigenous-trees/,https://www.hlrn.org.in/documents/Forced_Evictions_2018.pdf,https://foreignpolicy.com/2019/03/17/modis-bullet-train-dreams-are-hitting-rural-roadblocks/,"Ankur Paliwal, Kumar Sambhav Shrivastava"," Ankur Paliwal:    Ankur is an independent journalist who mostly writes stories at the intersection of science and social justice. He focuses on the stories of underreported people and places. He has reprorted from India, Ethiopia, Ghana, Tanzania, Kenya, Germany and the United States for  Scientific American ,  Undark ,  Nautilus ,  GQ , PBS and several Indian news publications. He is a co-founder of of Land Conflict Watch, a unique, research-based data journalism project that maps and documents ongoing land conflicts across India. Ankur won the Next Generation of Science Journalists Award 2016 at the World Health Summit in Berlin. He currently lives in New Delhi.   Kumar Sambhav Shrivastava:   A journalist for over a decade, Kumar Sambhav Shrivastava writes on human rights and political economy of India. He is a co-founder of Land Conflict Watch, a unique, research-based data journalism project that maps and documents ongoing land conflicts across India. Sambhav has been awarded the National Award for Excellence in Journalism from the Press Council of India, the Young Journalist from the Developing World Award from the Foreign Press Association of London, the Global Shining Light Award from the Global Investigative Journalism Network and the Asian College of Journalism Award for Investigative Journalism. He lives in New Delhi.  <h1> </h1>",,,
United States,"USA TODAY, the Arizona Republic, and the Center for Public Integrity.",Big,Participant,Innovation (small and large newsrooms),"Copy, paste, legislate: We looked for legislation that was written by special interests. We found it in all 50 states.",04/03/19,"Investigation,Explainer,Multiple-newsroom collaboration,Database,Open data,Fact-checking,Illustration,Infographics,Chart,Video,Politics,Corruption,Business","Animation,Scraping,D3.js,Three.js,Canvas,Json,Adobe,Creative Suite,CSV,R,RStudio"," This story was produced as part of a collaboration between USA TODAY, The Arizona Republic and the Center for Public Integrity. More than 30 reporters across the country were involved in the two-year investigation, which identified copycat bills in every state. The team used a unique data-analysis engine built on hundreds of cloud computers to compare millions of words of legislation provided by LegiScan. "," The impact: Fewer model bills, and the special interests behind them, will go undetected.   The phenomenon of copycat legislation is so pervasive that it is difficult to imagine it being curtailed without years of reforms across many states. The impact of our reporting has been felt, however, in dozens of communities, where our stories revealed the motivations behind many model bills and empowered citizens with the knowledge of who is behind them.       We interviewed more than 40 state lawmakers across the country about bills they sponsored. Many said they didn’t know they had introduced a bill written by a corporation or didn’t understand how bill language was crafted to help the company.   One lawmaker in Pennsylvania sponsored more than 70 copycat bills -- without recognizing, he said, the source of the bill in many instances. “I had no way of knowing…” Rep. Thomas Murt said.   The effort continues. As part of the project, we built two tools that give the public access to information on copycat bills in ways that didn’t previously exist.    Our public facing tool -- launched in November and the product of months of development work -- allows online readers to explore tens of thousands of possible model bills by state or keyword. We update it weekly with newly filed model legislation, making this tool a nearly real-time resource.   Our internal tool allows Gannett reporters to explore the copycat bills we’ve confirmed and report on the lawmakers in their local communities who sponsored them. So far, we have trained more than 100 reporters on the tool. "," How do you find 10,000 needles in 50 haystacks?   That, in effect, is what journalists and developers with  USA TODAY  and  The Arizona Republic  set out to do two years ago: Identify among the roughly 100,000 bills introduced in the 50 states each year what’s been copied from drafts pushed by special interests.   Here’s how we did it.   Using data provided by LegiScan, which tracks every proposed law introduced in the U.S., we pulled in digital copies of nearly 1 million pieces of legislation introduced between 2010 and Oct. 15, 2018. The data included a limited number of bills from 2008 and 2009.   We then asked a dozen reporters covering state legislatures for  USA TODAY  Network newsrooms across the nation to build a list of model bills by searching special-interest groups’ websites, scouring news coverage and interviewing lobbyists and lawmakers. We identified more than 2,100 models, a list that is far from complete because many groups don’t make their models public.   We then used a computer algorithm designed to recognize similar words and phrases and compared each model in our database to the bills that lawmakers had introduced.   These comparisons were powered by the equivalent of more than 150 computers, called virtual machines, that ran nonstop for months.     "," Our scoring system is based on three factors: the longest string of common text between a model and a bill; the number of common strings of five or more words; and the number of common strings of 10 or more words.   Based on those factors, bills received scores on a 100-point scale. The closer to 100, the more likely a bill was copied from model legislation.   For its analysis,  USA TODAY / Arizona Republic  used only bills that scored 80 or higher. At that level, substantial amounts of text have been duplicated.   Another estimated 10,000 bills below the 80-point threshold were likely copied from model legislation but matched less of the model’s text. Out of caution,  USA TODAY / Arizona Republic  cited in its investigation only bills with substantial portions copied from a model. In addition, if legislators copied an idea but not the precise language, a bill would not be flagged.   Joe Walsh, a former data scientist at the University of Chicago, used what’s known as the Smith-Waterman algorithm to create the Legislative Influence Detector, which also finds similarities between model legislation and bills. His system has been used by reporters around the country to find model bills.   Walsh reviewed  USA TODAY / Arizona Republic’s  investigation and findings and applauded its scoring system for showing when a bill has been substantially copied from model legislation.   “It’s really clear, the numbers are nice and round, and it’s easy to show and explain,” Walsh said. “I wish that we were able to do some of this stuff. I am glad someone is.” ","  Can I examine the results?     USA TODAY / Arizona Republic  continues to search legislation and compare it with known model bills from around the country, furthering its investigation of outside influences on state lawmakers.   Initially, the system is being rolled out to USA TODAY Network journalists for use in reporting on state legislatures.    How were bills categorized?    Special-interest groups, both liberal and conservative, have for years crafted and lobbied for model bills. Generally, the organizations that craft the bills have a clear mission or ideological bent. The American Legislative Exchange Council, the best-known and one of the most prolific model-bill factories, supports conservative ideas and efforts. The State Innovation Exchange, once known as ALICE, is in effect ALEC’s liberal counterpart. We classified bills based on the mission or ideological orientation of the organizations that created each model. In some cases, groups with a conservative bent also push bills that benefit industry. We labeled each bill according to the most dominant characteristic.This story was produced as part of a collaboration between USA TODAY, The Arizona Republic and the Center for Public Integrity. More than 30 reporters across the country were involved in the two-year investigation, which identified copycat bills in every state. The team used a unique data-analysis engine built on hundreds of cloud computers to compare millions of words of legislation provided by LegiScan. ",https://www.usatoday.com/pages/interactives/asbestos-sharia-law-model-bills-lobbyists-special-interests-influence-state-laws/,https://www.usatoday.com/in-depth/news/investigations/2019/04/03/abortion-gun-laws-stand-your-ground-model-bills-conservatives-liberal-corporate-influence-lobbyists/3162173002/,https://www.usatoday.com/videos/news/investigations/2019/02/06/buying-our-statehouses-how-copy-cat-bills-become-your-laws/2791495002/,https://www.usatoday.com/in-depth/news/investigations/2019/04/03/abortion-gun-laws-stand-your-ground-model-bills-conservatives-liberal-corporate-influence-lobbyists/3162173002/,https://www.usatoday.com/in-depth/news/local/arizona/2019/06/19/abortion-laws-2019-how-heartbeat-bills-passed-ohio-missouri-more/1270870001/,https://www.usatoday.com/in-depth/news/investigations/2019/04/03/takata-airbag-gm-ignition-switch-recalls-used-car-dealers-sue-deaths-crashes-honda/3162202002/,https://publicintegrity.org/politics/state-politics/copy-paste-legislate/how-we-uncovered-10000-times-lawmakers-introduced-copycat-model-bills-and-why-it-matters/,"Introductory graphic: Veronica Bravo, Ramon Padilla, Mitchell Thorson, Pim Linders, Rob O'Dell, Michael Squires, Josh Susong, Shawn Sullivan","    Full project credits:     REPORTING AND ANALYSIS:  Natalie Allison, Chris Amico, Daniel Bice, Giacomo Bologna, Ben Botkin, David Boucher, Jon Campbell, Amy DiPierro, Paul Egan, Dustin Gardiner, Ronald J. Hansen, Greg Hilburn, Greg Holman, Joe Hong, Lisa Kaczke, Keegan Kyle, Kaitlin Lange, Pamela Ren Larson, Aamer Madhani, Patrick Marley, Kelsey Mo, Dan Nowicki, Rob O’Dell, Geoff Pender, Nick Penzenstadler, Agnel Philip, Justin Price, Nick Pugliese, Yvonne Wingett Sanchez, Jeff Schwaner, Chris Sikich, Michael Squires, Matt Wynn    FROM THE CENTER FOR PUBLIC INTEGRITY:  Jared Bennett, Kristian Hernandez, Sameea Kamal, Rui Kaneya, Mark Olalde, Pratheek Rebala, Peter Smith, Liz Essley Whyte    EDITING:  Chris Davis, John Kelly, Amy Pyle, Michael Squires, Kytja Weir  (CPI),  Gordon Witkin  (CPI)     GRAPHICS AND ILLUSTRATIONS:  Andrea Brunty, Veronica Bravo, Lauren Lapid, Pim Linders, Ramon Padilla, Jim Sergent, Shawn Sullivan, Mitchell Thorson    PHOTOGRAPHY AND VIDEOGRAPHY:  Patrick Breen, Chris Powers, Pat Shanahan    DIGITAL PRODUCTION AND DEVELOPMENT:  Robert Barnes, Christian Baucom, Andrea Brunty, Tom Foster, Tyler Hawkins, Spencer Holladay, Ryan Marx, Annette Meade, Josh Miller, Michael Varano, Stan Wilson    SOCIAL MEDIA, ENGAGEMENT AND PROMOTION:  Mary Bowerman, P. Kim Bui, Anne Godlasky, Danielle Woodward   ",,,
United States,USA TODAY,Big,Participant,Best visualization (small and large newsrooms),Slavery's explosive growth: How '20 and odd' became millions,21/08/19,"Explainer,Multiple-newsroom collaboration,Database,Open data,Illustration,Infographics,Chart,Video,Map","Animation,D3.js,Three.js,Canvas,Adobe,Creative Suite,R"," The challenge before the graphics desk was to research and communicate the scale of those directly affected by slavery in the lands that would become the United States. Learn more about this project and its many contributors here: <a class=""Hyperlink SCXW66236507 BCX2"" href=""https://www.usatoday.com/in-depth/opinion/2019/08/21/slavery-america-behind-usa-todays-1619-series-black-history/2032393001/"" rel=""noreferrer noopener"" style=""text-decoration: none; color: inherit;"" target=""_blank""> https://www.usatoday.com/in-depth/opinion/2019/08/21/slavery-america-behind-usa-todays-1619-series-black-history/2032393001/ </a>     ","   Nichelle Smith, an investigations team editor at USA TODAY, recalls attending a lecture at the Library of Congress in early 2018 where she listened to scholars discuss the landing 400 years ago of enslaved Africans at the British colony of Virginia. A name soon caught her attention: ""Angela,"" among the first Africans brought to Virginia in 1619. Angela survived the first leg from Angola on a slave ship, was taken hostage by British pirates and eventually sold to the commander of Jamestown Island. Her age and the date of her death remain unknown.  Smith, along with Deborah Barfield Berry, Kelley Benham French, Rick Hampson and Jarrad Henderson, spent months meticulously reporting and preparing our ambitious series. They were supported by dozens of USA TODAY colleagues – writers, editors, designers, producers, developers, visual specialists – who produced 1619: Searching for Answers, remembering the first enslaved Africans to be brought to the English-speaking colonies that became America.   "," We built on innovations from our previous work, combining data analysis, vector animation, JavaScript visualization, and illustration to convey the explosive growth. Our interrn Shuai, working with as seasoned data reporter, compiled detailed records of slavery in the Americas, converting it into succinct visulaizations. Animator Ramon Padilla and illustrator Sheldon Sneed collaboated to create a datavizulazation that transforms into a powerful illustration, making an attempt to convey the vast scale of the impact of slave trade on our nation. "," The 1619 series, <a href=""https://www.usatoday.com/in-depth/news/nation/2019/08/21/wanda-tucker-angola-slavery-1619-history-america-black-family/2016591001/"" target=""_blank"">1619.usatoday.com</a>, overseen by Managing Editor Kristen Go,   is an exhaustively researched examination of the journey, the protagonists who defined that moment of history and the pain and repercussions that continue today.   Our journalists traveled from Virginia to Angola and beyond to produce a vivid, multi-part series that includes the Tucker family’s quest to connect with its past.   Wanda Tucker, who traces her family roots to the 1800s in Virginia, has been trying to connect her family history back to   William – the first   recorded   African baby baptized in Virginia, a child born to Anthony and Isabella, survivors of the White Lion ,  a privateer that anchored at Point Comfort, where its captain traded human beings for supplies.   We traveled with Wanda Tucker to Angola; USA TODAY underwrote her journey so we could be alongside her to document her quest, starting at the port city of Luanda and moving deep into the interior to the point of origin of the Portuguese trade, the historic Ndongo Kingdom. Portuguese slave ships, one even named San Juan Bautista   – Saint John the Baptist – would sail from Luanda with innocents below deck, never to see their homeland again.   The historical research was extessive, but the concept itself was the most difficult to acheive. It took multiple iterations and ideas from across the team to get it right.     "," Pairing reporters of various diciplines and skill levels with visual experts yeilds excellent excellent results, especially when paired with an ambisitous goal.   “I have found myself searching for my own history,” said USA TODAY reporter Berry, who traveled to Angola. “I, too, have done the same. My grandmother’s last name is Tucker, and she was from Virginia.”   Berry recently learned of her own ancestry results while reporting from Angola. She'll tell her compelling family story in coming installments.   ""The landing of the first enslaved Africans in 1619 is one of the most important events and dates in our history, but it hasn't been treated as such,"" said USA TODAY Editor-in-Chief Nicole Carroll. ""We set out to correct that. Our goal is to educate and inform Americans about the history that continues to shape and influence the country we are today.”   Carroll also used the project as an opportunity to further educate our newsroom.   Carroll and Smith led a tour of USA TODAY colleagues to the <a href=""https://nmaahc.si.edu/"" target=""_blank"">Smithsonian National Museum of African American History & Culture</a>, a true public trust. ",https://www.usatoday.com/pages/interactives/1619-african-slavery-history-maps-routes-interactive-graphic/,,,,,,,"George Petras, Ramon Padilla, Shuai Hao, Sheldon Sneed, Jim Sergent, Shawn Sullivan, Mitchell Thorson, Javier Zarracina and Mark Nichols, USA TODAY","  <h2>Meet the team behind this project </h2>     Reporting and research:  Deborah Barfield Berry, Nichelle Smith, Rick Hampson, Kelley Benham French, Nicquel Terry Ellis     Editing:  Kristen Go, Kelley Benham French     Photography and videography:  Jarrad Henderson      Visual editing:  Christopher Powers, Andrew P. Scott     Graphics and illustrations:  George Petras, Ramon Padilla, Shuai Hao, Sheldon Sneed, Jim Sergent, Shawn Sullivan, Mitchell Thorson, Javier Zarracina and Mark Nichols.    Digital production and development:  Andrea Brunty, Spencer Holladay, Craig Johnson, Ryan Marx, Annette Meade, Mike Varano, Stan Wilson     Copy editing:  Robert Abitbol, Michael B. Smith, Lauren Olsen     Social media, engagement and promotion:  Julie Burton, Dana Mitchell, Alex Ptachick, Cara Richardson, Elizabeth Shell      Communications:  Chrissy Terrell, Hayley Hoefer, Stephanie Tackach     Augmented reality:  Ray Soto, Will Austin, Alan Davies, Alex Daley-Montgomery, Mykal McEldowney (studio recording)  ",,,
United States,The Arizona Republica and USA TODAY,Big,Participant,Best data-driven reporting (small and large newsrooms),Ahead of the fire,22/07/19,"Investigation,Explainer,Multiple-newsroom collaboration,Database,Open data,Illustration,Infographics,Chart,Video,Map,Satellite images","Animation,Scraping,D3.js,Three.js,QGIS,Canvas,Json,Adobe,Creative Suite,CSV,R,RStudio,PostgreSQL"," After last year's deadly wildfire in Paradise, California, The Arizona Republic set out to find the places in Western wildfire country that face similar risks.   Working with partner newsrooms in the USA TODAY Network, we found many threatened communities are in our back yards, our vacation spots, and are home to friends and relatives. Understanding the risks across 11 western states is a national story, but in our newsrooms it is a local story, too.         "," The disaster of the Camp Fire was caused both by fire hazard and human risk factors: who could escape, who could survive. Those hazards and factors are present in hundreds of small communities across the West.   A lookup tool published with the project <a href=""https://www.azcentral.com/storytelling/wildfires-risks-map-california-arizona-oregon/"" target=""_blank"">allows you to see each community's risk factors</a> in relative to all communities in the West.      "," Evacuation constraints were calculated by dividing the number of households by the number of major roads that exit a community.   Larson's analysis also estimated the share of residents who live in mobile home parks using Department of Homeland Security infrastructure data. Fire spreads exceptionally fast through mobile home parks, which typically have limited evacuation routes.     Mitchell Thorson, Ryan Marx, Ramon Padilla and Shawn Sullivan of USA TODAY analyzed and visualized the data for display.    As this report shows, when the fire arrives, the risks will be human risks, and preparation might be the difference between a close call and becoming the next Paradise.  "," Investigative reporter Dennis Wagner, who has covered wildfires in the West for more than 35 years, including the Dude, Rodeo-Chedeski, Wallow and Yarnell Hill fires, reported on the risk factors. For this project, interviewed scientists, forestry officials and community leaders.    Visual journalist Thomas Hawthorne, who covered the aftermath of the Camp Fire, traveled with reporting teams and led the process of developing and editing video stories on each community.    Reporters and visual journalists from across the West joined the effort, identifying communities in their states with high hazard scores and other risk factors. The Desert Sun in Palm Springs, the Redding Record Searchlight, the Coloradoan in Fort Collins, the Great Falls Tribune in Montana, the Salem Statesman Journal in Oregon and the Kitsap Sun in Washington sent teams to interview residents and officials and photograph their communities from the ground and air.    In all, nine reporters and seven visual journalists across eight states found communities working to reduce their risks of wildfire even as they confront the inevitable: Big fires will happen. "," Reporting for this project began with data reporter Pamela Ren Larson's unprecedented analysis of Western communities at risk for wildfire, including how a disaster in those communities could be compounded by residents’ age and disability, limited road networks, and housing type.   The analysis began with the U.S. Forest Service’s Wildfire Hazard Potential (WHP), which assigns a score to every 18-acre parcel of land in the country. The higher the score, the higher the probability the place will experience a catastrophic wildfire.   That data was paired with more than 5,000 Census-designated places, the most comprehensive information on where people live in the United States. WHP scores were averaged for each community, plus the area one mile beyond their boundaries.   The analysis focuses on communities of fewer than 15,000 households. Large cities’ wildfire potential is most accurately determined by neighborhood, like a canyon community, than the broader metropolitan area. For example, even in a city like Los Angeles, where wildfire is a threat, most of the population lives in areas with low wildfire potential.   Wildfire hazard was combined with the other characteristics that make a community vulnerable to a disaster: percentage of residents who are disabled or elderly; percentage of residents who don't speak English well; whether a community is authorized to broadcast alerts to all cellphones in range; and residents’ ability to evacuate in a disaster. ",https://www.azcentral.com/in-depth/news/local/arizona-wildfires/2019/07/22/wildfire-risks-more-than-500-spots-have-greater-hazard-than-paradise/1434502001/,https://www.azcentral.com/storytelling/wildfires-risks-map-california-arizona-oregon/,https://www.azcentral.com/in-depth/news/local/arizona-wildfires/2019/07/22/wildfire-risks-ahead-of-the-fire-about-this-report/1784203001/,https://www.azcentral.com/in-depth/news/local/arizona-wildfires/2019/07/22/wildfire-risk-pine-arizona/1427448001/,https://www.azcentral.com/in-depth/news/2019/07/22/wildfire-risk-cascade-chipita-park-green-mountain-falls/1460432001/,https://www.azcentral.com/in-depth/news/2019/07/22/wildfire-risk-merlin-oregon/1449975001/,https://www.azcentral.com/in-depth/news/2019/07/22/wildfire-risk-riggins-idaho/1467623001/,"Mitchell Thorson, Ryan Marx, Ramon Padilla, Veronica Bravo, Karl Gelles, Shawn Sullivan","  <h2>Contributing to this report</h2>     Reporting : Pamela Ren Larson, Dennis Wagner, Jacy Marmaduke, Zach Urness, Anna Reed, Chris Henry, Sam Metz, Damon Arthur, David Murray    Photo and video:  Thomas Hawthorne, Timothy Hurst, Kelly Jordan, Anna Reed, Jay Calderon, Omar Ornelas, Mike Chapman, Rion Sanders, Maghen Moore    Data analysis and graphics:  Mitchell Thorson, Ryan Marx, Ramon Padilla, Veronica Bravo, Karl Gelles, Shawn Sullivan    Digital design:  Spencer Holladay,  Andrea Brunty, Josh Miller    Social media:  Danielle Woodward, P. Kim Bui, Jennifer Hefty, Mary Bowerman    Digital development:   Stan Wilson, Ryan Marx, Craig Johnson, Evan Sundwick    Project manager:  Annette N. Meade    Copy editing:  Becca Dyer, John Paul McDonnall, Kelsey Mo, Robert Abitbol,     Editing:  Michael Squires, Josh Susong    Executive editor:  Greg Burton ",,,
Egypt,Alhadaqa,Small,Shortlist,Best visualization (small and large newsrooms),The Unwelcomed,20/08/19,"Investigation,Explainer,Illustration,Chart,Map,Immigration,Human rights","D3.js,Adobe,Creative Suite,Microsoft Excel,CSV,R,RStudio"," Migrants & Refugees are dying crossing borders. In a time period does not exceed 6 years, between 2014 and 2019, the International Organization for Migration (IOM) has recorded 5,506 death or missing incidents of migrants and refugees across borders. I tried to visualize those incidents twinning time and space at which they were happened in an interactive dashboard and throughout a short simple story. "," Awarded the Pudding Cup of best data driven stories of year 2019. Featured by big names like Alberto Cairo who talked about it in his personal blog and promoted it among his network. From its publishing data (about 4 months ago) till now It has around 6000 views from around the globe. Too many positive feedbacks received talking about how effective is the story and how beautifully it was designed and created. Lastly, a special episode on Dataviztoday Podcast was produced talking about the project, the process of its creation, and how using 2 charts together can make a great impact. "," I used mainly R for data preparation, analysis and finding the main insights. Then the project was totally programmed and coded using html, css, javascript, d3js. For the illustrations I used Adobe illustrator  "," The hardest part is the creation of the narrative (the fictional symbolic short story), where i had to go through many true stories that were talking about different stories happened in different locations then concluded all those stories in one symbolic one that may touch every story happened before. Also, the formulation of the main final message that they are wishing to communicate to everybody around the world and specifically to the decision makers. "," Decide on your main objective and your main message, then make every single decision in service of this main objective. ",http://alhadaqa.com/2019/08/the_unwelcomed/,http://alhadaqa.com/ar/2019/08/the_unwelcomed_ar/,https://dataviztoday.com/shownotes/48,http://www.thefunctionalart.com/2019/10/the-unwelcomed-mapping-ongoing-tragedy.html?m=1,https://pudding.cool/process/pudding-cup-2019/,https://www.centerforglobaldata.org/viz-challenge,,Mohamad Waked," I’m a Mechanical Design Engineer who turned into a Data Scientist, then into a Data Visualization Designer.   I have an extreme passion towards analyzing data, exploring its dark sides, examining it from different angles, breaking it down till roots, understanding the connections between its small dots, discovering and uncovering its deep secrets, then summarizing those findings in a graph that is pleasing to its viewer. I founded ALHADAQA which is my personal DataViz Lab, and there I am expressing part of my passion towards data exploration and data visualization.   @mohamad_waked ",,,
United States,Gizmodo,Small,Participant,Best data-driven reporting (small and large newsrooms),Goodbye Big Five,22/01/19,"Investigation,Explainer,Long-form,Database,Culture","Personalisation,Scraping,Json,CSV,PostgreSQL,Python,Node.js"," Reporters Kashmir Hill and Dhruv Mehrotra spent six weeks blocking Amazon, Facebook, Google, Microsoft, and Apple from getting their money, data, and attention, using a custom-built VPN. Here’s what happened. "," The series recieved over one million reads and provided a striking look into how these companies control internet infrastructure, online commerce, and information flows. The piece furthered the existing discourse about technology monopolies and just how integrated they are in our lives.   "," In our initial analysis, we wanted a basic understanding of how much data was flowing to a tech giant given a specific behavior, so Mehrotra built network monitoring software to independently conduct experiments to gather data. For instance, when Kash wanted to go on a run, she would direct the software to begin monitoring her network traffic and then assign the data-capture a label. This is how Gizmodo linked network activity to specific behavior. The software then used WHOIS lookups to categorize where each packet was headed.   The next step was to actually block outgoing traffic to each tech giant. To do that we first needed to identify the various IP networks that each company operates. Internet infrastructure relies on a certain level of transparency in order for data to be routed appropriately through the multitudes of networks that comprise it. As such, we were able to utilize the <a href=""https://github.com/dmehrotra/ThwartingTechGiants/blob/master/AS.csv"">public Autonomous System Numbers of the various tech giants to identify their IP networks</a>.   Armed with a means to categorize IP addresses,we crafted firewall rules on our VPN to drop packets associated with the five tech giants. A firewall rule specifies criteria for how your computer should handle internet packets. For example, if your VPN spots data traveling to 75.126.39.35 on port  5222,  our packet filter would recognize it as traffic to WhatsApp, a Facebook-owned company, and drop it.  "," When designing our system, we did not consider how the prevalence of content delivery networks, or CDNs, would affect our blockade. Many websites and apps are not actually sent to your browser directly from their hosting provider. Instead, often times there is a middle-man, a CDN, that acts as a buffer between your browser and the company’s servers.   The reason for this is speed and security. A CDN will store versions of a company’s content in multiple geographical locations in order to deliver it to the end user faster. If you think of the internet as a bunch of wires, instead of as a kind of omnipotent cloud-like thing, the reason for this is quite obvious: the closer you are to your content, physically, the faster you will get it.   For our purposes, what this meant was that when we were blocking the companies that do web hosting as a service—such as Amazon, Google, and Microsoft—websites they host would evade our firewall if they used a CDN from a third party because it didn’t look like the website was being sent from a tech giant. That’s how Airbnb and Gizmodo itself, which are both hosted by AWS, <a href=""https://gizmodo.com/i-tried-to-block-amazon-from-my-life-it-was-impossible-1830565336"">broke through our Amazon blockade</a>. "," All of our code is open source, and we wrote a tutorial so others can also experience a tech-giant free life.  ",https://gizmodo.com/c/goodbye-big-five,https://gizmodo.com/want-to-really-block-the-tech-giants-heres-how-1832261612,https://gizmodo.com/i-tried-to-block-amazon-from-my-life-it-was-impossible-1830565336,,,,,Dhruv Mehrotra and Kashmir Hill," Dhruv Mehrotra in an Investigative Data Reporter at Gizmodo.  His work has been supported by Eyebeam, The ACLU, and NYU's Courant Institute of Applied Mathematics.     Kashmir Hill is a technology reporter at The  New York Times.   ",,,
United States,The Wall Street Journal,Big,Participant,Best visualization (small and large newsrooms),The National Debt: Visualized,08/03/19,"Explainer,Database,Illustration,Infographics,Chart,Politics,Economy","Scraping,D3.js,JQuery,Json,Adobe,CSV,Node.js", Breaking down the U.S. National Debt into digestible and quantifiable ways. , Was discussed in the U.S. congress. Several congressman tweeted it out to their constituents , I used a scraper to grab the daily number from the Treasury department and calculated it down to the closest second. Talked with the Treasury about the breakdown and visualized it using 'scrollytelling' , Working with the Treasury to have daily access to the latest number and making sure that traffic from our site wasn't pinging their site too often causing it to crash. Why should it be selected? Because it gives viewers real time up-to-the-second access to just how much debt the U.S. owes and by showing just how massive this number is and what it means going forward for the country. , How big the U.S. debt truly is. Who owns all of it. How many other things the debt can buy. ,https://www.wsj.com/graphics/us-national-debt-visualized/,,,,,,,Brian McGill," Brian McGill is the multimedia editor in The Wall Street Journal's Washington Bureau. McGill works as part of WSJ's national team focused on politics and economics.    Prior to joining WSJ, McGill worked as a news artist for National Journal, The Tampa Tribune and the Springfield (Mo.) News-Leader. He received his degree from the Missouri School of Journalism at the University of Missouri. ",,,
Switzerland,SWI swissinfo.ch,Small,Participant,Best data-driven reporting (small and large newsrooms),Women artists struggle for visibility in Swiss museums,06/07/19,"Investigation,Infographics,Arts,Culture,Women","Google Sheets,CSV,R"," Just a quarter of Swiss museum exhibitions over the past decade have featured women artists, according to our research. We contacted 125 Swiss art museums by e-mail to ask them if they could send us a list of all the temporary, individual and collective exhibitions organised between 2008 and 2018, by gender. We received data from 73 museums, and for seven others, we were able to get the data directly on their website. ", It triggered a national debate on the representation of women in the art world and calls for mandatory quotas.  ," Here is a link to the data on the procedure, definitions, method and data analysis: koa87.github.io/2019-06-museums/?_sm_au_=iVVfSSNQN5q6vnFB    The preprocessing and analysis of the data was conducted in the R project for statistical computing. The RMarkdown script used to generate this document and all the resulting data can be downloaded under this link: koa87.github.io/2019-06-museums/rscript.zip .   Through executing main.Rmd, the herein described process can be reproduced and this document can be generated.  "," Definitions: museums resisted being compared to other museums.    Queer artists: Since the main topic of our research is the visibility of female artists, as compared to male, we decided to adopt a binary gender definition when asking the museums for their data. However, not everybody identifies with one of these genders. Thus, when sending us the requested information, some of the museums pointed out that in the same period they exhibited queer artists as well. These artists were not included in the final numbers.  ", Data is everywhere and can shed light on areas where institutions don't want to provide answers. ,https://www.swissinfo.ch/eng/female-artists-switzerland-exhibitions/45013956,,,,,,,"Alexandra Kohler, Céline Stegmüller"," Céline just graduated from the Académie du journalisme et des médias (AJM) at the University of Neuchâtel but has been filming, writing and interviewing people all over Switzerland long before that. She joined swissinfo.ch in 2018 as videojournalist for Nouvo in English.   Alexandra completed a Bachelor's degree in Media and Communication Studies and English Literature in Fribourg, Switzerland, followed by a Master's degree in World Politics and Society in Lucerne. After seven years at the NZZ as news editor and data journalist, she joined Swissinfo in 2019. Here she creates data and visually-driven stories.  ",,,
United Kingdom,The Courier,Small,Participant,Best data-driven reporting (small and large newsrooms),ScotRail apology tracker,18/12/19,"Breaking news,Illustration,Chart,Politics,Business","Scraping,CSV"," The story used data scraping, analysis, and visualisation to track customer satisfaction levels with the main Scottish rail network - Scotrail. ", The impact of the project was to highlight variations in customer satisfaction that are not availble in other formats and to correlate this with the many times throughout the year we have reported on service disruptions.   Due to the nature of the contract Abellio/ScotRail they are not generally subject to FOI requests (with small exceptions such as cimmunications with the government agencies they come into contact with etc). This was therefore a more creative way to collect data to quantify the nature of the issues with the business.     ," This project starts out in Workbench:   https://app.workbenchdata.com/workflows/9628   I used workbench to beging scraping tweets from the @ScotRail twitter account. As Workbench is limited by the twitter api to only pull the last 3000 or so tweets - I left the scraper running with the tweets set to auto-accumulate.    Continuing in workbench I used a series of regex extractions, excel formulae, and data transformations to calculate the number of times the ScotRail account used the word ""sorry"", ""apologies"", or ""apologise"" every day and calculate the % of tweets every day that were apologies.   This scraper has been used a few times, and for visualisation I would either use Flourish or Datawrapper.  This time I used datawrapper. "," The project was not intended to be used on the day it was.   I have had the scraper running for nearly a year and was intending on doing a full analysis when I had a year of data. However, on the 18th of December the Scottish Government announced that they were stripping Abellio (the parent company) of the national rail contract due to the consistent performance issues - which seemed like a perfect time to show our data.   I then had to take the data from Workbench and pull out key dates, investigate the reasons behind these, and produce a visualisation within a few hours. As such, the final graphic isn't as polished as I would have liked, but such are the limitations of working in a small newsroom limited to open source/free resources. ", Having scrapers/tools set up and running like this can come in hugely handy to add to breaking news at the right time. ,https://www.thecourier.co.uk/fp/news/scotland/1044954/graph-shows-scotrails-mountain-of-sorry-tweets-in-2019-with-111-in-a-single-day/,https://app.workbenchdata.com/workflows/9628,https://www.thecourier.co.uk/fp/news/local/fife/1044869/scotrail-scottish-government-strips-abellio-of-rail-contract/,,,,,Lesley-Anne Kelly," Lesley-Anne Kelly is the sole data journalist at DCT Media, covering The Courier and the Press & Journal, in Scotland.   She's originally from Glasgow but now lives in Dundee with her husband and dog. Her main tools are Excel, R, OpenRefine, with bits and pieces of everything else as the project dictates.   She loves turning her projects into visualisations and was once called ""the most super of superusers"" by Katie Riley of Flourish. ",,,
United Kingdom,The Courier,Small,Participant,Best data-driven reporting (small and large newsrooms),The Dundee drug death crisis,19/07/19,"Quiz/game,Chart,Politics,Health","Microsoft Excel,R,RStudio","The Courier is a regional newspaper based in Dundee and Dundee is in the grips of a drugs death crisis - at various points in the last few years Dundee has had the highest drug death rate in Europe, and arguably the world. We created several data driven stories as a result of the yearly drug death publication from the National Records of Scotland, as well as to coincide with a Drugs Deaths Commission that was set up in Dundee to tackle the crisis. The first story provided is the article created the morning the statistics were released, which has"," With the first story we aimed to provide an in depth analysis of the statistics as they were released. The impact was raising the profile of the crisis and educating readers as to the magnitude of it.   With the second story - we talked about how to show people the path that many drug users in our city go down. We do so much reporting on the crisis that we wanted to do something a bit different - so we used Twine to create a ""choose your own adventure"" interactive story. We received lots of feedback that whilst it was grim, it had the desired effect of helping people to understand the various issues and everything that comes into play with individuals at the forefront of the crisis. "," For the initial report I used a mixture of Excel and R, and visualisations were down in Flourish.   For the second report we used very not technical post it notes to storyboard, and it was built in Twine.  "," It's such an emotive subject and we were very aware of the sensitivities around it. Once we had a prototype together we went through the story with an organiser of a local rehab program, to check for accuracy and tone and made tweaks from there.   The paths in the story are based on years of reporting (particularly in court and seeing the snap choices and decisions that led to the outcome) and every end point (and many in between) are backed up with local data and statistics. ", Thinking outside the box in how we present important stories so that they dont just get lost. ,https://www.thecourier.co.uk/fp/news/936367/breaking-scotland-has-highest-rate-of-drug-deaths-in-europe-according-to-shocking-new-stats/,https://www.thecourier.co.uk/fp/news/local/dundee/961506/how-hard-is-it-for-a-dundee-drug-user-to-get-clean-find-out-with-our-interactive-simulation/,https://www.thecourier.co.uk/wp-content/uploads/twine/A-Close-Path-Final.html,,,,,Lesley-Anne Kelly and Paul Malik," Lesley-Anne Kelly is the only data journalist at DCT Media, working for both The Courier and the Press and Journal.   Paul Malik is the Politics Editor of The Courier. ",,,
Japan,Nikkei Inc.,Big,Participant,Best data-driven reporting (small and large newsrooms),The impact of China's version of GPS,19/08/19,"Investigation,Map,Politics","Scraping,QGIS,Python"," We investigated China's ambition in the space. China launched 18 positioning satellites BeiDou in 2018 alone and now operates more satellites than U.S.'s GPS. We used the satellite planning data and calculated the daily maximum number of observable satellites at each capital city in the world. As a result, we figured out that the majority of countries have security concerns from the growing number of BeiDou. "," We published the article both in Japanese and English. Both received high engagement from our readers especially from outside Japan. Our analysis was mentioned in the annual report of the U.S.-China Economic and Security Review Commission, also in the executive summary (See the Project link 2). "," We scraped the data from the tool offered by U.S. receiver company Trimble. Trimble's GNSS (Global Navigate Satellite System) planning tool is very helpful to understand where satellites circulate. We used Selenium with Python and collected the number of observable satellites for each country. To plot the data for each capital city in the world, we used QGIS for mapping. "," Japanese government operates its own positioning satellite QZSS and feels a threat by China's ambition in the space. We tried to collect interviews from government officials and experts, but they were reluctant to speak about China's strategy. Our colleague visited Washington D.C. to ask comments for our analysis.    Initially, we tried to visualize the orbit for every satellites by using 3D modelling, but accurate visualization was very difficult and we chose to visualize it on a static world map as a number of satellites observable from each capital city in the country. "," What we learned from this article is that simplifying the complicated data is very important. Also, it is possible to tell a story about national security issues by using publicly available data.     ",https://asia.nikkei.com/Business/China-tech/China-s-version-of-GPS-now-has-more-satellites-than-US-original,https://www.uscc.gov/annual-report/2019-annual-report,,,,,,"Kazuhiro Kida, Shinichi Hashimoto", Kazuhiro Kida learned data journalism through Lede program at Columbia Journalism School in 2018 and now leads data projects in the newsroom. He has more than 17 years of career as a reporter and has basic skills of programming.   Shinichi Hashimoto is a member of the Investigative reporting team at Nikkei. ,,,
South Korea,KCIJ Newstapa,Small,Participant,Best data-driven reporting (small and large newsrooms),Journalism Reform Dashboard,17/10/19,"Investigation,Database,Open data,News application,Chart","D3.js,Json,Google Sheets,CSV"," The Dashboard unveiled Korean media industry’s deformed business model. Thanks to such odd, Korea-specific business models, media outlets rarely go out of business.    Korean media outlets - both print and broadcast - often publish advertorials not notifying that they’re ads, as if they’re actual news stories.    The outlets also organize business forums targeting executives at businesses and government organizations. They almost force the organizations to buy conference tickets or sign up for multi-million-KRW programs. These strategies are aimed at attracting PR budget - beyond simply obtaining ads.    KCIJ launched the Dashboard to unveil the status quo and abolish such practice. "," Media industry insiders vaguely knew how Korean media companies make money, but there has been no news story unveiling the status quo with actual data. We’ve collected news and payment data and opened the database to public.    The KCIJ’s Dashboard is the first of its kind in local media industry to build up a database of published advertorials and another database of each government organization’s PR expenditure payment breakdown. As new data comes in with new advertorials being published, we regularly update the Dashboard.   After KCIJ’s stories were released, a group of Korean lawmakers submitted a bill to regulate advertorials to the National Assembly in December last year. The bill requires media outlets to notify advertorials as ads and impose fines if an outlet fails to comply.   We’ll continue the series until the bill passes the Assembly and become legislated. "," The Dashboard visualizes data in different formats such as bar charts, line graphs, tree maps and bubble charts, using data visualization libraries like D3.js and Chart.js. We developed the website with Vue.js, one of JavaScript frameworks.    As the Dashboard has to be regularly updated with new advertorials being published every day, we designed the website with Google Spreadsheet to allow all participants of this project can easily add new data any time. We coupled the Dashboard and Google Spreadsheet so that any changes made on the Spreadsheet can be reflected instantaneously on the Dashboard. "," KCIJ collected and reviewed tens of thousand news stories to spot advertorials, build a database and compare it with the database from Korea Advertising Review Board. To gather government and quasi-government organizations’ PR expenditure records, we attempted different methods including FOIA request.    Publishing advertorials not indicating it’s an ad and selling conference tickets are two abnormal and somewhat unethical revenue stream for a media outlet’s business model. It is a unique topic that only a non-profit independent newsroom like KCIJ can report objectively. "," One can understand advertorial publishing trends in Korean newspapers and broadcast channels, by ranking of media outlets in numbers of published advertorials, ranking of advertisers, and size of government organizations’ PR expenditure spent on advertorials.   One can view above-mentioned data by media outlets and advertisers. The data is available month by month and year by year. Links to each advertorial story are also provided.   One can easily grasp the size of government expenditure spent on advertorials and buying conference tickets with visual elements like tree maps and bubble charts.  ",http://pages.newstapa.org/n1907,https://newstapa.org/article/V8aIX,https://newstapa.org/article/cWY7i,https://newstapa.org/article/hrUGK,https://newstapa.org/article/JE-I-,https://newstapa.org/article/i5Iz3,https://newstapa.org/article/TTprn,"Dongyoon Shin, Dahye Yeon, Sangjin Han, Joohwan Hong, Kangmin Kim, Songyi Lim, Kichul Kim, Youngcheol Shin, Joonsik Oh, Sangchan Lee, Hyeongmin Jeong, Hyeongseok Choi","  Dahye Yeon    Yeon is a data journalist who joined KCIJ-Newstapa in 2016. She has done data analyses in two massive projects in 2016 ""Medals and Power"" and the ""the Collaborators of Park Geun-Hye and Choi Soon-Sil regime,"" both of which largely involved data analyses and fact-checking procedures.    Songyi Lim    Lim Songyi is a data journalist with KCIJ-Newstapa’s Data Journalism team. She specializes in data visualisation and produces interactive visualization web pages. She has been working as a data journalist since 2016, and has participated in Newstapa’s ongoing featured investigation ‘Centenary of the Republic special report: Who rules Korea’.     Kangmin Kim    After joining KCIJ-Newstapa in 2013, Kim took a part in different data-driven stories, ""NIS Scandal Allegedly Manipulated Public Opinion on Twitter to Influence the 2012 Presidential Election,"" ""The Nuclear Revelation 2014"" and ""Medals and Power."" He was also heavily involved in data analysis for breaking news such as the impeachment of ex-president Park Geun-hye. He also participated in a data analysis project, which unveiled high-level public officials' personal assets.      *Due to word limits, we only provided bios of data journalists who participated to this project.   ",,,
India,The Hindu,Big,Participant,Best data-driven reporting (small and large newsrooms),Shrinking shape of water,30/06/19,"Explainer,Solutions journalism,Map,Satellite images,Environment","QGIS,Microsoft Excel,Google Sheets,CSV,OpenStreetMap"," In 2015, Chennai - the capital of Tamil Nadu, the southern most State of India - faced a devastating flood which killed hundreds and displaced many more. However, in 2019 it faced a water scarcity. While climate change is partly to blame, government apathy combined with public disregard for water resources resulted in bone-dry reservoirs, sewage-polluted rivers, lakes shrinking due to construction. While the government needed to be reminded of its responsibility towards its citizens, people need to know the extent of damage inflicted upon the water bodies due to their callous nature. In two parts, we achieved that.  "," The story was shared widely on twitter and facebook. It was also noted and appreciated by opposition members of the State assembly. However, the government was as usual silent.   Interestingly, north-west monsoon rains in 2019 exceeded the expected levels in Chennai. This time around, the residents were ready. Many apartments in the city had rain-harvesting technology in place. Reports of individual houses and apartments saving lakhs of liters of rain water and using it for daily needs were heart-warming. However, what role the two-part story played in this new beginning cannot be measured. As The Hindu, the paper I work for, is the number one daily in terms of circulation in Chennai, i think we can safely assume that it did play a significant role. The groundwater levels too improved at the end of 2019 compared to the levels seen in the previous years.      "," Satellite images from Copernicus Sentinel, LandSat (USGS/NASA) were procured for various time periods by zooming into various lakes and water bodies in Chennai. Using the maps, water surface extent was calculated and compared across time periods. While the former method was used to visually convey the decrease in water extent, the latter was used to convey the same using hard data.    In the second part of the story, we mapped how construction projects by both the government and the civilians reduced the city’s lakes in size. To do this, the old maps of the lakes were snapshot from Digital Globe and Landsat and processed using google engine. Comparisons with lake extent in 1954 (sourced from a map developed by the US govt at that time) and satellite maps shot in 1973 were carried out.    Similarly, the small rivers which criss-cross the city were mapped and all the sewer canals which drained into them were identified and plotted. The authorities can now precisely know the meeting point of sewer drains and rivers and curb the menace.          "," The hardest part was collection of old maps and making sense of them. Also, it was tough to set the exact zoom levels thereby enabling comparison of maps. Cloud cover was a challenge, because similar dates must be maintained as much as possible to make the comparisons valid (if the dates are not similar then  comparison becomes a farce as seasons vary wildly in India). Digitising old maps was difficult, especially mapping the lake extent from old maps was a challenge. If it was not done accurately, then it cannot be compared to present day maps to show the level of reduction in lake levels due to construction of buildings. Rivers that criss cross Chennai had to be followed inch by inch to see where the sewer canals meet them. It's a painstaking process given that sewer canals are extremely narrow and thus wont get highlighted in google maps often.  "," Random collection of old maps showing the extent of river and lakes may sound futile, however that's precisely what helped in making the core of our argument. Collect old files whenever you come across and archive them using referenceable labels, one day you will find a use in them.  ",https://drive.google.com/open?id=1_RabaAxahnwxgfbh_FOoIDgX21_WPBrt,,,,,,,"Vignesh Radhakrishnan, Raj Bhagat Palanichamy, K. Lakshmi"," Vignesh Radhakrishnan is a senior data reporter with The Hindu. He handles numbers for the Daily across all verticals including sports, business, economy, politics, climate and daily news stories.  ",,,
Japan,Nikkei Inc.,Big,Participant,Best data-driven reporting (small and large newsrooms),China's ambitions in international payment system,20/05/19,"Chart,Economy",Python," China is building its own international payment system, CIPS (Cross-Border Interbank Payment System) to compete with global standard SWIFT. The participation from global banks is growing, but the exact volume of transactions was not well known. We collected the PDF files from the Chinese central bank, People's Bank of China and scraped data written in Chinese. We figured out that transaction volume increased dramatically in recent years and pointed out that it would transform the global payment system landscape. "," The engagement from the financial sector was exceptionally high. Japanese banks are expanding their business in China and after the article, the movement has accelerated. We think it was a best timing to publish the article. ", We used python package 'pdftotext' and regular expression to extract the figure from the PDF written in Chinese. ," Since scraping data from Chinese government official site is prohibited, we needed to download a large amount of PDF files manually. Also, regular expression didn't work for some special characters. ", We learned that regular expression worked for Chinese langueage. ,https://asia.nikkei.com/Business/Markets/Rise-of-the-yuan-China-based-payment-settlements-jump-80,,,,,,,"Kazuhiro Kida, Masayuki Kubota, Yusho Cho", Kazuhiro Kida and Masayuki Kubota are data journalists and lead the project. Yusho Cho is the bureau chief of Shanghai and supported the project. ,,,
Kazakhstan,Factcheck.kz,Small,Participant,Open data,Aslan Seitov,25/10/19,"Investigation,Explainer,Database,Open data,Fact-checking,Illustration,Infographics,Chart,Culture","Adobe,Microsoft Excel,Google Sheets,CSV"," Our team tried to think about the impact of cultural and political issues on choosing name to newborn children. We made the hypothesis on this and with help of open data and visualization tools found out that yes, culture and politics has straight impact on choosing name to newborns.  "," Among journalists this article became so popular even local TV chanel asked for interview. But at the same time it is very interesting because every person has a name so this topic is really for everyone. People understood that their names are not special and their parents was just following the trends. For exmaple for now we can show some actions of Islamization in Central Asia region. How we can describe it? I don't know but that's the fact. And among all of the negative news this reliable and at the same time funny article based on data makes people think about it and understand that they may be under impact of some manipulations but they even don't know about this. Of course, after reading this article they going to be more sceptic about everything and I hope they will check information and try to be more objective.  ", We used government open data which I think we can rely on. Because there is no need to manipulate with this data. We used excel to clean the data and rarely google sheets' tools for cleaning. After making flat table we got the cleanest data that we can use for visualization. Mostly we used Flourish. For free I can do there almost everything. Living in Kazakhstan and generally being from Cetral Asia is quite uncomfortable for getting data or for access some services. But floursh is for everyone and for free.  , Of course the hardest thing was to find the idea and to work with data. But I enjoyed it. At this moment we didn't know about scraping so we are proud of ourselves for making such interesting content with using minimum skills with working with data. , May be vizualization. And searching tools for google. And short ways to use them.  ,https://factcheck.kz/socium/aleksandr-vs-aruzhan-istoriya-kazaxstana-v-imenax-novorozhdyonnyx/,,,,,,,"Aslan Seit, Myahriban Seyitlieva, Madina Bulatova, Mavzuna Tabarova, Zulfiya Raisova"," Factchecker, Journalist,    Not journalist, but can use Excel   Teacher in Kostanay State University    Journalist from Tajikistan   Hardworking journalist ",,,
Russia,7x7 online-journal,Small,Participant,Best data-driven reporting (small and large newsrooms),Regional Kings of State Auctions,28/03/19,"Investigation,Open data,Fact-checking,Corruption","Personalisation,Microsoft Excel"," The quality of food in schools, medicines for patients, repair and construction of housing and roads often causes deserved discontent among Russians. The state regularly spends billions of rubles for these purposes through the public procurement system. And in every region there are people who managed to adapt their business to huge budget money, and usually these are not some “Putin friends” from Moscow, but their own - local ones. ""7x7"" represents the ""kings of government orders"" of the Murmansk and Vladimir regions, the Republics of Karelia and Chuvashia.           "," The main impact is that society, residents of small cities, through our investigations, found out which businessmen have close ties with regional governments and officials. Russian reality is such that it is practically impossible to directly influence this situation, but usually in such situations both businessmen and officials become more careful when allocating budget money. They begin to fear publicity. "," Journalists of our online magazine used the public procurement database. There is information about all auctions, the winners of which received budget money. Having compiled a list of the largest entrepreneurs close to the state, journalists used economic databases to determine the relationships between the companies. Then each journalist wrote about 10 requests for information to state authorities and companies, interviewed company employees and competitors. "," The most difficult part of the project is the analysis of a large amount of data. These are thousands of government purchases. Journalists wrote out the sum of the contracts for each of them, put them together and got the TOP-10 entrepreneurs who got rich at the expense of budget money. "," The most important result of the project is that readers have learned that often companies receive profitable government contracts only because they are friends with officials, and not because they are good businessmen. This is indirect corruption. ",https://lr.7x7-journal.ru/koroli/,https://semnasem.ru/koroli/koroli-murmansk,https://semnasem.ru/koroli/koroli-kareliya,https://semnasem.ru/koroli/koroli-chuvashiya,https://semnasem.ru/koroli/koroli-vladimir,SYKTYVKAR,,"Gleb Yarovoi, Sergey Markelov, Natalya Petrova, Vladimir Prokushev, Anna Yarovaya (producer)"," Gleb Yarovoi, Sergey Markelov, Natalya Petrova, Vladimir Prokushev are journalists of the regional online magazine ""7x7"", each of them has experience from 3 to 10 years. But for most of them, this was the first experience in investigative journalism. In their work, journalists paid much attention to corruption, the impact of politics on decisions, protests and public initiatives.     ",,,
Russia,TASS Russian News Agency,Big,Participant,Best visualization (small and large newsrooms),Getting the Pulse on Blood Donations in Russia,14/06/19,"Explainer,Documentary,Fact-checking,Illustration,Infographics,Map,Health","Adobe,Creative Suite,Microsoft Excel"," The special project tells us about why it is essential to become a donor and how the issue of blood donation has changed in Russia over the past ten years. There is the interactive tile grid map of Russia, which shows how donations have been progressing nationwide on five key parameters. In addition, the project answers key donation questions and dispels the most popular blood donation myths. "," Blood donation is a very important part of health care in many countries. We put together some unique statistics, comparing data, went through the history of the situation and outlined the blood donation problems in Russia. "," We have used Microsoft Excel and Tableau Desktop to data analysis, Adobe illustrator to create diagrams and visualizations, and Procreate to make various illustrations. Project was built using Readymag, HTML, CSS and JavaScript. "," One of the toughest things to come up with a story for the regular donors and a wider audience. There are two parallel storylines in TASS special project for that reason. In the end, the reader would know how much he or she would have donated during reading the material. This widget was one of the hardest part for developers. "," This TASS special project is to create conditions to draw attention to the blood donation problems in Russia. The main message of the project is to tell why it is necessary to donate blood and why it should be done regularly. Illustrations, visualizations and maps are designed to motivate readers to donate some blood. ",https://donor-krovi.tass.ru/,,,,,,,"Author: Timur Fekhretdinov Editors: Sabina Vakhitova, Artyom Ivolgin Designer: Vasily Yegorov Illustrator: Daria Yastrebova Developers: Anton Mizinov, Andrey Strizhkov"," TASS Infographics Studio and Special Projects team were based in Moscow in 2016. The main direction of our activity is infographic and multimedia projects about history, science, technology and art. ",,,
Russia,TASS Russian News Agency,Big,Participant,Innovation (small and large newsrooms),"Mercator. It's a Flat, Flat World",22/08/19,"Long-form,Documentary,Illustration,Infographics,Map,Arts,Culture","D3.js,Three.js,QGIS,Json,Adobe,Creative Suite"," The interactive digital special project ""Mercator. It’s a Flat, Flat World"" tells the story of a famous map, published in 1569 by the flemish cartographer Gerhard Mercator. We describe the historical and scientific context of the epoch and share the personal story of its' creator. The key visual of the project is the map itself. We point at the mistakes, some curious facts and the unconditional advantages of this innovative projection, that glorified Mercator's name. "," Gerardus Mercator has made a purely innovative map projection which is still used in some fields. A trace of Mercator’s heritage is found even in space research: all images of the Earth taken by the Sentinel-2 and Landsat satellites are shown in his projection. However, we think, it's important to remind the audience, especially those who eager to rely upon maps while debating about the omnipotence of their countries. In reality Russia is not as big as this popular projection shows us. In reality, Russia is nearly half the size of Africa. Greenland seems to be the same in magnitude as South America, though in actual fact it is only one-eighth of its territory. Canada is only a little bit larger than the United States, yet it looks twice as big. Maps may breed risky delusions. And as we've observed, these comparisons of depiction of Russian territories in different projections actually raised discussion among readers. Sometimes it's useful to doubt. "," All the pictures were created in Adobe Illustrator and the map as a key visual was collected and processed in QGis. The site itself was built in HTML and Javascript, using Data Visualization Libraries "," The hardest part was to digitalize the map correctly. Today only three surviving printed copies of the map are known and are kept in the National Library of France, the Maritime Museum Rotterdam and the Basel University Library. All three sets of sheets are slightly different from one another, quite possibly because time had no mercy on the paper that the map had been printed on. Another tricky point was to translate the text on the map from Latin to Russian. "," The interactive digital special project ""Mercator. It’s a Flat, Flat World"" tells the story of a famous map, published in 1569 by the flemish cartographer Gerhard Mercator. We describe the historical and scientific context of the epoch and share the personal story of its' creator. The key visual of the project is the map itself. We point at the mistakes, some curious facts and the unconditional advantages of this innovative projection, that glorified Mercator's name. ",https://mercator.tass.com/,,,,,,,"Authors: DARIA DONINA, TIMUR FEKHRETDINOV Editors: SABINA VAKHITOVA, KRISTINA NEDKOVA Illustrator: ANASTASIA ZOTOVA Art director: ANTON MIZINOV Translator: ANDREI STARKOV Style editor: PHILIP AGHION"," TASS Infographics Studio and Special Projects team were based in Moscow in 2016. The main direction of our activity is infographic and multimedia projects about history, science, technology and art. ",,,
Taiwan,READr,Small,Participant,Best data-driven reporting (small and large newsrooms),Decline of East District: the rent is not the only cause.,25/06/19,"Explainer,Solutions journalism,Long-form,Database,Open data,Chart,Map,Business,Culture,Economy","D3.js,Json,Google Sheets,CSV,R,RStudio"," By combining data of shops rental(rentalprice, shops-being-empty-for-rent time, numbers of shops waiting to be rented ) and MRT travel volume, we explored whether the East District of Taipei is really in decline as other media have said. The fact of""EastDistrict is declining"" was later proved by the data we collected. We then tried to find out the causes and possible solutions through interviews. "," This feature tried to use data as evidences to confirm whether the phenomenon of""EastDistrict  Declining"" is true rather than just seeing other media reports individual shops’ closing. We did not set an assumption at the beginning. Instead, we started from examining data, asking questions from the data, trying to figure out different factors of the causes.  We then consulted experts and local shop owners with data we found, trying to be an alternative model of investigative reporting. "," We used html to convert the data from Taipei City Office of Commerce that was originally in the format of excel file and with inconsistent columns into analyzable csv format.  Next we used R studio for data cleaning. The shopping districts areas were assigned by Taipei City Office of Commerce. We selected shop addresses that are in the area of East District. After locating these addresses, we examined whether the number of shop-opening was reducing and the number of shop-closing is increasing on yearly basis.  Finally, by using d3.js to visualize the set of data, we used d3 to draw a simplified version of the map. With locations of the shops, we can see numbers and locations of shops that were opened or closed in different years on the map. The set of maps became the animation of this feature’s landing page. In the text, we cooperated with real estate industry businessmen to obtain shop rental information. After collating Taipei MRT travel volume within the time period required by the feature, we compared it and found that the rents of East District have declined, but visitors has also declined. On the contrary, the rents in Ximending area have increased, but visitors has continued to soar. "," The most difficult part is to find evidence of  ""Declineof East District"".Although there are rental data of major shopping districts of Taipei provided by real estate developers, we were still lack of original data to back up the claim. As a result, we studied original shops registry and closure data from Taipei City Office of Commerce. We located shopsin East District from addresses, then comparedthe opening and closing datesone by one. Finally we came out with the data set that shows shop statues of each address in seven years. The sourceconfirmed that East District is indeed in decline. This kinds of approach of data and question-asking have not be seenin other Taiwanese mediaoutlets.  "," it can be learnt that not to presume a conclusion at first, using data as a starting point then to explore various possible answers. When there is no original information and only secondary information provided by others, people can try to use the government's public information to find an algorithm that can answer questions we discussed about. For stories or events that have already be reported by other media outlets, try to examine them from the perspective of data, some new findings or angles may surface.  ",https://www.readr.tw/project/eastern-district-of-taipei,,,,,,,Producer: Chien Hsin-chan Executive Producer: Chen Tzy-Tyng Journalist: Liu Tzu-Wei Design: Hsu Ling-Wei Photographer: Li Chao Yen Data: Chien Hsin-chan Web developing: Tan Hsueh-Yung," The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, it is almost one of the few relatively well-established teams in Taiwan. Therefore, many of the topics produced by READr have an indicative impact on the development of data journalism in Taiwan.       READr is not just a data newsroom, it is also a digital innovation team. Therefore, we always hope to make breakthroughs in every topic, not just using traditional news methods to tell stories.       The difference between our team and the general data newsroom is that the members have a background in engineering and photography. Therefore, the various roles of the team members(includingjournalists, designers, engineers, community managers, product managers, etc.), everyone's opinions are equally important. Each member can make the best contribution based on his own experience and professionalism. Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",,,
Taiwan,READr,Small,Participant,Best data-driven reporting (small and large newsrooms),Who are the Internet Water Army from China that Twitter confront?,22/08/19,"Investigation,Explainer,Breaking news,Open data,Elections,Politics","R,RStudio"," On August 19th 2019, Twitter announced the news of deleting the suspected Chinese Internet Army account. Twitter suspended 936""China-originated""accounts and said that these accounts have recently published a large number of disinformation related to Hong Kong’s protests. We analyze these deleted accounts within three days, try to understand what they have done in their past. Through data visualization, we also found the interaction and link these accounts. "," This report notonly the only one report in Taiwan, but also thefirst one to analysis the original data released from Twitter. Almost all the media only quoted fromTwitter’s press release, while ignoring the batch of data theyreleased. After further analysis, wefound more fun facts further thanTwitter claims to be.For example, we found that many accounts werefansof Korean pop singers in the past. "," We use R to cleaning, analysis data. Using web crawlerto collect the information from other accounts who interacted with these deleted accounts. We also use Gephi to visualize the connections between these accounts.  "," During operation, the maximum number of data reach millions. To visualize the huge amount of data, and uncover the meaning behind them is difficult, but very interesting. When almost all the media only quoted Twitter news release, we are the first media to analysis their data. "," How to analyze data from a number of social networking sites quickly and find their interaction patterns and associated with each other, dig out the official press release did not say a thing. ",https://www.readr.tw/post/2017,,,,,,,"Producer: Chien Hsin-chan Journalist: Lee Yu Ju Design: Chen Yi-Chian Data: Lee Yu Ju, Chien Hsin-chan, Kuan Hsien Wu, Li Chao Yen, Chen Yen-Yu"," The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, it is almost one of the few relatively well-established teams in Taiwan. Therefore, many of the topics produced by READr have an indicative impact on the development of data journalism in Taiwan.       READr is not just a data newsroom, it is also a digital innovation team. Therefore, we always hope to make breakthroughs in every topic, not just using traditional news methods to tell stories.       The difference between our team and the general data newsroom is that the members have a background in engineering and photography. Therefore, the various roles of the team members(includingjournalists, designers, engineers, community managers, product managers, etc.), everyone's opinions are equally important. Each member can make the best contribution based on his own experience and professionalism. Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",,,
Taiwan,READr,Small,Participant,Innovation (small and large newsrooms),How Did Twitter Identify Large Clusters of Accounts Amplify Messages Related to The Hong Kong Protests?,16/09/19,"Investigation,Explainer,Cross-border,Database,Open data,Infographics,Elections,Politics","AI/Machine learning,R,RStudio,Python"," On August 19, 2019, Twitter published for the third time the behavior of the political cyber army they monitored, and publicly deleted the profile of the account. We try to figure out the activity characteristics of deleted accounts by machine learning, to speculate on the possible reasons for Twitter to delete these accounts, and to find a way to identify the ""cyberarmy"".       We found that the deleted twitter accounts did have some special behavior characteristics. This is the first time in Taiwan that media has applied machine learning to news reports. "," On the published data, we can use a scientific method to deduce the reason behind the data, and use the result and insight as a report, so that readers can have a better context to understand the circumstances under which Twitter was deleted these accounts, and how they define them with the support of Twitter materials, and find out what kind of patterns the Cyber Army has discussed in recent years, and what they will actually look like.This report is not just about data visualization or simple analysis to find insights, but through a greater amount of data and machine learning methods, using computer science to return to a more realistic situation.   This also allows us to find that in the field of data journalism, in fact, there are more analysis methods available to do the report. This allows data journalism not only to find correlation or cause and effect through cross-analysis of the data, but to drill down into the data to find more facts. "," The most important part of this topic is the machine learning. In machine learning, how to find the suitable model is the most important. So we tried several basic and important models, including Decision Tree, Xgboost, and Randomforest. At the beginning, the data must be cleaned to be usable in the Python sklearn module, and the fields that we think are important and can be analyzed are used as learning characteristics.   In Twitter ’s published of deleted accounts, it includes two groups of accounts and all content posted by users in the two groups of accounts were published. So we can take one group as the object of learning and the other group as the object of verification. Of course, there needs to be a sufficient number of random Twitter accounts and the published content of these accounts as a control group for learning and verification. To ensure that the model we use has good results with the parameters set for the model. "," In order to better achieve the effect of machine learning, it is really important to find a sufficient number of random Twitter accounts and the content that these accounts have published. Because if there is no way to have a sufficiently random account, under the mechanism of machine learning, it will make the computer too easy to find the corresponding features for grouping. And compared to the 940 accounts published by Twitter, we also need to ensure that we can find a close number of accounts so that machine learning can have enough data to learn.   At present, more complete Twitter account information and published content are probably on the Internet's AI data platform Kaggle, but most of the datasets on Kaggle will generate data with certain specific attributes and insufficient data fields. Finally, we found that the two data sets have complete fields, so we used some of the data from these two data sets, and obtained the Twitter accounts of some public figures, and then went to grab the postings of these accounts, following After the data set is mixed, it is divided into training group and verification group. Only in the end can a better training result be achieved. "," At present, for most data journalism, we are very focused on comparing data sets, or finding correlations between different data sets, trying to find the cause of these data, or understanding the various results obtained under conditions. In some cases, it may be that more computer science methods (such as machine learning) can be used to actually find closer to the facts and actually affect the cause of the results. Although in general, AI and machine learning both use existing data to predict or achieve one of the results we expect. But machine learning can also be used as another kind of reverse engineering, telling us under what conditions, the final results will become those we already know.   Therefore, although we are not using the general application of machine learning to predict future results, how to make good use of these different technologies to complete more scientific reports is also what we can learn from this process. ",https://www.readr.tw/post/2029,,,,,,,"Producer: Chien Hsin-chan Journalist: Lee Yu Ju Design: Chen Yi-Chian Data: Chien Hsin-chan, Hung Chih-Chieh"," The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, it is almost one of the few relatively well-established teams in Taiwan. Therefore, many of the topics produced by READr have an indicative impact on the development of data journalism in Taiwan.       READr is not just a data newsroom, it is also a digital innovation team. Therefore, we always hope to make breakthroughs in every topic, not just using traditional news methods to tell stories.       The difference between our team and the general data newsroom is that the members have a background in engineering and photography. Therefore, the various roles of the team members(includingjournalists, designers, engineers, community managers, product managers, etc.), everyone's opinions are equally important. Each member can make the best contribution based on his own experience and professionalism. Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",,,
Taiwan,READr,Small,Participant,Best data-driven reporting (small and large newsrooms),When A Group of Volunteers With Religious Background Enter Primary School Classrooms Teaching ‘Life Education',12/09/19,"Investigation,Open data,Illustration,Chart,Culture","Json,Google Sheets,RStudio"," In August 2019, some people accused that a Christian background of allegations of""RainbowLoving Family Life EducationAssociation"" become volunteers of elementary school, spread particular religious idea in the time before the formal curriculum. What they had taught not only suspected of illegal content, but also shows the system of volunteers lack of good practices. Even parents can not understand what these volunteers had taught to the children. We investigated the 2632 elementary schools in Taiwan, found that 40% of the volunteer groups of had religious background. "," These religious volunteers caused huge criticism in Taiwan, many parents found out that if they want to know who is teaching their children, absolutely no one can provide them information.We did something even the government could not do. We expose all elementaryschools which have volunteers stationed, and presented through visualization who these groups are and where these groups. We also offer interrogator so that parents can quickly query. "," This project uses google sheet to arrange data, and fetch data through google sheets API. And we use CSS and Vue.js to visualize data. "," The most difficult part is to get information on more than two thousand elementary schools. We cooperated with a few of city councilors to get a small part of the information, and mostof the information was directly call the school to ask. We havea total of more than two thousand phonecall. When this issue has been criticized in Taiwan, without knowing the whole picture of the situation, the fear is easy to be enlarged. For example, many people will simply just start criticizing those who volunteer to help others, and think they all are people who want to pass specially religious thought. Through our survey the whole picture, so that readers can objectively understanding this issue. "," If only reported certain cases, lack of whole picture, it is easy to cause panic. The report should be responsible to provide the whole picture to the readers, data can be done about it. ",https://www.readr.tw/project/extra-curriculum,,,,,,,"Producer: Chien Hsin-chan Executive Producer: Lee Yu Ju, Chen Tzy-Tyng Journalist: Huang Yi-Hsun, Lee Yu Ju Design: Chen Yi-Chian Web developing: Tan Hsueh-Yung"," The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, it is almost one of the few relatively well-established teams in Taiwan. Therefore, many of the topics produced by READr have an indicative impact on the development of data journalism in Taiwan.       READr is not just a data newsroom, it is also a digital innovation team. Therefore, we always hope to make breakthroughs in every topic, not just using traditional news methods to tell stories.       The difference between our team and the general data newsroom is that the members have a background in engineering and photography. Therefore, the various roles of the team members(includingjournalists, designers, engineers, community managers, product managers, etc.), everyone's opinions are equally important. Each member can make the best contribution based on his own experience and professionalism. Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",,,
Taiwan,READr,Small,Participant,Best data-driven reporting (small and large newsrooms),Disclosing Taiwan Merchants Receive Subsidy from China Government.,07/07/19,"Investigation,Explainer,Long-form,Database,Open data,Illustration,Chart,Politics,Economy","Json,Google Sheets,R,RStudio"," Taiwan enterprises Want Want Group was broke the news that they receive a lot of Chinese government subsidies on facebook, causing public discussion. And on June 2019, there was a""againstChina infiltrate media"" parade broke a record number of participants in Taiwan. These cases shows the“Chinafactor” anxiety of Taiwanese people. We drill down through more than 100 thousand copies earnings Taiwan to understand the trends in Chinese government grants, but also interpret the significance grants for the reader, to provide a comprehensive perspective about this issue. "," We survey Taiwanese business in China and Hong Kong, found China government issued a subsidy of NT $ 66.3 billion for the 42 Taiwanese business in the past 10 years. There were three groups accounted for 60% of the subsidy. Subsidy may be followed on the size of the company, the industry situation and even political positions related. We also found that, although the China government frequently used subsidies to attract talents, but the status of Taiwan businessman in recent years has been not as in the past. "," This project is made by Vue.js and the official router library to implement a single page application. In this project, we use fullPage.js to create a fullscreen scrolling experience landing page before the article page, and we use IntersectionObserver to implement section detection within the article so that readers can always know which section reading right now, we use HTML anchor to let readers jump to the section they interested quickly, all these features create a better experience when reading a long-form article. "," The most difficult part is to search for the company's earnings in different China, Hong Kong Web site, case by case basis and recorded on file. We went through a total of more than eight thousand copies earnings, meaning that opened more than 8,000 pages, in order to fully disclose such information. However, we thought thatthan to talk about some certaincases, to provide the reader a wholepicture of the issueis a more responsible act. "," If only reported certain cases, lack of whole picture, it is easy to cause panic. The report should be responsible to provide the whole picture to the readers, data can be done about it. ",https://www.readr.tw/project/china-company,,,,,,,"Producer: Chien Hsin-chan Executive Producer: Lee Yu Ju, Chen Tzy-Tyng Journalist: Chen Li Ya Researcher:Lee Yu Ju Data:Lee Yu Ju, Li Chao Yen, Hsu Ling-Wei Design:Chen Yi-Chian Web developing: Hsiung Kai Wen"," The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, it is almost one of the few relatively well-established teams in Taiwan. Therefore, many of the topics produced by READr have an indicative impact on the development of data journalism in Taiwan.       READr is not just a data newsroom, it is also a digital innovation team. Therefore, we always hope to make breakthroughs in every topic, not just using traditional news methods to tell stories.       The difference between our team and the general data newsroom is that the members have a background in engineering and photography. Therefore, the various roles of the team members(includingjournalists, designers, engineers, community managers, product managers, etc.), everyone's opinions are equally important. Each member can make the best contribution based on his own experience and professionalism. Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",,,
Taiwan,READr,Small,Participant,Best visualization (small and large newsrooms),How Did The Internet Water Army From China Amplify Messages Related to The Hong Kong Protests on Twitter?,16/09/19,"Explainer,Database,Open data,Crowdsourcing,Illustration,Politics","R,RStudio"," On August 19th 2019, Twitter announced the news of deleting the suspected Chinese Internet Army account. Twitter suspended 936""China-originated""accounts and said that these accounts have recently published a large number of disinformation related to Hong Kong’s protests. We analyze these deleted accounts try to understand what they have done in their past. Through data visualization, we also found the interaction and link these accounts. "," This report notonly the only one report in Taiwan, but also thefirst one to analysis the original data released from Twitter. Almost all the media only quoted fromTwitter’s press release, while ignoring the batch of data theyreleased. After further analysis, wefound more fun facts further thanTwitter claims to be. "," We use R to cleaning, analysis data. Using web crawlerto collect the information from other accounts who interacted with these deleted accounts. We also use Gephi to visualize the connections between these accounts.  "," During operation, the maximum number of data reach millions. To visualize the huge amount of data, and uncover the meaning behind them is difficult, but very interesting. When almost all the media only quoted Twitter news release, we are the first media to analysis their data. "," How to analyze data from a number of social networking sites quickly and find their interaction patterns and associated with each other, dig out the official press release did not say a thing. ",https://www.readr.tw/post/2028,,,,,,,"Producer: Chien Hsin-chan Journalist: Lee Yu Ju Data: Lee Yu Ju, Chien Hsin-chan, Kuan Hsien Wu Design:Chen Yi-Chian"," The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, it is almost one of the few relatively well-established teams in Taiwan. Therefore, many of the topics produced by READr have an indicative impact on the development of data journalism in Taiwan.       READr is not just a data newsroom, it is also a digital innovation team. Therefore, we always hope to make breakthroughs in every topic, not just using traditional news methods to tell stories.       The difference between our team and the general data newsroom is that the members have a background in engineering and photography. Therefore, the various roles of the team members(includingjournalists, designers, engineers, community managers, product managers, etc.), everyone's opinions are equally important. Each member can make the best contribution based on his own experience and professionalism. Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",,,
Russia,TASS Russian News Agency,Big,Participant,Innovation (small and large newsrooms),The Buran. The Soviet space shuttle success story,01/04/19,"Long-form,Documentary,Illustration,Infographics","3D modelling,Adobe,Creative Suite,Google Sheets"," This special project is dedicated to the 30th anniversary of the first and only flight of the spaceship ""Buran"". The ""analysis of the flight"" goes into detail about the most difficult stages of the Soviet rocket plane’s mission, including the landing, which for the first time in history was fully automatic. Each of the project’s four chapters contains interactive inserts: a comparison of the Soviet system ""Energy — Buran"" with the American Space Shuttle system, a step-by-step layout of the spacecraft’s take-off and landing, as well as complete information about the surviving flight specimens and full-size models. "," This special project is dedicated to the 30th anniversary of the first and only flight of the spaceship ""Buran"". Each of the project’s four chapters contains interactive inserts: a comparison of the Soviet system ""Energy — Buran"" with the American Space Shuttle system, a step-by-step layout of the spacecraft’s take-off and landing, as well as complete information about the surviving flight specimens and full-size models. The ""analysis of the flight"" received positive attention and feedback from users and professional community. "," We have used Adobe illustrator to create plans for the the spaceship and 3Ds Max and V-Ray to make realistic technical illustration. Project was built using HTML, CSS and JavaScript. ", One of the most complex (and complicated) visualizations of the project is a detailed 3D model of the Buran. It contains image of all 38 000 heat-resistant tiles. ," The ""analysis of the flight"" goes into detail about the most difficult stages of the Soviet rocket plane’s mission, including the landing, which for the first time in history was fully automatic. ",https://buran.tass.com/,,,,,,,"Authors: KRISTINA NEDKOVA, TIMUR FEKHRETDINOV Editors: ALEXANDER BYCHKOV, SABINA VAKHITOVA Style editor: PHILIP AGHION 3D-model: ALEXANDER VOLKOV, ANASTASIA ZOTOVA Photo-editor: PAVEL KUKOLEV Video editor: MAXIM MAKAROV Art Director: ANTON MIZINOV"," TASS Infographics Studio and Special Projects team were based in Moscow in 2016. The main direction of our activity is infographic and multimedia projects about history, science, technology and art. ",,,
Taiwan,READr,Small,Participant,Open data,Fact Checks of the 2020 Presidential Election,21/08/19,"Database,Open data,Fact-checking,Illustration,Video,Elections,Politics",Google Sheets," Taiwan’squadrennial presidential elections, candidates run in full swing, there are a lot of news output every day, but is this information true or false? Starting from September 2019, we have turned the public conversations of presidential candidates into texts through crowd-sourcing, and we have worked with 14media and research institutions to check them. We have also served readers in the only presidential debate. Provide instant fact checks. "," The project generated 5,990 video clips, plus 24 direct donations of transcripts. The total transcript hours reached 48 hours. At least 1432 netizens volunteered to participate as transcripts or transcript verification volunteers, and the transcript completion rate has never been lower than 50%.        So far, 3227 conversations have been separated from public speeches, of which 678 contain verifiable content, and 11 media have completed 230 checks, of which 47 error messages and 60 one-sided facts have been found. 13 media partners used the audit data to make in-depth application reports. "," This plan is primarily through third-party services. We upload the video to youtube and segment the video with parameter settings. Then create a google form to use the question form to help users fill in the transcript content and verify that the transcript is correct, and finally import the data into the database created with google sheet. "," The concept of Open data is not popular in the news media because exclusivity is the most important thing in the news.       However, after many communications and discussions, we established a common goal: to give voters in Taiwan an objective choice and to reproduce the important role of the media-presenting the facts. And that made all the difference, to make the project work.      "," The role of the news media has recently been entertained, but we believe that as media people, we still hope that news should give readers the truth. It is difficult to challenge the needs of readers, and it is even more difficult to combat reading rate. But as long as we gather the media together, we can do it. And the concept of open data is like this, sharing and opening data to maximize social benefits, and this will be the core value of READr unchanged. ",https://www.readr.tw/project/fact-check-2020,,,,,,,"Producer: Chien Hsin-chan Executive Producer: Lee Yu Ju, Chen Tzy-Tyng Design: Hsu Ling-Wei, Chen Hsing Ying Web developing: Chen Tzy-Tyng, Tan Hsueh-Yung Data: Li Chao Yen"," The development of information news in Taiwan media is still not perfect at present. Although the READr is only a small information newsroom, it is almost one of the few relatively well-established teams in Taiwan. Therefore, many of the topics produced by READr have an indicative impact on the development of data journalism in Taiwan.       READr is not just a data newsroom, it is also a digital innovation team. Therefore, we always hope to make breakthroughs in every topic, not just using traditional news methods to tell stories.       The difference between our team and the general data newsroom is that the members have a background in engineering and photography. Therefore, the various roles of the team members(includingjournalists, designers, engineers, community managers, product managers, etc.), everyone's opinions are equally important. Each member can make the best contribution based on his own experience and professionalism. Without the framework of traditional thinking, the team can make the report more creative and also keep the news professional by presenting stories in a true and complete way. ",,,
United Kingdom,BBC England Online,Big,Participant,Best data-driven reporting (small and large newsrooms),Climate change: Tree planting rise 'needs to happen quickly',30/07/19,"Explainer,Long-form,Open data,Chart,Map,Environment","AI/Machine learning,Microsoft Excel,CSV,R,RStudio,Python"," Experts on climate change say a huge programme of tree planting is needed if the UK is to have any chance of reducing its carbon emissions to effectively zero.  The BBC England data team examined just how many trees have been planted, and whether the government is on course to meet its ambitious targets.  And it personalised the story by providing local versions so readers could see how many trees had been planted in their area with government funding.   The story fulfills one of the BBC’s key priorities to report the effects of climate change on the environment.  "," BBC News ran programming across multiple platforms highlighting both the level of local tree-planting as well as the scale of the challenge ahead in meeting the Committee on Climate Change's recommendations for carbon cutting. As well as a story on the BBC News website we also provided shorter versions and radio,  geographically tailored to different regions of England that told people the number of trees planted in their area and linked to the longer story from our local news topic pages, thanks to natural language generation software, allowing us to give audiences a story tailored to where they live.   Radio colleagues also had access to scripts written using the same natural language generation tools, listing all of the local councils in their areas and the number of government-funded trees planted since 2010. These scripts were written as though they would be ready for a presenter to read out if they so wanted but in practice were used as the building blocks for regional broadcast content, providing the essential local data in an easy to digest way, coupled with the essential national level context. It avoided colleagues needing to wade through long explanatory notes or filter spreadsheets in order to get to the data that mattered to their audiences. We often provide localised figures in our stories but often these will end up focusing on a few places, whereas this project - containing figures for more than 300 individual local authority districts - gave us the chance to use the local, more personal, aspect as a way into the story for the audience. "," The main data visualisations were created using R and the BBC's bbplot and mapping packages, including an annotated time series highlighting planting rates going back to the 1970s and the geographical breakdown of new trees planted in England with government funding since 2010.   The accompanying local stories and local radio scripts were written using Arria's natural language generation software, with local data filling in the gaps in sentences but also being used to change the structure of each paragraph according to the data it contained. Python was used to combine local authorities into their respective BBC radio station area.   This workflow produced a story for each local authority district in England, with one summary story for all of London.   Each of the stories contained relevant local statistics that were also put in a national and regional context. The local versions of the story linked to the longform article about the UK trend.   The <a href=""https://www.bbc.co.uk/news/live/uk-england-devon-49083027"" rel=""nofollow"">liveblog for Devon and Cornwall</a>, for example, includes an <a href=""https://www.bbc.co.uk/news/live/uk-england-devon-49083027?ns_mchannel=social&ns_source=twitter&ns_campaign=bbc_live&ns_linkname=5d4023a672af49066398061b%2643%2C800%20government-funded%20trees%20planted%20in%20Torridge%20in%20eight%20years%20%262019-07-30T15%3A50%3A52.716Z&ns_fee=0&pinned_post_locator=urn:asset:87aa3ecd-6caa-44a0-9315-fc9e5c962986&pinned_post_asset_id=5d4023a672af49066398061b&pinned_post_type=share"" rel=""nofollow"">update about Torridge</a>, one on <a href=""https://www.bbc.co.uk/news/live/uk-england-devon-49083027?ns_mchannel=social&ns_source=twitter&ns_campaign=bbc_live&ns_linkname=5d4023a951ae87065597c850%2660%2C600%20government-funded%20trees%20planted%20in%20West%20Devon%20in%20eight%20years%20%262019-07-30T14%3A47%3A44.446Z&ns_fee=0&pinned_post_locator=urn:asset:adc3b7ff-64b7-4d34-b514-6c4e7aae3ca8&pinned_post_asset_id=5d4023a951ae87065597c850&pinned_post_type=share"" rel=""nofollow"">West Devon</a>, one about <a href=""https://www.bbc.co.uk/news/live/uk-england-devon-49083027?ns_mchannel=social&ns_source=twitter&ns_campaign=bbc_live&ns_linkname=5d4023a851ae87065597c84f%2655%2C500%20government-funded%20trees%20planted%20in%20East%20Devon%20in%20eight%20years%20%262019-07-30T14%3A27%3A49.596Z&ns_fee=0&pinned_post_locator=urn:asset:7ea49b7c-8042-48a6-ba83-702f5225a50a&pinned_post_asset_id=5d4023a851ae87065597c84f&pinned_post_type=share"" rel=""nofollow"">East Devon</a>, another about <a href=""https://www.bbc.co.uk/news/live/uk-england-devon-49083027?ns_mchannel=social&ns_source=twitter&ns_campaign=bbc_live&ns_linkname=5d4023a328b0fb067d70ed59%2676%2C100%20government-funded%20trees%20planted%20in%20South%20Hams%20in%20eight%20years%20%262019-07-30T14%3A25%3A10.100Z&ns_fee=0&pinned_post_locator=urn:asset:d5a1244d-05fc-4e47-aa71-d8423f2a4b92&pinned_post_asset_id=5d4023a328b0fb067d70ed59&pinned_post_type=share"" rel=""nofollow"">South Hams</a>, the <a href=""https://www.bbc.co.uk/news/live/uk-england-devon-49083027?ns_mchannel=social&ns_source=twitter&ns_campaign=bbc_live&ns_linkname=5d40239d28b0fb067d70ed56%26No%20government-funded%20trees%20planted%20on%20the%20Isles%20of%20Scilly%20in%20eight%20years%20%262019-07-30T13%3A48%3A41.226Z&ns_fee=0&pinned_post_locator=urn:asset:36c2ec48-d375-45c6-98d1-e7ffe17a1be9&pinned_post_asset_id=5d40239d28b0fb067d70ed56&pinned_post_type=share"" rel=""nofollow"">Isles of Scilly</a>, and another about <a href=""https://www.bbc.co.uk/news/live/uk-england-devon-49083027?ns_mchannel=social&ns_source=twitter&ns_campaign=bbc_live&ns_linkname=5d4023b751ae87065597c852%2620%2C000%20government-funded%20trees%20planted%20in%20Exeter%20in%20eight%20years%20%262019-07-30T13%3A45%3A01.105Z&ns_fee=0&pinned_post_locator=urn:asset:80b0d07f-8ee2-4831-977b-5b6306c375ac&pinned_post_asset_id=5d4023b751ae87065597c852&pinned_post_type=share"" rel=""nofollow"">Exeter figures</a>     "," We were looking at a subject and a theme for coverage but also trying to build knowledge about how we can use data and technology to power local journalism in the future.   Just because climate change is of huge interest, particularly for younger audiences, does not mean that every story about it will connect with people.   The project therefore had to be relatable.    We knew that the issue of tree planting and tree removal had been of great interest after protests about felling in Sheffield, Yorkshire.   So the task was to find data that showed what was happening with trees across the country and then make that relevant to our audiences.   This was where the idea of natural language generation came in, using shorter stories on the rates of planting since 2010 as a way into the story for our audiences.   A technical challenge of this was that stories had to be tagged with a relevant location identifier in order to appear on the BBC’s online local topic pages.   As part of the process, BBC News Labs built a tool to preview stories and the related tags before they were sent to the BBC’s online news content management system for publication.   The logistics involved in publishing such a high volume of localised stories in one day was also a challenge, with the output of stories on the BBC publishing system more typical of a large live news event. "," It is possible for data to tell audiences how a story is locally relevant to them.    This is often done with interactive maps, dashboards and postcode lookup tools. However, with natural language generation we have the opportunity to provide entire stories that are locally relevant.   We can unlock public datasets and tell them in a language, style and tone that makes them accessible to audiences.   Far from being any sort of replacement for journalists, this is a means of enhancing the work they would already do to open up the data that was previously only there for those who knew where to find it.   Automated content can be combined with more in-depth reporting to play to the strengths of both, making stories more relevant to a wider range of audiences, without having to rely solely on automated versions of the story.   The story also demonstrates the role that liveblogs and other rolling coverage platforms (e.g. Facebook pages) can play in serving automated content to bring new audiences to in-depth content.   This was an experimental piece for BBC England but the technology has been used by other teams in the BBC's general election results coverage, with semi-automated stories providing the constituency by constituency results.   The trees project is also a story of collaboration between newsrooms.   The scripts we wrote and sent out to colleagues gave them access to the data relevant for the local audiences they served without requiring them to sort and filter spreadsheets or try to piece their story together from multiple rows of a CSV. ",https://www.bbc.co.uk/news/uk-england-47541491,https://github.com/BBC-Data-Unit/tree-planting,https://bbcnewslabs.co.uk/news/2019/salco-trees/,https://bbc.in/370kSi9,https://bbc.in/3a88gqP,,,"Daniel Wainwright, Rob England, Paul Bradshaw, Conor Molumby","  Paul Bradshaw  works as a consulting data journalist with the <a href=""https://github.com/bbc-data-unit"">BBC England Data Unit</a>, and also runs the <a href=""http://www.bcu.ac.uk/media/courses/data-journalism-ma-2018-19"">MA in Data Journalism</a> and the <a href=""http://www.bcu.ac.uk/courses/multiplatform-and-mobile-journalism-ma-2018-19"">MA Multiplatform and Mobile Journalism</a> at Birmingham City University. A journalist, writer and trainer, he has worked with news organisations including The Guardian, Telegraph, Mirror, Der Tagesspiegel and The Bureau of Investigative Journalism, and his awards include the CNN MultiChoice Award for an investigation into people trafficking in football. He publishes the <a href=""https://onlinejournalismblog.com/"">Online Journalism Blog</a>, is the co-founder of the award-winning investigative journalism network HelpMeInvestigate.com, and has been listed on both <a href=""http://Journalism.co.uk"">Journalism.co.uk</a>'s list of leading innovators in media, and the US Poynter Institute's list of the 35 most influential people in social media.   His books include <a href=""https://leanpub.com/scrapingforjournalists"">Scraping for Journalists</a>, <a href=""https://leanpub.com/spreadsheetstories/"">Finding Stories in Spreadsheets</a>, the <a href=""https://leanpub.com/DataJournalismHeist/"">Data Journalism Heist</a>, <a href=""http://leanpub.com/snapchatforjournalists/"">Snapchat for Journalists</a>, the <a href=""https://www.routledge.com/The-Online-Journalism-Handbook-Skills-to-survive-and-thrive-in-the-digital/Bradshaw/p/book/9781138791565"">Online Journalism Handbook</a> and most recently <a href=""https://www.routledge.com/Mobile-First-Journalism-Producing-News-for-Social-and-Interactive-Media/Hill-Bradshaw/p/book/9781138289314"">Mobile-First Journalism</a> with Steve Hill. He can be found on Twitter <a href=""https://twitter.com/paulbradshaw"">@paulbradshaw</a>.    Daniel Wainwright  leads the BBC England Data Unit, having been a founder member of the team in 2015. He specialises in original data-led projects that are often shared across BBC regional and network news. He began his career at the Express & Star, which at the time was Britain's biggest selling regional newspaper, rising to the role of political editor and was named 2014 columnist of the year at the Midlands Media Awards. His work at the BBC has seen him host podcasts on data journalism for the BBC Academy and give a keynote speech for the European Forum for Geography and Statistics Conference in 2019. He is on Twitter <a href=""https://twitter.com/danwainwright"">@danwainwright</a>.    Rob England  has been working for the BBC England Data Unit for just over a year, unearthing original stories on a wide range of topics from climate change to homelessness. He was previously a reporter for BBC News online in the South West - covering Devon, Cornwall and the Channel Islands - an assistant producer at various BBC radio stations in southern England and a researcher for investigative programmes for ITV. In 2016, Rob graduated with an MA in Broadcast Journalism from Cardiff University. He is on Twitter <a href=""https://twitter.com/England_Rob_"">@England_Rob_</a>.    Conor Molumby  is a journalist in BBC News Labs. His work focuses on data, automation, newsroom tools and new audience experiences. He previously worked as a business news reporter and in enterprise software. ",,,
Ireland,The Irish Times,Big,Participant,Best data-driven reporting (small and large newsrooms),How green are Ireland's homes?,12/06/19,"Investigation,Database,Open data,Infographics,Environment",Google Sheets, This project examined how energy efficient Ireland's homes are using data published by Ireland's Central Statistics Office (CSO) from 2009 to 2019. The data as presented in the article allowed readers to understand how efficient houses are both on a national and local level by utilising the data collected on homes which have had energy assessments carried out. The data also documented the dominant type of heating used in each county.  ," With climate change to the forefront of many people's minds, this story provided an insight both into the energy efficiency of Ireland's homes and also showed how reliant the country is on using fossil fuels to heat them. Analysing the data showed that just one in 20 homes has the top energy rating of A, the majority of these in Dublin, the capital. The county with the lowest number of A rated homes was in Leitrim, the least populated part of the country, which along with Roscommon, also had the  highest proportion of G rated homes (the least energy efficient rating) in the country. While analysing the data showed discrepancies in the energy efficiencies between different counties, it also showed the differences in energy ratings within each of the Dublin postcodes showing that older, more ""established"" areas (ie Dublin 4), had less A rated houses than ""newer areas"", further away from the city, such as Dublin 13 which had the highest number of A rated houses in Dublin.     "," The primary tool I used for this project was Google Spreadsheets. I downloaded the information from the CSO website and carried out all cleaning and analysis using this programme. The cleaning primarily involved grouping different ratings together to provide an easy to understand picture of each rating (ie merging together A1 and A2 ratings to give an overall A rating for each county). This was done in line with the SEAI's own categorisation of ratings (ie A, B, C). I then ran simple analysis by filtering the various columns to ascertain the counties with the highest - lowest energy ratings etc. I also ran calculations to ascertain the number of houses and apartments BERs had been carried out in so I could inform the reader ie, ""One-fifth of all assessments have been carried out in apartments, with the rest being houses"". I also used Google Spreadsheets when analysing the main space heating fuel used in each home and again used the filter option to ascertain if there were any fuels that were more dominant in some counties than others (ie the reliance on solid fuel in Offaly). Datawrapper was then used to create the graphs.  "," There has been much talk in Ireland, as in most countries, about climate change and the steps needed to reduce our carbon footprint. I believe this article should be considered as retrofitting of homes and making them more energy efficient is one of the issues that comes up often in the context of fighting climate change and as such I felt it necessary to examine how efficient, or indeed inefficient, our homes really are and provide some context through data around the issue. I believe the article does just that; gives an insight into the true energy efficiency of our homes on a national scale and also provides insight into our reliance on fossil fuels (particularly gas and oil) to heat our homes. I believe this project adds to the important conversation around climate change and analysing and presenting the data as per the article gives people a greater understanding of not just the scale of retrofitting that may be required to reduce our home's energy consumption and therefore carbon footprint, but also how their home, particularly for those with a low rated home, is having some impact on emissions. The most difficult part of the project was the condsieration given to how to present it in an easy way for the reader. While maps were considered and tested, presenting the graphs in the format found in the article was found to be the easiest way for the viewer to get to grips with the data. ", From this project other journalists could learn the value of looking past the press releases issued by bodies such as the Central Statistics Office and examining the vast amount of data that informs them to see what other stories can be gleaned from the data.  ,https://www.irishtimes.com/news/environment/how-green-are-the-homes-in-your-county-only-one-in-20-houses-has-top-energy-rating-1.4106088,https://www.cso.ie/en/releasesandpublications/er/dber/domesticbuildingenergyratingsquarter32019/,https://docs.google.com/spreadsheets/d/1c5ejoXWnRkidQYEaQbwepVS3mOnT90LJszMqFpm7wTo/edit?usp=sharing,https://docs.google.com/spreadsheets/d/12l8SQxL0omnI8dCagOmPkxNBa6vRC6OnFpf_yDxZPDs/edit?usp=sharing,,,,Nora-Ide McAuliffe," Having graduated with an MA in Journalism from Dublin City University in 2015, I have worked with The Irish Times as a digital production journalist since 2016 on the breaking news team. In 2019 I graduated with a professional diploma from a one year part-time data journalism course at University College Dublin. While still a digital production journalist, I now also work on data projects covering a wide range of topics. <a href=""https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302"">https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302</a> ",,,
Ireland,"The Irish Times, University College Dublin",Big,Participant,Best data-driven reporting (small and large newsrooms),What has your MEP been doing for the last five years?,21/05/19,"Investigation,Database,Chart,Elections,Politics","Scraping,Google Sheets"," In the run up to the European Parliament elections in May 2019, this project highlighted the ""performance"" of Irish members of the European Parliament (MEPs) under eight different areas using data collected by the European Parliament from 2014-2019 and made available on their website under each MEP's profile.  "," The impact of this project highlighted to readers how active their MEP had been in the past five years under areas such as attendance, contributions to plenary debates, their work as rapporteur or shadow rapporteurs on reports and opinions, the number of motions for resolution put forward etc. While the article did not paint a whole picture of all of the MEPs activities during their tenure, it did give readers an understanding of how their MEP performed in certain areas compared to other Irish MEPs from their constituency or outside of it. The article was published three days before the European election while candidates were still canvassing for support.  "," I primarily used Microsoft Word, Google Sheets, ParseHub and Datawrapper. As the data was collected by the European Parliament and made available on its website under each MEP, in some cases it was possible to copy and past the information into Google Sheets and count up for example, oral questions or motions for resolution and get the number for each MEP in this way. To ascertain the attendance of each MEP, I clicked into each session of the parliament from 2014-2019 (http://www.europarl.europa.eu/plenary/en/minutes.html#sidesForm), pasted each session into a Word Document, searched each MEPs name which gave me the number of times each MEPs name appeared and then went through entry to check if they had been marked as absent or present. ParseHub was used in particular to ascertain the number of Written Explanation of Vote by each MEP due to the sheer number of explanations provided and the format it was provided in by the European Parliament which was not conducive to copying and pasting into a Google Spreadsheet due to the way it was presented on the website.  "," Compiling the database was quite time consuming due to the amount of copying and pasting required, particularly in capturing the attendance data which involved individually clicking into each parliamentary session over five years and copying and pasting the information into the Word Document. Figuring out the exact set of commands in order for ParseHub to collect the information I wanted was also quite challenging as it took me a while and numerous readings of explainers and YouTube videos before I found the right combination that I needed. I believe this project should be selected as it used readily available data in an interesting and fresh way to show readers something that they otherwise would not have known about unless they went through each of the MEPs profile on the European website themselves and had days to spare doing so.  Further, for those MEPs seeking re-election, it gave voters something other than the candidates' manifesto to consider as they could see they could easily see the candidates' record in certain key areas, though the data presented. ", Other journalists can learn that sometimes a combination of simple tools such as Google Spreadsheets and Microsoft Word can be used effectively to navigate through data.  ,https://www.irishtimes.com/news/politics/what-has-your-mep-been-doing-for-the-last-five-years-1.3899736,https://www.europarl.europa.eu/meps/en/home,https://docs.google.com/spreadsheets/d/1lV3RkiV-L1zk2pyGndwtvOoz5BP_bRwHrHOlLx9Eago/edit?usp=sharing,https://docs.google.com/spreadsheets/d/1q6rnOMVxc1Wqgl8n9xZKl9P9w0D_Zw0-6mYus18_frI/edit?usp=sharing,,,,Nora-Ide McAuliffe," Having graduated with an MA in Journalism from Dublin City University in 2015, I have worked with The Irish Times as a digital production journalist since 2016 on the breaking news team. In 2019 I graduated with a professional diploma from a one year part-time data journalism course at University College Dublin. While still a digital production journalist, I now also work on data projects covering a wide range of topics. <a href=""https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302"">https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302</a> ",,,
Ireland,The Irish Times,Big,Participant,Best data-driven reporting (small and large newsrooms),Divorce rates in Ireland,21/05/19,Investigation,Google Sheets," In the run up to the referendum on divorce in Ireland in May 2019, this project set out to look at rates of divorce in Ireland since 2000, four years after divorce was legalised in 1996. Before the May referendum (which passed), to apply for a divorce, couples had to be living apart for four of the five previous years and the 2019 referendum asked voters if they wish to remove this condition or keep it in the constitution.  ", The impact of the project was that it showed readers what the rate of divorce was really like in Ireland and showed the counties with the highest and lowest rates of divorce from 2000-2017.  This gave voters greater context around divorce in Ireland in the lead up to the referendum. It also highlighted to readers the divorce rate in their own county from 2016-17 and the number of applications received to give them a snapshot of divorce  in their own area. It also informed people of the number of divorce applications from 2000-2017 to see how the numbers have fluctuated over time.  , For this project I used divorce data collected by the Court Service of Ireland and used Google Sheets to create the database and to carry out the analysis. I also used population data from the Central Statics Office in order to calculate the rate of divorce per 1000 population in each county.  , This was a relatively straightforward data story that did not include many challenging elements.  , NA  ,https://www.irishtimes.com/news/social-affairs/carlow-has-highest-divorce-rate-in-ireland-data-shows-1.3898759,http://www.courts.ie/courts.ie/library3.nsf/pagecurrent/80c3478a9bd754f680258014002e8c05?OpenDocument&Start=1&Count=1000&Expand=21&Seq=1,https://docs.google.com/spreadsheets/d/1Qc8xMYVsORSE0IjFV-NT1_q64JdZCP_MXxvJ0pu3F40/edit?usp=sharing,,,,,Nora-Ide McAuliffe," Having graduated with an MA in Journalism from Dublin City University in 2015, I have worked with The Irish Times as a digital production journalist since 2016 on the breaking news team. In 2019 I graduated with a professional diploma from a one year part-time data journalism course at University College Dublin. While still a digital production journalist, I now also work on data projects covering a wide range of topics. <a href=""https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302"">https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302</a> ",,,
"Hong Kong S.A.R., China",South China Morning Post,Big,Shortlist,Best data-driven reporting (small and large newsrooms),100 days of protests rock Hong Kong,17/09/19,"Explainer,Long-form,Breaking news,Database,Fact-checking,Mobile App,Illustration,Infographics,Chart,Video,Map,Politics","Animation,QGIS,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,OpenStreetMap"," “100 days of protests rock Hong Kong” was created to help readers navigate a fast-moving and complex story. As many details began to get blurred in the noise and chaos surrounding the protests in Hong Kong, our graphics team set out to bring clarity and inform a curious international audience about the underlying causes.   The protests began on June 9, 2019, when a million people demonstrated against an amendment to an extradition bill and quickly snowballed into a mass leaderless anti-government movement. The events that followed captured the world’s attention. "," Using data we were able to provide readers with an impartial bird’s-eye view of the first three months of the civil disturbances that captured the world’s imagination.    To date the piece has received more than 57,000 unique views and a 4:50 minute read-time. Journalists and academices have told us they regularly refer to it in order to verify the sequence of events. The story also resulted in invitations for the team to talk at various international conferences and events "," We also analysed video footage and photographic records, exploring and filtering the data manually. We categorised the type of violence and weapons used by both sides on a case-by-case basis and built a meter to indicate to readers the level of violence showing how it ebbed and flowed.    We combined on-the-ground reporting with charts, maps, DataViz, illustrations, videos and photographs to create a visual narrative and timeline.   To incorporate every protest into a single story required the use of almost 100 photos for visual evidence. We also employed time lapse videos to record the mass protests that brought the city to a virtual standstill. Twenty locator maps compare legal and illegal demonstration routes and we also recorded the location of all demonstrations to show how quickly the protests spread from the Central business district to new towns bordering mainland China. More than 14 explanatory diagrams were created to analyse the use of tear gas, laser pointers, rubber bullets etc. ", The hardest part of this project was keeping up with the fast-moving events. We needed to plan ahead so that we could realease the story on the annivesary of the 100th day of the protests. ," We believe ""100 days of protests rock Hong Kong"" is a great example of how to use data to keep an emotive narrative impartial ",https://multimedia.scmp.com/infographics/news/hong-kong/article/3027462/hong-kong-100-days-of-protests/index.html?src=arcade,,,,,,,"Pablo Robles, Darren Long and Dennis Wong", The South China Morning Post’s graphics team is based in Hong Kong and brings together a team from four continents with diverse backgrounds to tell stories visually. Our goal is make the complex simple. ,,,
Ireland,The Irish Times,Small,Participant,Best data-driven reporting (small and large newsrooms),Has the number of Irish Ministers going abroad for St Patrick's day increased?,03/12/19,"Investigation,Politics",Google Sheets," Each St Patrick's Day (March 17th) Irish ministers and other government members go abroad to promote Ireland and maintain links with Irish emigrants around the globe. This project looked at these visits since 2011-2019 (during the Fine Gael administration) and found the number of countries visited by the Government on St Patrick’s Day has increased sevenfold since 2011 with a greater focus on Europe, Asia and South America.  "," This project highlighted the changing and expanding nature of the Irish government's St Patrick's Day visits abroad since 2011. It highlighted how not only how the number of countries visited has expanded, but hand in hand with that, how the number of government members sent overseas to represent Ireland, has also increased significantly.  "," Google Sheets, Datawrapper and Tableau were the main tools used for this project. Google Sheets was used to create the working database and used for simple analysis on the data collected. Datawrapper and Tableau were used to create the charts for the article.  "," The most difficult part of the project was compiling the database. The information was collected using government press releases, government expenses, individual minster’s newsletters, and Irish Times news articles to ensure accuracy of countries visited and the minsters who visited them. These measures were needed as often the final destinations or the ministers’ itineraries were changed (ie a Minister due to go to France may have ended up not goign at all due to an issue that cropped up in the days leading up to the trip) and a new press release was not issued with the updated information and so ensuring accuracy involved going beyond relying on the initial government release. I believe this project should be considered as it was a very fresh take on the annual St Patrick's Day trips abroad and the data showed how the St Patrick's Day  programme has been expanded in recent years to not only include a greater number of US cities, but also the Middle East and South America.  ", Other journalists can learn that a story being covered each year will allow for a lot of data on the matter to be built up and that can allow for a fresh angle on the issue.  ,https://www.irishtimes.com/news/politics/mapped-where-do-all-the-ministers-go-on-st-patrick-s-day-1.3822975,,,,,,,Nora-Ide McAuliffe," Having graduated with an MA in Journalism from Dublin City University in 2015, I have worked with The Irish Times as a digital production journalist since 2016 on the breaking news team. In 2019 I graduated with a professional diploma from a one year part-time data journalism course at University College Dublin. While still a digital production journalist, I now also work on data projects covering a wide range of topics. <a href=""https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302"">https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302</a> ",,,
Ireland,The Irish Times,Big,Participant,Best data-driven reporting (small and large newsrooms),How many hours a day is your police station open?,17/08/19,"Investigation,Database,Map",Google Sheets, This project looked at the opening hours of Garda (police) stations in Ireland to ascertain what opening hours around the country were truly like. My analysis found that almost 60 per cent of Garda stations are open for three hours or less Monday to Friday while many are closed at the weekend.  ," Cuts to Ireland's police force and in particular the rise of crime in rural Ireland has been a top issue in Ireland for the last number of years. While much has been reported about the full closure of Garda stations, cuts to personnel and resources, the issue of opening hours in stations that are still functioning has been less reported on. The impact of this project was to illustrate what opening hours in areas that have Garda stations are truly like to allow people gain a better understanding of how frequently or indeed infrequently their station is open. By illustrating the data through a map, readers' were not only able to look at specific opening hours for all stations around the country, they were also, due to the colour scheme of the map, able to see the spread of 24 hour Garda stations and their locations.  "," I used Google Spreadsheets to create the database and to run the data analysis in this project. The main calculation techniques I used were addition and percentage calculation to find out the number of stations open one hour, two hours etc. Tableau was used to visualize the data.  "," There were a few challenging aspects to this project. One was the creation of the working database as this involved copying and pasting the information from the 564 Garda station directory made available online, into Google Sheets which took a considerable amount of time. Another challenging aspect was having to phone some Garda stations to confirm their opening hours (as the Garda press office could not confirm) as it was evident some of them had not been entered incorrectly (ie giving hours as 2am to 6pm in rural areas which I believed seemed strange, this for example usually turned to to be 2pm -6pm) but trying to get hold of a garda in the station to confirm the correct hours, was, in some cases difficult and took a number of attempts, often over a few days, due to the limited opening hours etc. Also due to the nature of the data and the database I created, calculating the opening hours of each station had to be done manually and so this was also quite time consuming when coupled when having to double and triple check the work. I believe this project should be selected as it puts the much disucssed issue of Garda resourcing into a data context in a way that has not been done before. Through the data, readers can get a real insight into Garda opening hours and what the current state of play is regarding this aspect of policing on both a local and national level.  ", NA  ,https://www.irishtimes.com/news/crime-and-law/six-in-10-garda-stations-open-for-three-hours-at-most-on-weekdays-1.3988627,https://www.garda.ie/en/Contact-Us/Station-Directory?preview[theme_id]=201040452&preview_as_role=end_user&use_theme_settings=false?p=u,https://docs.google.com/spreadsheets/d/1GlRDEqLOcJ8hGFlEMJ3Ic7S1EAOKNVWF7BMJCqRmjDo/edit?usp=sharing,,,,,Nora-Ide McAuliffe," Having graduated with an MA in Journalism from Dublin City University in 2015, I have worked with The Irish Times as a digital production journalist since 2016 on the breaking news team. In 2019 I graduated with a professional diploma from a one year part-time data journalism course at University College Dublin. While still a digital production journalist, I now also work on data projects covering a wide range of topics. <a href=""https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302"">https://www.irishtimes.com/profile/nora-ide-mcauliffe-7.4138302</a> ",,,
Kenya,The Elephant (www.theelephant.info),Small,Participant,Best data-driven reporting (small and large newsrooms),Who is Policing the Police? Kenya's Lame Duck Oversight Mechanism,12/05/19,"Investigation,Explainer,Solutions journalism,Long-form,Database,Open data,Fact-checking,Politics,Gun violence,Human rights",Personalisation,"After years of clamoring by the civil society and other actors within the human rights fraternity, Kenya eventually established a civilian led statutory Independent Policing Oversight Authority (IPOA) in 2012. The agency was mandated to oversee the working (and reforming) of the country's police service, its core designation being receiving complaints from vicitms of police excess. At the end of its inugiral six year term, IPOA had received nearly 10,000 complaints against the police, and had secured a paltry 3 convictions, a performance which elicited heavy criticism from the Kenyan public. This piece investigates the workings of IPOA using data","No individual piece of journalism either by a Kenyan or foreign journalist has ever reported on the workings of the country's Independent Policing Oversight Authority (IPOA) the way this piece of longform reportage did as will be evidenced in its ten parts, where each segment speaks to a specific component of what makes IPOA what it is, bottlenecks and all. For starters, in the course of researching on the piece, I made contact with the leadership of the agency requesting evidence based answers as to how IPOA is working to address the now over 12,000 complaints by aggrieved civilians against the Kenya police, seeing that the agency now has a new management board which took its leadership in late 2018. These querries forced the agency to compile a 15 page report, which was an exclusive. As such, since the release of IPOA's 2018 end of term report when it's inaugural board was exiting office, the report given to me by the agency was the second comprehensive communication the agency was giving to the Kenyan public, albeit through a journalist. The leader of the agency's technical committee told me as much. This means that apart from being the go to piece of journalism by anyone looking to read on Kenya's struggling police oversight mechanism (including by a UN Special Rapportuer who commended the reporting), this project became the trigger which jolted IPOA back into action, a reminder that the public is watching, and that if they imagined no one was questioning their workings, then my querries were a reminder that someone had an eye on them. The project forced IPOA to do what it is supposed to do but it wasn't doing, which is to release substantive progress reports, the one instigated by me being their second comprehensive report in 8"," The push for the establishment of a civilian led police oversight mechanism in Kenya had been in the works for decades before the country's 2010 inaugurated new constitution provided for such a provision in law. It however took a further push and pull for the agency to finally take shape two years later in 2012. As such, reporting on this subject meant going back memory lane, meaning speaking to civil society players who had been part of the journey and revisiting tens of relevent reports, including WikiLeaks cables which evidenced the push by both the American and British embassies in Nairobi for the establishment of such an agency while the Kenyan state resisted. I therefore dug up a lot of historical references to illustrate the long journey travelled.   The second step was for me to assmeble data collected since the 2012 establishment of the agency, where up to date there have been over 12,000 verified reported cases of police malpractice, a good percentage being cases of extrajudicial killings in Nairobi slums. This data was then juxtaposed with the historical demand for IPOA, yet upon its establishment, the agency has so far managed only 6 convictions, to mean that either the agency isn't fit for purpose, or as some suspect, it was set up to fail.   After visiting the archives and connecting the dots from the need for IPOA, its establishment, the monumental number of cases in its hands, and its documented poor performance, I then investigated why the agency was under perfroming, backing this up with corresponding data including low budgets and a lack of adequte members of staff. I then concluded by weaving in recommendations, based on police recruitment data, which showed a flawed recruitment pattern. The project therefore not only reported IPOA's failures, but offered a way forward.  "," One of the hardest element in this project was the opaqueness of both the Kenya Police and the Independent Policing Oversight Authority (IPOA), such that no individual employee of either the police or IPOA (including their spokesmen) wishes to be quoted on record for speaking for their respective institutions. This therefore meant canvasing through the use of personal referals and the likes, to a point where the IPOA had to convene a board meeting for them to respond to my querries since non of its board members wished to be seen to be the ones speaking on behalf of the agency.   After going around the opacity, the other lingering challenge remained the eternal threat to journalists seen to be poking their noses in police malpractices, since it exposes both the police as law breakers while simultenously blowing the cover of mitigating agencies such as IPOA and the Kenya police's internal affairs unit, both of which receive thousands of complaints against the police annually but are forever dragging their feet in prosecuting wrong doers. Tackling this subject is therefore seen as threatening the peace within the police and the oversight agencies, thereby putting the journalist's safety at risk.   Lastly, working on this project and looking at the attendant risks confimed the limited cover journalists have, since depite reporting these potential dangers to editors and grant givers (the project was as a result of a grant awarded to me by the Institute of War and Peace Reporting), it remained apparent that for the most part, journalists are left to their own devices. All that was offered were half hearted promises that in case of any danger some form of intervention would follow, yet there was no evidence or written commitment about the same, including their declining to offer relevant insurance policies.          ","The big take away from this project is that no matter how voluminous, sensitive and complicated the data is, applying the skill of narrative and weaving through the data set a step at a time eventually yields a readable, informative and enjoyable piece of journalism, even if the subject matter is sad and heartbreaking. This project reminds journalists that in staying true to the profession's primary role of informing/educating/entertaining the public and holding power to account, it is important to not forget that these should be coupled with the need to employ language, technology and similar aids so as to remain a good storyteller. For without getting audiences hooked/enagged to a story, then the story may not serve the purposes its meant to. The project therefore illustrates the need for getting the right mix of facts and figures and marrying them to elaborate storytelling, what one may call a win-win scenario. The second point the project presents is that data has to be made accessible to diverse audiences in the most simplified version possible without losing its essence and legitimacy. Sometimes, the whole concept of data journalism may seem inaccessible to either journalists or audiences, something which this project has successfully demystified through proper step by step context building in an informative and entertaining/readable fashion, eventually giving the heavy data a soft landing since it comes cushioned in relatable and accessible enablers. The other point this project enhances is that a lot of quality data is under ultilized. This is because in countries such as Kenya, a lot of governement agencies release piles of data domiciled in their websites, which no one hardly capitalizes on. This is therefore a reminder of the huge volumes of readily available data t utilization doesn't have to be boring or limited to . under",https://www.theelephant.info/features/2019/12/05/who-is-policing-the-police-kenyas-lame-duck-oversight-mechanism/,https://www.theelephant.info/features/2019/07/11/my-sons-are-dead-a-mothers-cry-for-justice/,https://www.theelephant.info/features/2019/08/08/dying-for-justice-who-killed-oscar-kingara-and-george-paul-oulu/,https://www.wikileaks.org/plusd/cables/09NAIROBI1014_a.html,https://www.the-star.co.ke/news/big-read/2018-05-13-weve-done-all-thats-humanly-possible--ipoa/,https://www.awaazmagazine.com/volume-16-issue-1/cover-story/item/1062-motion-without-movement-an-insider-s-reflection-on-policing-reforms,http://www.ipoa.go.ke/wp-content/uploads/2018/05/IPOA-BOARD-END-TERM-REPORT-2012-2018-for-website.pdf,Mwalimu Mati," Isaac Otidi Amuke’s Op-Eds, longform reportage and nonfiction have been published by the literary journal  Kwani? (Nairobi) , Commonwealth Writers (London), Wasafiri (London), World Policy Institute (Washington, D.C.), Adda Stories (London), Solitude Atlas (Stuttgart), The New African Magazine (London), The Chimurenga Chronic (Cape Town), BBC Storyworks (London), The Mail and Guardian (Johannesburg), The Sunday Nation (Nairobi) and The Elephant (Nairobi). He contributed  Safe House , the title piece for '' Safe House; Explorations in Creative Nonfiction''  (Dundurn/Cassava Republic 2016) – an anthology of nonfiction from Africa edited by Ellah Wakatama Allfrey. He received the 2013 Jean Jacques Rousseau Fellowship from the Akademie Schloss Solitude in Stuttgart, Germany, was a finalist for the 2016 CNN Multichoice African Journalist of the Year Awards in Johannesburg, South Africa – where he received the Highly Commended Features Award – and was shortlisted for the Gerald Kraak Award in 2018 in Johannesburg, South Africa. He has previously attended a number of writing workshops including the 2014 Commonwealth Writers Nonfiction Workshop in Kampala, Uganda, the 2015 Farafina Creative Writing Workshop in Lagos, Nigeria, and the 2015 African Writers Trust (AWT) Editorial and Publishing Workshop in Kampala, Uganda. In 2017, he was commissioned by Penguin Random House to co-author the forthcoming memoir of South Sudanese actor, model and UNHCR Goodwill Ambassador Ger Duany. In 2019, he received a double grant from the Institute of War and Peace Reporting (IWPR), resulting in five pieces of longform journalism focusing on criminal human rights reporting published by The Elephant. He works as a freelance journalist in Nairobi, Kenya and is working on a memoir on asylum seeking among other nonfiction book projects.     ",,,
"Hong Kong S.A.R., China",South China Morning Post,Big,Participant,Best data-driven reporting (small and large newsrooms),Hong Kong protests: key events of six months of anti-government anger,12/09/19,"Explainer,Long-form,Open data,Illustration,Infographics,Chart,Video,Map,Politics","Scraping,QGIS,Canvas,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV"," Hong Kong’s streets have been consumed by chaos and destruction since the anti-government movement sprang into life in June. Protesters have locked horns with riot police in often violent clashes. We used data to show how the crisis has seen the transport system paralysed and businesses with mainland China links attacked, while the government continues to refuse to cede to all five demands from protesters. "," By combining on-the-ground reporting with charts, maps, DataViz, illustrations, videos and photographs we were able to create a visual narrative and timeline to bring clarity and inform a curious international audience about what was happening in Hong Kong. The data also provided an impartial bird’s-eye view of the first six months of the civil disturbances that captured the world’s imagination.  "," To report the “Key events from Hong Kong’s anti-government protests” we monitored daily coverage over six months of protests that roiled Hong Kong. We analysed live coverage from our frontline reporters and studied photos and videos from our own journalists as well as other trusted news sources and combined them with daily police briefs.   To build this project we created a DataViz for each day and separated events into peaceful protests, violent protests and government announcements which included data such as the numbers of arrests, injuries, tear gas rounds and rubber bullets fired and public transport disruptions and train stations vandalised. "," As the protests dragged on, lurching between peaceful protests and violent clashes, many details began to be lost in the noise as the government, police and protesters tried to to control the narrative for their own ends. The South China Morning Post graphics team addressed this by combining objective and factual data with photographic and video evidence to distil events to their essence. We used the scale of the video/photo to indicate the level of violence of an event – the larger the image the greater the level of violence. We made sure to include all events and provide context to avoid bias to police or protesters ", All the data was collected manually and interpreted visually to show readers key events for the first six months of protests. Reporting the data this way makes it easy for readers to understand the complex series of events at a glance. ,https://multimedia.scmp.com/infographics/news/hong-kong/article/3032146/hong-kong-protests/index.html?src=arcade,,,,,,,Pablo Robles, Pablo Robles joined the South China Morning Post from Costa Rica in Setember 2017 to help strengthen the graphics team's digital output. Robles focuses on interactive solutions to create immersive experiences that push the boundaries of digital storytelling. ,,,
France,mind Media (Groupe mind - Frontline Media),Small,Participant,Best data-driven reporting (small and large newsrooms),Adtech Finder,29/11/19,"Explainer,Database,Chart,Business","Scraping,Google Sheets,CSV"," mind Media is a French business outlet dedicated to analyze online advertising and companies specialized in advertising technologies. We have developed a tool that allows us to know, for each website that sells online advertising, the list of some of the service providers to which it is connected. It allows us to follow the impact of the main adtech trends and the relationships between its actors. We have drawn many articles from it since 2017. And we've developed an interface to let our subscribers to directly query our database for their own analysis.  ","  Context : online advertisement is now powered by tracking tchnologies, open auctions and managed by trading desks. Transparency is a legal requirement and the ability to identify and monitor technology providers gives an overview of a publisher's strategy.   The analysis of these files allows us to shed light on the market on several points: what technological architectures have the publishers implemented to sell their inventories (video, banners, native advertising) on desktop and mobile web? How many resellers do they work with? With which SSPs have they signed a commercial contract? Our study showed that a large number of SSPs are competing for the inventory of French publishers, despite a concentration around the main sellers in the market. We also showed that the sub-agencies generally work with the same SSPs as the publishers themselves.    We also use it as a thermometer to track certain trends in online advertising. French publishers are increasingly adopting header bidding? The number of their sales partners continues to grow, with an increasing use of resellers.    This tool also allowed us to reveal the names of the first publishers to connect to Eyeo's ad exchange, the German company that ironically is behind the world's most used adblocker, Adblock Plus. We also discovered the signature of some contracts between adtech companies and ad agencies, such as Deezer with the Triton Digital audio adexchange, or the presence of DMP Weborama at a Dutch publisher, the Telegraaf Media Groep, for ""an R&D project"". "," In 2017, adtech did not shine by its transparency and it was urgent to reassure advertisers who were struggling to understand the workings of programmatic advertising (automation of transactions) and saw a significant portion of their investments captured by fraudsters. A practice in vogue at the time, known as ""domain spoofing"", consisted of usurping publishers' domain names in order to pass themselves off as the publishers to the brands and thus take a share of their advertising revenue.    To remedy this, the Internet Advertising Bureau (IAB), the professional association in charge of putting order in online advertising, created the Ads.txt system in May 2017: each publisher is required to write on a text file at the root of its site - for example at the url lemonde.fr/ads.txt address - the list of service providers (SSP (supply-side platform) and adexchanges (market place for online advertising)) that it authorises to sell its advertising space. Buyers thus know that if a service provider who is not on the list tries to sell it to them, they are probably dealing with a fraudster.    At the beginning of 2018, to make it easier for us to keep track of the subject, we developed a crawler that visits more than 800 websites every night, which we have associated with nearly 400 publishers in the United States and Europe. In order to obtain usable statistics, several long correspondence tables, which we have created ourselves, make it possible to associate several advertising domains with a single adtech and several websites with a single publisher.    We export the results in .csv format in order to analyse them with Google Sheet and Excel. "," A crawler automatically imports into a database, every night, the ads.txt file of 827 websites (the list of URLs can be found here), operated by 368 content publishers from nine of the world's largest advertising markets: the United States, the United Kingdom, France, Germany, Italy, Spain, Portugal, Belgium and the Netherlands.    In each of these countries, we have relied on audience sources (Similar Web) and selected the most important publishers, often members of a professional organization, such as Digital Content Next in the United States, the Association of Online Publishers in the United Kingdom, or the Asociacion de Medios de Informacion in Spain. In France, we have also included several e-commerce sites - those with the largest audiences - whose revenues are increasingly derived from advertising.    We currently crawl the sites of 124 publishers in France, 76 in the United States, 37 in the United Kingdom, 23 in Germany, 44 in Italy, 22 in Spain, 12 in Portugal, 15 in Belgium and 13 in the Netherlands.    We made sure to link each URL to a publisher, each publisher to a category (media, e-commerce...) and to a country (France, USA...). In the case of France, we even went so far as to specify the control room associated with a publisher.    Similarly, to make it easier for Ads.txt Scan users to read the relationships between publishers and service providers, we have associated each advertising system domain name with the company that owns it (271 at the end of August 2018). For example, if appnexus.com is displayed in an ads.txt file, the user interface of our monitoring tool will display Xandr. As a result of mergers and acquisitions in adtech, a provider may be attached to several advertising domain names, not always very explicitly. "," Market intelligence and the path to exclusive content for B2B medias is directly linked to our ability to monitor online datas and traces. As an organization strategy the integration of data analysts in the newsroom should be mandatory. In some ways a new tradition of modern muckrackers is born.    This project, as well as several others initiated by mind Media, illustrates how journalists can use the digital traces left by companies that experiment open data somewhat in spite of themselves, to get around the communication wall. This allows us to better understand their activity and to shed light on certain practices that are rarely known to the uninitiated, by finding answers to questions that companies never address in their press releases and press conferences, and about which they deliberately remain vague in interviews.    Our newsroom has a dozen journalists, each specialized in covering relatively opaque sectors (media and online advertising, finance, health, etc.). The data-journalism unit has trained and sensitized them so that they know how to identify data sources that are not very visible to the uninformed eye: APIs, lists, Excel and PDF files hosted on the websites of professional unions, ministries, certifying bodies... We also sometimes scrape the HTML code from the websites of the companies we study or their accounts on social networks.   In addition to the Ads.txt files, we have for instance specified the type of data collected by online advertising actors thanks to another IAB initiative, the Global vendor list of the Transparency & consent framework. We showed which mobile advertising trackers, especially geolocation trackers, are installed in French media applications, thanks to the open platform Exodus Privacy.     ",https://www.mindnews.fr/article/17056/ads-txt-scan-1-3-les-editeurs-francais-ont-encore-augmente-le-nombre-de-leurs-vendeurs-et-revendeurs-programmatiques/,https://www.mindnews.fr/article/17008/ads-txt-scan-2-3-250-ssp-et-adexchanges-ont-acces-a-l-inventaire-d-au-moins-un-editeur-francais/,https://www.mindnews.fr/article/17018/ads-txt-scan-3-3-qui-sont-les-prestataires-qui-figurent-dans-le-fichier-des-editeurs-americains-et-europeens/,https://www.mindnews.fr/ads-txt-scan,https://www.mindnews.fr/article/12535/ads-txt-scan-presentation-et-methodologie/,,,Aymeric Marolleau," Aymeric Marolleau is in charge of data journalism within the group mind. In this position, he helps its four editorial teams (mind Media, mind Fintech, mind Health, Planet Labor) to produce data-driven articles and to put data at the heart of the work of its journalists. From 2015 to 2018, he was a specialized media and adtech journalist at mind Media. He has been an economic journalist for about ten years. In 2013, together with Capital magazine, he won the Prize for the Best Investigation as part of the ""Magazines of the Year"" awards, organised by the Syndicat des éditeurs de la presse magazine (SEPM), for a data-journalistic work on the remuneration of major French bosses. ",,,
Serbia,BIRN Serbia,Small,Participant,Open data,Mothers in Serbia feel the brunt of new law,29/05/19,"Solutions journalism,Open data,Crowdsourcing,Women",Microsoft Excel,"  BIRN team designed a digital maternity leave calculator after new Law on financial support to families with children was addopted leaving the public confused.  Citizens can use it for calculating compensations, finding forms and deadlines for submission. They can compare if they lost or gained with the new law, how to split the leave with a partner or how the law applies to a child with disabilities. The calculator is a central part of the article which explains the context in which controversial law was adopted, exposing some of its core flaws through six in-depth interviews with mothers.   ","  The article reached over 100,000 people through social media, the tool was used more than 4000 times during the first month.      We launched the data crowd-sourcing campaign through social media calling women to fill in the form and help us harvest the data.  A month after it was published, we managed to collect more than 4000 inputs.       In this way, we tried to measure the effects of the law as well as raise awareness among the public about this important issue.     This project helped us overcome the lack of official data. Instead of analysis, we offered our readership a tool to navigate through complex and unclear procedures.  This was our response in the situation where the Ministries denied access to reports and data about the implementation of the controversial law, despite the Commissioner's decision in our favor.     On the bright side, the project was recognized by Bebac.com, the largest website in Serbia on family planning and pregnancy with more than 250 000 registered users. The maternity leave calculator BIRN team developed is embedded on their website and the application.     The project was significant for decision-makers as well since they organized a working meeting with the civil society representatives a week after the article was published. Prime minister Ana Brnabic, Ministry of Labour, Employment, Veteran and Social Policy Zoran Djordjevic and Minister without portfolio responsible for demography and population policy Slavica Djukic-Dejanovic set together with more than 10 NGO's and showed interest in fixing problematic law.  ","  BIRN team analyzed Law on financial support to families with children and different maternity leave policies to develop an algorithmic model of the legal procedure. This was the first step in creating Baby's first abacus an online tool that compares the user's inputs with the legal norms.      Instead of informing our readers - while on the leave your compensation can not be higher than 3 average monthly salaries, we thought - type your monthly income here and see where you stand.      By developing this calculator we found a way to craft the information for each reader. Young families can use it to understand the basic steps of maternity leave procedures, deadlines, and forms. The ones with more children can inform themselves what the new law brings, whether they can apply for state aid and longer leave. Both can use it to compare the allowance between the partners and choose how to split the leave. Also, the calculator offers the users to compare whether they gained or lost with the law changes.      In the end, users are asked to leave their data for the research purpose as part of the first data crowdsourcing campaign. In the future, our goal is to make this code open-sourced so people from different parts of the world can improve and easily implement it.  ","  The hardest part of this project was finding relevant data. On the institutional level, our access was denied so we couldn't work with the relevant sources. The data crowdsourcing campaign was far from perfect too. It mainly reached women from two larger cities with above-average salaries meaning it wasn't helpful with large scale conclusions. Also, there were a lot of unfinished questionnaires or the ones with inconsistencies.     In that way, we failed to understand the bigger picture trough data, whether the new law made it worse for Serbian mothers on average.       On the other hand, our tool raised awareness among the women about the importance of open data, explaining them how it could be used for better understanding of problems our communities face and the legislative part that regulates it. We learned the sad lesson about our goverment too who failed to understand the importance of sharing the data and mesuring the effect of the laws with it.       The article we wrote offers answers to the questions mentioned, political and social context in which this change was made.  Interviews with women from different social backgrounds, areas, and salaries show they are feeling the same - like the system failed them when they need it the most.  ","  This project is a good example of doing data journalism in countries with closed governments.      Our initial idea was to obtain a dataset of maternity leave beneficiaries analyze it to answer questions such as - who are Serbian mothers today, where they live and work, how much they get while pregnant, is that more than they had before the new Law.       After a few failed attempts, when we asked the Minister of Labour and Social Issues why they are denying us access, he said: This data is important to the state and If we wanted you to analyze it, we would have called you to be a part of a working group.      At this point, we knew we should get creative. So, the main lesson we learned from this project was to use situations like this to our advantage and adapt along the way.      Our response was to make a digital maternity leave calculator that would serve as a questioner. In that way, we could meet the demand for information both on individuals and the level of community. We launched the first data-crowding campaign through social media and partner organizations to collect data directly from citizens and inform them about the importance of open data.  ",https://javno.rs/analiza/majke-u-srbiji-na-udaru-zakona,https://docs.google.com/document/d/1WLarcffXCTvXM4lR3ie2IxE_-gwYQxNQZib6QzOcJzw/edit?usp=sharing,,,,,,"Slobodan Georgiev, Milorad Ivanovic, Ana Curic, Natalija Jovanovic","  Editorial:     Slobodan Georgiev is Belgrade based journalist and he serves as a Programme Coordinator in BIRN Serbia. He works as a journalist in Vreme weekly from 2001 covering organized crime and sports.      Milorad Ivanovic is an Editor in Chief at BIRN Serbia in Belgrade. His investigations have included work on human trafficking, the employment of Balkan mercenaries by British and US security firms in Iraq, and arms trafficking from Ukraine into Serbia. He is also a jury member of the annual awards given by the Independent Journalists Association of Serbia, IJAS, and the US embassy in Belgrade.     Investigative reporter:     Ana Curic is an investigative journalist based in Belgrade. She started working in BIRN Serbia, which is part of Balkan Investigative Reporting Network, in 2017, where mostly covers misuse of public funds, justice and financial crime. She has a degree from the Faculty of Political Science at the University of Belgrade and speaks fluently English and Spanish. Ana was nominated for The Annual Prize for Investigative Journalism in Serbia 2019 for the series of articles about the illegal engagement of high officials from the ruling party at the Medical College in central Serbia.     Data journalist:     Natalija Jovanovic works as a journalist for BIRN Serbia. She is interested in finding stories through data and interactively presenting them. Natalija covers social topics covering maternity leave policies, culture or environmental issues. Her work also includes bringing tech innovation to local media. She is currently doing a master’s in Social Science and Computing at the University of Belgrade.  ",,,
Hungary,Átlátszó,Small,Participant,Best data-driven reporting (small and large newsrooms),"Why does the Hungarian Air Force fly to the Canary Islands, Las Vegas, Dubai, Malta, or Panama?",12/12/19,Investigation,"Microsoft Excel,CSV"," In 2018 Prime Minister of Hungary, Viktor Orbán traveled frequently with newly acquired military aircraft and posed proudly next to them. Since then, however, flight route data on two passenger and two luxury jet aircraft purchased by the Hungarian Defense Forces has been hidden from flight tracker websites. Atlatszo managed to obtain all of the four aircraft’s archive routes, revealing that the Defense Forces two Airbus A319s and two Dassault Falcon 7x aircrafts have been flying to places like the Canary Islands, Dubai, Las Vegas, Malta or Panama in the past two years. ​ "," This was the second large investigation in a series of articles on how government officials and business elite use private jets, yachts and defence force's aircrafts for private business purposes after we published <a href=""https://english.atlatszo.hu/2018/09/24/hungarian-government-elite-including-orban-uses-luxury-yacht-and-private-jet-registered-abroad/"">the first in September 2018</a> which won the most prestigious journalism prize in Hungary, the Transparency-Soma Prize for the best investigative report in 2018. The new investigation had larger impact in terms of engagement, impressions, shares, due to the fact that now the Defense Forces' aircafts were used for shady purposes, and when the article hit the public, the Ministry of Defense classified the information and data of the routes for 30 years. In Hungary is hard to gain nationwide attention with corruption cases, but this series of articles again raised the public's attention because the stories were about luxury, and even the prime minister Orbán was involved.  "," We used very simple tools, like Excel, Carto and Flourish. Excel for the analysis, Carto and Flourish for showing the data.  ", The hardest part was connecting the dates of different flights and the movement of the luxury yacht from different datasets.  ," Tracking and monitoring flights and vessels used by high government officials and businessmen allied with the government can reveal secret meetings between them outside Hungary. With the evidence of data and series of photos, drone-footage you can enforce them to react which is quite rare in Hungary.  ",https://english.atlatszo.hu/2019/12/12/why-does-the-hungarian-air-force-fly-to-the-canary-islands-las-vegas-dubai-malta-or-panama/,https://english.atlatszo.hu/2020/01/29/luxury-yacht-and-private-jet-linked-to-hungarian-government-elite-crossed-paths-on-the-adriatic/,,,,,,"Katalin Erdélyi, Tamás Bodoky, Attila Bátorfy"," Átlátszó.hu was established in 2011 by investigative journalist Tamás Bodoky. Átlátszó is the first crowdfunded investigative center in Hungary. During the years the team was awarded with international awards like Breaking Borders Award, European Citizen Prize, Theodor Heuss Medal, Pulitzer Memorial Prize, Index on Cenzorship Award, and journalists got individual accolades like Transparency-Soma Prize for best investigative reporting, Quality Journalism Prize, Junior Prima, etc.     Tamás Bodoky is the editor-in-chief of investigative journalism center Átlátszó.hu  Katalin Erdélyi is an investigative journalist of Átlátszó.hu  Attila Bátorgy is a journalist of Átlátszó.hu ",,,
Russia,RBC.RU,Big,Participant,Best data-driven reporting (small and large newsrooms),RBC team portfolio: Series of data-driven stories from Russia,31/12/19,"Explainer,Long-form,Breaking news,Database,Open data,OSINT,Infographics,Satellite images,Politics,Environment,Corruption,Health,Economy","Scraping,Json,Microsoft Excel,Google Sheets,CSV,Python", RBC.Ru special projects team is focused on data since 2017 though every member of the newsroom contributes to the data-driven reporting going further back. We use open data and the information disclosure legislation to show trends and controversies and challenge the government's narrative when that possibly serves public good. In 2019 our work and effort were invested in disclosing the data that is not published by the government. We generated both short data-driven news and long-term investigations to produce social impact and make the officials accountable. ," We discovered lack of medicines in different regions while analyzing thousands of government procurement contracts. That helped to provide patients with the medicines – before the officials made less efforts. We opened the data on HIV mortality and morbidity rates that was swept under the carpet by the Ministry of Health. We opened data on alcohol-related mortality that never was published by the officials. We made the Moscow government to admit the problem of low-quality asphalt material provided by the cartelized companies. We made local governments to admit that doctors’ salaries remain low in the Russian regions despite Putin’s orders to raise them. We found out that Moscow landfills are the biggest in Europe and showed how they grow rapidly to the wide audience to make people feel accountable for the amount of waste they produce.       We measure our work impact with thousands of views, government officials’ reactions and changes we bring to the local communities.       "," Python, Git, ABBYY Finereader, Tabula PDF, Excel, Google Spreadsheets, Tableau Public, HTML, CSS and Javascript, Adobe Illustrator, Sketch, Mapbox ", We work hardly to give the the real-time response and the most proficient analysis to serve public good. We provide RBC's audience with the profound stories based on data at rapid-fire pace of a news agency. ," We teach people to open data to public as we participate in the hackathons, university life and civil activists' conferences. In Russia not only the journalists can use freedom of information acts but also common people, and that is our goal - to show many people that they can hold the government accountable.  ",https://www.rbc.ru/society/11/04/2019/5cacc3ef9a79476dc9ae4f4b,https://www.rbc.ru/society/13/05/2019/5ca1d0229a7947593d373650,https://www.rbc.ru/society/11/03/2019/5c6c0ae49a79477703d915b7,https://www.rbc.ru/society/21/08/2019/5d5a951b9a7947b5b3dc6022,https://www.rbc.ru/society/23/08/2019/5d5e6cff9a7947916e53dda2,https://www.rbc.ru/society/11/03/2019/5c6c0ae49a79477703d915b7,,RBC.Ru staff, The whole newsroom participates in the data projects. Specific bios could be provided by request. ,,,
"Hong Kong S.A.R., China",South China Morning Post,Big,Honorable Mention,Best visualization (small and large newsrooms),Why your smartphone is causing you ‘text neck' syndrome,25/01/19,"Investigation,Explainer,Solutions journalism,Long-form,Open data,Illustration,Infographics,Video,Lifestyle,Health","Animation,QGIS,Creative Suite,Microsoft Excel,Google Sheets,CSV"," Mobile phones are now generally seen as essential to our daily lives. Texting has become the way most of us communicate and has led to rapidly increasing numbers of people suffering from 'text neck'. For our visualisation, “Why your smartphone is causing you ‘text neck’ syndrome” we researched how the angle of your neck when you look at your phone can effectively increase the weight of your head by up to 27kg. This in turn can damage posture and, if you text while walking, expose you to all kinds of dangers  ", This data visualisation caused much debate on social media and was translated into Spanish and republished by artesmedia.com ," We collected data about mobile phone internet access by country. Using dataviz and diagrams, graphics and our own video footage we detailed how extensive mobile phone use leads to curvature of the spine. We also recorded more than 10 hours of video to analyse how people use their mobile phones in Hong Kong when walking and crossing streets. The data confirmed the study made by the University Of Queensland.    We also use data research to explore mobile phone addiction and to explain how users ‘zone out’ on their phones. We hope that our innovative storytelling will make readers aware of their own habits and understand how their actions impact those around them as well as themselves. "," We also recorded more than 10 hours of video footage of mobile phone use on the streets of Hong Kong to corroborate an academic study from the University Of Queensland. We pepper the story with short videos to demonstrate how peripheral vision is restricted when using mobile phones, how your gait changes and to illustrate the dangers people pose while texting and walking in the street and using public transport. ", We believe this data visualisation helps make readers aware of their own habits and understand how their actions impact those around them as well as themselves. ,https://multimedia.scmp.com/lifestyle/article/2183329/text-neck/,,,,,,,Pablo Robles, Pablo Robles joined the South China Morning Post from Costa Rica in Setember 2017 to help strengthen the graphics team's digital output. Robles focuses on interactive solutions to create immersive experiences that push the boundaries of digital storytelling. ,,,
"Hong Kong S.A.R., China",South China Morning Post,Big,Shortlist,Best visualization (small and large newsrooms),Key events from past four months of Hong Kong's anti-government protets,10/09/19,"Investigation,Explainer,Long-form,Breaking news,Database,Fact-checking,Illustration,Infographics,Video,Map,Politics","Animation,Adobe,Creative Suite,Microsoft Excel", For our data visualization “Key events from Hong Kong’s anti-government protests” we analysed the daily coverage of the first six months to show how the protests roiled Hong Kong. We used the live coverage of our frontline reporters and studied photos and videos from trusted news sources as well as the daily police briefs. ," By combining on-the-ground reporting with charts, maps, DataViz, illustrations, videos and photographs we were able to create a visual narrative to bring clarity to a fast evloving series of events and inform a curious international audience about what was happening in Hong Kong. The data also provided an impartial bird’s-eye view of the civil disturbances that captured the world’s imagination.   Journalists and academics in Hong Kong use this story as a go-to source of information on how the protests evloved. "," To build this project we created a DataViz for each day and separated events into peaceful protests, violent protests and government announcements which included data such as the numbers of arrests, injuries, tear gas rounds and rubber bullets fired and public transport disruptions and train stations vandalised.    We used the scale of the video/photo to indicate the level of violence of that event – the larger the image the greater the level of violence used. ", All the data was collected manually and interpreted visually to show readers the key events over the first six months of protests. Reporting the data this way makes it easy for readers to understand the complex series of events at a glance. , We fiest published this project to mark the fourth month of protests and pdated with fresh information at the end of each month until December. We believe this is a good example of keeping a news story evergreen and fresh ,https://multimedia.scmp.com/infographics/news/hong-kong/article/3032146/hong-kong-protests/index.html?src=arcade,,,,,,,Pablo Robles, Pablo Robles joined the South China Morning Post from Costa Rica in Setember 2017 to help strengthen the graphics team's digital output. Robles focuses on interactive solutions to create immersive experiences that push the boundaries of digital storytelling ,,,
Hungary,ATLO Team / Átlátszó,Small,Participant,Best visualization (small and large newsrooms),State advertising as an instrument of transformation of the media market in Hungary,09/07/19,"Explainer,Politics,Corruption","Animation,Google Sheets"," The submitted project is an appendix to our journal paper <a href=""https://www.tandfonline.com/doi/full/10.1080/21599165.2019.1662398"" rel=""noopener noreferrer"" target=""_blank"">“State advertising as an instrument of transformation of the media market in Hungary”</a> , published in East European Politics. In the paper we examine and analyze data on state bodies’ (the government, ministries, authorities, state institutions, state owned companies) advertising practices between 2006 and 2018 through three following governments. ", The submitted project and it's appendix is the last phase of our ongoing scientific investigation on the allocation state advertising expenditures in Hungary started in 2016. Our work is often cited not only by news media but also journal papers and at last the regulation of state advertising is on the agenda of the European Commission.  ," We use data obtained from market research company Kantar and we store it in Google Spreadsheet. For the visualization we used Tableau before, but for the submitted project we used Flourish.  "," The methodology is the hardest part because we needed to avoid vague arguments on which media outlets can be considered as ""pro-government"". The importance of this project is that we have enough big data on proving the anti-market and anti-competition nature of public funding of the media in forms of state advertising. We always stated that the corrupted allocation of state advertising is not only an academic question, that's why we used visualizations for the general public. We also think that our project is a good example of the collaboration of the academia, journalism and visualization.  "," We truly believe that same amount of data could reveal similar hidden patterns also in those countries where the state's and the government's influence on media is considered low. We helped in data requests and analysis our <a href=""https://vsquare.org/pole-and-hungarian-brothers-be-eu-member-states-fuel-the-rise-of-pro-government-propaganda-with-taxpayers-money/"">Polish</a> and Czech colleagues.  ",https://atlo.team/allamihirdetesek/,https://www.tandfonline.com/doi/full/10.1080/21599165.2019.1662398,,,,,,"Attila Bátorfy, Ágnes Urbán"," ATLO is the visual storytelling project of Átlátszó.     Attila Bátorfy is a journalist az Átlátszó and master teacher of journalism and media at the Media Department of Eötvös Loránd Science University Budapest.  Ágnes Urbán is a media economist, and associate professor at the Department of Infocommunications of Corvinus University Budapest.  ",,,
Indonesia,Tempo.Co,Big,Participant,Best data-driven reporting (small and large newsrooms),ITE Law's Malleable Terms : A Tale of Supressive Twins,29/11/19,"Investigation,Long-form,Database,Illustration","JQuery,Json,Google Sheets,CSV"," The project tells the story of individuals who are punished by a relatively new law introduced in the Indonesian criminal code called UU ITE. This law, with its lax definitions, have been weaponised by certain political groups to attack those who criticise their actions. The story visualises data taken from 400 recipients and divides them into their profession, and showcases each victim's story. "," The project raised awareness of the effects of the law towards normal people, especially the way these political groups litigate relentlessly to tie up the matter in court. The story has currently amassed a combined 551 retweets from our main twitter profile @tempodotco and reignited debate on the matter for the Indonesian parliament to revise the law.  ", The article uses html and jquery in its construction. Data was collated and saved in a JSON file format to be taken using a javascript function. Animation effects are pure css.  ," The hardest part of this project is the volume of it, and showcasing it effectively. We had to think outside of the box to show that many people are affected by this law, while also balancing the visualisation so it wouldn't feel too crowded and losing the reader's attention.  Eventually divvying it up by profession was an elegant solution to keep that balance. The Atari-like design was also used to allay this problem.. ", Others can learn how to effectively showcase huge amounts of csv data and how to showcase it in an interesting and non threatening way to readers.  ,https://interaktif.tempo.co/proyek/pasal-karet-uu-ite-sejoli-pembungkam-kritik/index.php,,,,,,,"Syailendra Persada, Kris Pradipta, Ginanjar Pamungkas"," Tempo Media Lab is a newly created division of Tempo and is its digital media research and development arm. Other than developing new media strategies, we are also attached to different desks of the newsroom to help them create better longfroms with new ways of storytelling - particularly with a focus in data journalism. ",,,
"Hong Kong S.A.R., China",South China Morning Post,Big,Participant,Innovation (small and large newsrooms),Why your smartphone is causing you ‘text neck' syndrome,25/01/19,"Explainer,Solutions journalism,Long-form,Open data,Illustration,Infographics,Chart,Video,Map,Lifestyle,Culture","Animation,Sensor,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV"," Mobile phones are now seen as essential to our daily lives with texting the way most of us communicate which has led to rapidly increasing numbers of  'text neck'.    For our visualisation, “Why your smartphone is causing you ‘text neck’ syndrome” we researched how the angle of your neck when you look at your phone can effectively increase the weight of your head by up to 27kg. This in turn can damage posture and, if you text while walking, expose you to all kinds of dangers "," This data visualisation caused heated debate across social media and was translated into Spanish and republished by <a href=""http://artesmedia.com/"" target=""_blank"">artesmedia.com</a> "," After using data to explain how the angle of your neck can increase the weight of your head we use a pressure sensor which activates an animation showing how neck angles change when looking at smartphone, potentially altering the weight of your head to 27kg.   We also recorded more than 10 hours of video footage of mobile phone use on the streets of Hong Kong to corroborate an academic study from the University Of Queensland. We pepper the story with short videos to demonstrate how peripheral vision is restricted when using mobile phones, how your gait changes and to illustrate the dangers people pose while texting and walking in the street and using public transport. ", We use data to explore mobile phone addiction and to explain how users ‘zone out’ on their phones. We hope that our innovative storytelling will make readers aware of their own habits and understand how their actions impact those around them as well as themselves. , The use of video and interactive elements helps make readers aware of their own habits. We believe this fun and engagning form of storytelling is a more effective way for reader to understand and change how their actions impact those around them as well as themselves.  ,https://multimedia.scmp.com/lifestyle/article/2183329/text-neck/,,,,,,,Pablo Roble, Pablo Robles joined the South China Morning Post from Costa Rica in Setember 2017 to help strengthen the graphics team's digital output. Robles focuses on interactive solutions to create immersive experiences that push the boundaries of digital storytelling. ,,,
Russia,RBC.Ru,Big,Participant,Best visualization (small and large newsrooms),Scale of the pollution: growth of Moscow landfills,13/05/19,"Explainer,Long-form,Database,Open data,OSINT,Infographics,Map,Satellite images,Environment","Animation,Json,Google Sheets,OpenStreetMap,Python"," Every year Russia produces 70 million tons of waste. The tenth part of it – in Moscow. Most of this garbage settles in landfills. RBC shows how they grew around the capital. The research shows that 23 landfills around Moscow occupy about 608 hectares. Some of them, according to the plans of the Moscow region authorities, will grow further. Among the landfills that continue to operate (15 out of 23), one of the largest landfills in the world and the largest operating in Europe, ""Timokhovo"" — it occupies almost 114 hectares.  "," Many civil activists that strive against new landfills in the Moscow and nearby regions shared this story in their Vkontakte and Facebook groups and Telegram chats. The story gained 50,000 views which is more than average in the RBC website. Some landfills were excluded from the expansion plan of the Moscow government.  "," Google Earth, Russian Land registry requests to open the data on the owners and the growth of the Landfills, Wikimapia, Python, JSON, Sketch, Adobe Illustrator, Google Spreadsheets, Factiva to get old publication on the story of lanfills, OpenStreetMap, HTML, Javascript ", The hardest part of the project was to get information on when government allowed new parts of land to be joined and transformed into the dump. We read lots of documents and got the information from the land register to get it. We discovered not only the current and the past sizes of the landfills but also that some of them are illegal and can not be situated in the places they are.  , Journalists and public activists can learn using more Google Earth for their projects as well as opening the data through the land register ,https://www.rbc.ru/society/13/05/2019/5ca1d0229a7947593d373650,,,,,,,"Dada Lyndell, Julia Sapronova, Anastasia Antipova. Design: Alexander Vagichev, Darya Baryshnikova, Evgeniy Tarasenko. Infografics: Damir Yanayev. HTML: Anastasia Zayceva"," Dada Lyndell - a data journalist interested specifically in OSINT, open data; topics - medicine, ecology, corruption, government procurement contracts.   Julia Sapronova - a chief producer, working both with data and visual stories.   Anastasia Antipova - a producer, working both with data and visual stories. Has a background in podcasts.  ",,,
Indonesia,Tempo.Co,Big,Participant,Best data-driven reporting (small and large newsrooms),Jailhouse Story,10/02/19,"Investigation,Long-form,Quiz/game,Database,Illustration,Infographics,Crime,Human rights","JQuery,Json,Adobe,Google Sheets,CSV"," The project is about convicts being tortured by the Indonesian Police in order for them to extract a confession from them. The project shows how often it happens, how hard it is to prove innocence after being convicted, and the potential devastating consequences that it can result in. "," The project rose awareness on the fact that police brutality is commonplace in Indonesia, especially in regional police. After the story was published, a spokesperson for the Indonesian police had promised to reopen several cases thought to have been closed using coerced confessions. ", The article uses html and jquery in its construction. Data was collated and saved in a JSON file format to be taken using a javascript function. Animation effects are pure css. ," The hardest part of this project was cleaning the data. All in all the project had gone through hundreds of victims collected under a four year period. Some of these data featured incomplete fields and forced our team to get back on the ground to complete it. An additional challenge was to accurately portray the story and the experience of the victims in several different stortelling formats, be it as a newsgame or an animated video. Each victim's story needed to be sensitively handled, especially because some had even resulted in the death of the victim, so we needed to tread carefully to bring their stories to life. "," Others can learn how to show a volume of data in an interesting, non-intimidating way, and how to create different types of experiences in a single article to showcase differing stories and viewpoints in an elegant way. ",https://interaktif.tempo.co/proyek/kisah-di-balik-terali-besi/index.html,,,,,,,"Syailendra Persada, Kris Pradipta, Ginanjar Pamungkas"," Tempo Media Lab is a newly created division of Tempo and is its digital media research and development arm. Other than developing new media strategies, we are also attached to different desks of the newsroom to help them create better longfroms with new ways of storytelling - particularly with a focus in data journalism. ",,,
Singapore,Reuters,Big,Participant,Best data-driven reporting (small and large newsrooms),India election: The figures behind the faces,18/05/19,"Explainer,Politics","Scraping,Python"," The scale of the Indian election meant there was an immense amount of data being recorded by a variety of government sources. The graphics team scraped eight separate data sources programmatically. This included over 400,000 pdfs which were scoured using an algorithm to retrieve specific data points the team were interested in.    Data from these websites were then joined together using javascript, allowing us to discover new connections and make compelling visual projects such as this piece which was the first to actually show all candidates, but also visualised their criminal cases and wealth. ", The piece was a hit with Reuters clients but was also shared widely in India as well as elsewhere in the world. , Lots and lots of javascript work to compile and merge the data. R was used to then analyse that master dataset.    The presentation itself is a combination of rendering in the browser using custom code and polishing in Adobe’s Creative Suite.      ," The key challenge for this piece was flawlessly matching two completely different data sets that were scraped so as not to put a wrong photograph or name against an incorrect number of criminal cases.       By joining these datasets together, we became to first news organisation to put every candidate’s face to a name along with their criminal cases and assets. The presentation of the project was a creative way to illustrate the scale of the election. ", Preparing data well in advance can be a powerful weapon when preparing for a big event. Data scraping started months ahead of India’s election day.     ,https://graphics.reuters.com/INDIA-ELECTION-CRIMINAL-CANDIDATES/0100925031T/index.html,,,,,,,"Gurman Bhatia, Manas Sharma"," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Malaysia,Asia Online Publishing Group,Small,Participant,Best data-driven reporting (small and large newsrooms),Heightening Awareness of Dangers of the Dark Web,13/12/19,"Investigation,Explainer,Infographics,Chart,Crime","Personalisation,Scraping,Adobe,Microsoft Excel,Google Sheets","The dark web is a We realized that although people have heard about the dark web, not many are actually aware of what it is, how it can be accessed, who usually accesses it and what actually goes on there. The truth is that dark web activities affect us more than ever in this digital age, and businesses especially have much to lose. We embarked on this project to shed light on the direct and indirect risks of the dark web and why people need to have a strategy to counter the threats emanating from the unknown depths of the"," The heart of the project was a survey of 100 Malaysian IT professionals that gave us deep insight into their understanding of the dark web, their companies own use of the deep or dark web and their appreciation of the dangers that dark web can pose.    The survey results were analysed and written into a e-book download which was made freely available from our cybersecurity news portal. Malaysia's government body for cybersecurity ""Cybersecurity Malaysia"" also requested we shared anonymised raw survey data with them for their own increased understanding and also promoted our findings.   The survey's findings were considered important enough to share in more detail directly with a number of senior Cyber Security officers from a group of some of Malaysia's largest finance companies. Together with commercial sponsor Ingram Micro, we funded a live event where these findings were published.   Our aim with the entire project was to make more businesses aware of the threats of the dark web and more importantly to understand that private and confidential data from their own companies may very likely be ""out there"" on the dark web without their knowledge.   In era where cybercrime, legal compliance and online reputation being cognisant of this fact is vital for every business. with thi sin mind one of the most compelling impacts the project had was the number of large Malaysian companies (over 30) who as a direct result of this work, went out and secured services to scan check their own dark web exposure.   Whilst most kept the results of these assessments private, three companies who have asked to remain anonymous allowed us to view the results of their dark web assessments. In all three cases live and current data including user names and passwords were discovered on the dark web. "," The survey was conducted using ""old fashioned"" telephone-based interviews.    We specialise in highly targetted IT news portals. Our oldest title is ""data & storage asean"". This specific campaign revolved around one of our newer news portals ""cybersecurity asean"".   By running a highly targetted portal that focuses on a very niche subject (cybersecurity) we have been able to build a database of cybersecurity officers and administrators who work across the south-east Asian region. We were able to tap into this subscriber data to profile our targeted set of ""subjects"" for the survey interview.    We designed the survey questions, using statistically proven questioning methodology and captured the interview answers using google forms. All telephone interview surveys were also recorded.   The results were analyses using Microsoft BI and visualisations/infographics of the results were created using piktochart.   We have developed our own download engine (aopgdownloads.com) which captures and stores details of every document downloaded from any of our news portals. This was used to capture data on who downloaded the survey write up. This was used for impact analysis, with built-in follow-up questions we were able to identify people/companies that acted on what they read such as arranging a dark web assessment.   In order to actually delve into the dark web and find stolen private data we worked in partnership with Ingram Micro and CyberInt, the latter providing the dark web scanning technology the former providing the expertise to run the scans.     "," There are three areas which make this project particularly hard to undertake.   The first is credibility and trust. Companies and IT security officers do not like to share details that could compromise their own security. They are always naturally sceptical about sharing any information about their own company's security and data privacy. Our publication has developed a reputation for not being tempted to sensationalise cybersecurity news. We are known for reporting, informing and educating about cybersecurity issues. This enabled us to gain trust when conducting survey interviews, it also enabled our journalists to convince a number of companies to share the results of their dark web security scans with us.   The second was acquiring high-quality data. Our project did not collect large amounts of data, rather we collected deep insights from a highly select group. being able to identify and approach senior IT Security Officers in large Malaysian enterprises requires the ability to have the database available (which have through our subscriber base) and then being able to assess the correct people to approach and interview.   The third was securing the correct partners to make the project possible. Having support from the Malaysian government agency for cybersecurity and then bringing in the companies with the skills to help us navigate dark web issues and the dark web itself was critical but required credible approaches and convincing to get the right supporting organisations behind us.         "," The information we collated and produced in the report is in itself a valuable learning resource for literally anyone. We all need to understand what the dark web is and how it can be used. Whilst many of us will never ""surf"" the dark web, understanding how it is used against us, in itself makes people more cyber aware.    In terms of the project itself, its a demonstration of how a small newsroom needs to use multiple platforms to spread the content we produce. We focus on digital, but this project deployed telephone canvassing as well as face to face event to drive the content we created into the hands of the people we wanted to direct it to. Further by teaming with a government agency we were able to syndicate the content wider than over our own web platform alone.    The most valuable lesson is for other ""small newsrooms"". To be relevant and ""punch above our weight"" we need to achieve domain expertise. As a small newsroom, it's almost impossible to compete with the big news publishers if we try to go wide. But by being very focused on the news we cover and maintaining highly focused news portals (e.g. cybersecurityasean.com) we are able to do more than compete. We develop core expertise that the larger news agencies cannot rival. In the area of cybersecurity we are more credible than our bigger rivals.  By focusing on this core competency we were able to hinge together this project, be taken very seriously and deliver compelling content that educated and informed our target audience in a way a larger more genralist newsroom would not be able to do     ",https://cybersecurityasean.com/white-papers/how-well-do-malaysians-understand-dark-web,https://cybersecurityasean.com/daily-news/threat-intelligence-dark-web,https://cybersecurityasean.com/news-press-releases/dark-web-threat-every-malaysia-business-must-be-vigilant,https://aopgdownloads.com/file/how-well-do-malaysians-understand-the-dark-web,,,,"Andrew Martin, Aron Raj, SAH Syed Nadzari, Christina Yeap and the rest of the AOPG editorial team"," AOPG is Southeast Asia’s premier IT news publisher. We are recognised as experts in focused IT News reporting which we do through our three flagship news portals:   www.datastorageasean.com   www.disruptivetechasean.com   www.cybersecurityasean.com       Andrew Martin - Group Publisher (and co-founder)    Originally from the UK and a graduate of Loughborough University, Andrew's background is working for IT vendors. He was Vice President for APJ for a number of IT vendors, before seeing a gap in the market for an expert and focused IT, news publisher. With no one filling the gap, he teamed up with some partners and launched Asia Online Publishing Group just over 5 yeasr ago. In addition to deep knowledge in IT Andrew's BSc was in Sociology which gives him the foundation to lead research projects such as the one in this award nomination       Aron Raj - Journalist and Head of Production.   Aron has worked in media for many years, most of his tenure has been in Malaysia large national news organisations (e.g. Astro), where he has doubled up in TV and written journalism, interviewing Wimbledon champions through to formula one legends during his time. Seeing the news industry change, Aron saw the chance to sharpen his skills and join a smaller organisation that focused solely on IT news.        Syed Nadzari - Senior Journalist   Syed started his working life as an IT administrator, he left that to become one of the countries leading written translators for Malay to English transcription, something he did for many years for one of the countries largest broadcasters. two years ago he brought his both ends of his working life together combining his strong writing skills with his earlier background in IT to quickly become one of the region's most recognised IT news journalists.   This team along with their talented ""supporting cast"" are consistently recognised as the one of the leading enterprise technology journalism teams in the region, often being flown to international events by the likes of Dell, VMware, Cisco, Huawei .           Among the other key members of the editorial team for this particular project include our two tech writers/journalists, SAH Syed Nadzari and Aron Raj, and graphics designer, Christina Yeap. ",,,
Singapore,Reuters,Big,Participant,Best data-driven reporting (small and large newsrooms),How India mobilised a million polling stations,22/05/19,"Explainer,Elections,Politics","Scraping,QGIS,Python"," This piece utilized the data from the 400,000 electoral roll PDFs that were scraped programmatically. They gave us the number and sex of voters at every polling station. We then matched exact coordinates for each station in order to find out other interesting aspects such as accessibility or altitude.    We were able to explain some of the extreme locations and show the lengths to which the election commission goes in order to provide a polling station, sometimes only for one or two voters. ", The piece was a hit with Reuters clients but was also shared widely in India as well as elsewhere in the world. , Javascript and Python were used to scrape and merge the data. R was used to then analyse that master dataset.    QGIS was used to map out all of the stations in the project before being polished in Adobe’s Creative Suite and built into a web page. ," This was a massive data scraping challenge. Every state had a completely different way of storing this data so each had to be tackled separately. Electoral roll PDFs we only available in the format of a scanned/photocopied image which made scraping almost impossible. A specific tool had to be made to do that task and it took a long time to run through all 400,000 of them. ", Preparing data well in advance can be a powerful weapon when preparing for a big event. Data scraping started months ahead of India’s election day. ,https://graphics.reuters.com/INDIA-ELECTION-STATIONS/010092FY33Z/index.html,,,,,,,"Manas Sharma, Simon Scarr, Marco Hernandez"," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Romania,"OCCRP, The Guardian - UK, Süddeutsche Zeitung - Germany, Newstapa – South Korea, El Periodico – Spain, Global Witness and 17 other partners who can be viewed here: https://www.occrp.org/en/troikalaundromat/about-the-project",Big,Winner,Best data-driven reporting (small and large newsrooms),The Troika Laundromat,03/04/19,"Investigation,Explainer,Breaking news,Cross-border,Multiple-newsroom collaboration,Database,Open data,Fact-checking,Infographics,Chart,Video,Politics,Corruption,Money-laundering,Crime,Economy,Employment,Human rights","AI/Machine learning,Drone,D3.js,JQuery,Microsoft Excel,CSV,R"," We exposed a complex financial system that allowed Russian oligarchs and politicians in the highest echelons of power to secretly invest their ill-gotten millions, launder money, evade taxes, acquire shares in state-owned companies, buy real estate in Russia and abroad, and much more. The Troika Laundromat was designed to hide the people behind these transactions and was discovered by OCCRP and its partners through careful data analysis and thorough investigative work in one of the largest releases of banking information, involving some 1.3 million leaked transactions from 238,000 companies. A video explainer: <a href=""https://youtu.be/uteIMGxor0o"">https://youtu.be/uteIMGxor0o</a> "," First published in March 2019, with stories being added on an ongoing basis, the impact of of the Troika Laundromat was immediate and widespread. Raiffeisen, Citibank, Danske Bank, Nordea Bank, Swedbank, Credit Agricole, and Deutsche Bank were all seemingly implicated, and two banks --  Raiffeisen in Austria and Nordea in Finland -- deeply involved in the Laundromat saw their shares tumble. Twenty-one members of the European Parliament demanded sanctions against bankers whose financial institutions were involved in the money-laundering scheme. They also called for an “<a href=""http://hokmark.eu/letter-to-juncker-on-the-troika-laundromat-case/"">EU-wide anti-money laundering supervisory authority</a>.” At the same time, the Parliamentary Assembly of the Council of Europe (PACE) called <a href=""https://assembly.coe.int/nw/xml/XRef/Xref-XML2HTML-en.asp?fileid=27553&lang=en"">for swift and substantial action</a> to strengthen anti-money laundering provisions and improve international cooperation in the fight against laundromats.   The investigation triggered <a href=""http://www.xinhuanet.com/english/2019-08/18/c_138316966.htm"">a major political crisis</a> for the president of Cyprus as we revealed that a law firm he established and co-owned, and in which he was a partner at the time, was arranging business deals linked to a friend of Russian President Vladimir Putin, the infamous Magnitsky scandal, and a network of companies used in various financial crimes.   It also ignited investigations into some of Russia’s most powerful politicians including an investigation <a href=""https://www.elperiodico.com/es/economia/20190328/un-intimo-de-putin-uso-la-trama-de-blanqueo-para-comprar-una-villa-de-lujo-en-la-costa-brava-7374794"">in Spain into the property owned by the family of Sergei Chemezov</a> – the president of the main State owned technology conglomerate in Russia, Rostec Corporation, and a former partner of Vladimir Putin in their KGB heydays in Dresden, East Germany.   More recently, Sweden's SEB bank was revealed to be caught up in the Laundromat when leaked data raised questions about its dealings with non-resident clients.    Overall, the Troika Laundromat put the European banking system under increased scrutiny and is currently brought up in the European institutions as a main reason to clean up the European financial system. "," We received the data in various formats, including PDFs, Excel files and CSVs. We built our own virtual banking database, code-named SPINCYCLE. After grouping the source data by the given columns and format, we were left with 68 different structures. For each structure, we built individual Python parsing scripts that would feed data into the SPINCYCLE database. In the database, we organized the transactions so the data would link up. We used a proprietary IBAN API to pull details on banks that were missing in the data. For monetary values, we performed currency conversion at the time of the transaction, so we linked SPINCYCLE to an on-line table of historic exchange rates. We also tagged the accounts for which we had received information so that we could look at the overall flow of funds from the money laundering system. The neural net was trained using data from company registries and the Panama Papers, and it helped us to pick the names of 22,000 individuals from the 250,000 parties involved in the money laundering system. To make the data available to our members, we provided a web-based SQL interface. Later, we added a full-text search index based on ElasticSearch, which could be searched using Kibana as an interface. We also used Aleph, our home-grown open source data analysis engine.    On the landing page we aimed to present an overview of the whole network with a chord diagram and a dashboard that sets the model for the whole exploration: a big graphic on top followed by a dashboard with main key points. For the data visualization section we used client side Quasar Framework over Vue.js and D3.js for the graphs, all designed in Adobe Creative Suite.   The collaboration took place via the OCCRP secured wiki and Signal. "," The Troika Laundromat was born out of data work done on a large set of very dry banking transactions. We had to look for patterns in order to identify and isolate transactions that stemmed from what we later defined as the Troika Luandormat (TL). You can think of the TL as a TOR-like service meant to anonymize banking transactions. We had to look for the error, for the bad link, in order to identify who was the organizer and who were the users of the system. We finally found out through careful data analysis that the bankers putting this together made a small but fatal mistake: they used only three of their offshore companies to make payments to formation agents in order to set up dozens of other offshore companies that were themselves involved in transacting billions of dollars. These payments which were only in the hundreds of dollars each were of course lost in a sea of millions much larger transactions so we had to find them and realize that they were part of a pattern. The whole Troika Laundromat came in focus after this realization.   Another hard part with this particular project was the security of the team's members. The people we reported on were very powerful in their own countries and across borders and we had to insure the communication with reporters in Russia, Armenia and other places was always done via secure channels.    Last but not least the factchecking had to be done across borders and across documents and audio in many languages so this took quite a bit of time and effort to make sure we had things right. "," We learned, once again, that it is the combination between deep data analysis and the traditional footwork that makes good investigative journalism. It is the ability to zoom in and out between the data and the reality in the field that can find you the hidden gems.    We had a data scientist working with the investigative teams and this cooperation proved to be a recipe for success.    We also insured that journalists had multiple entry points, trimmed down to their technical abilities, with the data. The secured wiki where we shared our findings had a section where we described in detail how the information can be accessed through different systems. This was also a place where advanced journalists shared their ready made formulas so that others could apply them on top of their data of interest.   We have also learnt in previous projects and applied it here that the data scientist and our data journalists need to be available via Signal to the new arrivals in the collaborative team and be ready to explain how the systems work, what we already found in the data etc.  This made their integration much easier and improved efficiency as the new journalists in the project did not have to start from scratch.    Another important lesson that we drew is that it is not just cooperation across countries and between very smart reporters that makes a good project but cooperation across leaks can give you a fuller picture. In addition to the new leaked files, reporters on the Troika Laundromat used documents from previous ICIJ investigations, including <a href=""https://www.icij.org/investigations/offshore/"">Offshore Leaks</a>, <a href=""https://www.icij.org/panamapapers"">Panama Papers</a> and <a href=""https://www.icij.org/paradisepapers"">Paradise Papers</a>. It’s crucial that at some point in time we unify all these datasets as there are many untold stories in the current gaps between them. ",https://www.occrp.org/en/troikalaundromat/vast-offshore-network-moved-billions-with-help-from-major-russian-bank,https://cdn.occrp.org/projects/kremlins-laundromat/#/overview/companies,https://www.occrp.org/en/troikalaundromat/laundromat-money-leads-to-spanish-paradise,https://www.occrp.org/en/troikalaundromat/death-in-vienna,https://www.occrp.org/en/troikalaundromat/prince-in-wonderland,,,"Coordinators: Paul Radu, Sarunas Cerniauskas. Reporters: Olesya Shmagun, Dmitry Velikovsky, Alesya Marohovskaya, Jason Shea, Jonny Wrate, Atanas Tchobanov, Ani Hovhannisyan, Irina Dolinina, Roman Shleynov, Alisa Kustikova, Edik Baghdasaryan, Vlad Lavrov","  Šarūnas Černiauskas  is a regional editor for OCCRP, based in Vilnius, Lithuania. He leads Siena.lt, the first Lithuanian non-profit organization entirely dedicated to investigative reporting. Černiauskas has contributed to numerous cross border investigations, including OCCRP's Troika Laundromat, ICIJ's Panama Papers and Paradise Papers. One of his stories exposing the misuse of EU funds by members of the European Parliament was shortlisted for the European Press Prize in 2017. Černiauskas has received several national awards and became the first laureate of the Investigative Journalism Prize established by the Vilnius University, awarded for reviving investigative journalism in Lithuania.        Paul Radu  (@IDashboard) is a director and co-founder of the Organized Crime and Corruption Reporting Project <a href=""http://www.reportingproject.net/"">www.</a><a href=""http://occrp.org/"">occrp.org</a> a co-creator of the Investigative Dashboard concept <a href=""http://www.investigativedashboard.org/"">www.investigativedashboard.org</a>, of Visual investigative Scenarios visualization software <a href=""http://vis.occrp.org/"">vis.occrp.org</a> and a co-founder of RISE Project <a href=""http://www.riseproject.ro/"">www.riseproject.ro</a> a platform for investigative reporters and hackers in Romania. He has held a number of fellowships, including the Alfred Friendly Press Fellowship in 2001, the Milena Jesenska Press Fellowship in 2002, the Rosalyn Carter Fellowship for Mental Health Journalism in 2007, the 2008 Knight International Journalism fellowship with the International Center for Journalists as well as a 2009-2010 Stanford Knight Journalism Fellowship. He is the recipient of numerous awards including in 2004, the Knight International Journalism Award and the Investigative Reporters and Editors Award, in 2007, the Global Shining Light Award, the Tom Renner Investigative Reporters and Editors Award, the 2011 the Daniel Pearl Award for Outstanding International Investigative Reporting and a 2015 European Press Prize. Paul is an Ashoka Global fellow and a board member with the Global Investigative Journalism Network <a href=""http://gijn.org/"">gijn.org</a> and other organizations.   Paul was working the Panama Papers and the Russian, Azerbaijani and the Troika Laundromat.    Other team members who made this project:     Adem Kuric and Kenan Ibrovic, web production; Jason Papakheli, Samuel Gallicchio, and Friedrich Lindenberg, data analysis; and Edin Pasovic and Sergiu Brega, graphic design. Interactives designed by Quickdata (Romania); Researchers for OCCRP include Karina Shedrofsky, Jelter Meers and Katarina Sabados. OCCRP video team includes Sergiu Brega, Cristi Dimitriu, Madeleine May and Matt Sarnecki; with videography from Armenia - HETQ.am, Dana Chladek, Thierry Humeau and Ferran Masip. Promotion by Alex Cooper. Project coordinator was Paul Radu. Editors for OCCRP are Jodie DeJonge, Sharon L. Lynch, Ilya Lozovsky, Jody McPhillips, Drew Sullivan, and Julia Wallace.   Quickdata: Cosmin Nitu, Catalina Nitu ",,,
Austria,Der Standard,Big,Participant,Best visualization (small and large newsrooms),Is Europe Moving To The Right?,19/05/19,"Explainer,Database,Illustration,Map,Elections,Politics","Animation,D3.js,Canvas,Google Sheets,CSV"," There is frequent talk in journalism of things like right-wing populist movements, of parties moving to the center or of politicians shifting to the left. One might think that this spatial metaphor would be best shown using graphic illustrations, but in practice, such graphics are few and far between. With our project ""Is Europe Moving to the Right?"" however, we have produced an ambitious multimedia feature that does just that. Our goal was no less than placing every European Union member state on a left-right political axis and following each country's political shifts over time. "," The project was initially published in German on May 19, 2019, on derStandard.at and has received around 50,000 page impressions and a cumulative retention time of 4,500 hours. The article generated 320 posts from users and, by the standards of Austrian media, enjoyed significant resonance on social media outlets. We also received a great deal of international feedback on the German-language version of the graphic, even before we published the English version, which made it onto several data viz newsletters. "," The peg for the multimedia graphic was the European Parliamentary elections held in the 28 member states from May 23-26. In cooperation with our infographics department, an adapted version was included in the print edition of STANDARD on the weekend of May 18-19, 2019. From the very beginning, however, our priority was the digital version, which was created with the help of JavaScript library D3. We were particularly focused on a user-friendly presentation as well on desktops as on smartphones and tablets, since more than half of our users now read our content on mobile devices. "," Data-driven stories are created in two ways. Either a mass of data reveals a surprising set of circumstances that demands to be shared. Or it begins with a supposition that must be examined either with the help of data already in existence or with information that must first be gathered. Our project falls into the latter category, but initially it seemed that data availability might turn out to be an insurmountable hurdle. How were we, in 2019, to determine whether Finnish parties and their voters in 1999 leaned more to the left or more to the right? How far to the left or right? And what did ""centrist"" mean at the time?   Fortunately, social scientists at North American and European universities have long been focused on such questions. Since 1999, the organizers of the Chapel Hill Expert Survey have been polling political scientists on the political leanings of parties in their home countries. The survey now encompasses 268 national parties, allowing them to be situated on an abstract left-to-right spectrum at the time of each parliamentary election – from 0 (the left end of the democratic spectrum) to 10 (the right end).   By analyzing individual party positions together with the share of votes they received in a given election, we were able to calculate a long-term national average for each EU member state. The trick was to find an adequate method for presenting that information. Instead of a simple line chart, we decided to place the outline of each individual country on an animated, vertical timeline. The right-left movements of the country as it travels down the timeline depict its political leanings. This method enabled us to produce a seemingly straightforward portrayal of each country's political shifts over time "," Don't give up on a visualization idea just yet because the data is available, the leading question has been asked countless times (""Is Europe Moving To The Right""), and the representation is obvious (left/right movements of countries).   Chances still are no one has done it before and you can create something new and unique. ",https://www.derstandard.at/story/2000104392842/is-europe-moving-to-the-right,https://www.chesdata.eu/our-surveys,,,,,,"Michael Matzenberger, Sebastian Kienzl", *Michael Matzenberger* studied journalism and history at the University of Vienna before working as a freelance journalist and joining the Austrian news website and paper /Der Standard/ in 2011. After working on data driven stories for different areas and fields within the newsroom he became managing editor in 2017 and head of data journalism in 2018.    *Sebastian Kienzl* studied graphic design and mathematics and works as a data journalist and developer for interactive graphics in the data journalism team of /Der Standard/. The main focus in his work is the design and implementation of data visualizations and online storytelling formats. ,,,
Singapore,Reuters,Big,Participant,Innovation (small and large newsrooms),A plateful of plastic,31/12/19,"Explainer,Environment,Health",Creative Suite," The Reuters graphics team came up with a striking way to visualise how much plastic the average person eats in their diet. Using real plastic that was ground down into shavings, weighed out on digital scales, and pictured at table settings, we were able to drive the message home in a powerful and compelling visual format. ", The refreshing change with this project was that no major technology was used. Hard work and smart ideas made this possible. Physically weighing out plastic on measuring scales in the office and tipping it onto white plates and bowls was very satisfying and a refreshing change to reporting and visualising data programmatically on a computer.    The entire piece was also made in an environmentally friendly way with no plastic wasted.    The photographs were shot with a DSLR camera and tripod in front of a natural light source in order to create the nice shadow. They were then edited in Photoshop and placed on a cleanly designed web page. , The project was widely shared on social media and featured on websites of environmental groups. It is still widely shared as a striking explainer on the dangers of microplastics. ," The straight-line projection is simple math. Showing the results in a way that would be readily understood proved to be a challenge. Even with the innovative idea of physically weighing out the plastic, there were still big obstacles to overcome.    First of all, a high volume of finely ground plastic was needed. At first, the author tried to shave down his own bottle caps manually but after a couple of hours and only a spoonful of plastic later, it became obvious this was not the solution. To get the type and volume of plastic required, professional help was needed.    After trawling the internet to find places that would import large amounts of shredded plastic into Singapore we realised this was a terrible solution for environmental reasons. A Singapore recycling initiative agreed to provide plastic chips in a size that was suitable and in a range of colors. They could be returned afterwards to be recycled.    The next major challenge was the photography. Determining suitable table settings and assuring consistent lighting conditions took a lot of work. After the white tableware was returned to its rightful owner, the team decided to go even bigger and visualise the amounts of plastic over longer periods of time such as 10 years and a lifetime.    For this, larger plastic items were donated by other companies in Singapore, all with the agreement it could be returned afterwards for the sake of the environment. "," Sometimes, an innovative idea can drive a powerful visual data project. This piece was based on only a handful of numbers but with some innovative thinking around how to show the data, how to get the materials needed, and how to execute the final product, a strong story can be delivered. ",https://graphics.reuters.com/ENVIRONMENT-PLASTIC/0100B4TF2MQ/index.html,,,,,,,https://graphics.reuters.com/ENVIRONMENT-PLASTIC/0100B4TF2MQ/index.html," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
India,Indiaspend.com,Small,Participant,Best data-driven reporting (small and large newsrooms),Why Masses Of Jobless Youth May Reduce The BJP In UP,04/10/19,"Long-form,Open data,Economy,Employment",Google Sheets," The story was part of the series on growing unemployment in India. It explained how the poorly executed policies of the government led to a massive joblessness and layoffs in the companies who were left with no options other than reducing their production, do cost cutting in absence of cash in their hand.  ", The government refused to accept the problem of joblessness persisting in India's biggest state due to the badly executed anti-industries policies made by them but it later started working on generating jobs by opening temporary vacancies in government departments and even made an accouncement to smoother the functions of banks for startups which helped in creating jobs.  , Used government websites to collect the data.  , The hardest part was to get the comment from government about the problem highlighted in the story as they continuosly refused to accept that there was this problem persisting in the country , How to use government websites and its data to report a story ,https://www.indiaspend.com/why-masses-of-jobless-youth-may-reduce-the-bjp-in-up/,India,,,,,,Saurabh Sharma," I am Saurabh Sharma, 28, a freelance journalist from Uttar Pradesh. I quit the mainstream media in August 2016 to pursue my interest in meaningful journalism that supports our society and raising issues that are ignored by mainstream media.     After being trained in Journalism from the University of Lucknow in 2014, I began my journalism career with United News of India (UNI), an International news agency. After completing 18 months at Lucknow and New Delhi bureau of UNI, I joined Newstrack.com in Lucknow to learn digital journalism and worked there for eight months.    I started freelancing for 101reporters.com in 2016 and have been doing special, breaking, as well as offbeat stories since then. I have always been inclined toward working with people. It has been more than 5 years in journalism but even today I feel like I just started off with reporting.    I have also won the Lorenzo Natalie Media Award’s Grand Prize in 2017, been given by the European Commission, for my story Shanno's life.   I was also the finalist of the Thomson Foundation’s Young Journalist Award in 2019 for my story on how women in a small North Indian village are fighting against 400 years old prostitution tradition.    You can find my work on following websites:      Reuters, Times UK, FIRSTPOST, INDIASPEND, NEWSCLICK,  NEWSLAUNDRY ",,,
Singapore,Reuters,Big,Participant,Innovation (small and large newsrooms),How many protesters took to the streets on July 1?,07/04/19,"Investigation,Breaking news,Multiple-newsroom collaboration,Fact-checking,Politics",Creative Suite," When large-scale demonstrations first plunged Hong Kong into political crisis, the size of the crowds became a contentious issue, with police giving low numbers and protest organizers much higher estimates.   For Reuters, this was an opportunity to calculate our own estimate and provide an unbiased statistic. Collecting and analysing a solid dataset would be pivotal in the success of the piece.      After speaking to crowd science experts and working out the analytics, we embarked on one of our most ambitious and innovative data exercises yet.   This immersive presentation shows how we arrived at our final calculation.    "," The piece is the definition of public service journalism, delivering a valuable independent statistic at a time when the size of the crowds was being used as a political tool. The project resulted in an exclusive story and figure that was not matched in-house by any other news organisation.    The piece was shared widely on social media in Hong Kong and across the rest of the world. "," After speaking to crowd science experts and working out the analytics, the next step was to study the three-km route of the next march step by step, spending hours to pin down locations where protesters would funnel through. These would be the places to monitor the flow rate of people, and thus calculate crowd size.   Armed with zoom lenses, DSLR cameras, tripods, GoPros, iPhones, extra batteries and folding fishing chairs, the teams manned their counting stations.    Bursts of HD video were recorded at specific times and later played back in Adobe After Effects in order to count the heads of people passing a line in the road.    As data was being calculated, the graphics team also edited video from other cameras to make striking timelapses and visualised the data we had collected in order to explain the entire process to the reader.    We felt it was extremely important to be transparent about how we arrived at this number.   The visuals, graphics and text were presented in an immersive experience. "," There were a few difficulties to overcome. The first and most important issue was working out the science of crowd counting. This is extremely difficult when a moving march is being studied, rather than a static crowd. The narrow streets and concrete skyscrapers of Hong Kong add to the challenge. The team spoke to crowd science experts and worked on theory and testing before everyone decided we were well placed to execute the idea.    The next difficulty was identifying positions on the route that would be good for people flow AND had an elevated position we could set up and comfortably occupy for a full day. Early exploration was done in remotely in Google Earth and after narrowing locations down we physically went there to do some tests. There were some issues in checking out views from one of the city’s hotels so we had the bright idea of combing through traveller’s photos from the hotel on Trip Advisor and managed to find some shots out of the window of the room looking directly at the road we wanted. We booked the room ahead of the next big protest.    Actually counting the heads of people was an enormous task in itself. Luckily it wasn’t raining or too sunny so there were very few umbrellas in the crowd. Counting such a high volume of people, even in bursts, was extremely taxing on the team members but we knew we had to get this published as soon as we could after the march. Some counting was even done on the plane as some of the team who weren’t based in Hong Kong headed home. ", There are often creative ways to tackle a lack of available data. This is a good example of that. These techniques can be challenging but extremely rewarding. ,https://graphics.reuters.com/HONGKONG-EXTRADITION-CROWDSIZE/0100B05W0BE/index.html,,,,,,,"Simon Scarr, Manas Sharma, Marco Hernandez"," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Kyrgyzstan,Kloop.kg,Small,Participant,Best visualization (small and large newsrooms),Working women in Kyrgyzstan almost have no spare time - they spend it cleaning house,02/08/19,"Explainer,Infographics,Video,Audio,Culture,Women,Economy,Employment","Animation,Scraping,Adobe,Microsoft Excel,Google Sheets,CSV,Python"," Project touches upon one of the most vulnerable and sensitive issues not only in Kyrgyzstan, but in Central Asia, – gender equality. Persistent cultural norms, patriarchal views and women abusive stereotypes lead to the fact that women spend quarter of their daily time doing unpaid housework. With that men spend only 5% of their daily time doing chores. The project is a combination of video, narrative and a poster. Based on data we reconstruct the average day of the working couple – even both spouses work, women continue working, but unpaid when returning home, while men enjoy more leisure. "," Kloop.kg is one of the few independent media outlets in Kyrgyzstan, and its audience is fewer compared to the large national media outlets. Therefore we were very happy to achieve the following results with each of our projects reaching the broad audience and getting so many responses.   1. First of all, the project is innovative in a sense that it is the first video reconstruction based on data, not only in Kyrgyzstan, but in Central Asia.   2. The project ended up to be the TOP ONE article by page views during the month it was published – February 2018.   3. The average time on page amounted up to 2 minutes 48 seconds compared to 1 minute 57 among all the articles that were published between February, 2019 and January, 2020.   4. Eventually it took 69<sup>th</sup> place among over 3000 of articles published at Kloop.kg during one year.   5. It also received over 5000 of reactions, comments and shares on Facebook page.   6. The project also entered Global Investigative Journalism Network’s GIJN’s Data Journalism Top 10: <a href=""https://gijn.org/2019/02/14/gijns-data-journalism-top-10-datashare-document-analysis-visualization-talkies-and-kyrgyzstans-labor-imbalance/?fbclid=IwAR3m4Yp16LoacW2y2J8MO1iMCXHv_F5ya-CQZfC-xEzlMc8tT4OpJb_RFSw"">https://gijn.org/2019/02/14/gijns-data-journalism-top-10-datashare-document-analysis-visualization-talkies-and-kyrgyzstans-labor-imbalance/?fbclid=IwAR3m4Yp16LoacW2y2J8MO1iMCXHv_F5ya-CQZfC-xEzlMc8tT4OpJb_RFSw</a> "," Data sources:     National Statistical Committee of the Kyrgyz Republic. URL: http://stat.kg/ru/publications/obsledovaniya-byudzheta-vremeni/   IMF data   World bank data     Methodology. The dataset contains distribution of time (in minutes) spent on daily life duties (chores, work, leisure) disaggregated by gender, areas, regions, marital and occupational statuses, age etc.   We did the cross sectional data analysis, in order to calculate the average distribution of time devoted to unpaid housework by women and men. In order to prove the gender differences we estimated the differences between women and men in elasticities of unpaid housework to paid occupation and leisure. The elasticity for women is on average lower than for men (even though, when women start working, they still tend to withdraw less time from unpaid housework than men. This means that working women still devote unequally large amount of time to housework, while men enjoy more leisure).   In the narrative and a poster we provide fact-sheet of data-based differences between women and men in terms of their time spend on chores and how it affects their life-being. For example, one of the consequences is that women have less economic and education opportunities – less than half of working age females are employed, and their salaries are 30% less than males’. "," The hardest part was working with data and finding appropriate way for data visualization.   The initial dataset consisted of multiple tables – all of them in non-machine-readable format. The data needed to be scraped, converted into machine-readable format and cleaned. Finally, we obtained dataset, disaggregated by gender, age, region, occupation, time spent on chores, education and work.   We discussed a number of different formats, including longread, scrollytelling and finally agreed to do a very simple, but informative format – video reconstruction based on data. We calculated the averages of the time spent on doing chores, split them into categories and re-constructed an average day of the working couple before and after work. "," The issue of gender inequality in Kyrgyzstan is one of the most sensitive issue among many other social problems.   From the one hand it is declared by the law that there is no gender inequality in legal economic and social rights. From the other hand there is a number of societal norms and stereotypes leading to post-ante unequal outcomes.   One of these outcomes is unequal distribution of time between women and men spent on household chores. It is believed in Kyrgyzstan that women from an early age are obliged to maintain household’s wellbeing. Especially it is mandatory for new brides who just got married and entered husband’s family (also known as “kelin”). As a result women are exempted from effective time spending, which they could have devoted  towards their education, paid work and carrier opportunities.   As a result of our findings we concluded the following   1. There is unequal distribution of household chores between women and men, which restricts economic opportunities and labour force participation of women.   2. There is a clear unfairness in the Kyrgyz society. We proved that even when both partners work, women work more but unpaid, doing household chores, while men enjoy more leisure. ",https://kloop.kg/blog/2019/02/08/u-rabotayushhih-zhenshhin-v-kyrgyzstane-pochti-net-svobodnogo-vremeni-oni-tratyat-ego-na-uborku-doma/,http://bit.ly/KRwomen_chores,https://www.youtube.com/watch?v=FPWjRynf8UI&feature=youtu.be,https://docs.google.com/spreadsheets/d/18jPaC9i81HGq5AstuypsQ-BAQ6LwDu_M7dQhzUVd7H8/edit#gid=1254731807,,,,"Savia Hasanova, Anna Kapushenko, Altynai Mambetova, Arseny Mamashev, Tilek Beishenaliev, Aizada Karataeva, Bekzhan Asylbekov, Katya Myachina, Dmitry Motinov, Tatiana Zelenskaya"," Bio of Savia Hasanova (applicant):   For the past two years I have been working closely with journalists, teaching them how to use data for making data-driven stories, investigating corruption and supporting reportages with data. Along with fellow colleagues from School of Data Kyrgyzstan we have trained over 100 of journalists from Central Asia, mentored over 50 of data stories and organized the first Central Asian conference on open data.   I am an author of a number of data-driven stories. One of them – «Kyrgyzstan survives on money made by migrant workers, but it doesn’t know how to spend it» – was named one of the 2018 Top journalism articles in Russian language by the International Journalists’ Network. During 2018-2019 my colleagues and I published 8 data-driven articles in different formats – longreads with data visualization, data-tests and video.     ",,,
Spain,Personal project,Small,Participant,Innovation (small and large newsrooms),The Overview Effect,12/02/19,"Cross-border,Documentary,Quiz/game,Database,Open data,Map,Satellite images","Creative Suite,Microsoft Excel,CSV","   The overview effect is a cognitive shift in awareness reported by some astronauts during spaceflight, often while viewing the Earth from outer space.It is the experience of seeing firsthand the reality of the Earth in space, which is immediately understood to be a tiny, fragile ball of life, ""hanging in the void"", shielded and nourished by a paper-thin atmosphere.        From space, national boundaries vanish                     And why, I don’t know. I don’t know to this day. I had a feeling [the Earth] it’s tiny, it’s shiny, it’s beautiful,it’s home,   and it’s fragile.        — Michael Collins, Apollo 11   "," ""The Overview Effect"" project is product of instrinsec motivation .Some months ago a friend explained me what this abstract concept  mean and I felt so curious that I decided that I wanted to visualize it. After all, I am a teacher, so I am driven by making the complex simple and appealing. As a result, I created some maps and published them into my personal portfolio.   Because this project was just a self-distraction in order to research new paths for storytelling, I didn't design neither a communication plan or a reporting dashboard. I just didn't shared it far away fronm my personal website. Consequently, I can't measure the impact of the project by analyzing social media. "," I used <a href=""https://carto.com/"">CARTO</a>, a Location Intelligence platform, and Excel. "," The hardest part of this project was going beyond the abstract concept (the overview effect) and translate it into graphical elements in order to create an appealing visualization. Apart from that, it was also quite complicated to geolocalize some data in order to create a consistent and coherent visualization "," The major challenge of the project is to broad the perspectives beyond the common worldwide maps. We are used to be exposed to worldwide maps made from national boundries. What about if we could reshape the world on the basis of another geographical data? The most interesting effect of the project is that we can be able to identify the world, the continents, even some national boundries, without them being drawn.       It is usually said that ""What is not named, does not exist"". This project aims to reflect on this idea and goes beyond: ""What is not charted does not exist...?"" ",https://ainaceco.wixsite.com/ainceco/world-on,,,,,,,Aina Cebrian Cormand,"   Hi, I'm Aina! When it comes to describe myself I tend to keep from using  neither adjectives nor too technical words but expressing the things I love. I'm passionate about  data,   education  and  design . I'm driven by making the complex simple and appealing.       I really enjoy grasping datasets and growing them into stories. The combination between narrative, graphical components and statistics can converge into new engaging formats for storytelling.     But not merely stories. Data visualization shouldn't provide definite answers, but generate accurate questions that bring out critical thinking and create outside the box thinkers - just as any educational system should.       After almost 10 years studying apparently unrelated fields, such as Data Visualization and Journalism, Education and Audiovisual Communication, and with experience working in public and private sector and university I currently find myself discovering new junction points. I am currently working developing Data Analyitics and Visualizations for the Spanish Ministry of Education and also creating some personal designs.                         ",,,
South Africa,"Oxpeckers Investigative Environmental Journalism, Earth Journalism Network",Small,Participant,Open data,#WildEye: Mapping wildlife crime,28/01/19,"Investigation,Explainer,Solutions journalism,Cross-border,Multiple-newsroom collaboration,Database,Open data,Crowdsourcing,Infographics,Map,Audio,Environment,Corruption,Crime","Personalisation,Scraping,D3.js,QGIS,Json,Adobe,Google Sheets,CSV,OpenStreetMap,Python"," <a href=""https://oxpeckers.org/wildeyemap/"">#WildEye</a> is a pioneering geojournalism tool that, for the first time, provides access to consolidated data on wildlife crime in greater Europe. Developed by journalists for journalists, it aggregates and shares information on seizures, arrests, court cases and convictions relating to illegal wildlife trade. Journalists use the tool to look for data, patterns and trends, and #WildEye hosts a growing dossier of investigative reporting. It is also used by civic organisations, law enforcement agencies and policy-makers. Pioneered by Oxpeckers Investigative Environmental Journalism, #WildEye is partnered by the Earth Journalism Network, and is currently being replicated in Asia. "," We have seen significant successes in making wildlife trafficking data publicly accessible. By shining a light on the scale of illegal wildlife trade taking place, we have sparked the interest of other reporters, law enforcement, and organisations monitoring and investigating wildlife crime.   We have mapped nearly 400 incidents of seizures, arrests, court cases and convictions on the #WildEye map. These span the entire continent and record information on close to 100 different endangered species.    We have formed ongoing relationships with organisations across the globe, including wildlife monitoring agency TRAFFIC, the Global Initiative against Transnational Organized Crime, the Wildlife Justice Commission and USAID. We have also interacted with law enforcement agencies, including Interpol, Europol and local law enforcement.    More than 25 investigations have been published. These have been viewed and shared hundreds of times. Topics include an exposé of illegal tiger farms in the Czech Republic; songbird smuggling from Italy to Malta; the poaching of sturgeons in Romania and Bulgaria; the black market for parrot eggs smuggled into Europe from the Amazon; and how traffickers use the world's largest reptile market to sell protected species.    We met with journalists funded by our partners at Earth Journalism Network, and at convenings in the UK and Germany. Here, we trained them on how to use the map and workshopped story ideas that incorporated #WildEye data.    In November 2019, we hosted a live webinar, where close to 30 participants from around the world received that same training, this time in the context of the updated #WildEye map. We looked at why Europe, and why now; explained to participants what #WildEye shows and taught them more about how to use the tool; and had a lively discussion on the difficulty of accessing wildlife crime-related data. Listen to the webinar here: <a href=""https://oxpeckers.org/2019/11/talking-wildlife-crime/"">https://oxpeckers.org/2019/11/talking-wildlife-crime/</a>. "," #WildEye was built by developers using an open-source platform called Mapbox. This was customised to fit in with Oxpeckers' style and to suit our needs as data journalists.    #WildEye’s main feature is a map of Europe showing where law enforcement agencies have been involved in action against wildlife trafficking. Each case is identified by an icon that signifies either a seizure, an arrest, a prosecution or a conviction.   Move your cursor over any icon and a text box will pop up, providing detailed information about what products were seized, who was arrested, and how much they were fined, for example.   The tool includes a search function to help users filter information and find topics of interest. If you want to learn about the illegal trade in birds, for instance, simply type “birds” in the search box and you’ll get a host of results covering seizures, arrests, court cases and convictions involving that word.   Data is uploaded on Mapbox back-end via a Google spreadsheet that is updated on a weekly basis, sometimes more frequently. Methods of data collection range from scraping the net to obtaining reports from several dozen organisations who monitor wildlife crimes. We convert bulky reports into spreadsheets that can be analysed and added to #WildEye. By engaging with organisations such as TRAFFIC and the Wildlife Justice Commission, we have been able to access some of this data more easily. "," Our biggest challenge has been accessing data – mainly because #WildEye is the first platform to collect, collate, analyse and make public data on legal processes relating to illicit wildlife trade in Europe.   We always knew that we were creating a tool that maps and makes the data public, but quickly came to understand why we were doing this when we hit dozens of roadblocks and had to fight to get information.    This led to a lot of crowd-sourcing, scraping and meeting with organisations to give them a better idea of why this data should be accessible to anyone and everyone. It took some creativity and teamwork, but our efforts became clear each time a new journalistic investigation was published and when organisations working in the field started using the platform for their own research.    Platform challenges included customising Mapbox templates to update data quickly and to show all data points properly and in full; adding a spiderfier functionality to Mapbox as the data sets increased (and the map began to look too busy); creating searchable tags and clickable menus; making it easier to add and access #WildEye data as a member of the public; and explaining how to use the tool as new users.    These issues were solved by doing two things: first, we included the data manager in the tech-related updates and back-end functionality of the tool, and encouraged them to work closely with the developers. Second, we created a more community-friendly tool, which now includes user guides, direct links to adding and accessing the data, and a cleaner, more simplistic-looking map. These changes were supplemented by running a webinar to encourage users to experiment with the tool and provide feedback on its usefulness to their own work. "," The main thing that others can learn from #WildEye is that making data on wildlife trafficking accessible is crucial in the fight against it. We cannot expect people to understand these issues if they cannot get information and read stories about them.    #WildEye is a resource that was developed by journalists for journalists, and is a centralised platform used to tell compelling data-driven stories about illegal wildlife trade.    We started by focusing on Europe because of its growing importance as a trafficking hub, and the fact that it is a source of wildlife products that are high in demand in other regions. Until now, there has been no single place to access information easily on efforts to crack down on wildlife crime on the continent. #WildEye addresses these gaps by  tracking the scale of Europe’s role in the illicit trade, and by helping journalists to increase media coverage of the problem.   By exploring the data, journalists can look for patterns and trends that can inspire new investigations. For example, why are there more seizures in some countries than others — is it due to more intense controls or to the preference smugglers have for certain routes? Why do so few seizures result in prosecutions and (fewer still) convictions?    #WildEye is a fantastic customisation of open-source technology, which we hope encourages others – especially journalists – to test similar methods of visualizing large datasets of their own. We have been vocal about the tech-related challenges we faced throughout the creation of this tool during in-person interactions, on panels and in the webinar, and hope that others learn from our experiences. Making important data sets look good and easily accessible does not have to be difficult, and #WildEye is a prime example of this, within the context of data journalism.  ",https://oxpeckers.org/wildeyemap/,https://earthjournalism.net/special-reports/wildeye,https://oxpeckers.org/2019/01/wildeye/,https://oxpeckers.org/2019/11/talking-wildlife-crime/,https://earthjournalism.net/program-updates/ejn-gathers-journalists-covering-the-illegal-wildlife-trade-in-europe,https://earthjournalism.net/resources/be-a-part-of-the-wildeye-community,https://oxpeckers.org/guide-for-journalists/,"Fiona Macleod, Roxanne Joseph, James Fahn, Charlotte Schep, Mike Shanahan, Sara Schonhardt, Anina Mumm, Mark Hartman, Tristan Mathiesen, Paul Kennedy"," #WildEye is a project by Oxpeckers Investigative Environmental Journalism (Oxpeckers), supported by the Earth Journalism Network (EJN). Oxpeckers is Africa's first journalistic investigation unit focusing on environmental issues. It combines traditional investigative reporting with data analysis and geo-mapping tools to expose eco-offences and track organised criminal syndicates.    EJN was developed by Internews to enable journalists from developing countries to cover the environment more effectively. It is a global network with reporters and outlets in every region of the world. It trains journalists to cover a variety of issues, develops innovative environmental news sites and produces content for local media.    ScienceLink is a digital media specialist outfit which focuses on interactive apps, dataviz and digital story-telling. It developed the #WildEye functionality along with graphic design agency Artman Designs.  ",,,
United Kingdom,BBC,Big,Participant,Best data-driven reporting (small and large newsrooms),Electric cars,04/05/19,"Investigation,Explainer,Solutions journalism,Open data,Chart,Map,Environment","Personalisation,Json,Microsoft Excel,Google Sheets,CSV,R,RStudio"," The UK government wants to “lead the world in zero emission vehicle technology” by encouraging drivers around the country to go green and choose an ultra low or zero emissions vehicle.    The BBC’s Shared Data Unit used open data to examine whether the infrastructure of electric car charging points was in place to meet the anticipated rise in demand.   We found there was an uneven geographical distribution of charging devices, with more than a third of local authorities having ten or fewer locations where drivers can plug in their vehicles.      "," The story featured across the BBC's network news programmes including the News Channel and Radio 5 Live, as well as regional radio stations.  As part of the Shared Data Unit's commitment to make its data journalism available to the wider regional news industry, the story generated local stories in 30 local titles across the UK.   The story provoked a wider debate about the availability and reliability of open data on the chargepoint network.  We analysed data from Open Charge Map, a crowd-sourced website of charging locations with an open API.  Not every location features on Open Charge Map, but it aims to be a ""reliable single point of reference for charging equipment location information"".  It certainly had thousands more charge points registered than the government's own National ChargePoint Registry, while data held by private companies including the electric vehicle charging platform Zap-Map was not available to us.  As proponents of open data, we felt our audience would benefit from the most comprehensive analysis of charge point data we could bring them in order for them to make informed choices.   Six months later, the Department for Transport published its first experimental statistics on charge point coverage, sourced from Zap-Map's platform.          "," We used R Studio and the Haversine formula to perform 49 million calculations for each distance (as the crow flies) between each of the 7,000 charging points, using their latitude and longitude coordinates and then storing the shortest distance. That allowed us to report the average distance between charging locations for each local authority area; a measure of how the existing infrastructure compared across the UK. We published the script we used in a <a href=""https://github.com/BBC-Data-Unit/electric-car-charging-points"">GitHub repository</a> so anyone interested could follow our method – we publish our methodology in this way for all of our reports - and recreate it themselves.   We interrogated the API of Open Charge Map and used Open Refine to convert the data from JSON format into a csv, which we published as a public Google Sheet we shared with our network of local news partners and our online readers.   We used Carto to make an interactive map showing the average distance between charge points across the UK.     "," The hardest part of the project was understanding the limitations and availability of the data on charge points.  Private providers suggested Open Charge Map was not the best source, and were resistant to us telling a story about a ""patchy"" network.  Though we felt by using the most complete open source data available to us, we would be providing helpful and informative information into the public domain.   We sourced additional data from the Driver and Vehicle Licensing Agency to calculate a rate of charge points per registered electric cars for each local authority area in the UK.  The calculation allowed our audience to explore those areas where charge point infrastructure was described as ""patchy"" by the RAC.     "," Through highlighting the limitations of available data, journalists can put contribute to the wider policy debate around what should be published by public authorities and private providers.  We highlighted an issue of ""patchy coverage"" using technical skills to analyse and present the open data held by Open Charge Map in a compelling and personalised package.     We also demonstrated our model - that through equipping journalists with the skills to interrogate data - we can improve and enhance local news coverage for all our audiences.  The reporter that worked on the story, Aimee Stanton, was on secondment with the BBC from JPI Media.  During her time with the Shared Data Unit, she developed the technical skills alongside the core team to be able to tell the story. ",https://www.bbc.co.uk/news/uk-47696839,https://github.com/BBC-Data-Unit/electric-car-charging-points,https://youtu.be/jNZPAQP4y2s,,,,,"Aimee Stanton, Pete Sherlock, Alex Homer, David Gregory"," Aimee Stanton is a data reporter for JPI Media, who spent three months on secondment at the BBC Shared Data Unit.   Alex Homer is a senior data journalist with the BBC Shared Data Unit.   Pete Sherlock is the Assistant Editor of the BBC Shared Data Unit.   David Gregory is a news correspondent for BBC Midlands Today. ",,,
Finland,Helsingin Sanomat,Big,Participant,Best visualization (small and large newsrooms),Suuret suunnitelmat / Big Plans,24/12/19,"Illustration,Infographics,Video,Map","Animation,3D modelling", Helsingin Sanomat (HS) staff graphic designer Uolevi Holmberg created a 3D model of 50-year-old Helsinki Metropolitan Transportation plan using open source 3D data of Helsinki as a base. In the end of ‘60s Finnish engineers with the help of American Wilbur Smith & Associates visioned eight-lane motorways over the city of Helsinki and massive intersections next to seaside islands and old stone brick house areas. Using game engine techniques HS visualized an unseen and intense scrollable flyover experience throughout the whole story.      , There's a debate in Helsinki weather to build a motorway tunnel under Helsinki or not. The vision has been also critized of being too automobile-centric. This article gave historic perspective to that debate.  ," For the article an alternative 3D environment of Helsinki was created. HS used an open data 3D model of the city and built new motorways on top of the model. The end result was visualized using game industry's Unreal Engine. Other tools used were Modo, Meshlab, and After Effects. The article was hand-coded, but it used a web publishing platform made in Helsingin Sanomat. "," Creating the alternative 3d model took weeks, because it was built manually. The first version of the story was made to theater stage, as it was one part of Helsingin Sanomat's Musta Laatikko (Black Box) journalistic theater show. For web the story needed a lot of changes tweaking, because we wanted the reader to be able to advance in the 3d world by scrolling. This was done using video key frames in the code. "," The techiques used were advanced. Unreal Engine is well suited also for journalistic needs, and the key frame technique in video offers a immersive scrollytelling experience. ",https://dynamic.hs.fi/2019/smith-polvinen/,,,,,,,"Kalle Silfverberg, Uolevi Holmberg, Elisa Bestetti, Maria Lähteenmäki, Lauri Malkavaara, Emma-Leena Ovaskainen"," Kalle Silfverberg is the HS's city desk editor. Uolevi Holmberg is a motion graphic designer specializing in 3D. Elisa Bestetti does UX coding for Helsingin Sanomat. Maria Lähteenmäki in an experiemced picture editor. For decades Lauri Malkavaara has been the editor of HS's celebrated monthly supplement Kuukausiliite, but currently he is an editor in web projects. Emma-Leena Ovaskainen is a visual producer currently working on digital news. ",,,
Qatar,Al Jazeera,Big,Participant,Innovation (small and large newsrooms),Can you draw symbols of India's political parties?,13/05/19,"Explainer,Quiz/game,Politics,Culture","AI/Machine learning,Personalisation,Canvas"," In 2019, more than 600 million voters cast their ballots in the world's largest election. Historically, due to high illiteracy rates, India’s election commission assigned visual symbols to political parties to help voters identify their candidate of choice.   These symbols include everyday items like a flower, an elephant and even a USB stick. To mark this major event, Al Jazeera created its very first machine learning game which challenged our digital audience to draw some of India's 2,300 political party symbols.   The results were both enjoyable and insightful and demonstrated that innovative technology can engage a global audience.  "," One of the big discussions in 2019 was how could newsrooms innovate with technologies such as Machine Learning (ML) and Artificial Intelligence (AI).   These technologies can yield tremendous opportunities for journalists but just like any other technology, they should be applied to the right story.   As Al Jazeera, one of our key considerations when producing data-driven stories is to ask ourselves how we can amplify the cultural and human stories from the global south to a global audience.   Following its launch, this interactive game quickly became one of our highest engaged stories on our website as well as on social platforms specifically Facebook. Tens of thousands of users from all over the world competed with one another to draw a variety of symbols ranging from an elephant to a bicycle. Many viewers from India challenged one another to draw their party symbols and to learn the meaning behind these symbols.   Organisationally, and for the region, this opened up the doors to experiment more with interactive storytelling using machine learning. This is vital given that these new techniques are often considered out of reach for most news organisations.   Produced and developed over the course of three weeks by one data journalist using open-source technologies, this story sparked a conversation in the newsroom about the adoption of new interactive techniques to storytelling.         "," Under the hood was a convolutional neural network trained using Keras, an open-source neural-network library written in Python. The data itself was the open-source Quick Draw dataset which contains around 50 million drawings from 345 classes. https://quickdraw.withgoogle.com/data   For our story, we utilised 20 classes which corresponded to India's biggest political parties. Once the model was trained we utilised Google's TensorFlow.js. which allowed us to run the entire deep learning module directly in the user's web browser.   This made the application very accessible and meant that users did not need to download anything to start drawing on their desktop/mobile.   Several rounds of optimisations were performed to ensure the best user experience on both desktop and mobile. The UI experience for the game was built using React.js, a Javascript framework which powered the drawing canvas while assigning scores for drawing the correct symbols. ", The most difficult part of the process was making the technology seamless in order for users to just focus on the storytelling and user experience. The success of the project opened up huge possibilities for future machine learning based stories which are expected to become an integral part of the data-driven newsroom. , The biggest takeaway from this project is that machine learning can have its place in every data journalist's toolbox. As algorithmic accountability and machine bias is increasingly playing a role in journalism it's important for data journalists to experiment with new storytelling techniques that captivate a global audience's attention.  ,https://interactive.aljazeera.com/aje/2019/can-you-draw-indias-political-party-symbols/index.html,,,,,,,Mohammed Haddad," Mohammed Haddad is a data journalist and instructor. He leads Al Jazeera’s interactive team, @AJLabs, where he uses data to tell compelling visual and analytical stories from the global south.  ",,,
Argentina,Infobae Argentina,Big,Participant,Open data,The secret decrees of the dictatorship,24/03/19,"Investigation,Documentary,Open data,Human rights","Microsoft Excel,Google Sheets,CSV,Python"," We analyzed 7,114 secret decrees signed by the last Argentine military dictatorship between 1976 and 1983 that, until now, had never been fully processed and reviewed. From a  quantitative  and  qualitative analysis,  we identify orders and dispositions, from arrests and deportations, to the arms trade, editorial bans and real estate operations, through million-dollar contracts to counteract the bad image of the dictatorship abroad. Everything was decided without society knowing and was revealed in the work presented to apply for this award. This series of articles began to be published on March 24, 2019 and extended until May 26, 2019. "," Between 2013 and 2015, the government of Cristina Kirchner ordered the declassification of a large number of secret decrees of the dictatorship that had not been published at the time in the Official Gazette. But this declassification was done in a disorderly way, at different stages over three years, and in a little bit visible section on the Official Gazette website. In practice, the vast majority did not meet publicly, nor did anyone analyze them in their entirety.  We carry out a deep journalistic research of historic background and political context to help the public to understand possible reasons for classification by dictatorship governments and to exposed their arbitrariness.  Through a series of requests based on the Law on Access to Public Information we found that there were almost 1,500 decrees signed by the dictators that still remained secret and that it was not known what they were, nor the reasons why they were not declassified along with the rest.  In this process, we also identified 8 decrees that had not been declassified by Cristina Kirchner and we revealed their content and the possible reason why they remained hidden.  We decided to extend the analysis to the secret decrees issued in democratic periods from 1958 and we identified, thus, that there were almost 600 norms signed by constitutional presidents that remained outside the public light and nobody knew about them  Our investigation exposed those decrees that still remained secret and the area of ​​the Government they were referring. We identified which were in the process of being declassified by Mauricio Macri administration. Through a request of the Law on Acceso to Public Information we advanced the content of a dozen of them. The open data of all decrees were shared with society. "," From the Infobae Data Unit we proceed to the complete download of the secret decrees, including 354 attached documents. After the opening of data (a PDF conversion was performed for TXT), the documents were uploaded to the Document Cloud platform, which allows access to them in the original format and in text format. We used an OCR system (optical recognition characters) to open the data. Then, the counting of the decrees for the time series was automated. Data parsing was performed using Python. Repeat patterns were relieved, through different variables that were put under study. In order to visualize the volume of secret decrees during the dictatorship, in comparison to other periods in Argentine history, we used Flourish. The open data of the secret decrees in text format were shared with the audience for the first time, through a Google Drive link for public consultation. "," In order to process and analysed the 7,114 secret decrees into a series of journalistic notes, the Infobae Data Unit proceeded to the complete download of all of them, including its 354 attached documents.The most difficult part was the reconstruction of loans taken by other nations for the purchase of military weapons. The design of the spreadsheet had to be done manually, to reconstruct the timeline of money and the handling of figures in different currencies. Then perform the calculations of the value of money to report what value in dollars the indebtedness would have at current figures. As it was not just to analyze only numbers, but text, another difficult part was to detect repetition parameters on words. First, a sample of 400 random decrees had to be read to identify them. "," The team that worked on this project was composed of Mariel Fitz Patrick as a reporter, Daniela Czibener as a programmer, Ezequiel Di Martiis as DataViz and Sandra Crucianelli as an analyst.  There was was not only a text-based data analysis but also journalistic research of background and context. The opening task combined digital and manual work, through information requests and manual data entry. We only use free tools that could be used by any other reporter. We were only moved by the desire to open information, process it and expose it in the light of public opinion. ",https://www.infobae.com/politica/2019/03/24/los-decretos-secretos-de-la-dictadura/,https://www.infobae.com/politica/2019/03/24/los-libros-que-la-junta-militar-prohibio-mediante-ordenes-reservadas/,https://www.infobae.com/politica/2019/03/31/el-gasto-secreto-de-la-ultima-dictadura-en-compra-de-armas/,https://www.infobae.com/politica/2019/03/31/la-lista-completa-de-prestamos-del-exterior-y-convenios-de-adquisicion-de-armamento-belico-por-decretos-reservados/,https://www.infobae.com/politica/2019/04/19/que-dicen-ocho-decretos-secretos-dictados-en-democracia-que-cristina-kirchner-mantuvo-bajo-reserva/,https://www.infobae.com/politica/2019/05/05/hay-aun-1-500-decretos-de-la-ultima-dictadura-que-permanecen-en-secreto/,https://www.infobae.com/politica/2019/05/06/el-gobierno-publico-nueve-decretos-secretos-de-la-dictadura/,"Sandra Crucianelli, Mariel Fitz Patrick, Daniela Czibener, Ezequiel Di Martiis","  Sandra Crucianelli  is a research journalist and specializes in data journalism. Coordinates the Data Unit in Infobae, Argentina. She is a member of the International Consortium of Investigative Journalists. He was part of the Panama Papers, Paradise Papers and Implant Files team for ICIJ. He is a Knight Fellowship of the International Center for Journalists (ICFJ). As an instructor, she teaches online courses for the Knight Center of the University of Texas and the Digital Journalism Training Center of the University of Guadalajara, Mexico . Mariel Fitz Patrick is a research reporter for Infobae. She is a member of the International Consortium of Investigative Journalists. He was part of the Panama Papers, Paradise Papers and Implant Files team for ICIJ. Daniela Czibener is a MS in Computer Science. She has skills in several programming languages, data processing and data visualization. Ezequiel Di Martiis is a graphic designer with data visualization skills.    Mariel Fitz Patrick  is a journalist specialized in judicial matters, Investigation and Data Journalism. He works at Infobae Argentina, currently the Spanish-language website with the largest audience in the region, and the most visited in Argentina. In addition to covering current issues, she integrates the Data Unit team. Previously, she was a journalist of different graphic media, such as the magazine Noticias (of Editorial Perfil) and publications of the Clarín Group.  He was part of the journalism team of “Periodismo para Todos”, a research program conducted by Jorge Lanata, aired nationally on Channel 13, which received numerous awards in Argentina and was awarded the Silver Medal at the New York Festival in 2014, the year in which he was also nominated for the Awards Emmy in the Current Affairs category.  She is a member of the International Consortium of Investigative Journalists (ICIJ), and participated in the global investigation Panama Papers - winner in 2017, among other awards, of the Pulitzer, and of the Paradise Papers investigation - who won the 2018 National Freedom of Expression Award of Editorial Perfil in Argentina. In 2018, she took part in The Implant Files investigation.  In 2019 he won - along with Sandra Crucianelli - the Digital Media Investigative Journalism Award of the Argentine Journalism Forum (FOPEA). ",,,
Nigeria,PositiveNaija,Small,Participant,Best news application,"PositiveNaija: Nigeria, Nigerians & Excellence",07/07/19,"Explainer,Solutions journalism,Breaking news,Quiz/game,Open data,News application,Fact-checking,Infographics,Video,Culture",Microsoft Excel," PositiveNaija is a multimedia news, information and forum platform dedicated to informing the world on the positive progress and achievements associated with Nigeria and Nigerians globally.  The aim is to positively - inspire, educate and redefine the standards of the Nigeria’s values-based system.    Vision : To inspire hope and pride in every Nigerian.    Mission : As a human-centered enterprise, to continuously strive towards creating a good, well-informed and empathetic society for Nigeria and Nigerians through news management and impactful initiatives.    Values : Truth, reliability, respect, valid sense of judgement and excellence. ","<ul>  Reversing the faulty national value system: Nigeria’s national value system continues to deteriorate rapidly with obscure definition of what principles, standards of behaviour and judgement of what is acceptable by every Nigerian in daily practice towards national success. The root causes of this can be attributed to the prevailing decline in education (critical-thinking), misinformation, falsehood, lack of empathy, and incoherent national vision for Nigeria.   Improving on the deficient national image: Nigeria has a national image problem both at home and abroad hinged on lack of trust towards the Nigerian society to maintain an objective approach to ideal standards of good-natured co-existence. The socio-economic and cultural limitations of this remains enormous internally and externally even as Nigeria records huge financial losses in form of investments to poor image yearly. The root causes of this can be attributed to the increasing absence of love / respect by Nigerians for self, compatriots (neighbours) and country as well as poor leadership and ethics of governance.  </ul>", Excel Spreadsheet - Data analysis ," 1. Gaining the attention and trust of the public due to prevalent instances of fake news, uninspiring leadership, negativity, etc.   a. We have been able to succeed by maintaing objectivity in our publishing style.   b. We also have been able through the competitions we organise on our platform, demonstrate explicitly, transparency and accountability.   c. We have also been able to engage at various times, resourceful and skilled persons to provide strategic and operational support in enhacing our work.   d. We are also apolotical in our general approach - limiting/elimating political influences.   e. Continous and strategically engaging the public on various platforms - both online (social media) and offline (through the use of stickers).   f. We have upheld the principles of truth, respect, reliability, valid sense of judgement and drive unto excellence. "," 1. Continous innovation yet with simplicity   2. Truth as the basis for Trust.   3. As a media entity, unique approach for revenue generation outside advertising. ",https://www.positivenaija.com/,https://www.positivenaija.com/positivenaija-bi-annual-newsletter-july-december-2019/,https://www.positivenaija.com/positivenaija-love-and-heroes-2019/,https://www.positivenaija.com/positivenaija-love-and-heroes-2019/,https://www.youtube.com/channel/UCCbTIH79ZF1s63YWD6atiDw,https://www.instagram.com/positivenaija_1/,https://www.positivenaija.com/wp-content/uploads/2019/04/Excerpts-PNS-2018-LOP-2019-Toju-Ogbe.pdf,Toju Micheal Ogbe," Toju Micheal Ogbe is a Nigerian author, strategist, sociologist, journalist, researcher, and poet.   He is passionate about a Nigeria that is genuinely united by love and with a common vision towards peace and sustainable progress.   He is the Founder and Editor-in-chief of PositiveNaija (since January 2015).   His academic qualifications include a Bachelor of Art (BA) in International Studies & Diplomacy [First Class] from Benson Idahosa University, Nigeria and a Master of Business Administration (MBA) in Oil & Gas Management [Merit] from Coventry University, United Kingdom (UK).   He also possesses a Diploma in Intelligence Gathering, Analysis and Management from the Summit Leadership Institute, Policing Centre of Excellence (UK) and the Nigeria Police.   Toju Micheal Ogbe has provided and maintained relevant contributions to the field of sustainable energy development in Nigeriain the international development sector as well as Nigerian public and private sectors.He is a consultant with expertise in monitoring and evaluation; social audit; energy management; capacity development; publishing; data, information and knowledge management; communication and human resource management. ",,,
Finland,Helsingin Sanomat,Big,Participant,Best data-driven reporting (small and large newsrooms),Inequality and childhood: the things that affect your success in life,25/12/19,"Explainer,Quiz/game,Illustration,Health,Economy","Personalisation,Json,Microsoft Excel"," A personalized and interactive story about things that affect your success in life. What was your childhood home like, how old were your parents, how was school, did your parents read to aloud? And so on. The data in the background is based on over 80 scientific reserach papers - this is how we can say that these thing really have an affect in the peoples lives.   The story is interactive: you are asked questions as the story goes on and the text is personalized for you according to your answers. "," The story was widely read, shared and discussed. It got over 350 000 visits (in a country of 5,5 million people). From social media and feedback it is possible to see that the story really made people think about their childhood and how self made their success really is.   One impact is that the story introduced an advenced way to report about scientific research. Every question or factor has a source. If we say that moving home a lot as a child might not be good for you we can point the source research for this. A group of 28 researchers were involved in the process. They analyzed the relevant reserch papers for us to use.   The story has 15 questions. From analytics we can see that it had a tight grip: over 90 percent of those who answered the first questions also answered the last ones.         "," The reporter wrote the story in a table format. If you answer question nr 1 ""Yes"" and question nr 2 ""No"", this is how the text will look like to you.   The tools used to get the story together: html, css, js and vue.js         "," Once we got the data from the research team the hardest part was to write the storyline in a way that it feels personalized (""This story is talking to me and about me"") and also like a quality journalistic feature article - not just something that is put together from random cells in a table. To better understand this, here is an example of the structure:  <ul>  Intro text (same text for everyone)   First set of questions:    When you were born, did your mother have at least a secondary education degree, that is, had she graduated or had a vocational qualification? Yes / no   When you were born, were at least one of your parents under 20? Yes / no   Did you know if your mother had depression when she was waiting for you? Yes / no       Text explaining things about the topics in the questions (same for everyone).   Paragraph about question 1. There is a different version for answer ""yes"" and ""no"". Your answer determines the one that is shown to you.   Paragraph about question 2. There is a different version for answer ""yes"" and ""no"". Your answer determines the one that is shown to you.   Paragraph about question 3. There is a different version for answer ""yes"" and ""no"". Your answer determines the one that is shown to you.  </ul>",<ul>  Don't be afraid to work together with researhers.   An important but sometimes distant topic (things that create inequality in early years of life) can be made into an immersive experience using interactive and tailored storytelling.  </ul>,https://dynamic.hs.fi/2019/elaman-evaat/,,,,,,,"Heidi Väärämäki, Kimmo Taskinen, Elisa Bestetti, Juho Salminen"," Heidi Väärämäki: journalist focused on health and social issues   Kimmo Taskinen: visual journalist, photo journalist, illustrator   Elisa Bestetti: graphic designer and developer   Juho Salminen: data journalism producer ",,,
Finland,Helsingin Sanomat,Big,Participant,Best visualization (small and large newsrooms),Sailing to Tallinn,08/09/19,"Long-form,Cross-border,Open data,Illustration,Infographics,Video,Map","Animation,Scraping,D3.js,Canvas,Json,Creative Suite"," We scraped the data of boat routes, types and home country in the Finnish Gulf during one day in August 2019. The boat routes in Finnish Gulf are among the busiest in the world with daily ferries between Finland and Estonia and tankers going to St. Petersburg and back. In the story the reader can follow a sailing boat's trip from Helsinki to Tallinn along others at the same time in the same sea area. Annual traffic data of the route was also illustrated in the graphics of the story.  "," The story got very positive feedback over its fluent user experience and on-the-spot feel. It combined reportage storytelling to traffic data during the sail which reporter and photographer made their story. It also broadened readers awareness how it actually is to navigate in the one of the busiest boat routes in the world. The story was made right after a deadly collission in the Finnish Gulf, so it also had strong educational mission: the route is really busy and you should act accordingly as a private boater as well. "," GPS data was collected for all the duration of the sailing. At the same time marine traffic data was fetched from the API <a href=""https://slack-redir.net/link?url=http%3A%2F%2Fwww.digitraffic.fi%2Fen%2Fmarine-traffic%2F"" rel=""noopener noreferrer"" target=""_blank"">www.digitraffic.fi/en/marine-traffic/</a>. The dataset were joined and visualised on a map using d3.js. ", Challenge after the scraping was to reduce the datasets to an acceptable size for the web. We worked especially hard on combining the data and the narrative storytelling to create a feel of a boat trip that is going forward and at the same time provides the reader with a lot of information. , The story made us realize it's possible to combine live traffic data and visual storytelling into format that follows classic reportage but still creates new digital storytelling with strong data. ,https://dynamic.hs.fi/2019/purjekyyti-tallinnaan/,,,,,,,"Elisa Bestetti, Emma-Leena Ovaskainen, Saara Tammi, Kaisa Rautaheimo, Iines Vikiö."," Elisa Bestetti, design and code. Bestetti is staff designer.   Emma-Leena Ovaskainen, producer. Ovaskainen is staff design producer.   Saara Tammi, text. Tammi was staff reporter for the summer 2019.   Kaisa Rautaheimo, photos and videos. Rautaheimo is staff photographer.   Iines Vikiö, graphics. Vikiö was staff graphic designer for the summer 2019. ",,,
United States,Florida Today and USA Today,Big,Participant,Innovation (small and large newsrooms),Apollo 11 50th Anniversary,16/07/19,"Explainer,Long-form,Multiple-newsroom collaboration,Documentary,Podcast/radio,Mobile App,Video,Audio,Culture","Animation,AR,3D modelling"," A celebration and remembrance of the people and engineering that made the first landing of humans on the moon possible, telling the story with text, video and augmented reality. "," Literally hundreds of thousands of people made the Apollo 11 moon landing possible, many in small but critical roles, from assembling spacesuits to cleaning offices to protecting what was a top-secret program of national importance and prestige. Our series of stories and videos brought these everyday heroes out of obscurity and gave them long overdue credit for their service to history. ""You made me a legend to my family and community and across the county,"" said Dorothy Johnson, who worked in NASA's typing pool and was one of the African American ""hidden figures"" of NASA fame. Many hundreds of letters and emails expressing similar gratitude came pouring into the office. And for generations who weren’t born during Apollo, it gave them a chance to experience the launch of Apollo 11 as if it were a live event taking place today - to listen in on mission control, watch the footage and follow the rocket and spacecraft in AR, brining history to life while making it fresh, accessible and familiar. Tens of thousands used the 321 Launch App to create the excitement of that historic launch in the palms of their hands. "," The 321 Launch experience was built on the Unity game engine, but the app’s development wasn’t just limited to hardcore programming. Our emerging tech team in Washington, D.C. and space reporter Emre Kelly in Florida cracked open frayed Apollo-era presskits and books to find detailed mission parameters and design elements; NASA experts weighed on in tough decisions related to translating Saturn V from reality to 3D; and even veteran engineers who worked on the storied rocket offered their input. Professional video editing applications helped edit content to run in sync with the 3D events and the “live” chat on the Scribble platform, all of which focused on the mission as if it were happening in real-time. "," The major challenges we faced were, first, how to create a virtual experience that would bring the wonder of Apollo to new audiences and generations, and, second, finding a fresh way to tell a well-trodden, 50-year-old story.   We chose augmented reality as the solution to the challenge of recreating the Apollo mission, from lift off to splash down, in real time. The sheer magnitude of the research and mathematical calculations to pull this off were extremely daunting. We had to follow a precise timeline for all events. We had to compute the complex trajectories of the Apollo 11 flight to such precision that it would match the actual path of the real rocket. This was done using Apollo-era documents and back-of-the-napkin calculations to ensure the AR rocket launched on time, separated from its stages properly, docked in orbit, touched down on the moon and completed its mission in real time exactly as did Apollo 11 in 1969. The AR model-building always showed the spacecraft in its proper position and made the experience - even the vibration and roar of the rocket engines - as real as possible.   The second challenge was no less formidable. Florida Today in Brevard Country, the so-called Space Coast and home of the Kennedy Space Center, was founded, in-part, to report on the moon program. Every major moon landing anniversary had been commemorated by the paper. What could we say this time that felt fresh? We decided to tell the story of the people of Apollo, to piece together the personal stories of aging workers who made the mission possible, from the most menial to the architects of the whole project. We used text, video, photos, a podcast and a documantry film to recreate their experiences and tell the story of a technological wonder. "," Journalism isn’t always about sitting down in front of a keyboard and writing. For the Apollo project, it was about collaboration with a team of people journalists and heavy-duty programmers from the gaming industry. It was getting on the horn with a variety of sources, and staying in touch with them throughout the process to ensure accuracy. And in the end, it was about integrating them all together into a product that successfully merged cutting-edge tech, traditional journalism, and Space Race-era aesthetics.  ",https://www.floridatoday.com/moonlanding/,https://www.usatoday.com/story/augmented-reality/2019/08/21/interactivestory-experienceid-apollo/2058082001/,https://www.321launchapp.com/,https://www.floridatoday.com/videos/tech/science/space/2019/07/17/documentary-people-apollo-moon-landing/1758612001/,https://soundcloud.com/peopleofapollo/the-people-of-apollo,https://www.youtube.com/watch?v=dWCf6auWf0g,,"Bobby Block, Britt Kennerley, John Torres, Tim Walters, Mara Bellaby, Tom Mardis, Emre Kelly, USA Today's Emergening tech team"," Mara Bellaby if Florida Today's Editor in Chief. She has written and edited for newspapers since 1989, working in Arizona, Indiana, Ohio and Florida in roles ranging from freelancer to managing editor.   Bobby Block is Florida Today's Managing and Watchdog Editor. He is an old long-serving foreign correspondent and national security reporter who has found a new lease on life on the frontlines of local news. He also covered and worked in the space industry.   Tom Mardis brought coding and AR and VR development skills to the project.   Emre Kelly is a space reporter focusing on rocket launches, the business of spaceflight, and the industry’s impact on the Space Coast.   Britt Kenerley has written and edited for newspapers since 1989, working in Arizona, Indiana, Ohio and Florida in roles ranging from freelancer to managing editor.   John Torres is a News Columnist for Florida Today. He has reported from Italy, Indonesia, Africa, Haiti and other places but these days writes mainly about the justice system.   Tim Walters is a video journalist who grew up watching rockets launch from his backyard, and who now helps chronicle those events and so much more for his home county of Brevard.   USA TODAY's Emerging Tech team, led by Ray Soto and Annette Meade, fuses traditional journalism storytelling with AR, 3D design and programming. The team has built AR experiences around space, sports and even history.     ",,,
Portugal,"INESC TEC, Ci2 - Smart Cities Research Center - Polytechnic Institute of Tomar, University of Porto, University of Kyoto",Big,Participant,Best news application,Tell me Stories,14/04/19,"Investigation,Solutions journalism,News application,Fact-checking,Mobile App","Json,Python"," Ever wondered if you could revisit the history of the war in Syria with a single click? Remember the details of the election of Donald Trump to President of the United States of America? Know more about climate change? Or what was said and written about the Iran nuclear deal? Tell me Stories [tellmestories.pt] is a website that automatically creates temporal summaries of a given topic. Thorugh a generated timeline, users are offered the chance to sift through a collection of news articles to discover the most relevant information and related stories. "," During the last decade, we have been witnessing an ever-growing number of online content posing new challenges for those who aim to understand a given event. This exponential growth of the volume of data, together with the phenomenon of media bias, fake news and filter bubbles, have contributed to the creation of new challenges in information access and transparency. For instance, following the media coverage of long-lasting events like wars, migration or economic crises can be oftentimes confusing and demanding for users and journalists. Media outlets often use temporal summary as a solution. However, manually building such timelines can be very laborious and time-consuming. One possible approach to overcome this problem is to automatically summarize a large amount of news into consistent narratives through timelines. Such tools may play an important role in a large spectrum of users looking for the most valuable and useful stories within large amounts of information. This may be the case of journalists, policymakers, students or casual readers in need of getting context about a given story or interested in checking a fact. Imagine how useful it would be to quickly obtain a timeline of news about a candidate to an important public role or background information to answer questions regarding an unexpected disaster. Tell me Stories offers users the opportunity to quickly access the story of an event over time providing a contextualized overview of it. This tool is the result of our participation at the 41st European Conference in Information Retrieval (ECIR 2019) where we won the Best Demo Presentation. A related preliminary project of Tell me Stories first appeared in 2018 (though in that version it was adapted to the Portuguese web-archive collection). "," Tell me Stories is a user-friendly interface that allows running queries on news sources and exploring the results in a summarized and chronologically organized manner with the help of an interactive timeline. Given a user query, the system automatically identifies relevant dates and the most important headlines to illustrate the story. To this purpose, we rely on a 4-step pipeline: (1) News Retrieval; (2) Identifying Relevant Time Intervals; (3) Computing Headline Scores and (4) Deduplication. The first step in the pipeline is to run the query against any data source of interest, and fetch matching documents. Tell me Stories is built on top of the Signal Media Dataset [<a href=""https://research.signal-ai.com/newsir16/signal-dataset.html"" target=""_blank"">https://research.signal-ai.com/newsir16/signal-dataset.html</a>], a one-million news articles collection (mainly English, but also non-English and multi-lingual articles) which were originally collected from a variety of news sources (such as Reuters) for a period of 1 month (1–30 September 2015) and indexed on a database through ElasticSearch technology. However, our solution can be easily adapted to other scenarios including different kinds of data sources (e.g. social media posts, etc) and languages since it is mostly language-independent. This may be understood as an important contribution for anyone interested in having access to a summarized temporal view of their data. Next, we select relevant time periods, by applying a strategy that forces the system to select intervals with at least one peak of occurrence. Following, we rely on YAKE! [<a href=""http://yake.inesctec.pt"" target=""_blank"">http://yake.inesctec.pt</a>], a keyword extraction statistical method developed by our team [best short paper of ECIR 2018] to select the most important headlines over a huge number of documents. Finally, in an attempt to reduce the amount of duplicated content, we make use of deduplication algorithms. The source code for our temporal summarization framework, as well as examples of how to adapt for different data sources, are available online [<a href=""https://github.com/LIAAD/TemporalSummarizationFramework"" target=""_blank"">https://github.com/LIAAD/TemporalSummarizationFramework</a>]. "," Tell me stories lie on top of a complex structure that involved the development and the assembly of several tools to make it work. While manually constructing stories from different and disparate sources is possible, it turns out unfeasible and a time-consuming task in the long-run. Tell me Stories tries to fulfill this gap by offering an easy to play tool that automatically creates narratives over time. To make this happen, we had to build a search engine infrastructure, which makes available a collection of one million documents, that we had to previously index in a database. Documents are searchable by means of a typical query interface, however, unlike conventional search engines (such as Google), which are more focused on retrieving recent single web pages, we aim to offer users a comprehensible story of an event over time in a way that prevents them from having to grasp the entire web. Knowing that dozens of webpages can be related to the query event, raises, however, several concerns related to the information overload problem that we had to deal with. The difficulty here is to select the most relevant parts of the story without burdening users with too much information. To tackle this problem we begin by selecting the most relevant time periods of the story and devised a keyword extraction algorithm tuned to select the most important headlines. Users are then offered not only a timeline to navigate in-between the different time-periods, but also the most relevant news regarding that particular time-frame. Our tool may be of the utmost importance for journalists seeking high relevant data to write an article or for preparing an interview. In the era where Artificial Intelligence puts so many questions, helping journalists through this kind of tool may be the answer. "," In this project, users are offered the chance to submit a query on Tell me Stories, either by selecting one of the pre-defined topics that we show on the first page or by issuing their query (naturally subject to the temporal window defined by the dataset, in this case, September 2015). Once a query is issued, the user is shown a timeline summary about the topic. This interactive visualization enables the user to navigate back and forth through time-periods supporting the understanding of long-lasting events like wars, international or financial crises. For each selected timeframe, users are offered the top-20 most relevant titles from that time period. In addition to this, users are also offered a word cloud that summarizes the most relevant keywords appearing in the set of documents. For instance, issuing the query ""iran nuclear deal"", a hot topic nowadays, shows some of the efforts done by President Barack Obama to set a deal at that time. However, moving to the advanced search feature (which is visible once a query is issued) offers users the chance to play with other datasets. In this project, we provide (as an example) access to the Portuguese web-archive collection (which offers access to millions of documents over more than 10 years long). Issuing the query ""acordo nuclear irão"" will give us a more comprehensible story from 2010 onwards. A user interested in getting to know more information about the issued topic can then click on the corresponding headline to access the preserved webpage (which no longer exists in the conventional web). Can you imagine how great would it be for a journalist to query his/her own dataset and get some new insights that he/she was not aware of? Or just remember some forgotten details about a given topic? ",http://tellmestories.pt,https://link.springer.com/chapter/10.1007/978-3-030-15719-7_34,http://www.ccc.ipt.pt/~ricardo/ficheiros/ECIR2019Poster1.pdf,https://github.com/LIAAD/TemporalSummarizationFramework,https://play.google.com/store/apps/details?id=com.app.projetofinal,http://yake.inesctec.pt,,"Ricardo Campos, Arian Pasquali, Vítor Mangaravite, Alípio Jorge, Adam Jatowt","Ricardo Campos is an assistant professor at the Polytechnic Institute of Tomar [http://www.ipt.pt]. He is an integrated researcher of LIAAD-INESC TEC [https://www.inesctec.pt/en/centres/liaad], the Artificial Intelligence and Decision Support Lab of U. Porto, and a collaborator of Ci2.ipt [http://www.ci2.ipt.pt/], the Smart Cities Research Center of the Polytechnic of Tomar. He is PhD in Computer Science by the University of Porto (U. Porto). He has over 10 years of research experience in Information Retrieval and Natural Language Processing and has been given several awards for his works and reviews at international journals and conferences. He his, together with Alípio Jorge and Adam Jatowt one of the organizers of the Text2Story workshop [text2story20.inesctec.pt/] and a principal researcher (together with Alípio Jorge) of the Text2Story project Text2Story – Extracting journalistic narratives from text and representing them in a narrative modeling language. More in http://www.ccc.ipt.pt/~ricardo Arian Pasquali is a researcher associated to University of Porto and the LIAAD-INESC TEC research center. He has MSc in Computer Science by the University of Porto with a specialization in data mining. He has received serveral awards for his projects. Currently in an inviting researcher at the Signal AI company. His research interests involve machine learning applied to text mining, in particular information retrieval and natural language processing. More in http://www.dcc.fc.up.pt/~apasquali Vítor Mangaravite is a PhD candidate at Universidade Federal de Minas Gerais (UFMG) and an external research collaborator LIAAD-INESC TEC. He has MSc in Computer Science by UFMG in Database Lab. His research interests are Information Retrieval, Machine Learning, and Information Theory, with special interests in Ranking Models on Entity and Expert Finding. He has been researching in the fields of Information Extraction, specifically on keyphrase extraction algorithms and temporal summarization, having been given a few awards. Alípio M. Jorge is an associate professor and head of the Department of Computer Science of the Faculty of Science of the U. Porto and the coordinator of LIAAD/INESC TEC - INESC since 2012. He is PhD in Computer Science by U. Porto. His research interests are Data Mining and Machine Learning, in particular association rules, web intelligence and recommender systems. He lectures on information processing and data mining. He co-chaired international conferences (Discovery Science 2009, ECML/PKDD 15 and EPIA 01), workshops and seminars in data mining and artificial intelligence. He was Vice-President of the Portuguese Association for Artificial Intelligence. He is in the coordination board for the Artificial Intelligence Portuguese",,,
South Africa,Mail&Guardian,Small,Participant,Best data-driven reporting (small and large newsrooms),Counting Every Vote,04/12/19,"Long-form,Breaking news,Database,Infographics,Chart,Elections,Politics,Women,Human rights","Scraping,D3.js,Three.js,Canvas,JQuery,Json,Microsoft Excel,Google Sheets,CSV,Python"," M&G Data Desk analysed voting results for the past 20 years ensuring every vote and area was part of the discussion leading up to May 8.      Three months prior to the National Elections in May we extracted, compiled and analysed election data from the past 20 years yielding stories about how the youth wouldn't vote, how one smaller party would cause an upset and the decline of the ruling party.    We were innovative in approaching the coverage of these elections focusing on using new technologies and directly engaging voters who desperately wanted to be heard.  "," The M&G Data Desk was innovative in how it approached the elections using technologies that fit into our minuscule budget yet producing work that can rival any newsroom and can be used by our readers and society to make the best decisions.   The 2019 National Elections were the most hotly contested and this year instead of focusing on the politics of parties and personalities, the M&G Data Desk felt compelled to focus on the voters and what were the most important issues to them. But we didn't only want to focus on anecdotal evidence. The Data Desk created databases with elections records spanning over 20 years of how voters would either turn out or not. It was imperative to also analyse the progress made in key areas where voters had protested for instance and where voter turnout had been the lowest. The Data Desk engaged the electorate about issues that were important to millions and would dictate which party would make strides and which would fall to the wayside.  We questioned the data we had gathered and ask questions such as  What do Women Want?  The data showed clearly that more women than men registered to vote and possibly turned out at the polls yet their interests were not proportionately represented by political parties. We also focused on why certain areas all over the country did not turn out to vote -  Where voters won’t come out to vote –  and which areas were using their democratic right to chop and change who they wanted as their representatives. These are all issues affecting the electorate which have not been adequately covered by the media. "," We used Python to extract the data from the dozens of pages elections data had been compiled in. The data was in various formats and had to be cleaned in CSV. The data was then analysed using Excel, Tableu and    Once we had the data prepared in various CSV worksheets and the data had sent us to tell the stories in particular areas we used Flourish to create interactive storytelling methods to better deliver the content.    Graphics were created with DataWrapper and Flourish.    We then created the Live Results Page using PYTHON by connecting it to the Electoral Commission of South Africa's API, ensuring zero lag time between results allowing voters and readers to engage and ask pertinent questions about their preferred party's.  This allowed for on-time reader engagement to direct the content our readers needed.   This platform had more than 72 000 unique views over three days, this is unheard of in a newsroom.    From our analysis, we first noted that we were able to correctly predict how a right-wing political party would gain a substantial number of votes in the 2019 National Election.   Our data was credited and discussed across the media spectrum.   The stories and interactive graphics were viewed and shared hundreds of times on all platforms. Most importantly the Data Desk created content backed by technological and scientific means to open conversation on television and radio shows. "," We learned that the lack of resources in newsrooms does not mean the inferior quality of journalism. There are numerous avenues a team like ours can tap into to ensure citizens are better informed and their stories are told, backed by intense research and data analysis.  Partnerships are one of the most important aspects of ensuring journalism continues to thrive and governance and government are held to account.  On the other hand, the Data Desk spent weeks creating the Live Results Map which viewers spent more than 8 minutes at any given time on yet we did not turn this into a commercial success and this is what newsrooms need. We learned a very difficult lesson here and going forward more team members must be brought in to flesh out how best such digital success can be turned into commercial success.   In the six months of its establishment, the Data Desk has changed how the M&G newsroom and other platforms approaches stories and specifically this election. The team has produced data-driven stories that can be used by our readers to make better decisions, stories that drive the national debate and bring facts and figures to the national discourse. "," Data journalism is not simply about the numbers and graphics. It's about journalism and how the numbers affect people. Though we had a large amount of data, it did not mean much unless citizens were engaged and understood what it all means. This is what we did with this election. We used publicly available data to find how the elections would turn out. We were directed by the data showing us where the voiceless were.   The 2019 National Elections were the most hotly contested and instead of focusing on the politics of parties and personalities, the M&G Data Desk felt compelled to focus on the voters and what were the most important issues to them. But we didn't only want to focus on anecdotal evidence. The Data Desk created databases with elections records spanning over 20 years of how voters would either turn out or not. It was imperative to also analyse the progress made in key areas where voters had protested for instance and where voter turnout had been the lowest. The Data Desk engaged the electorate about issues that were important to millions and would dictate which party would make strides and which would fall to the wayside. ",https://mg.co.za/article/2019-05-03-00-jozini-where-youth-actually-vote/,https://mg.co.za/article/2019-04-12-00-where-voters-wont-come-out-to-vote/,https://mg.co.za/article/2019-04-18-00-umdoni-residents-want-change/,https://mg.co.za/article/2019-04-26-00-small-parties-are-shrinking-except-for-freedom-front-plus/,https://mg.co.za/article/2019-05-14-south-africa-this-is-your-new-parliament/,https://public.flourish.studio/story/33299/?utm_source=embed&utm_campaign=story/33299,https://mg.co.za/article/2019-05-17-00-what-the-low-voter-turnout-really-shows/,"Athandiwe Saba, Jacques Coetzee"," Athandiwe Saba is a multi-award-winning investigative data journalist, author and data editor. She has worked for the biggest weekly newspapers in South Africa with an eye for research-based and data-driven stories to provide in-depth articles on a vast range of topics. She also has experience in storytelling using multimedia and data visualisation. Three time speaker at The African Investigative Journalism Conference Recipient of News Corp Fellowship with Wall Street Journal and The Times.  She is passionate about data, human interest issues, and good governance.  She has been internationally recognised for her work in data journalism by the Global Editors Network.    ",,,
Italy,European Data Journalism Network,Big,Participant,Best visualization (small and large newsrooms),Don't Miss the Train,19/12/19,"Long-form,Cross-border,Multiple-newsroom collaboration,Environment","Scraping,D3.js,Google Sheets,CSV,Python"," As train travel becomes more and more popular in view of its relatively low environmental impact, we’ve investigated some of the main barriers preventing European citizens from taking the train more often. We mapped all active passenger stations in 16 EU member states and crossed those data with the population grid, to get a fine-grained picture of the reachability of train stations across the different European regions. This work was complemented by on-field reporting and national in-depth analyses, contributing to the collaborative and transnational character of the investigation, which is available in 11 languages. "," The project was published right before Christmas 2019, so it hasn’t been circulating for a long time – it is meant to focus on long-lasting trends and to exert an impact over time, as it circulates more and more in its different linguistic versions. This is typical of the projects of the European Data Journalism Network, which doesn’t have a large audience of its own but relies extensively on the involvement of its member organizations, building upon and disseminating the network’s investigations to their own public. Early feedback by readers and partners was encouraging however, and more stories based on the collected data are going to be published in the coming weeks.  "," In order to immerse and engage the audience in the navigation, we focused on the relation between the topic of the investigation, typography, and the visualizations. Indeed, the whole visual relies on the concept of travel: a metaphorical line that connects two distant points, echoing train travel between two stations. This was the starting point for the design, which relied heavily on a semantic approach: visuals behave as the meaning they drive. Not only visuals: typography was also specifically designed in connection with the topic, as some letters were deliberately stretched in order to convey the concept of distance.    In terms of technology, we used d3.js to develop dynamic visualizations that animate when the reader scrolls. We implemented a serpentine chart for d3, which wasn’t existing before – especially to express distances. We were inspired by the early serpentine-based timelines for “How Long Do Animals Live?” designed by Otto and Marie Neurath in 1939. The challenge was to adapt a chart initially designed to represent time spans to a chart representing distances. More specifically, we had to calculate and correct the distortion caused by the different radiuses in the curve of the serpentines, so to let the chart represent the data correctly. "," Coordination was the hardest part of the project, as a dozen of different partners based in different European countries were asked to contribute to it. This was the second large collaborative investigation run by the European Data Journalism Network, so we couldn’t count on well-established workflows. Network members were asked to contribute to the data collection/analysis by checking or integrating collected national data, while the following phases were managed by Journalism++ (in charge of data research and of the text for the main article), Sheldon.studio (in charge of the design of the main article), and OBC Transeuropa, which provided overall editorial coordination and took care of the impact and dissemination strategy.    Collaborative transnational data-driven projects can bring a lot of added value, as a single media organization would hardly be able to collect and master such an amount of information and to navigate through national specificities. Yet large collaborative investigations are very challenging in terms of coordination and management, as news organisations have different skills but are not used to working together, and they often have different expectations and standards. It was not trivial to come out with a product that would meet the demands of the network members, be in line with the budget and standards of EDJNet, and work reasonably well across borders – as we were presenting the same main story to a Swede, a Greek, or a Hungarian, coming entirely in his or her own language.  "," In the last years, only a few projects connected data visualizations with typography. In particular, it was rare for typography to be specifically designed or chosen in connection with the topic of a given data-driven story. For this reason, we believe that our project contributes to innovation in data and visual journalism projects, by feeding the debate on design and highlighting the immersive potential of apparently simple techniques.    The other feature of the project we are particularly proud of is its truly transnational character, which makes an ambitious work of data journalism directly accessible to a variety of audiences, including those based in European countries where data journalism is still not strong. It is possible – and valuable – to develop a data-driven project in several languages, but this requires effective workflows and the early adoption of a multilingual and multi-country perspective starting from the stage of the investigation’s design.    However, country comparisons may tell little of actual relevance to a given reader and to his or her daily experience: that is why we chose to include 16 different countries on one hand, but to go deep down to the local level on the other hand. In this way, we provided readers with a story that offers them both the large European picture and fine-grained data linked to their own everyday reality. Regional specificities and similarities across borders come out more clearly in this way, and it becomes easier for both national and local media across Europe to build their own stories out of data collected by the European Data Journalism Network.  ",https://datavis.europeandatajournalism.eu/obct/en/dont-miss-train/,https://www.europeandatajournalism.eu/eng/Investigations/Don-t-Miss-the-Train,,,,,,"Journalism++ (Leonard Wallentin, Sascha Granberg), OBC Transeuropa (Lorenzo Ferrari), Sheldon.studio (Matteo Moretti, Daniel Rampanelli), with the contribution of several other EDJNet members"," J++ is an international team of data journalism experts, including specialists in research, data analysis, datadriven storytelling, newsroom programming, graphical design and more. Leonard Wallentin are Sascha Granberg data journalists at J++ based in Stockholm.   Sheldon.studio is the first studio that focuses on the design of informative- experiences: projects that tend to decrease the distance between reality and perception, highly immersive experiences capable of informing an audience on complex and multifaceted topics. The newborn studio is funded by a group of professionals and academics (Matteo Moretti, Maurizio Lepore, Daniel Rampanelli) to bridge the gap between industry and university, applying the research carried out by Matteo Moretti at the Free University of Bolzano. The studio background allows to design regardless of a specific medium.   Osservatorio Balcani Caucaso Transeuropa is a think tank based in Trento (Italy). It was launched in 2000 and it is focused on South-East Europe, Turkey and the Caucasus. Over time, OBC Transeuropa expanded its scope and now reports on the socio-political and cultural developments of the entire Europe, combining journalism with research and education. It is a founding member and coordinator of the European Data Journalism Network. Lorenzo Ferrari has a PhD in the history of political integration and is one of the two editorial coordinators of the network. ",,,
Taiwan,Business Weekly,Big,Participant,Best visualization (small and large newsrooms),Taiwan's New Crisis: Inability of Industrial Waste Management,19/12/19,"Investigation,Explainer,Solutions journalism,Multiple-newsroom collaboration,Database,Infographics,Chart,Video,Audio,Environment,Business,Health,Economy","Animation,Drone,Adobe,Creative Suite,Microsoft Excel"," More than 100 Taiwanese companies, which built factories in China for the past 20 years, shifted their production capacity back to Taiwan in 2019 because of the U.S.-China trade war. However, Taiwan’s industrial waste disposal is already problematic ahead of the great shift. This investigative report tracks the past environmental records of these companies and reveals that when the Taiwanese government offers incentives to attract them to reinvest Taiwan, it did not consider measures to deal with their industrial waste, which further weighs on the environment. "," The next day when this project was published, Taiwan's Environmental Protection Agency announced policies including to inventory the land available for factories that process industrial waste. At the same time, more than 600 online readers and opinion leaders in Taiwan have shared this project through their social network, which help making this website reached 110,578 viewers in ten days. These reactions show that the project have awaked the public to the soaring crisis of industrial waste. "," We build up this website with HTML, CSS and Native JavaScript. For the motion chart, we use CSS Animation and SVG, with GIF and AE animation.   And we also put aerial video, aerial photo and audio in website to diversify the way of storytelling, make this hard topic reading-friendly. "," This project tracked the environmental records of 120 Taiwanese companies who have applied for the government’s “reinvest benefits”. The research includes their subsidiaries in China, which makes a total amount of 600 entities. We compared and crosschecked the databases of environmental protection agencies in both China and Taiwan, and the data collected from non-profit organizations. To examine these data is time-consuming and requires consideration.   The results show 58% of these 120 Taiwanese companies have violated environmental laws in Taiwan or China such as water pollution, air pollution, waste or poison between 2014 and 2019.   We also checked the statistics from the Taiwan government and found that 6.14 million metric tons of industrial waste were hoarded and unprocessed in the past ten years, proving the seriousness of industrial waste problem in this island country.   This investigative project reveals Taiwan’s capacity of processing industrial waste is very insufficient while the government is luring Taiwanese manufacturers to reinvest the island amid trade war. The project suggests that the government should reviews the investment applications carefully, especially those who have poor environmental records in the past. ","   Journalist should participate in the making of website in early stage:  The way of storytelling of digital and printed magazine is totally different. Journalist should participate in the production as early as it can, to make the digital narration more fluently.    Check list of materials is a must in making digital feature story:  The amount of photo, soundtrack, video and text of a digital website is at least two times complicated than a printed magazine, especially when you’re not familiar with making a digital feature story. In this way, to name a check list of all the materials you may need in beginning of the production is a must to prevent chaos.    Digital marketing is an art:      To marketing your feature story to your fans, but in the mean while not to makes them feel annoying is an art. How to weigh and calculate your post through all social media channels needs a system.  ",http://bw.businessweekly.com.tw/event/2019/IndustrialWaste/,,,,,,,Tien Hsi Ju/Wu Chung Chieh/Ta Jen Lee/Sheila Lee/Ting Ann Lee/Lin Fang Ju/Hou Liang Ju/Hui-Ying Cha/Astrid Chang/Hsu Chun Chieh/Chen Chung-I/Ling-Fen Huang/Yang Yi Chien/Ko Lin/Ta Li Ju/Lai Shih Jen," Business Weekly is a leading financial media which founded in 1987 in Taiwan. We currently has the highest circulation among all kind of magazines in Taiwan by average circulation of 100,000 every week, and with more than 1,300,000 readers base.   In recent years, we are on our journey of digital transformation. Besides running Facebook fans page, Line@ account and Youtube channel, we also try hard on producing digital feature story, in order to satisfy our readers and explore the possibility of digital news.   For this feature story, we team up with journalist, researcher, photographer, video journalist, programmer, social media manager and editors. More than half of this team is around 30. ",,,
South Africa,"Viewfinder, Daily Maverick, GroundUp, Checkpoint on eNCA",Small,Participant,Best data-driven reporting (small and large newsrooms),Killing the Files: IPID's cover-up of police brutality in South Africa,10/07/19,"Investigation,Long-form,Multiple-newsroom collaboration,Documentary,Database,Fact-checking,Infographics,Video,Politics,Corruption,Crime,Gun violence,Human rights","Animation,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,R,Python"," Decades after apartheid, police brutality remains pervasive in South Africa. The perpetrators are almost never held accountable. Why? This project exposed that SA’s police watchdog, the IPID, had a long history of “completing” cases without proper investigation. This was done to inflate performance statistics, while obstructing justice for victims. In reporting this story, I accessed IPID’s database for more than 36,000 criminal complaints against the police. We analysed the progression of these cases. Whistleblowers, leaks, research and the experiences of victims augmented the data analysis to reveal the reasons, extent and true human cost of IPID’s statistical manipulation. "," This project launched with the broadcast of a video documentary on a national television broadcaster (Checkpoint on eTV); the publication of a long-form exposé in partnership with the Daily Maverick and GroundUp; and, a series of newscasts on Eyewitness News’ radio service. On launch day, we approved requests by News24 and IOL – two of SA’s largest online news sites – to syndicate the exposé in full. Through the launch week, our exposé was picked up as a hard news item by various other newsrooms and current affairs shows on national radio and television.   Civil society and policing sector experts immediately called on IPID to account. Notably, the Institute for Security Studies (ISS) condemned the cover-up and called for an independent inquiry. The African Policing Civilian Oversight Forum (APCOF) scheduled a roundtable discussion for sector experts on IPID’s role and challenges. APCOF is in the process of finalising a policy paper based on Viewfinder’s findings. We understand that it contains recommendations for improvements in case management and oversight mechanisms at IPID.   In the midst of this public scrutiny, IPID presented its 2018/19 annual report to Parliament. That presentation was overshadowed by our findings of statistical inflation contained in successive IPID annual reports. MPs called on IPID’s management to account and committed to more closely monitor IPID’s progress on addressing statistical manipulation. Weeks later IPID presented to Parliament again. It acknowledged that some degree of statistical manipulation did occur.   Meanwhile, Viewfinder established a reporting line for victims of police brutality who felt that their cases were poorly handled. We are in the process of assisting numerous such victims to access their dockets from IPID and to analyse whether those investigations were properly done or covered-up. ","The first technique I used was to submit a Promotion of Access to Information Act (PAIA) request to the Independent Police Investigative Directorate (IPID) for access to their Master Register databases. My request was granted. I also used OCR to lift d Then, I used two investigative techniques to develop a clear understanding of what the data represented and how the data was produced and compiled: i) I studied IPID's Standard Operating Procedures, specifically as it related to the progression of cases on the computer case management system ii) I interviewed informants and whistleblowers to understand the institutional environment and pressures which informed the digital aspects of case management and data compilation. Through this latter process I learnt more about how cases were manipulated to inflate performance statistics. Then I worked with data scientists to clean and align dozens of spreadsheets so that they may be compiled into one mastermaster database which allowed us to track the progression of each of more than 36,000 criminal complaints against the police as these progressed through IPID's case management system. This was a wholly collaborative process, which I directed on the basis of my knowledge about IPID's processes and what the data at each stage of the case management process. We variously used Python and R to do this cleaning and compilation. Then, with my direction, we analysed the data to identify various red-flags and test various hypotheses: that the conviction / accountability ratio for these crimes was very low, that case ""completion"" indeed happened at or near performance reporting deadlines (whistleblowers claimed many such cases were poorly investigated), that certain cases were closed or completed in contravention to SOPs. Then I employed the services of a motion graphics professional to visualize certain elements of the data for inclusion in my film and"," Viewfinder was founded on the understanding that “public interest” journalism in post-apartheid South Africa should be rooted in the imperative for greater equality and redress in our society. We are creating a space for long term investigations into abuses of power which impact directly on the poor and marginalized majority of South Africans. We are producing journalism which engages audiences across the class spectrum and empowers them with the information and knowledge to hold large institutions accountable on such abuses. Such a dual mandate has been sorely missing from other SA newsrooms in general, and investigative journalism outfits in particular.   This project was based in an investigation that took longer than a year to report. On a shoestring budget and with minimal other support, we collated and analyzed scores of spreadsheets; submitted more than a dozen access to information requests; trawled through large troves of public records; obtained leaks; produced a 25 minute documentary film and traveled to remote parts of the country to speak with whistleblowers and victims of police brutality. We committed a substantial part of our funding to fact-checking and legal vetting – to ensure watertight accuracy and credibility. The result, I believe, is a demonstration of what is possible when bringing best practice in data analysis, conventional investigative techniques, research, solutions orientated journalism, field reporting and storytelling to bear on abuses of power which impact on the poor and marginalized majority in SA.   The hardest part of this project was to keep the faith that we could achieve that objective, over fourteen months of arduous investigation and preparation. "," As a former news journalist committed to reporting on social justice struggles, I used to believe that data-journalism somehow existed in a different realm from what I was doing. For me, as I suspect it is for many other reporters, data journalism was an opaque world of infographics and fancy tools produced by people with different skillsets and interests to myself.   In reporting this project, those assumptions were exploded. My project was less a “data-journalism” project (in the way I had previously conceived such to be) and more a project of investigative and explanatory reporting which “stood on a foundation of data and analysis”. This was a revelation, and one that I would like to share with other reporters that have not yet seen the potential of data as a legitimate and important source for bolstering accountability and investigative journalism projects.   As a reporter I have always valued the importance of good storytelling to engage one’s audience. I think that my project may demonstrate to others the potential of data as a source of narrative inspiration. Data is often seen as depersonalized, neutral and dry. My reporting demonstrated the human story behind the data and demystified the processes which act upon the people implicated in the data and the management of thereof. In my story, the reader is reminded that the numbers represent real victims of rape, torture, murder or assault by police officers. They represent real perpetrators, who often act with unchecked impunity. The premature “completion” of cases is the outcome of decisions and processes by IPID investigators (real people) who are driven to cover-up due to the incredible constraints and pressures of their work. ",https://viewfinder.org.za/kill-the-files/,https://viewfinder.org.za/month-end-at-the-ipid-a-time-for-killing-files/,https://viewfinder.org.za/special-closure-the-high-water-mark-of-ipids-cover-up/,https://viewfinder.org.za/key-take-aways-ipids-cover-up-of-police-brutality-in-sa/,https://vimeo.com/369767127,,,"Daneel Knoetze, Timothy Gabb, Alex Noble, Anton Scholtz, Laura Grant, Tomas Knoetze"," Daneel Knoetze bio   I am the founder of Viewfinder, an accountability journalism unit in South Africa. Viewfinder does long term investigations into abuses of power which impact on the public interest – specifically as it relates to the need for equality and redress – in post-apartheid South Africa. We provide our audiences with the knowledge they need to demand accountability from large institutions on the specific abuses of power that we expose. My debut investigation exposed a national cover-up of police brutality in South Africa. The investigation made a significant impact, leading to SA's police watchdog being called to account in Parliament and ongoing calls from civil society for policy reform within the criminal justice system.    I was a fellow with the US State Department's Hubert H. Humphrey Fellowship in 2017/18. HHH fellows are selected on displayed qualities of leadership and public service in their respective fields and home countries. During my fellowship year, I studied best practice in investigative- and public interest journalism at Arizona State University. I did a professional affiliation with Reveal from the Centre for Investigative Reporting in San Francisco - the newsroom upon which Viewfinder is modeled.    Previously, I worked as the communications officer for Ndifuna Ukwazi (2015 - 2017). NU is a civil rights NGO and law centre which advocates for desegregation in Cape Town through well-located affordable housing development. Here, I was instrumental in launching and advancing Cape Town's most formidable ""Urban Land Justice"" campaign, which gave birth to the Reclaim the City social movement. RTC and NU's impact on the advancement of desegregation, evictee support and affordable housing development in Cape Town is well known.    Between 2012 - 2015, I was a news reporter at the Cape Argus and GroundUp. I mainly reported on community protests, land struggles and other issues related inequality and marginalization in the townships and farmlands surrounding Cape Town.    I am a graduate of Rhodes University (UCKAR) in Makhanda, Eastern Cape. ",,,
France,"La Montagne, Le Parisien, Ouest France, Le Progres, Le Telegramme, La voix du Nord, Sud Ouest",Big,Participant,Best data-driven reporting (small and large newsrooms),Collectif data+local,30/07/19,"Investigation,Explainer,Long-form,Multiple-newsroom collaboration,Database,Open data","Scraping,Google Sheets,R,Python"," Hello,   I'm not sure about the category or even about the fact that I can entry the Sigma Awards, but here is the story.   We, at La Montagne, have created a consortium of data journalists from more than 20 newsroom of local dailies in France : La Montagne, Sud Ouest, Le Parisien, Le Telegramme, Ouest France, La Voix du Nord...  "," . january 2018, first story about parking fines. We have collected pricing decisions for each town and created a shared spreadsheet to be able for each newsroom to compare the prices for each town of France.   --> the ministry of transportation ask us if we could give them our spreadsheet...    . winter 2019-2020 : we began a huge investigation in november wich as made the front pages of more than 10 dailies newspaper about transparency into the public medical system.    --> since this investigation several hospitals have created a deontology comittee, several doctors who haven't declared the right amount received from pharmaticals have resigned...  ", More than 30 data journalists are working together for these stories.   We're using two tools : slack for the conversation / organisation and Google Docs for shared docs.  ," This is a unique collaboration effort : there is no other group like that in France (in Europe ?) that put all the ""old"" newspapers into a common work.  ", Working together is a major way to have an impact. ,https://collectif-datalocal.github.io/,https://www.lamontagne.fr/clermont-ferrand-63000/actualites/la-secheresse-a-t-elle-entame-le-niveau-d-eau-des-nappes-phreatiques-de-votre-departement_13615074/#refresh,https://twitter.com/search?q=transparencechu&src=typed_query&f=live,,,,,"Cedric Motte (La Montagne), Frédéric Sallet (Sud Ouest)"," Cedric Motte is at the head of the newsroom, in charge of innovation and digital products, at La Montagne Centre France. He created the ""collectif data+local"" 2.5 years ago with Paul-Alexis Bernard, who was his team mate (and passed away last year)   Fredéric Sallet is a data journalist at Sud Ouest. ",,,
Sweden,Sveriges Television,Big,Participant,Best data-driven reporting (small and large newsrooms),The Unsafe Curves of Sweden,26/09/19,"Investigation,Breaking news,Database,Open data","Scraping,QGIS,Json,Microsoft Excel,CSV,PostgreSQL,PostGIS,OpenStreetMap,Python"," The data team at SVT used methods not previously used by journalists - or experts in the field - to reveal extensive flaws with Swedish roads.They found almost 16 000 curves on Swedish roads that are unsafe to travel on. The roads are constructed in such a way that it's impossible to keep the car on the road if you travel at the set speed limit. The team was also able to show that more accidents happen in these curves than those who are constructed correctly. A description in English can be found here: <a href=""https://medium.com/the-svt-tech-blog/finding-16-000-unsafe-curves-427889afde8b"">https://medium.com/the-svt-tech-blog/finding-16-000-unsafe-curves-427889afde8b</a> "," To start with, the findings were used in a 45 min documentary that was broadcasted at Sep 25th. There the result of the analysis was only mentioned briefly. On the morning of Sep 26th, there were broadcasts and articles published on the web, in both national and local news. All 24 local news outlets did local stories of their own. There was a panel at the national morning show and news segments in all broadcasts from morning until night. At the nine o'clock news that night the Minister for Infrastructure, Tomas Eneroth, called for change and that there should be signs posted along the unsafe curves. We've also had a follow-up question in the parliament from a member of the parliament to the minister. There were over 100 articles written about the story in mostly local media - and the team was also invited by a group of analysts from the Swedish Transport Administration (the agency that are responsible for road safety in Sweden) to discuss how the data was used and the methods of the project.  "," Every state road in Sweden is measured by a measuring car that drives around and collects all kind of data about the road: the radius of the curves, how uneven the surface is, how much (or how little) the road leans to give a few examples. The data is collected meter by meter, but it’s stored with an average of 20 meters. This data is stored in a database that is publicly available through an API. (<a href=""https://pmsv3.trafikverket.se/"" rel=""noopener nofollow"" target=""_blank"">PMSV3</a>) A selection was made from the team to look at roads with at least one accident per year for the last five years. We excluded parts of the roads that had a speed limit lower than 70 km/h and also roads that were separated in the middle. In all we looked at 26 000 km of roads and we did this in 100-meter segments. For each segment we first determined whether it was a curve or not using another formula found among guidelines for constructing roads. (<a href=""https://trafikverket.ineko.se/Files/sv-SE/12046/RelatedFiles/2015_086_krav_for_vagars_och_gators_utformning.pdf"" rel=""noopener nofollow"" target=""_blank"">VGU 2015:086, s 106</a>) Once we concluded that the segment was a curve, we calculated a formula for friction for each segment. This formula calculates the friction needed to keep the car safely on the road using the speed limit on the road, the radius of the curve and the cross slope of the road.    We then had to turn these segments back to curves, and in all we did analysis on about 1.1 million segments. We found a problem with the data for round abouts and some cross roads. The measuring car either lost data or had data that was abviously wrong. We had to use OpenStreetMap to identify and exclude those ""curves"" that were not really curves.    See a longer description in English here: <a href=""https://medium.com/the-svt-tech-blog/finding-16-000-unsafe-curves-427889afde8b"">https://medium.com/the-svt-tech-blog/finding-16-000-unsafe-curves-427889afde8b</a> "," Knowing that our method was ok. We talked to several outside experts. We read a lot of scientific reports on the subject. We started to write a methodology very early on in the project. We also shared that methodology with all outside experts and also with the agency. We did this early on and asked for feedback - long before we did our interview.    Having a mixed team of journalists and developers was crucial for this project. Just dowloading the data and making all the calculations needed would have been hard without the expertise from the system developers involved in the project. Getting data and findings to our local news outlets was another challenge. We ended up creating unique pdf-files for each outlet, containing a description of the method and chosen roads from their area. We also spent a lot of time on the phone with our local journalists, explaining what we've done and suggesting angles and stories for them to do. This was also what really made the project, since the roads in question are mostly in the country side and through the reporting we heard voices from people affected by these roads. ", To work closely with developers - also for analysis and data gathering - not just for visuals. To make sure you write a methodology early and keep revising and adding to that through out the project. Not to be afraid to reach out to experts in the field - they can help you get things on track.  ,https://www.svt.se/nyheter/inrikes/tusentals-felutformade-kurvor-pa-svenska-vagar,https://www.svt.se/nyheter/inrikes/vagarna-pa-landsbygden-forsamras-men-prioriteras-bort,https://www.svt.se/nyheter/inrikes/trafikverket-vi-behover-prioritera-andra-saker,https://www.svt.se/nyheter/inrikes/sa-gjorde-vi-undersokningen-om-sveriges-osakra-kurvor,https://www.svt.se/nyheter/infrastrukturministern-maste-sanka-farten-i-kurvorna,https://www.svt.se/nyheter/lokalt/ost/folj-med-bargaren-jimmy-langs-vagen-med-26-osakra-kurvor,https://www.svt.se/datajournalistik/sveriges-osakra-kurvor/,"Helena Bengtsson, RIckard Andersson, Fredrik Stålnacke, Oskar Jönsson, Linnea Carlén, Lena ten Hoopen"," Helena Bengtsson is Editor for Data Journalism at Sveriges Television, Sweden’s national television broadcaster.   Rickard Andersson and Fredrik Stålnacke work as system developers/data journalists at SVT.    Oskar Jönsson is an investigative broadcast reporter at SVT.   Linnea Carlén works as an investigative on-line reporter at SVT.   Lena ten Hoopen is an investigative documentary reporter at SVT. ",,,
Belarus,TUT.BY,Big,Participant,Best data-driven reporting (small and large newsrooms),The ranking of the regions in Belarus: the best and the worst,09/02/19,"Investigation,Explainer,Long-form,Infographics,Chart,Map,Business,Immigration,Economy,Employment","Scraping,QGIS,Adobe,Microsoft Excel,CSV"," We have created a ranking of the Belarusian regions to see if the situation is really so bad (spoiler alert: yes) and what needs to be done to unleash the potential of individual parts of our country. Note that the text found at link No. 1 is dated January 31, 2020. The article is just a translation into English, users have not seen it. The original text was published in Russian on September 2, 2019. Link No. 2 leads to the original text. "," We highlighted the problem of the ongoing process of stratification of regions in Belarus. The article received positive feedback from our readers on social media and our forum, which leads us to the conclusion that we raised the public awareness on the problem. Our work and insights also inspired our colleagues to further research the issue and publish new texts, i.e., about the youngest region in the country or the one which lost the largest number of people. "," MS Excel, QGIS, Adobe Illustrator, Datawrapper. "," It was challenging to find and put data together. Then we had to analyze the data and choose the stories to show it in the most exciting way. This is when we found the regions affected by the Chernobyl accident with a high percentage of young population, the dramatically growing gap between Minsk and the rest of the country and others. "," The project shows how an analysis of several simultaneously mainly economic indicators can get you to the level of households and individuals. This way, one can observe people leaving regions, where large enterprises closed or entrepreneurs moving to the suburbs of big cities to open small businesses there, etc. ",https://news.tut.by/society/670884.html,https://finance.tut.by/news648075.html,,,,,,"Anton Devyatov, Svetlana Baksicheva, Elena Pashinina"," TUT.BY is the largest Belarusian independent media outlet. We have 70 journalists and editors working in the newsroom.   Anton Devyatov. Education: an energy engineer. I have been engaged in data visualization (search, collection, analysis and product development in various techniques from static illustrations to interactive JavaScript works) for more than 8 years. Over the past few years, I have also been producing editorial special projects.   Svetlana Baksicheva. In 2017, I  graduated from the philological faculty of Belarusian State University with a degree in Russian philology. In my student years, I worked as a freelance journalist for various media organizations.   I have been working as a TUT.BY journalist since November 2017.   Elena Pashinina. I have worked in media for seven years, and although I'm not a professional journalist (I'm a linguist and English interpreter by profession). I've been always interested in writing news, keeping people updated and sharing useful content. In the last academic year, I got a job in TUT.BY, one of the most popular Belarusian news outlets, where I joined Naij.com (later Legit.ng) team. Five years later I had my own team of editors and several African media platforms to write for (Tuko in Kenya and Yen in Ghana).  Now I work for BelarusFeed, the media project for and about Belarusians, foreigners living in and outside Belarus, ex-pats and anyone interested in one of the less discovered countries in Europe ",,,
Finland,Svenska Yle and Yle,Big,Participant,Best visualization (small and large newsrooms),"This is how educational level, unemployment and income affected the results in the Finnish EU-election 2019",28/05/19,"Investigation,Long-form,Multiple-newsroom collaboration,Database,Map,Elections,Politics,Economy,Employment","Personalisation,QGIS,Google Sheets,CSV,R"," A scrollytell-vizualisation that shows how demographic factors affected the election result in the EU-parliament election in May 2019.    The user can choose between exploring the data themselves or read the biggest findings in our scrolly tell.    This vizualisation for example shows that the success of the populist party cannot be explained by traditional left-right theories. The areas where the populist party won were not defined by income, age or unemployment but rather by voter turnout, education level and property ownership. "," Combining the voting data with demographic data for such small areas (most polling areas has a population of about 3000 people) gave us the possibility to make indepth-political analysis of the voting behaviour. We tried the concept after the genral election in April 2019 and tweeked the visualization for the EU election in May.   Many academics commented that the stories we did brought new light on how Finns vote and how voting still is somewhat related to social class. Most of all, our stories delivered new fact based information to the elections related public discussion. There is never too much of that around. "," Because this information was not available we had to gather it ourselves. First we got the geographical info about the polling districts (which we also used for visualisations in our election result service on election night). Then we asked the statistics official in Finland to combine the polling districts to statistical grid database (250 m x 250 m).    We also had two researchers who know this field very well help us with selecting relevant data and also analyzing the findings.   We had two data teams working with the project, from Finnish speaking and Swedish speaking departments  The project consists of 4 published stories: 2 in Finnish and 2 in Swedish:   For an initial analysis of the data, we utilized R. The data consisted of more than 100 demographic variables for each polling area. The demographic data was combined with the results data at party-level, and scatter plots describing result and demography were created for each variable and party. Visually examining the scatter plots, we could quickly identify correlational patterns between demography and support for particular parties. A handful of variables were chosen for the visualization. Population densities for polling areas were calculated with QGIS (only for the European Parliament version of the viz). "," - The demography data did not include all variables that might have shown interesting correlational patterns. For example, we did not have data the about mother tongue (Finnish/Swedish/other) or the immigrational status/share of foreign-born among the residents.  - Front-end animations and responsive grids on highly deadline sensitive story.  - We would have liked to publish all the material used for the stories as open data but we couldn't because of the contract with Finnish statistics official from whom we got/bought the data. ", Demographical data can be used not only to describe a certain area but also to write stories about society. Whebn it comes ti electionresults it was essential that we could get demographic data for the exact geographical areas that were the polling areas. When combining datasets in this matter you should have an open mind about what the findings will be.   The dataset also provided us with backgroundstories that could be used for other projects later.     ,https://svenska.yle.fi/artikel/2019/05/28/sa-rostade-man-dar-du-bor-kolla-hur-utbildning-arbetsloshet-och-inkomster,https://yle.fi/uutiset/3-10803950,https://yle.fi/uutiset/3-10803950,https://svenska.yle.fi/artikel/2019/04/17/samlingspartiet-vann-dar-de-rikaste-bor-sdp-i-omraden-med-hog-arbetsloshet-se,https://svenska.yle.fi/artikel/2019/06/28/var-bor-traktens-barnfamiljer-var-finns-det-flest-sommarstugor-testa-din,,,"Eemeli Martti, Linus Lång, Juha Rissanen, Malin Ekholm, Janne Toivonen, Juho Salminen"," Svenska Yle Data is a small datajournalism team within the minority media organisation Svenska Yle. Svenska Yle is a seperate organisation within the finnish public service company Yle. Svenska Yle produces tv-, radio- and online journalism in swedish for the 300 000 head strong swedish speaking minority in Finland. Svenska Yle Data consists of a project manager/datajournalist (Malin Ekholm) a datajournalist (Linus Lång) and a developer (Petter West). We work independently with a focus on online datajournalism but closely together with other newsrooms within the organisation. <a href=""https://mandrillapp.com/track/click/30284241/svenska.yle.fi?p=eyJzIjoiSUJBNFpGRE41c2Zfd1JmZXphcGM0N0NtT24wIiwidiI6MSwicCI6IntcInVcIjozMDI4NDI0MSxcInZcIjoxLFwidXJsXCI6XCJodHRwczpcXFwvXFxcL3N2ZW5za2EueWxlLmZpXFxcL2RhdGFqb3VybmFsaXN0aWs_cGFnZT0wXCIsXCJpZFwiOlwiNTM4NDJhNTg5NjBkNDA3Njk2NzlmODY2OTEyNjlmZWNcIixcInVybF9pZHNcIjpbXCIwMzhhNmU4NmFhOTVlMzIwZWU5M2RkMzFjZmZmNzIzNjY1MGMwNDU1XCJdfSJ9"" target=""_blank"">https://svenska.yle.fi/datajournalistik?page=0</a>   Yle Plus desk is one of Yles datajornalism teams. Plus Desk creates impressive and in-depth web journalism as a part of News and Current Affairs at Finnish Broadcasting Company (Yle). We specialize in data journalism, visual feature stories, virtual reality and interactive articles. <a href=""https://plus.yle.fi/"">https://plus.yle.fi/</a> ",,,
Finland,"Svenska Yle, Yle",Small,Participant,Best visualization (small and large newsrooms),Form your own government,17/04/19,"Explainer,Solutions journalism,Quiz/game,Infographics,Elections,Politics",R," This visualization lets the reader explore the complex process of forming a government in Finland. In a gamelike fashion you can try different coalitions and see if the parties you have chosen agree or disagree and if you can succeed to get a majority of the MPs to support you politics. The data used was th eelection reults in the 2019 Finnish Parliamentary election, the individual MPs answers to Yles election compass as well as the parties answers to the election compass.  ", The visualisation was well recieved and reposted every time there was a new twist in the forming of the government and later in the fall when there was a governmet crisis. It has been used in schools as a part of social studies education to show how a finnish government is formed.   The government game was one of the finalists in the application category at the NODA awards 2019 (Best Nordic datajournalism) and regarded to be the best application that shows how you form a government that the jury had seen.    The visualization was also translated in to finnish an published at Yle.fi and widely discussed among finnish political commentators. ," We had analysed the candidates answers in Yles election compass for anouther piece ealrier and done a political compass (<a href=""https://svenska.yle.fi/artikel/2019/04/04/centern-mer-vanster-an-sdp-se-var-partierna-finns-pa-den-politiska-kompassen"">https://svenska.yle.fi/artikel/2019/04/04/centern-mer-vanster-an-sdp-se-var-partierna-finns-pa-den-politiska-kompassen</a>) using R and psych. The visualization is built using Vue JS. The visualizastion calculates the parties mandate, where the MPs and the parties positioned themselves on the political compass, how many questions the parties agreed on and which questions that seperated them.   The storytelling goes from basic to more in depth information the further you read. "," We tried many different ways to visualize how to form a majority government. The level of nerdieness in the texts was also challenging. You need to be correct but still readable. We are happy that as young students 12-year olds have been able to understand it.    What data to show as a visual elemant and what to generate as text was also a issue we discussed a lot.     The visualisation was published just two days after the election so much of the preparation was made beforehand. We published five different datastories about the election results (maps, demographic datastory, the new MPs on the political compass etc) so effective time management was wery important for this to be possible. ", It is easy to outsmart the audience when it comes to topics that you are very familiar with. Not every one knows that much about politics as you would think. The biggest lesson from this was that it pays of to be as clear an understandable as you possilbly can even tough it is much harder to explain things that way.  ,https://svenska.yle.fi/artikel/2019/04/17/nu-far-du-vara-regeringssonderare-bygg-din-egen-regering-med-vart-regeringsspel,https://yle.fi/uutiset/3-10754560,,,,,,"Malin Ekholm, Petter West"," Svenska Yle Data is a small datajournalistic team within the minority media organisation Svenska Yle. Svenska Yle is a separate organisation within the finnish public service company Yle. Svenska Yle produces tv-, radio- and online journalism in swedish for the 300 000 head strong swedish speaking minority in Finland. Svenska Yle Data consists of a project manager/datajournalist (Malin Ekholm) a datajournalist (Linus Lång) and a developer (Petter West). We work independently with a focus on online datajournalism but closely together with other newsrooms within the organisation. <a href=""https://mandrillapp.com/track/click/30284241/svenska.yle.fi?p=eyJzIjoiSUJBNFpGRE41c2Zfd1JmZXphcGM0N0NtT24wIiwidiI6MSwicCI6IntcInVcIjozMDI4NDI0MSxcInZcIjoxLFwidXJsXCI6XCJodHRwczpcXFwvXFxcL3N2ZW5za2EueWxlLmZpXFxcL2RhdGFqb3VybmFsaXN0aWs_cGFnZT0wXCIsXCJpZFwiOlwiNTM4NDJhNTg5NjBkNDA3Njk2NzlmODY2OTEyNjlmZWNcIixcInVybF9pZHNcIjpbXCIwMzhhNmU4NmFhOTVlMzIwZWU5M2RkMzFjZmZmNzIzNjY1MGMwNDU1XCJdfSJ9"" target=""_blank"">https://svenska.yle.fi/datajournalistik?page=0</a> ",,,
Russia,RUGRAD.EU,Small,Participant,Best visualization (small and large newsrooms),Ancestral ties: Kaliningrad nepotism as a map,07/07/19,"Investigation,Database,Open data,Fact-checking,OSINT,Infographics,Politics,Business,Economy",Animation," This is how regional politicians, businesspersons, deputies, officials, security apparatus and judges originate ""influential clans"".   RUGRAD.EU, a Kaliningrad business portal, and Dvornik, a newspaper, under the auspices of <a href=""http://fij.org/"">Fund for Investigative Journalism (FIJ)</a> and <a href=""https://transparency.org.ru/"">Transparency International Russia</a> https://transparency.org.ru present ""Ancestral ties"" project. We have drawn up an interactive map of business, ancestral and public ties of Kaliningrad politicians, businessmen, deputies, public and municipal officers, security apparatus and judges who set up ""influential clans"" in the Kaliningrad region. The map helps to explain why and how certain regional processes come about. "," Law enforcement agencies and decision-making officials, as well as the public, have learned about corrupt connections of public officials that no one knew about before. Kinship, social and business ties create a large conflict of interest in the regional government system. This leads to unfair decisions that are made to the detriment of the public interest. After the publication of the project, many corruption links were destroyed - officials left their posts and now our project needs a serious update. "," We used a complex visual design, we had to draw every link between people and each other.   In addition, we had to use various investigative techniques and work with a large body of data to establish kinship and other relationships between people.   We combined work with public registries and databases and deep Internet search, analysis of user comments in the media and social networks that reported kinship and other relationships between politicians. We worked with a large array of data-we analyzed anti-corruption declarations of officials and deputies in order to identify relatives. We compiled lists of officials by authorities that are not public, by secondary sources, and combined them with registers and databases. "," The most difficult part of the project was interacting with designers and programmers who visualized the data. We had to come up with an optimal visual solution:  - to build data in the right logic, show relationships and trends   - so that the user can easily use our data   - to give the user an idea of the current situation based on our data   The jury should know that we have been working on this project for 3 years. We drew every link of every person. During this time, we had to completely redo our project once - in the Kaliningrad region, most of the heads of authorities were replaced and our data became irrelevant. "," It seems to me that other participants could learn how to interact with programmers, designers, and other technical staff. People who are engaged in the content part should think like people who will technically implement their ideas. Technical staff should also think like people who are engaged in the production of content ",https://rugrad.eu/projects/rodoviegnezda/,https://docs.google.com/document/d/183ePPlECCUwzi-lHCRDabDw5Bid71JZTakXJKImFRP4/edit?usp=sharing,,,,,,Roman Romanovskii," Roman Romanovskii, analyst Transparency International-Russia, investigative journalist ",,,
Italy,Knowledge Centre on Migration and Demography - European Commission,Big,Participant,Open data,Atlas of Migration 2019,18/12/19,"Open data,Fact-checking,Chart,Immigration","Personalisation,Json"," The Atlas of Migration is both an online tool and a book. It contains 198 country fiches, dividing EU from non-EU countries and territories.   It collects migration-related data from many official sources, harmonising, validating, keeping it updated and showing it in an easy-to-read format.   By making global migration data easy-to-access and use, the Atlas directly support policy-makers (European Commission) and contributes to an informed and evidence-based public debate on a topic especially sensitive and relevant for Europe nowadays and in the years to come.   The Atlas allows policy-makers, researchers, journalist and general-public to start a conversation on migration. "," Since its launch on 18 December 2019 at the UNHCR Global Refugee Forum in Geneva, the Atlas of Migration reached wide dissemination and received very positive feedback from a large and heterogeneous audience. It attracted the attention of high-level participants, including EU Commissioners and Heads of States.   In 44 days, from its publication on 18 December 2019, the tool scored a total number of 11260 views – an average of about 256 views per day. These statistics do not account for the PDF copies of the Atlas that have been downloaded, nor for the printed copies of the book that have been distributed.    Within the European Commission,  the Atlas has been identified as the tool providing background reference data to inform the policy-making level in all EC‘s departments dealing with migration. This includes in particular:  <ul>  the drafting of new legislative initiatives in the areas of International Migration, International Development and Cooperation, Asylum and International Protection, Integration of Migrants, etc.   the briefing of high-level representatives of the EC in preparation of official meetings.  </ul>   A  t national level,  the European Migration Network (EMN) – an official network chaired by the EC and gathering together experts appointed by the national authorities of the 27 EU countries to provide reliable and comparable information on migration - asked for:  <ul>  Translation of the Atlas into the 24 languages of the EU.   Integration of the Atlas with the EMN Country Fact Sheets, their annual reference publication.  </ul>   At international level , the International Labour Organization (ILO) has requested a series of  ad hoc  training sessions based on the Atlas.       The already positive and visible impact of the Atlas is expected to become stronger over time, with the increase in users, further optimisation of the product and with the development of Atlas-based activities with partners. "," As most of the online tool, the Atlas of Migration is based on both a frontend and a backhand.  <ul>   Frontend   </ul>  Let’s start with the frontend, which is the part visible to the user.   Tableau is the main tool used to develop the Atlas of Migration.   The Atlas is a single workbook, made of about 160 sheets organised in six dashboards. The current A4 format of the dashboards was adopted to match the printed version of the Atlas, which targets a less digital-skilled user. First and last dashboards provide introduction and sources-information respectively; the four central dashboards represent the core of the Atlas and are divided into two parts: EU Member States (second and third) and non-EU countries and territories (fourth and fifth). Each dashboard is divided into thematic areas such as demography, asylum, residence permits, integration of migrants, etc. Each thematic area shows one or more charts. Chart-description and additional info are provided in tooltip.   Made with Tableau Desktop and published on Tableau Server, the Atlas shows 60 different indicators belonging to 52 datasets from 12 international sources.  <ul>   Backend   </ul>  All the processes running in background – including the gathering, harmonising, validating, keeping and updating of data – make the backend.   Through a Tableau-MongoDB connector, the workbook draws on a centralised repository of cleaned, harmonised and validated data. The data repository, which is common to all KCMD tools, is constantly updated thanks to automatic routines that continuously check the original sources.     An additional routine (based on Tableau  tabcmd ) runs every night producing for each country a 2-page PDF that can be easily downloaded by the user. "," Both challenges and strengths of the project are included in its three pillars:      Respect the sensitiveness and complexity of the topic.      Migration is a complex and sensitive topic prone to manipulation through misinformation and ‘fake news’. The Atlas tackles this issues using a comprehensive approach that covers with data all aspects of migration and related fields. It informs people through solid evidence, without oversimplifying migration, but showing that it is about not only how many people move, but the context in which they move, how they fare in the host society, the relationships between countries, etc.      Adopt a user-centric approach.      Focusing on the user has been quite difficult since the Atlas targets a broad variety of users with different levels of knowledge on migration: policymakers, practitioners, journalists and the general public.   The objective of the project was to bring together the most relevant data on migration and related topics, which are generally scattered across the web, in a concise but comprehensive country-overview. We needed to avoid overwhelming the user with too much information, without oversimplify the topic. We did it by selecting 60 indicators out of thousands and visualising theme in two A4 sheets.   A second challenge was to select the most appropriate type of visualisation, looking for a balance between appeal and clarity.      Ensure impartiality and trustworthiness.      One of the main objectives was to minimise the risk of biased interpretation of the visualised data.   Trustworthiness was guaranteed by providing full description of the original data together with the direct link to their source.   In this way, the Atlas is a starting point for any further analysis.   It does not offer pre-cooked explanations nor miraculous answers, but it rather sets the bases to start a genuine conversation around reliable migration data. "," There are two main lessons that we learned while developing the Atlas of Migration:  <ul>  Focusing on the user is always a winning approach   Dealing with complexity is possible  </ul>  The Atlas was designed to target a variety of users with different skills and knowledge of migration. This is why we decided to design a single product in two different formats: an online tool and a printed book, reaching users with different digital skills. Moreover, since the target audience of the Atlas included policymakers, data-journalists, researchers and the general public, we made an effort to make the Atlas easy-to-read and understand, without oversimplifying the topic.   We can deal with complexity using a systematic and structured approach. Often data are not missing, but rather they are too much and too scattered across different organisation focusing on specific aspects. Dealing with migration required to apply a comprehensive approach. Yet the information were structured in topics and sub-topics, following a well-defined reading key (i.e. from the demography in the country, to the migrants’ population, to the number of residence permits issued every year and so on).   All the information were putted in the right context and we also included in the book a how-to-read section that explains with simple words how to read each chart; the same explanation was made available in tooltip for the online tool. ",https://bluehub.jrc.ec.europa.eu/migration/app/atlas.html,https://ec.europa.eu/knowledge4policy/atlas-migration_en,https://ec.europa.eu/knowledge4policy/migration-demography_en,https://ec.europa.eu/jrc/en/news/celebrating-international-migrants-day-new-atlas-migration,,,,"Dario, Tarchi; Francesco, Sermi; Sona, Kalantaryan; Martina, Belmonte; Simon, McMahon; Amalia, Gilodi."," All authors work at the Joint Research Centre of the European Commission, within the Knowledge Centre on Migration and Demography (KCMD).  <ul>  Dario Tarchi is a Senior Expert and Project Leader at KCMD. He has extensive experience in building tools and analysis for policy-making. He holds a PhD in Science of the Earth and a Master Degree in Physics.   Francesco Sermi is a Scientific Project Officer at KCMD. As data-scientist, he develops tools and performs analysis, mainly on the EU Asylum System and on the EU external dimension of migration. He holds a Ph.D. and a Master degree in Engineering, both from the University of Florence, where he worked also as post-doc.   Sona Kalantaryan is a Scientific Project Officer at KCMD. She holds a Ph.D degree in Economics from the University of Turin and previously worked at the European University Institute, ITC ILO, University of Turin, Collegio Carlo Alberto and Ministry of Economy and Development of Armenia.   Martina Belmonte is a scientific officer at KCMD. She is specialised in migration policy, field in which she obtained her PhD from the University of Milan. Prior to KCMD, she worked as policy analyst at OECD and as an evaluation consultant at EY and ICF on European Commission’s projects in the area of home affairs.   Simon McMahon is a researcher specialised in the politics of international migration. He has a PhD from King’s College London (UK) and has held posts at Coventry University (UK), Colegio de la Frontera Sur (Mexico), Pompeu Fabra University (Spain) and the European University Institute (Italy). He is currently based at KCMD.   Amalia Gilodi is a doctoral student at Luxembourg University researching immigrant integration. Prior to that, she has worked as a research trainee at KCMD. She holds a Master degree in Cultural Anthropology and one in Migration, Ethnic Relations and Multiculturalism from Utrecht University as well as a Bachelor in Psychology from the University of Padua.   </ul>",,,
China,"The Paper, Sixth Tone",Big,Shortlist,Best data-driven reporting (small and large newsrooms),Silent Cries of China's Depressed Netizens,09/10/19,"Explainer,Infographics,Human rights","Scraping,R,Python","In March 2012, a Chinese student named Ma Jie, also k published a post on Weibo. ""I have depression, so I'll just go die,"" she wrote. The next day, the young woman was found dead at her university in the eastern city of Nanjing. Over the years, Ma's final post has evolved into a ""tree hollow,"" or online gathering place for those living with depression. It has received more than 1 million comments. We analyzed more than 50,000 of these comments posted between July 16 and Aug. 16, 2019, in a bid to shed new light on this unique internet"," The project was viewed more than 2 million times on Weibo, and got more than 2000 reposts and likes.  "," 1) data sonification   Ma Jie documented her last years on Weibo. Her posts provide a unique opportunity for us to feel the emotional ups-and-downs of someone struggling with depression and suicidal ideas. In order to give readers a more intuitive idea, Sixth Tone and The Paper did sentiment analysis on all of Ma’s social media posts and then creatively visualized and sonified the results.   2) text analysis   Through a series of striking graphics, we visualized the entire “tree hollow,” including the level of activity of its members, their changing moods, and the connections between them. These innovative techniques bring into sharp relief the continued challenges facing millions of Chinese living with depression: the social isolation, prejudice, and lack of support many experience, as well as the lack of awareness of suicide prevention services. ", 1. To find this unique sample to shed light on the depression issue in China.   2. Text analysis is no easy task for the Chinese language. We had to test and adjust to make sure the analysis does make sense.   3. It is tough to illustrate all Ma Jie's posts and the comments under her last post. ," “The Silent Cries of China’s Depressed Netizens” provides an unprecedented glimpse into an important and underreported topic, and is our proud submission for the award. ",https://www.thepaper.cn/newsDetail_forward_4315606,http://www.sixthtone.com/news/1004543/the-silent-cries-of-chinas-depressed-netizens,,,,,,"Liu Chang, Wang Yasai, Zhang Yijun, Lin Yi, Fu Xiaofan, Lu Yan"," The Paper data news team focuses on data-driven stories and explanatory storytelling. The team consists of 20 people: half are data journalists, and half are graphics designers of all sorts, from information design to 3D modeling.   We cover stories on tight deadlines; We measure policy effects by data; We stretch ways to get data suitable for our stories; We try to push the envelope and use suitable storytelling for valuble stories. We also collaborate constantly with our English-language sister publication, Sixth Tone, and published some of our projects in bilingual. ",,,
United States,National Public Radio,Big,Participant,Open data,How Federal Disaster Money Favors The Rich,03/05/19,"Investigation,Database,Open data,Podcast/radio,Audio,Economy",Microsoft Excel," As climate change drives more severe droughts, floods and wildfires, the government spends billions helping victims. We found that disaster aid follows and perpetuates inequities in the U.S. economy.  NPR analyzed 40,000 federal property buyouts we obtained via FOIA lawsuit. Most were in neighborhoods that are over 85 percent white. We focused on how inequities in disaster funding affect families and communities. In Houston, we found that richer, whiter neighborhoods are more resilient to floods. We profiled two adjacent towns in New Jersey, which had different economic fortunes driven by disaster aid. We released the entire database to the public.         "," On April 18, 2019, Sen. Elizabeth Warren (D-Mass.) and Rep. Bennie G. Thompson (D-Miss.), Chairman of the House Committee on Homeland Security, formally asked the Government Accountability Office to determine the extent to which the structure and administration of federal disaster relief programs ""exacerbate racial and socioeconomic inequities in the United States and the extent to which they have a disparate impact on Native tribal nations."" Their letter explicitly cited the NPR series. The GAO agreed to study the issue. Relevant documents:  Sen. Warren news release on the request 4/18/19: <a href=""https://www.warren.senate.gov/oversight/letters/warren-thompson-seek-gao-review-of-federal-disaster-aid-programs-impact-on-inequality"">https://www.warren.senate.gov/oversight/letters/warren-thompson-seek-gao-review-of-federal-disaster-aid-programs-impact-on-inequality</a>  Sen. Warren news release on GAO agreeing to the study, 6/11/2019: <a href=""https://www.warren.senate.gov/oversight/reports/gao-agrees-to-investigate-federal-disaster-aid-programs-impact-on-inequality"">https://www.warren.senate.gov/oversight/reports/gao-agrees-to-investigate-federal-disaster-aid-programs-impact-on-inequality</a>   Also, during the reporting process and since our report was published, multiple academic sociologists have told NPR they are following up on questions raised by the trends NPR uncovered. Those academic studies are still forthcoming.         "," We analyzed the data via SPSS and Microsoft Access to track the demographic inequities in the disaster buyouts, in part by linking the database to U.S. Census datafiles from the American Community Survey. We linked the data in several waves. Each wave of census data was tied to the timeframe appropriate to the buyout, so that we could use the demographics that existed at the time of the buyouts. This was essential as some areas were dynamic demographically, as a potential effect of the buyouts. We found this in New Jersey, where two similar towns were profiled, highlighting the economic and social differences associated with disaster aid in both places. We used ESRI ArcMap to validate property coordinates, as a guide to finding disaster buyout clusters visually and to make decisions about where in the country to report. "," The hardest part of this project was obtaining the data. We filed a FOIA request for from the Federal Emergency Management Administration (FEMA) in 2014. It was denied, as was a subsequent appeal. We then sued FEMA in U.S. District Court (Civil Action No. 17-91.) We won the suit and obtained the data in November, 2017. Once we had the data we were able to convince some officials within the agency to cooperate in explaining it on background, and in getting an interview with the administrator who is responsible for the buyout program. The difficulty of getting the data from the government factored into our decision to provide the entire database to the public, to increase the overall transparency of the program and allow local communities to understand their own buyout patterns.     "," Main lessons from this project:   An agency's justification for withholding data may not be validated by a court.  Even if release of the data is without precedent (as this was), a court will still weigh the pros and cons of public release.  A reporter's prospects for obtaining legal assistance is correlated with the merit of their claim. Our attorneys agreed to take this case in part because they believed it would succeed.  When faced with the fact that their data will be released, a government custodian of the data is often motivated to explain it. Once released, they may have no interest in seeing it misinterpreted. That could potentially reflect poorly on them as well as on the news organization.       ",https://www.npr.org/2019/03/05/688786177/how-federal-disaster-money-favors-the-rich,https://www.npr.org/2019/03/06/700625573/new-jerseys-lost-valley-tells-the-story-of-government-home-buyouts,https://www.npr.org/2019/03/06/700873488/new-flood-control-systems-could-make-new-jersey-town-too-pricey-for-current-resi,https://www.npr.org/2019/03/05/696995788/search-the-thousands-of-disaster-buyouts-fema-didnt-want-you-to-see,,,,"Rebecca Hersher, Robert Benincasa, Meg Anderson, Barbara Van Woerkom"," Rebecca Hersher is a reporter on NPR's Science Desk, where she reports on outbreaks, natural disasters, and environmental and health research. Since coming to NPR in 2011, she has covered the Ebola outbreak in West Africa, embedded with the Afghan army after the American combat mission ended, and reported on floods and hurricanes in the U.S. Hersher was part of the NPR team that won a Peabody award for coverage of the Ebola epidemic in West Africa, and produced a story from Liberia that won an Edward R. Murrow award for use of sound. She was a finalist for the 2017 Daniel Schorr prize; a 2017 Pulitzer Center on Crisis Reporting fellow, reporting on sanitation in Haiti; and a 2015 NPR Above the Fray fellow, investigating the causes of the suicide epidemic in Greenland.   Robert Benincasa is a computer-assisted reporting producer in NPR's Investigations Unit. Since joining NPR in 2008, Benincasa has been reporting on NPR Investigations stories, analyzing data for investigations, and developing data visualizations and interactive applications for NPR.org. Benincasa's work at NPR has been recognized by many of journalism's top honors. In 2014, he was part of a team that won an Investigative Reporters & Editors Award, and he shared Robert F. Kennedy Journalism Awards with Investigations Unit colleagues in 2016 and 2011. In 2019, he was part of a team that won the National Academies of Science, Engineering and Medicine Communication Award for ""Coal's Deadly Dust,"" a series about the rise of severe lung disease among coal miners and regulators' failures in responding. Benincasa previously was database editor for Gannett News Service, and served on the faculty of Georgetown University's Master of Professional Studies program in journalism from 2008 to 2016.     ",,,
United States,The New York Times,Big,Participant,Best data-driven reporting (small and large newsrooms),The Border Between Red and Blue America,25/10/19,"Explainer,Multiple-newsroom collaboration,Infographics,Map,Satellite images,Elections,Politics","QGIS,Microsoft Excel,CSV,R,RStudio"," The American political landscape is polarized, and most political reporters identify the suburbs as an important electoral battleground. But a New York Times analysis found that the border between conservative and liberal America is more pronounced than the pundits realized. In fact, draw a line through the suburbs -- a line dividing the older suburbs built during the post-World War II era and the newer suburbs that have sprung up recently, and you'll find two distinct demographic and political worlds. "," These stories for the first time quantified the extent to which the U.S. has changed politically and demographically. For decades, the assumption has been that the great socieo-economic dividing line in the U.S. is between cities and suburbs. Many have sensed that the suburbs are becoming more like cities. However, these stories for the first time documented this trend with data and established that, in fact, the ""suburbs"" has truly become two categories. The newer, ""outer ring"" suburbs are totally different in almost every way from the older, ""inner ring"" suburbs. "," The U.S. Census Bureau has no official definition of a ""suburb"", and so academic researchers are left to devise their own methods. After reviewing these efforts, we came up with our own version, building on previous work. We started with neighborhood-level population density -- the U.S. is divided into more than 74,000 Census ""tracts.""  But the problem with this approach is that there are many industrial neighborhoods in urban places with low population density -- few live there, but they are clearly urban. So we took a second approach, using satellite imagery from the Multi-Resolution Land Use Consortium. The satellite imagery presented the U.S. as a set of several million pixels, with each pixel color-coded to represent land density. We wrote a script that joined the pixels to Census tracts, which then enabled us to create a development density index for each tract. So for each tract, we had population density and development density, which we were able to combine into an overall index of urbanness, on a scale of 1 (most rural) to 10 (most urban).  Our original plan was to call any neighborhood with a 1 or 2 rural, 9 or 10 urban, and everything in between suburban.   But those plans changed when we started working with data. We then created a database of demographic information at the tract level spanning 40 years. We also brought in neighborhood-level election results from the 2016 presidential race. When we grouped these data by our density index, the pattern was clear: Rural was definitely distinct, as was Urban. But our Suburban category had to be split as well - 2 through 5 were totally different than 6 through 8.    We then set about confirming these findings through traditional reporting - on the street and through experts. "," The project presented some major technological hurdles -- namely, figuring out how to convert a gigantic, high-density pixel map into something that would spatial-join with standard GIS shape files.   But the bigger challenge is then taking complicated findings and turning them into journalism that people will actually want to read. The first story was a partnership with colleague Sabrina Tavernise, who spent weeks on the ground in Kent County, Michigan, getting people in various neighborhoods to open up about their political beliefs. In this day and age, such conversations are not easy in the U.S., but Sabrina perservered and got perspectives from across the political spectrum.  The photos of Christopher Lee were especially effective at bringing the findings to live.   After a surprising local election in the State of Virginia, Sabrina once again set out to tell the story about the changing suburbs, this time on a tight deadline,  while I conducted additionanl layers of analysis specific to this state -- this is the third story in the package.   But the bottom line is, our analysis was complicated and difficult to ""storify"" -- but I think this represents a perfect marriage between advanced data techniques and traditional storytelling. "," The two main takeaways: Taking on the common knowledge of pundits is always a newsworthy exercise. In this case, our big get was showing how commentators talking about the ""suburbs"" as a single entity were in fact over-simplifying reality, and that reporters need to keep the inner ring - outer ring divide in mind at all times.   The second is the importance of narrative journalism. One of the stories in the series is my own data column about the findings. But this effort would not have been nearly as effective without the other two pieces, which allowed us to tell the story from a wide variety of perspectives. ",https://www.nytimes.com/2019/10/25/us/democrats-republicans-suburbs.html,https://www.nytimes.com/2019/10/29/upshot/suburbs-demographics-red-blue.html,https://www.nytimes.com/2019/11/09/us/virginia-elections-democrats-republicans.html,,,,,"Robert Gebeloff, Sabrina Tavernise, Christopher Lee, Lauren Leatherby, Jugal Patel"," Sabrina Tavernise is a national correspondent covering demographics and is the lead writer for The Times on the Census. She started at The Times in 2000, spending her first 10 years as a foreign correspondent.    Robert Gebeloff is a reporter specializing in data analysis. He works on in-depth stories where numbers help augment traditional reporting.   Jugal Patel and Lauren Leatherby are visual journalists in graphics department.   Christopher Lee is a freelance photographer based in Brooklyn, NY. ",,,
Singapore,Reuters,Big,Participant,Best data-driven reporting (small and large newsrooms),"Weapons of mass control, tactics of mass resistance",31/10/19,"Investigation,Multiple-newsroom collaboration,Illustration,Infographics,Chart","Scraping,Adobe,Creative Suite,Google Sheets"," As the year went on, the mass protests gripping Hong Kong became increasingly violent.       Reuters embarked upon a monumental data gathering exercise in order to quantify the escalating use of force. The team was able to plot the use of crowd control weapons at each protest and exactly how many rounds were fired.       The team combed through hundreds of police statements, scoured Reuters photographs and videos, gathered daily reporting, and requested statistics at press conferences in order to gather all of the information needed.       The unique data set and visualisations formed the backbone of this Reuters Special Report. "," This piece formed part of our Hong Kong graphics coverage and was used extensively during the year. It was shared widely on social media, providing a completely alternative angle to a story that was already heavily covered.   "," Data from the Wiki pages was scraped programmatically before being analysed and experimented with in R. Visualisations were then exported and styled in Adobe Illustrator before being placed on a web page using ai2html.    Animations, such as the opening summary box, were all made by replicating the ones on Wikipedia using HTML and CSS directly in the browser window. "," One of the hardest parts of this project was being sure to input the multitude of balancing voices. We needed to get comment from the police, government, Wikipedia, a Wiki editor involved in the pages, and also someone who specialises in this kind of online behaviour. There was a lot of reporting that went into this graphic.    Coming up with this fresh way to look at the HK story was also a challenge in itself. None of our Hong Kong pieces follow the typical line of news developments. We always try to create unique stories and visualisations. ", Sometimes a new and interesting idea is what makes a project special or unique. Creating something which stands out from the noise of everyone else’s coverage is a way to add value to an ongoing story ,https://graphics.reuters.com/HONGKONG-PROTESTS-VIOLENCE/0100B2L91Z7/index.html,,,,,,,"Simon Scarr, Gurman Bhatia, Marco Hernandez, Christian Inton, Anand Katakam, John Ruwitch, Felix Tam, Tom Westbrook, Clare Jim, Greg Torode, Anne Marie Roantree, Peter Hirschberg"," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Canada,The Globe and Mail,Big,Participant,Best data-driven reporting (small and large newsrooms),How Canada's suburban dream became a debt-filled nightmare,13/09/19,"Investigation,Explainer,Long-form,Illustration,Infographics,Chart,Map,Politics,Business,Economy","D3.js,Json,Microsoft Excel,CSV,R,RStudio"," Canadian household debt is at a record high of $2.2-trillion, fueled by a decade-long boom in home-buying. That said, we know very little about who is most at risk. For this project, we aimed to find out where household debt is concentrated, and what it means for Canada. The Globe undertook a multi-month data investigation that examined every aspect of Canada’s most indebted households: from location and family size, to commute and age of neighbourhoods. In doing so, we not only upended existing narratives on household debt, but also pinpointed where early signs of a housing crash could bubble up. ","  For years, the prevailing narrative has been that Canada’s most financially vulnerable people live in downtown Toronto and Vancouver, where detached home prices have rocketed well into seven figures. Turns out that isn’t true.   The Globe and Mail obtained exclusive information from a data analytics firm on the 100 neighborhoods with the highest debt-service ratio, or the percentage of disposable income households used to pay the interest payments on their mortgages, car loans, credit cards and lines of credit. We were surprised by the results: In fact, it’s the commuter cities where people have taken on crushing debt to chase their homeownership dreams, despite those homes being less expensive. The data showed that 98 of the 100 most financially stressed neighborhoods were in the suburbs.    From there, we undertook a census analysis to see what characteristics were common across these highly indebted neighborhoods. There were many. It wasn’t expensive heritage homes where people had deep financial burdens, but new subdivisions of starter homes that are traditionally better priced. Distressed households were considerably more likely to have larger household sizes, have multiple generations living under one roof, and commute to another city for work. In other words, home buyers have ventured further afield, to areas with scarce jobs, took on pricey car payments, and have rented out spare rooms to make ends meet.   This project — a major systematic look at household debt in the country — was published one month before the Canadian federal election, driving discussion on one of the most important political topics in Canada: home affordability. We offered Canadians valuable context through which to view parties’ pledges. The story stimulated heated discussions among our readers on social media, and a high level of engagement with more than 500 comments on our website.  "," The analysis part of the project was conducted by using a combination of Excel and programming language R. The Globe collected census profile data for the 100 most indebted neighborhoods and selected more than 200 metrics to trace out the characteristics shared by the neighborhoods.    The analysis pointed us to clusters of neighbourhoods in three provinces -- Ontario, British Columbia and Alberta -- and enabled us to distinguish the possible root cause of their high level of indebtedness. We also compared those neighborhoods with respective census metropolitan areas they are located in, to understand if the characteristics are unique to the select neighborhoods or should be ascribed to broader regional reasons. Some of the findings coincided with anecdotes and theories before the start of the project and provided strong support for the reporting followed.    We used CensusMapper, an API containing complete Canadain census data, and R to plot the communities other than the select ones, leveraging visualization to validate our metrics and build prototypes for our final presentation. The scrolling explainer was built with HTML and CSS. The static graphics were created in QGIS, polished in Adobe Illustrator and then converted into HTML and CSS, using ai2html. Satellite images from Google Earth were used to demonstrate the development of suburban homes in different parts of the country.  "," The hardest part was the initial analysis, which needed to prove that high indebtedness was as intense and prevalent as our anecdotal evidence seemed to suggest. Data analysis helped identify key characteristics of neighbourhoods that seemed to contibute to high debt levels and revealed how those factors built up and made the lifestyle no longer sustainable for people in commuter cities, with a higher interest payment and stagnant incomes.     The analysis also gave reporter Rachelle Younglai a better understanding of the situation and pointed her to the representative families and stakeholders to speak with, with questions on solid ground.      This led to the other most difficult part of the project: finding real people in the neighbuorhoods who were wiling to share detailed financial information on the record. It was particularly challenging in a community like Brampton where English is not commonly spoken. Younglai interviewed a minimum of 30 homeowners in Brampton, Edmonton and Coquitlam, B.C. Many homeowners were interviewed multiple times for hours at a time.    The combination of strong, robust data with real human stories was critical in making sure this story landed with maximum impact for our readers. For example, many readers commented on the subject in the lead of the story — it was a perfectly sourced anecdote that drove discussion all on its own. "," A lot of extra value can come from combining disparate datasets: Our original data on indebted neighbourhoods was interesting, and could have been a smaller story in itself, but it was in combining this data with robust census and demographic information that the true insights began to emerge, and gave our reporter fresh insights that led to compelling sources and an unexpected narrative.   Additonally, a lesson we learned is to simply ask for data: many sources, companies and researchers are eager to share their data with trusted journalists, and that can form the basis of a great story. ",https://www.theglobeandmail.com/business/economy/article-how-canadas-suburban-dream-became-a-debt-filled-nightmare/,,,,,,,"Rachelle Younglai, Chen Wang, Matt Lundy, Murat Yükselir", Rachelle Younglai is The Globe and Mail's real estate reporter.    Chen Wang is a data editor at The Globe and Mail.    Matt Lundy is an economics reporter at The Globe and Mail.   Murat Yükselir is a graphics editor at The Globe and Mail. ,,,
United Kingdom,"HuffPost UK, The Bureau of Investigative Journalism and the BBC Local Democracy Reporting Service",Big,Participant,Best data-driven reporting (small and large newsrooms),Grenfell property investigation,29/05/19,"Investigation,Long-form,Multiple-newsroom collaboration,News application,Fact-checking,Human rights",Google Sheets," This collaborative investigation explored the property dealings of Kensington and Chelsea in the run up to the Grenfell fire and brought to light significant new information about cash the council had to spend on the tower renovation works. For many months HuffPost UK, The Bureau of Investigative Journalism, and the BBC’s Local Democracy Reporting Service, worked to meticulously establish a timeline of financial decision making around Grenfell and the council’s property deals. This allowed us to reveal, for the first time, that the council had millions available to spend on the Grenfell works that it had not previously divulged. "," Grenfell Tower survivor and long-standing campaigner Ed Daffarn told us following publication of our story that the information and data made public by our investigation had allowed campaigners to hold the council to account and ask questions in a way that wasn’t possible previously.    Daffarn and other campaigners said the investigation had unlocked new information about the financial priorities of the Royal Borough of Kensington and Chelsea at the time at which it was making crucial funding decisions about Grenfell Tower. Campaigners and survivors also said it vindicated their belief that the council was “acting like a property developer” before the disaster at Grenfell Tower. They questioned why the council appeared to have placed property development and profit before tenants’ safety.    In this way specifically, the investigation has enhanced the understanding of one key aspect of this horrific disaster. The investigation has also enhanced public knowledge about the financial decision making processes surrounding the Grenfell Tower renovation works and money that was available to the council.   In one key quote, Daffarn said of the findings of our investigation: “If they’d been properly focusing on fire safety, rather than focusing on this, then Grenfell might not have happened.” David Lammy MP also spoke in strong language about the importance of the investigation, saying: “If the council had used its £129m in property sales to renovate Grenfell Tower, rather than buying up other properties, 72 lives could have been saved. This forms part of a picture of gross negligence.”   The investigation had a readership of hundreds of thousands and was picked up by other news outlets including the BBC and Inside Housing. The decision making processes that led to the Grenfell fire are now the focus of Phase Two of The Grenfell Tower Inquiry.  "," This investigation focused on property deals made by the Royal Borough of Kensington and Chelsea (RBKC) in the years leading up to the Grenfell fire and how these related to the financing of the Grenfell Tower renovation works. It was an investigation very much prompted by data and led by data-driven reporting.   The investigation came about because The Bureau had gathered data using Freedom of Information requests which showed details of the properties that RBKC had bought and sold from April 2014 to July 2018. RBKC initially refused the FOI request but this decision was overturned following an internal review lodged by The Bureau. This data was not in the public domain and HuffPost UK and The Bureau both felt it raised significant questions about the financial resources available to RBKC for social housing renovations in the years leading up to the Grenfell disaster.   The data revealed by these FOIs showed how much money RBKC had spent buying properties and how much it had made from selling properties in the years preceding the fire. The data drove our reporting as we investigated how these property sales and purchases were linked to the Grenfell Tower renovation works.    RBKC had claimed that despite its healthy reserves it was restricted in accessing funds to carry out the Grenfell renovation works because of ring fencing. But our investigation showed it could have used more of the £129million it made from selling property to finance the Grenfell refurbishment. Our research analysing RBKC’s accounts showed the council had £37m, specifically from the sale of property, in the bank that could have supported the Grenfell budget at the time in July 2014 when decisions were being taken to install cheaper, less fire-retardant aluminium cladding on Grenfell Tower.  "," One of the major hurdles we had to overcome during this investigation was establishing the rules and regulations surrounding how councils can spend money made from selling property, and whether this money could be used to renovate social housing.    A key moment in the investigation came when we were able to establish that RBKC had used money from the sale of one of the properties in our data to pay for the majority of the Grenfell Tower renovation works. RBKC had previously claimed that despite its healthy reserves it was restricted in accessing funds to carry out the Grenfell renovation because of ring fencing. But if it had been able to use millions from the sale of one of its property to finance the works, why couldn’t it use more of the £129million it made from selling property?   We struggled to get any clarity from RBKC on this point. When our journalists first put these questions to the council, a cabinet chief suggested the money from property sales was ring fenced. Eventually the council said it worked to its understanding of the rules at the time.   We pushed further on this and eventually secured confirmation from government that RBKC was free to spend the £129m cash from property sales on the Grenfell renovation works if it had wanted to. We also forensically investigated council accounts to confirm that RBKC actually held £37million in its bank account from property sales at the time decisions about funding at Grenfell Tower were being taken.   Crucially this money was available when £300,000 was cut from the cladding budget, which led to cheaper more combustible cladding being added to the building. The cladding was a key contributor to the speed with which the fire tore through Grenfell Tower, killing 72 people and leaving hundreds homeless. "," The core findings of our investigation show that RBKC had money from the sale of property that could have been used to prevent cost-cutting on the Grenfell Tower renovation works. Specifically, the council had £37million in the bank from the sale of property at the time when decisions were being taken to install cheaper, less fire-retardant cladding on Grenfell Tower.    Our findings show the council had greater flexibility over its funding of the Grenfell Tower works than has previously been known and show that RBKC could have chosen to use capital receipts from the sale of property to increase the budget for the Grenfell Tower renovation.   The core findings of the HuffPost UK, TBIJ and the BBC Local Democracy Reporting Service add to the understanding of the financial resources available to RBKC, specifically from capital receipts, at the time decisions were being made about the funding of the Grenfell Tower renovation works and specifically when discussions were underway in relation to cost savings on cladding for the building. The data also reveals more about the council’s investment priorities at this time. ",https://www.huffingtonpost.co.uk/entry/kensington-chelsea-council-property-sales-grenfell_uk_5ced6003e4b0bbe6e3342f04,https://www.huffingtonpost.co.uk/entry/grenfell-tower-rbkc-property-deals_uk_5ced0c7be4b00e036574384d,https://www.youtube.com/watch?v=aIe8HY12PsM&feature=youtu.be,https://www.youtube.com/watch?v=ulr7xTSlc0M&feature=youtu.be,,,,"Emma Youle, Gareth Davies, Julia Gregory","  HuffPost UK  is a news website owned by Verizon Media. It invests in investigative, public-interest led journalism and collaborates with other media organisations on investigative projects.    The Bureau of Investigative Journalism  is an independent, not-for-profit organisation which collaborates with other media to hold power to account at a local, regional and international level.    The BBC Local Democracy Reporting Service  created up to 150 new journalism jobs to help fill a gap in the reporting of local democracy issues across the UK.    Emma Youle  is special correspondent at HuffPost UK. An award-winning investigative journalist, she has reported on social inequality, homelessness and the contaminated blood scandal. She won the Private Eye Paul Foot Award in 2017.    Gareth Davies  is an award-winning local reporter who specialises in public interest and investigative journalism. As the chief reporter of the Croydon Advertiser, a weekly newspaper in London, he won ten awards in five years, including Weekly Reporter of the Year at the Regional Press Awards a record four times. He established an award-winning data blog, led a campaign which prompted new legislation on driving under the influence of drugs in England and Wales, and undertook an investigation which led to the conviction of a serious fraudster.    Julia Gregory  is a local democracy reporter covering the City of London, Hammersmith & Fulham, Kensington and Chelsea & Westminster.  ",,,
United Kingdom,The Bureau of Investigative Journalism and HuffPost UK,Big,Shortlist,Open data,Sold From Under You,03/04/19,"Investigation,Solutions journalism,Long-form,Multiple-newsroom collaboration,Database,Open data,News application,Fact-checking,Map,Politics","Google Sheets,CSV,PostgreSQL,PostGIS,Python,Node.js"," Sold From Under You is a large-scale data-led collaborative investigation into the sell-off of public spaces by local authorities, which revealed, for the first time, the scale to which the local government funding crisis is affecting public services, public spaces, and public servants.   The Bureau of Investigative Journalism led this ambitious investigation in collaboration with HuffPost UK, the Local Government Chronicle and 50 regional publications in the UK.   The Bureau managed more than 700 Freedom of Information requests. The data showed 12,000 buildings and pieces of land has been sold, transferred or otherwise relinquished by local authorities.  "," This was The Bureau’s biggest collaboration to-date, meaning we were able to engage a large number of people in the investigative process. In part we measure the success of the project by how many people took part in the collaboration. We held a hackday in Birmingham organised and run by our members and produced stories across many areas of the country.    Members of our network have done vital reporting with more than 50 stories published, energising a wave of local reporting at a time when scrutiny of local power most needs it. In Birmingham, Jane Hayes wrote for the Birmingham Mail about the heart of a local community being “ripped out” by council sell offs. In Lancashire, the council’s refusal to reveal key information under FOI led to an important article about transparency from Local Democracy Reporter Paul Faulkner. Writing for the Newham Recorder, reporter Hannah Somerville highlighted how the local council had sold off public assets to fund cost-cutting measures. Meanwhile, Adam Cantwell-Corn held Bristol Council to account, reporting how redundancies increased more than tenfold when asset sales were used to fund them. His story was published by The Bristol Cable.    The stories grabbed national attention throughout the week. A tweet by Labour leader Jeremy Corbyn including a link to HuffPost UK’s lead story was shared more than 2,000 times. The story itself was read by 100,000 people.The investigation was mentioned on TalkRadio and BBC’s Today, London News, Politics Live, Look East and Cambridgeshire programmes.    Additionally, our analysis of financial data published by Peterborough Council found the authority, which provides services to 200,000 people, may have broken the law by using £23million of money raised by selling assets to plug gaps in its budget. The story prompted a government investigation. "," Local authorities in England are required by the Local Government Transparency Code to publish annual lists of the land and buildings they own. Our research, however, found these lists could not be used as the basis for the investigation because the registers lack key details (who assets were sold to and for how much) and more than half are published in a way that makes tracking change over time impossible (councils are updating the same spreadsheet each year rather than publishing new ones).    As a result we decided that the Freedom of Information Act (FOIA) would be required. The Bureau submitted two separate FOIs to each of England’s 353 local authorities. The first sought details of every land or building asset disposed of since 2014/15, such as sale price and purchaser. The second asked what councils had done with the money when such assets were sold. From our research we were aware some had been using the money to pay for redundancies, and were keen to know how widespread this previously unreported practice was.   Managing more than 700 FOI requests was a huge task but over a period of months we were able to build a database. Robust knowledge of the FOIA meant we persuaded many councils which originally rejected the request to provide the information. This enabled us to build a comprehensive dataset showing which community spaces had been sold off, to whom and for how much.    We also created an interactive map which enables anyone to type in their postcode or the name of their local authority and see which public spaces have been lost where they live, a valuable open data resource.    We used Excel and Google Sheets to record the data. We used Python and other programming languages to create and maintain the interactive map. "," The scale of Sold From Under You is one aspect that sets the investigation apart. One hundred and fifty people signed up to take part in the project via The Bureau’s network of members across the UK, making Sold From Under You the Bureau’s biggest collaboration to-date. The multi-tiered nature of the project is also unique. The Bureau collaborated with HuffPost UK, the Local Government Chronicle and 50 local and regional publications across the UK. This meant we had journalists at every level - local, regional and national - exploring and reporting on the data we had obtained. A requirement of taking part in the investigation was to share findings before publication, strengthening and enriching the resulting stories.   But the project was not limited to journalists. The Bureau’s network also involves technologists, academics, local politicians, campaigners and members of the public passionate about the loss of public spaces in their areas. Members’ involvement ranged from sharing lived experiences of community spaces being lost and others offering data analysis and visualisation expertise, to local and regional journalists investigating and reporting on the data in their area.    In Birmingham, network members organised a hack day to see whether they could find information the council had refused to release under FOI. This wide-scale and multi-tiered approach meant stories produced during the Sold From Under You project were not limited to only the most often reported-on areas of England, but instead led to coverage of an important topic in as broad and representative terms as possible. "," The Bureau managed more than 700 Freedom of Information requests in the process of this story. Using this data, we revealed to the public for the first time the buildings and pieces of land sold or relinquished by local authorities. This enabled the public to see which community spaces had been sold off, to whom and for how much. Previously this information had either been spread across 353 local authority websites (held in countless more spreadsheets or PDFs) or not routinely published at all.   The Bureau made this data available in two key ways. Firstly, it shared the dataset a month before publication with its network along with a reporting recipe explaining its findings, the context and its methodology (this resource is now available for anyone to use as part of our commitment to being open about our methodology and the data we base our journalism on).   We also created an interactive map which enables anyone to type in their postcode or the name of their local authority and see which public spaces have been lost where they live. This made the key details mentioned above – what has been sold off, for how much and to whom – easily accessible, in an engaging way, for the first time.   On March 4, 2019 The Bureau and HuffPost UK launched a week-long series of stories under the Sold From Under You banner. We reported that thousands of public spaces had been lost to the local government funding crisis and that councils across the country were using money from selling community assets such as libraries, community centres and playgrounds to fund further cutbacks, including hundreds of redundancies.   The stories and the interactive map remain a valuable resource to other journalists, researchers and academics about the large scale loss of public spaces. ",https://www.thebureauinvestigates.com/stories/2019-03-04/sold-from-under-you,https://council-sell-off.thebureauinvestigates.com/,https://www.huffingtonpost.co.uk/entry/sold-from-under-you-explainer_uk_5c796bdee4b033abd14b61c8,https://www.thebureauinvestigates.com/stories/2019-03-05/has-peterborough-council-unlawfully-used-money-raised-from-selling-public-spaces,https://www.thebureauinvestigates.com/stories/2019-03-06/communities-fighting-back-against-council-sell-offs,https://www.huffingtonpost.co.uk/entry/council-worker-redundancies-sold-from-under-you_uk_5c7577abe4b0bf1662043271,https://www.thebureauinvestigates.com/projects/local-power/open-resources,"Gareth Davies, Charles Boutaud, Hazel Sheffield, Emma Youle, Nicola Slawson","  The Bureau of Investigative Journalism (TBIJ)  is an independent, not-for-profit organisation which collaborates with other media to hold power to account at a local, regional and international level.    HuffPost UK  is a news website owned by Verizon Media. It invests in investigative, public-interest led journalism and collaborates with other media organisations on investigative projects.    Gareth Davies  is an award-winning local reporter who specialises in public interest and investigative journalism. As the chief reporter of the Croydon Advertiser, a weekly newspaper in London, he won ten awards in five years, including Weekly Reporter of the Year at the Regional Press Awards a record four times. He established an award-winning data blog, led a campaign which prompted new legislation on driving under the influence of drugs in England and Wales, and undertook an investigation which led to the conviction of a serious fraudster.    Charles Boutaud  is a developer-journalist who has experience investigating data for stories using computational method. He won a Canadian Online Publishing Award for his work on public transport data in Montréal with the Huffington Post, Québec. Charles joins the Bureau Local from Trinity Mirror’s data unit in Cardiff having covered a wide range of social issues for the group’s many local papers.    Hazel Sheffield  is a freelance multimedia journalist and the founder of Far Nearer, a reporting project on local economies in the UK.    Emma Youle  is special correspondent at HuffPost UK. An award-winning investigative journalist, she has reported on social inequality, homelessness and the contaminated blood scandal. She won the Private Eye Paul Foot Award in 2017.    Nicola Slawson  is an experienced reporter and editor working for national newspapers, magazines and digital-only publications. ",,,
Spain,La Vanguardia,Big,Participant,Best visualization (small and large newsrooms),The CO2 we breathe,22/09/19,"Explainer,Long-form,Multiple-newsroom collaboration,Illustration,Infographics,Environment","3D modelling,Personalisation,Adobe,Creative Suite,Google Sheets,CSV"," This project is a visual journey that graphically represents all the carbon dioxide emitted by cars and how much forest mass is needed to absorb it. The article starts representing the consumption of CO2 of a car within a day and gradually changes the scale to represent the CO2 issued by a car during a year, by the whole car park of Madrid and Barcelona, by all vehicles registered in ​​Spain and finally by all the transport of the world. "," At least 25.000 people visited this article since it was published on september 2019. The story was also republished on instagram stories where it reached a different and diverse audience. Around 35.000 users saw the first storie and up to 20.000 followed the navigation until the final storie.   We published the article on the World Car-free Day when the Barcelona City Council was considering introducing restrictive measures for traffic within the city in order to reduce pollution. In this context, the article was very well received as it offered new data on this subject. And that allowed the reader to have tools to position themselves on the topic under discussion.       <pre>  </pre>"," This project has three clearly differentiated parts.     Documentation and data collection     In first place, we spent almost two weeks documenting ourselves on the thopic. We did research, calculations and verification in order to establish a comparison system that was the most reliable at scientific level and at the same time very visual and informative. Some exeperts in the field recommended us which sources and databases were better for our purpose (see atached methodology) and then we did a fact-checking of each step of the calculation process with them. We used Excel in order to manage the different csv we worked with.     Scrollytelling design     Once we had made clear which equivalences fit better with our purpose, we made a sketch to design the path the reader would follow when scrolling through the story. We wanted to make sure we chose the optimal way to chain the different steps.     Images and scales     For the visualizations we used Autodesk Maya for modeling and rendering (Arnold render) the 3D pieces. Once rendered, we retouched and adjusted them with Photoshop. Then, we used Adobe Spark as basis to construct an storytelling website model to integrate 3D images. As the CMS of La Vanguardia doesn't accept any kind of special page, we had to publish this project outside of the technical system of the newspaper. For this reason we had to add manually SEO and Analytics scripts.  "," One of the hardest part of documentation and data collection was to find the proper way to establish a comparison system. The academic articles use such a specialized language that makes them very complicated to explain to the reader in a simple sequence of images. As soon as we tried to simplify the explanations, the scientist we were working with alerted us that we were being inaccurate. The calculations and the final outline of the story were achieved thanks to two scientists with whom we verified multiple times that the story we built and the calculations were faithful to reality.   On the visual part, one of the most difficult steps was the development of 3D models and the proportional scaling of the objects to each other in order to recreate the real difference in dimensions between them. Moreover, we had to optimizate and the render the images making sure that the framing and the dimensions were useful for both desktop and mobile. "," This project is a good example of how colaboration between academics, data journalists and visual journalists can produce reliable articles in a very visual long format. This working methodology applied to this specific topic -co2 emissions- can provide citizens with tools and information to better understand the real impact of using private transportations and to have a global image of the amout issued each year to the atmosphere. In a time where climate change is one of the main challenges facing the 21st century, any content that draws readers into discussions, helps to raise awarness and build a more sef-sufficient citizenry.   On the other side, on a technical level this project is a good example of how to carry and create innovative journalistic projects despite the lack of resources the company invests in order to have better technical solutions when it comes to visual online journalism.  ",https://www.lavanguardia.com/especial/natural/20190921/Dia-mundial-sin-coches-2019/index.html,https://www.instagram.com/stories/highlights/18125190523043265/,https://docs.google.com/document/d/1ifxzaVm128Z3Ww_1s1eKWNxezZCtyJ7G3lLgTRj1i4s/edit?usp=sharing,https://docs.google.com/document/d/15zvHxkYz9sm6uTBABjp13sdTqp5s_GeAfX1DO-1mahc/edit,,,,Laura Aragó and Pablo González Pellicer," Laura Aragó is a data journalist at La Vanguardia, a Spanish newspaper based in based in Barcelona. After she graduated from a Master Degree on Data Journalism at Universidad Rey Juan Carlos, she started working for spanish media such as Nació Digital and El Mundo. Since then she has been awarded with a reporting grant from the European Journalism Centre, has collaborated in cross-border data investigations and taught as a lecturer at the International University of La Rioja.   Pablo García Pellicer it is a UX designer, motion graphics and a digital infographist at La Vanguardia with more than 15 years of experience in these field. He studied a Higher Degree in design at Elisava School and Geography and History at the University of Oviedo.  ",,,
Russia,Transparency International – Russia,Small,Participant,Best data-driven reporting (small and large newsrooms),Hotel room with a marble bath for 1.4 million rubles. How much money Russians pay for business trips of Minister Manturov and other officials,25/06/19,"Investigation,Open data,Fact-checking,Politics,Corruption","Scraping,Adobe,Google Sheets"," We analyzed the travel expenses of the Russian government and of all ministries and departments and found out that, while on business trips, many officials stay at extremely expensive hotels at the expense of the state budget. The price runs as high as $1,600 per person per night. "," This project reveals that millions of rubles have been spent illegally from the budget to cover the travel expenses of Russian officials. They rent hotel rooms for the prices exceeding the set travel expense limits. The government does not control expenses in any way; neither does it force officials to return the excessively spent funds. The latter is covered by the state budget. In other words, it is covered from the pockets of taxpayers.The lack of necessary control allows officials affording themselves such luxury. The budget for travel expenses could be significantly reduced, which would save lots of money.   Our project has demonstrated that there are tools for the control over the travel expenses of officials. It has also stimulated the authorities to control the spending of their employees. The Accounts Chamber of the Russian Federation has recognized that officials are massively exceeding the costs of business trips. Now we expect the return of excessively spent money to the budget. "," To conduct a profound investigation, we needed to find all the relevant government procurement (namely hotel rental) across all government agencies. Given the huge amount of public procurement contracts concluded by federal authorities and the fact that they were disguised, we had to use in-depth search techniques. We also tried to parse purchases by keywords, which greatly facilitated the search.   Most of the data was presented in scanned PDF format which can only be handled manually. We tried to automate the process as much as possible. We created an Excel table where we put all the costs and calculated whether there was an excess of the limit. Thus, we were able to calculate how much the authorities could save. "," The most difficult thing was to analyze a huge amount of data related to the cost of each trip of each particular official per year. Despite the complex data format, due to which we had to work manually a lot, we were able to automate the process of information retrieval and data analysis as much as possible.   We believe that our project deserves winning because we managed to not only extract a lot of data from a large array of numbers and discover trends and problems, but also to identify violations of the law using this data and legally qualify this. This will greatly help to change the current situation in the country. "," We believe that our project can help others to learn how to use the data efficiently so that it is not just data for the sake of data and beautiful visualization, but rather a tool that can be used to identify problems and violations of law and to make a change. ",https://travel.transparency.org.ru/1/,https://www.forbes.ru/obshchestvo/378567-prezidentskiy-nomer-za-14-mln-rubley-eksperty-podschitali-rashody-na-komandirovki,https://www.vedomosti.ru/politics/articles/2019/06/24/804918-rossiiskie-ministri,https://www.znak.com/2019-06-25/transperensi_glava_minpromtorga_denis_manturov_ostanavlivaetsya_v_sverhdorogih_otelyah,https://thebell.io/mramornaya-vanna-i-sushilka-dlya-nogtej-skolko-rossijskie-ministry-tratyat-na-komandirovki/,https://www.the-village.ru/village/city/news-city/354569-komandirovka,https://www.svoboda.org/a/30019418.html,"Alexander Avtonagov, Alexander Vavilov, Roman Romanovsky, Igor Sergeev","   Alexander Avtonagov, lawyer   Alexander Vavilov, analyst   Igor Sergeev, lawyer   Roman Romanovsky, investigative journalist   ",,,
Russia,Transparency International – Russia,Small,Participant,Best visualization (small and large newsrooms),Lobbying in the State Duma. Which deputies serve not only the people,30/09/19,"Investigation,Quiz/game,Database,Illustration,Map,Politics,Corruption","D3.js,Json,Adobe,Microsoft Excel,Google Sheets"," The project is an interactive map of the representation of interest groups (lobby groups) among deputies of the State Duma; whose interests, in addition to the interests of voters, these deputies are representing; and who is the beneficiary of the draft laws they introduce. This map also shows the lobbying opportunities of each deputy and the degree of their influence on the legislative process. "," The project is designed to draw the attention of civil society, the media, and lawmakers to the problem of the lack of legislative regulation of lobbyism in Russia. The problem is that lobbying in Russia has not been settled legally. The decision-making process itself takes place “behind closed doors”, which leads to the violation of public interest.    The project helps to make the process of political and economic decision-making more transparent and understandable for the citizens of Russia. The project data has become a kind of database for journalists which they can use for their investigations of lobbying practices among Russian deputies. The project data is also useful for the academic research on lobbying.   The key outcomes of the project are as follows: (1) a specialized media resource on lobbyism in Russia is about to be launched; (2) a well-known Russian media (The Arguments of the Week) has started publishing regularly the rating of lobbying opportunities of the members of the Federation Council of the Federal Assembly of the Russian Federation. "," We used only open data for the project. The sources of data are as follows: official websites of the State Duma, deputies' accounts on social networks, their personal websites, the Unified State Register of Legal Entities, the Federal Service for State Registration, Cadastre and Cartography (Rosreestr), websites of federal and regional government bodies, media, websites of commercial and non-commercial organizations.   We have compiled detailed biographies of deputies. We have investigated their relations with the commercial and non-profit sectors as well as with government bodies. We have collected and described the draft laws introduced by them, namely their essence, possible beneficiaries and terms of completion.   We have designed a matrix of interest groups as well a methodology for assigning deputies to a particular interest group.   We have examined a total of 48,000 sources. We have used the method of interviewing (we interviewed regional experts and industry representatives).   The technical part of our work on this project took 250 hours (please see below the technologies/ tools we used). "," The most difficult part was to collect data, because it was impossible to collect all the data automatically. This is especially true for the draft laws introduced by the deputies. We have analyzed only those draft laws that deputies introduced as authors. That is, the draft laws registered with the State Duma are signed by them. Draft laws where a deputy joined the group of authors after the draft had been introduced were considered as less important for the project or not taken into account at all. The official website of the State Duma allows its visitor to parse data only by the initiators of drafts rather than by the authors. Therefore, we have collected the data manually over a two-year period. It was also difficult to sort out who was really interested in passing this or that draft law, since rulemaking in Russia is usually not a direct action, and legislative initiatives themselves, as a rule, are introduced as something made for the public good.    The implementation of such a project required the painstaking and systematic work of three people over the course of one year, and another team of 10 people working on fact checking, data verification and technical implementation of the project. The project analytics takes approximately two thousand pages of text.   We believe that our project should win because the above-mentioned work resulted in an easy-to-use visual tool that might help various types of audience, be it civil society, media or researchers, to find out more about lobbying in Russia. "," Thanks to this project, one can analyze the activities of regional deputies, their lobbying practices and the transparency of political and economic decision-making at the regional level. Thanks to the project, voters can understand how much the deputy elected by them  represents their interests or the interests of the community surrounding them. This will help make a more rational choice next time. Besides this, the project may push lawmakers to legislatively regulate the lobbying institution in Russia. ",https://dumabingo.ru/,https://7x7-journal.ru/articles/2019/09/30/lobbizm-v-gosdume,https://www.znak.com/2019-09-30/chi_interesy_lobbiruyut_sverdlovskie_deputaty_gosdumy_issledovanie_transperensi,https://www.youtube.com/watch?v=oCoiYRCflFc,,,,"Svetlana Telnova, Elena Basmanova, Olga Berezovskaya","  Svetlana Telnova:  Head of the regional office of Transparency International — Russia (TI-R) in the Altai Region.. 34 y.o. Svetlana graduated from the Linguistic Institute of Barnaul State Pedagogical University. She is a member of the Association of Political Scientists of Russia. After graduation, Svetlana worked as a press secretary for a deputy of the State Duma. Then she worked in a number of regional NGOs. She was engaged in youth educational projects as well as the projects aimed at working with journalists, municipal deputies and local civic activists. Svetlana participated in research projects of the INDEM Foundation. She was the organizer of the annual international scientific and practical political and economic conferences. Svetlana also worked in the commercial sector for three years. She leaded the startup of an online store of farm products for two years and was nominated for the award “Kommersant of the Year 2016”. She then worked as deputy CEO of a regional fuel company for a year. In 2017, she headed the TI-R regional office in the Altai Region.    Elena Basmanova:  Lawyer of the regional office of Transparency International — Russia in the Altai Territory. Elena graduated from the faculty of law of the Altai State University. She worked in the Ministry of Internal Affairs and the Fire Department under the Ministry of Emergencies. Elena has a vast experience in legal practice in commercial organizations. Since 2014, she has worked in the regional center of TI-R. Elena deals with the problem of unauthorized buildings in the city of Barnaul and the illegal utility payments local residents of Barnaul are charged with.    Olga Berezovskaya:  Analyst of the regional office of Transparency International — Russia Regional Office in the Altai Region. She graduated from the Altai State Technical University (B.S.) and from the Linguistic Institute of Barnaul State Pedagogical University (M.A.). She worked in a number of Altai regional NGOs, oversaw youth projects and coordinated the School of the Committee of Civil Initiatives in the Altai Region. Since 2018, she has been working as an analyst at the TI-R regional office. ",,,
Brazil,Nexo Jornal,Small,Participant,Best visualization (small and large newsrooms),"30 anos de desmatamento da Amazônia, em mapas e gráficos",15/08/19,"Environment,Agriculture,Economy","Animation,Adobe,CSV,R,RStudio","  Visual stories revealing the deforestation in Amazon rainforest, with satellite images and data.   The Amazon Rainforest is home of about 10% of all known fauna and flora species in the world. ut such biodiversity is endangered: after a decade of decrease in the deforestation of the forest in Brazil, these numbers have risen again since 2015. This graphic combines the data from a time series of deforestation collected by satellites since 1988 and animations of critical points where damage of the rainforest was more considerable in the period.   ",  The publication was driven by the huge discussion about the theme during the fires that hit the forest in 2019. It aimed to produce visual impact to present the deforestation with data.  ," We used R to treat data and produce the chart, mainly Tidyverse packages.    The images were collected in the Google Earth Engine. The GIFs were made using Adobe Photoshop and the final layout in Adobe Illustrator. ", There are a lot of data visualizations that represent the deferostation in the Amazon Rainforest. The hardest part was to figure out a different approach that has a visual impact. The use of satellite images with the data  ," To think about different approaches about very sensitive issues, that already have a lot publications using data. ",https://www.nexojornal.com.br/grafico/2019/08/15/30-anos-de-desmatamento-da-Amaz%C3%B4nia-em-mapas-e-gr%C3%A1ficos,,,,,,,"Gabriel Zanlorenssi, Gabriel Maia, Thiago Quadros",  Gabriel Zanlorenssi . Data scientist and master in Political Science at Universidade de São Paulo.    Gabriel Maia.  Data science intern and undergraduate student of Oceanography at Universidade de São Paulo.    Thiago Quadros.  Art assistant and bachelor in Journalism at Universidade de São Paulo. ,,,
Russia,Transparency International – Russia,Small,Participant,Open data,Unrepresentative power,27/12/19,"Database,Open data,Fact-checking,Politics,Corruption,Women","Adobe,Microsoft Excel,Google Sheets,Python"," The project is a study carried out as part of a hackathon on discrimination and focuses on the representation of women in the regional parliaments of Russia. We uploaded and analyzed data on 500 thousand candidates at the municipal and regional levels (2013 −2017). We used open data published in open sources, conducted interviews with the representatives of regional parliaments and studied international documents and Russian legislation. As a result, we published a study according to which the number of women in the regional parliaments of Russia is less than 15% of the total number of parliamentarians. "," The main thesis of our work is that women face discrimination at all stages of elections to regional legislative assemblies. They are less likely to be elected to leadership positions and earn less than male deputies. We have found out that with a relatively large number of female civil servants, only a few hold leadership positions that allow them to make decisions.    Statistics show that the higher the position is, the fewer women can take this role. And vice versa, there are a lot of women in low positions. We believe that this has a direct negative impact on the standard of living in the country, including lack of protection of women's rights.    According to the UN estimates, the “critical mass” of women represented in legislative authorities should reach 30%, otherwise women will not be able to significantly influence any changes or decision-making processes and to promote their initiatives.    The publication of the study caused a stir in social networks. It was posted on the official website of Transparency International — Russia, and also posted in a number of large Russian media with a high audience reach.    We believe that in the current situation in terms of gender equality in Russia, it is important to raise awareness, ask questions about the activities of the state on the implementation of international agreements and the national gender plan, and thereby change the situation in the country. We wanted to make sure that the constitutional rights of every citizen and resident of the country are respected, and from this point of view, the project has fulfilled its function. "," We used the data from the <a href=""http://www.cikrf.ru/eng/"">State automated system ""Elections""</a> that contains information on all candidates who submitted applications to election commissions. To collect this data, we created a parser in Python and performed several iterations of data processing. It is worth noting that the indication of the gender of the candidate was determined automatically during parsing by identifying the end of the middle name (in Russian it depends on the gender). Problems arose when the middle name was not indicated, typos were found in the candidate’s name, or when the gender was difficult to determine. In addition, errors were found on the website of the Central Election Committee. For example, there were candidates born in 1903 or 1915. There were also some obvious typos and mistakes in the titles of political parties, etc. We also used various open databases. "," The main difficulties in the implementation of the project arose when we were trying to find Russian reports on the implementation of paragraphs of international documents regarding gender equality, as well as when we looked for information on the implementation of the National Strategy for Women.   The uniqueness, complexity and at the same time the advantage of our project lies in the fact that it brought together specialists in various fields united by the common idea of ​​combating discrimination. Our team included programmers, journalists and designers. Most of the work was done during the weekends, in our free time, under the mentorship of the representatives of Transparency International in Russia.    We believe that the strongest and most successful projects are implemented by those people who are ready to fight discrimination in any of its manifestations, and that is why we believe that our project deserves winning. ","We believe that our project is an illustration of the fact that in order to contribute to changes in society, it is not necessary to be a large organization or a large project with serious funding. It is enough to get together a group of enthusiasts. The information we analyzed is available in the public domain. This means that with the availability of human resources, such studies are quite feasible. When comparing the data, we made a number of important observations. For example, we have found an indirect correlation between the representation of women in parliament and the corruption perception index (CPI). We have articulated an evidence-based fact that the higher the position is and the more authority, access to the distribution of budgets and real power this position brings to a person, the less likely it is that a woman may occupy this position. We have drawn attention to the fact that despite Russia's active foreign and domestic policy regarding the promotion of women's rights, the real changes are insignificant and are rather nominal in nature. We have also pointed out that in Russia there is no single body dedicated only to promoting women's rights and improving their position. Our study emphasizes that women's voices help to take into account a wider range of public interests and bring additional knowledge and experience to the parliamentary debate. The participation of women, therefore, allows to take into account various points of view and ensure maximum awareness of parliamentarians in the development of public policy. These are simple things that are accessible to everyone, so we believe that our project has not only raised awareness about important problems in society, but also shows that ordinary people can and should be involved in the life of their communities and form an active civil",https://transparency.org.ru/special/womenassembly/,https://novayagazeta.ru/news/2019/12/27/158002-transperensi-zhenschiny-stalkivayutsya-s-diskriminatsiey-na-vyborah-v-regionalnye-zaksobraniya,https://daily.afisha.ru/news/33230-issledovanie-zhenschiny-rezhe-izbirayutsya-na-rukovodyaschie-dolzhnosti-i-poluchayut-menshe-deputatov-muzhchin/,https://7x7-journal.ru/articles/2019/12/31/transperensi-interneshnl-zhenshiny-v-pyat-raz-rezhe-muzhchin-stanovyatsya-deputatami-zaksobranij-i-zarabatyvayut-v-tri-raz-menshe,,,,"Evgenia Verbitskaya (the author of the text), and participants of the hackathon: Anastasia Platonova, Dmitry Usov, Ilya Tchaikovsky, Daria Titova, Evgeny Potapov","  Evgenia Verbitskaya (the author of the text):  Media specialist with an extensive experience directing PR campaigns in international, state and nonprofit organization environments. Professional experience includes projects for UNV, OSCE Election Observation Missions (2015, 2016), USAID, World Bank. Currently works as PR Manager at SEMrush ",,,
United Kingdom,Leeds Beckett University,Big,Participant,Best news application,The best constituency MP in Yorkshire,12/11/19,"Investigation,Explainer,Solutions journalism,Infographics,Chart,Elections,Politics",Google Sheets,"Journalism students at Leeds Beckett University employed public data to answer a simple question: which MP in their region is the most hard-working. Using data from multiple sources the students analysed information on each MP, including how often they voted, how often they rebelled from a party whip, how often they spoke in parliament, how often they asked formal written questions to government ministers and -- most importantly -- how often they mentioned the name of their constituency from the floor of the Houses of Parliament. They then mashed it all together and created a league table and a number"," The students followed up the data exercise with conventional reporting -- interviewing several MPs and other experts. The impact on the students is significant. At the start of the project, many of them couldn't name their local MP, let alone say what they stood for, how they had voted on their behalf, or how effective or busy their representative had been in the last 12 months. By the end of the projecmost students were highly engaged with party politics and had, along the way, learnt about many complicated facets of British public life, such as the Party Whip system (did you know that whips were banned from speaking in parliamentary debates?), the use of Hansard, the structure of government and so on. Other students looked at more specialised data, such as gender balance, or stance on leaving the European Union (we discovered that not one of Yorkshire's 60 MPs voted to leave the EU).  "," In terms of technology we used nothing more complex than google drive (sheets), a few infograms and a bit of HTML. But he real innovation is found in our method. Although being a humble university department with little actual budget and only two experienced journalists (in the form of the tutors) we did recognise our chief asset: human-power. There were 20 of us in total, each working eight hours a day for three days. That's 480 hours in total (or if one person had attempted the task alone, that's the equivalent of 12 working weeks).    In other words, this task could only be done in a group, so we crowd-sourced the work between us, first gathering all the data, putting it in a shared Google sheet and then cleaning the data, not once, but twice.      The main source of data was Hansard, the verbatim record of everything spoken in the UK Parliament, cross-referenced with other publically-available databases, such as The Public Whip and Wikipedia. Where necessary, the students used very old technology -- they rang up the constituency offices to check data discrepancies and ambiguities.                              "," The size of the task. Although many young people are interested in politics, there are many more who are disengaged. Even some of our journalism undergraduates often arrive at our doors seemingly uninterested in the more current affairs that dominate the public sphere. Our task, part of it at least, is to change all that. To get our young people, many from disadvantaged social backgrounds, to engage in politics: to see that they have a stake. This project satisfies that ambition. By placing our leaders under a kind of data microscope -- to see at a click how they all voted  -- and then rationalise the data, our students begin to experience confidence in a subject that has alienated them previously. It also creates a public good, our project is published on our own website and then disseminated across our social media channels. Our journalism creates a permanent league table of effort. At a glance, you can see clearly which of the region's MPs are engaged in the main business of representation -- what they should be doing -- debating and voting.  "," Our method of ""crowdsoucing"" data, sharing arduous work across a group to lighten the individual load is directly transferable to other projects. Indeed we have already begun to replicate the method with other data projects.        The information gleaned from the exercise is also replicable. Other regions of the UK could be scrutinised in a similar way. Indeed, think about it, if we had enough people (undergraduate journalism students) the entire country -- 650 MPs -- could be placed under this level of scrutiny.   ",https://www.leedsnow.co.uk/2019/12/11/conservatives-vs-labour-how-well-does-your-mp-represent-your-constituency/,https://www.leedsnow.co.uk/2019/12/18/rachael-maskell-best-mp-three-years-running/,https://www.leedsnow.co.uk/2019/12/18/rise-in-female-mps-within-yorkshire-and-humber/,https://www.leedsnow.co.uk/2019/12/18/the-mps-in-yorkshire-talking-about-climate-change/,https://www.leedsnow.co.uk/2019/12/18/conservatives-close-in-on-a-labour-yorkshire/,,,Sean Dodson and Katie Hall (tutors) plus 20 journalism students, Sean Dodson and Katie Hall are senior lecturers in Journalism at Leeds Beckett University.  ,,,
Brazil,Nexo Jornal,Small,Shortlist,Best visualization (small and large newsrooms),Como Bolsonaro votou nos últimos 20 anos na Câmara,22/04/19,"Investigation,Politics","Adobe,R,RStudio","  The current president of Brazil, Jair Bolsonaro, have served as a representative in the Chamber of Deputies (lower house) for 28 years. Based on data of his records between 1998 and 2018, we designed a visual representation of  his proximity with each party and each government. Known by his far-right positions, Bolsonaro had the lowest rate of concordance of the votes with leftist parties. One of our conclusions was that he tended to vote closely with right-wing representatives and members of gun’s lobby.  "," The impact of the publication was to provide information of the records of the current president of Brazil, while he was a congressman. It is far known his alignment with far-right issues, but the alignment with specific parties and the each government was not explored.  "," We used R to collect, treat and analyse data, and also to produce the visualizations. The data was collect through the API of the Chamber of Deputies.   The final layout was made in Adobe Illustrator. "," To understand exactly what can be considered an allignment of the vote in the Chamber of Deputies. Besides yes or no votes, the representatives can abstain, be absent or reject to vote (obstruction). For example, sometimes a abstention has the same pratical effect to a no.  "," It is one approach to explore the past records of important politicians. Sometimes, some ideological allignments are presumed to be true but they are not based on data. ",https://www.nexojornal.com.br/grafico/2019/04/22/Como-Bolsonaro-votou-nos-%C3%BAltimos-20-anos-na-C%C3%A2mara,,,,,,,"Gabriel Zanlorenssi, Caroline Souza, Rodolfo Almeida",  Gabriel Zanlorenssi.  Data scientist and master in Poltical Science at Universidade de São Paulo.    Caroline Souza.  Intern in the data team and undergraduate student of Geography at Universidade de São Paulo.    Rodolfo Almeida.  Visual journalist and bachelor in Journalism at Pontifical Catholic University of São Paulo.     ,,,
United States,The Washington Post,Big,Shortlist,Best news application,Which of these 2020 Democrats agrees with you most?,18/11/19,"Explainer,Quiz/game,Database,News application,Elections,Politics","Json,Adobe,Google Sheets,R,Node.js"," “Which of these 2020 Democrats agrees with you most?” is an interactive quiz to match readers to the Democratic primary candidates that they most closely align with on key issues.   This project sprung from a database of 86 questions and nearly 2,000 stances that we built over the course of 2019. We reached out to all of the Democratic campaigns, the biggest field in history, in order to compile this original reporting.   From this database, we pulled 10 questions – later expanded to 20 – that we thought revealed the most interesting differences between the candidates. "," The reader response to this project was extraordinary. Millions completed the quiz, and thousands took to social media to express their delight and surprise at the results. The feedback made one thing very clear: The project was getting readers to engage with tough issues, and to take a step back and think hard about whether their preferred candidates actually aligned with their worldviews.   This is a major achievement. Understanding the policies that a presidential candidate would enact is a critical part of the election process.   Every election, readers tell us that they are interested in reading more substantial policy coverage and less “horse race” coverage. In practice, this request is often aspirational. Any political reporter knows that lots of policy coverage is ignored by readers, even if heavily promoted. This quiz proved that you can get people to engage with this coverage – even coverage that is fairly in the weeds – if you are thorough with your reporting but also creative with your framing and design.   The project made an impact on our readers, but it also made an impact on our publication. The quiz got several thousand people to make a strong commitment to journalism and subscribe to The Post. It was the paper’s most successful project of 2019 in this regard. "," Technically speaking, this project was not that complex. The database of candidate stances was a sprawling Google Sheet. The layout of the quiz was done in React, but that is pretty common for our pages nowadays.   What really made the project shine was the design of the user experience. Policy deep dives can scare away readers that don’t consider themselves wonky. The design of the page – the question wording, the font sizing, the illustrations, the button shadows, the sticky bar, the explosion of confetti at the end if you answer everything – was in service of making the information and results as approachable as possible. There is lots of detailed information on the page, but it is only available once the reader has already made a decision to engage.   Most of these design decisions were subtle. We spent a good bit of time, for example, on deciding what the feedback for the buttons should be – how quickly the results and background information should fade in. But good design is greater than the sum of its parts, and without these interventions the page would not have worked nearly as well.   The most important tool we used, beyond design, was the old-fashioned reporting that got us the candidate stances in the first place. That was also the hardest part of this project, described in the next section. "," The most difficult part of the project was compiling our database of policy stances. From March through December, we published policy pages covering nine broad areas: health care, immigration, voting, climate, education, foreign policy, the economy, guns and criminal justice.   This was a major undertaking. We asked the candidates 86 questions. Given how much the Democrat field swelled over the year, this ended up working out to a final total of around 1,800 candidate stances.   Getting these stances was not as simple as sending along a questionnaire. For one, the questions needed to be meticulously crafted to be as clear to readers as possible without being too broad so that the candidates all sounded about the same. It’s often not in a campaign’s best interest to give straight answers on controversial subjects, and getting these campaigns pinned down on a tough stance was a difficult act of reporting that we had to repeat over and over again.   When campaigns passed on giving us answers, we researched their previous statements and legislation to get a sense of where we thought they stood. We kept the database constantly updated even as the candidate’s own stances shifted over the course of the year. In addition, we researched and wrote background information on all 86 questions to help explain to readers the nuance and importance of the policies we chose to highlight.   All and all, the quiz was just a capstone of a year of diligent campaign reporting that put us in position to create the most comprehensive interactive possible. "," As mentioned above, I think that readers could and did learn a lot from this project. For practitioners, I think there were four big lessons.   One: People who work on graphics teams are reporters like anyone else in the newsroom. Kevin Uhrmacher dreamed up our policy database, but was aware that the campaign reach outs it would require would be a big change for him. With some guidance from other parts of the newsroom, this project transformed him into a diligent and effective campaign reporter.   Two: You shouldn’t be afraid to be flexible with your plans and your teams. Kevin Schaul, Kevin Uhrmacher and I worked on the policy pages for a lot of 2019. The database by itself was, frankly, not connecting with readers that well.  We were a bit stuck, “too deep” in our own reporting. I brought in Brittany Mayes to help design the quiz, and it was her recommendations for framing and page design – coming from someone with an interest but not an obsession with policy – that really elevated it into something that would connect with millions of readers.   Three: Interactivity works, if well-designed and used appropriately. This has been an ongoing debate in newsroom graphics: Will readers actually click on your buttons? Or is it safer, especially in a mobile environment, to ditch that kind of interaction and lean into the scroll. We are pleased to report that over three quarters of readers who answered one question in the quiz answered every question, even after it was expanded to 20 questions.   Fourth: Policy coverage can work, and is worthwhile. But you need to be creative about your framing and presentation if you want to reach a broad audience. ",https://www.washingtonpost.com/graphics/politics/policy-2020/quiz-which-candidate-agrees-with-me/,https://www.washingtonpost.com/graphics/politics/policy-2020/,,,,,,"Kevin Uhrmacher, Kevin Schaul, Brittany Renee Mayes, Reuben Fischer-Baum"," Kevin Schaul is a senior graphics editor at the Washington Post. He graduated from the University of Minnesota with a degree in computer science, though all his professional work has been in newsrooms. He grew up in the Windy City suburbs, where he developed a love for Chicago Cubs baseball and deep dish pizza. In his free time, Kevin dabbles in photography and jazz piano.   Kevin Uhrmacher is a graphics editor for politics covering elections and public policy at The Washington Post. He was a Post intern in 2014 after graduating from the University of North Carolina at Chapel Hill, where he majored in journalism and political science. Kevin married his college sweetheart in 2019 and enjoys lending his voice to his church choir. He grew up in Rochester, NY.   Brittany Renee Mayes is a graphics reporter at the Washington Post and an adjunct professor at the University of Maryland. She is an alum of the NPR Visuals team, the New York Times Student Journalism Institute and the University of North Carolina at Chapel Hill. She's deeply involved in the journalism community, from mentoring in the ONA Student Newsroom to volunteering with Press Pass high schoolers. In her free time, her passions include falling deeply in love with every romance novel she reads, dancing, baking, cooking and napping.   Reuben Fischer-Baum is an assignment editor on the graphics team of The Washington Post. He previously worked at FiveThirtyEight and Deadspin. He grew up in Maine, graduated from Yale University, and tried out urban planning before getting into journalism. He enjoys backpacking and is trying to learn a little ballet.    ",,,
Brazil,Jornal Correio,Big,Participant,Open data,"1,000 Vidas (1,000 Lives)",17/07/19,"Investigation,Database,Open data,Crime,Gun violence","Scraping,Microsoft Excel,Google Sheets,CSV"," The Mil Vidas (1,000 Lives) project exists since 2011. It is published yearly, whenever the 1,000th homicide of the year happens in Salvador, based on daily reports from the Public Security Office. The project has been a finalist in the DJA, in its first edition. In 2018, without announcement, the Office removed all daily reports from its website, keeping only last month or so. In 2019, we opened the whole database, with data from +17,000 homicide victims since 2011. So, thanks  ONLY  to our  9 YEAR  diligent  data journalism work,  this data is again publicly available. "," The project had over 10,000 pageviews in 6 months, but, beyond the audience, the major impact was being the only ones able to re-publish a data that the Government, overnight, without announcement, decided to remove from its Public Security Office website. And we did it even in a better way, since when they were still online, the daily reports from the Government didn’t provide the data in a structured way. Each report was published on a different webpage, We, on the other hand, make the entire database available in CSV.    Besides, because of our diligent work, the Government got notified by the Prosecutor’s Office, for going against the Brazilia FOIA (LAI). That happened because when the Office removed the reports from their website, in early September 2018,  we hadn’t collected the data from August yet. It was the only month we lacked in the entire historical series since 2011, so we decided to wait for the response to open the database complete.    But we had the request rejected in all instances of the Government. After having our last appeal denied, we denounced the violation to the Prosecutor's Office. This process took months, and only after the Prosecutor's Office notified the Government, the information was finally sent. We were thus able to open the data in July 2019 and, since then, we keep the CSV updated.   Although it is ""only"" a CSV, this is the result of  a continuous 9-YEAR work . And today, due to the Office's policy of removing this data, this database is only available because of Correio’s work. We had feedback from professors who are now able to carry out their researches. Due to its granularity, it is an extremely important database for any study of violence in Salvador, covering an entire decade of data. "," The entire project, since 2011, is based on data published daily by the Public Security Office since 2011, on this link: http://www.ssp.ba.gov.br/modules/consultas_externas/index.php?cod=5.   In the early years of the project, the data was collected manually, but only the first 1,000 homicides of the year were collected, since our stories were always based on those. Then, the project was used to be left on stand-by until the following year. With the advance of the project we felt we wanted to start making broader analysis of the whole years' data. Of course, going back in many years over all the daily reports manually would be very hard. So, a former reporter at Correio, Thiago Freire, set up a scraper, and managed to scrape all previous data, since the first report, in 2011, thus obtaining the complete previous years.    From that moment on, we keep the data completely updated. And actually we keep doing it manually, because we need to access the reports for the daily coverage of violence in town. So when we access them, we already collect the data and update our database for the 1,000 Lives project. Then. monthly, we also update the CSV on the project's page.    "," I believe the great merit of this project is not necessarily to have opened the data, since this is already a widespread good practice, not even the volume or type of data that was opened. It is also not a project that has complex technologies or shiny data visualizations. Being a mid-size newsroom, in a poor region of a developing country, we don't have many resources (to give an idea, we don't even have any webdesigner or developers in the newsroom).   Nevertheless, we strongly believe the project is unique, and this uniqueness lies in the fact that we were able to open the data only because, during nine years, we did a CONSTANT work to update this database. We, and only us, have kept this database up to date for so many years. And only due to our work this data is still available, after the Security Office  decided, overnight, to remove it, in an anti-democratic decision that goes against Brazilian legislation. So, although the action of opening the data itself is common in other projects, what is unique in this work is the perseverance of our data journalism work, which allowed a quick and efficient response to an illegal decision of the government. We believe this process and the result of it can be a great reference for data journalists all over the world. "," As already mentioned, it may be that the project has little to contribute in terms of technology or data viz, but we believe that this project has a lot to contribute and can be a global reference in terms of the importance of persevering in building a database, in order to prevent policies changes and maintain authorities accountable.    When working with data, we can sometimes take for granted that historical data will always be there, for when you want to use it. So, you won’t keep your own files because you can always scrape data from the past. However, our project shows that this is not always true, especially in countries or communities where the culture of opening data is not yet consolidated.   Another great point to learn from the project is the importance to perserve not only in a personal level, but also on a team level. Notice that, being already a 9-year project (and counting), the 1,000 Lives was born by the hand of a specific journalist (Juan Torres), but year by year the project (and specially the culture of keeping the data updated) was passed to other journalists (reporters and editors), who keep the project still running.    Besides, the project also has a good point on insisting on FOIA requests and using all instances of it to obtain a public data.     ",https://www.correio24horas.com.br/mil-vidas/,http://bit.ly/Mil_Vidas_2019_print,http://bit.ly/Mil_Vidas_2011_FirstYear_5DaySeries,,,,,"Juan Torres,Clarissa Pacheco, Thiago Freire","  Juan Torres  is a (data) journalist. Works as Digital Media, Data and Innovation Coordinator at Correio* (Salvador, Brazil). Is also a board director of Abraji (Brazilian Association of Investigative Journalism) and a trainer of School of Data. Works at Correio since 2008, having occupied multiple positions and led many awarded projects. Was ICFJ fellow in 2016 for the Digital Path to Entrepreneurship and Innovation for Latin America Program. Has been awarded with INMA Global Media Awards (twice), Petrobras Journalism Awards and Tim Lopes Awards in Investigative Journalism. Has received honorable mention in Vladimir Herzog Awards for Amnesty and Human Rights and has been finalist of Latam Digital Media Awards (three times), Data Journalism Awards, Kurt Schork Awards, newsawards, and Esso Awards.      Clarissa Pacheco  is a journalist and currently works as a sub-editor of Correio, in Salvador. She has also worked for the newspapers A Tarde and Metrópole newspaper. She received the Petrobras Journalism Award, 2017 edition, with the ‘'The Silence of the Innocents””, and the INMA Global Media Awards 2016, for the same work. The project also received an honorable mention of the Vladimir Herzog Award for Amnesty and Human Rights in 2016. In 2015, she was a finalist in the ExxonMobil Journalism Award for the special 'Where's Geovane?', published in Correio.    Thiago Freire  is a journalist, graduated from the Federal University of Bahia. He was a trainee at Correio for two years, in addition to working with union and parliamentary communication. With a background in programming, he seeks to apply technology to solve journalism challenges. He is currently a Master's student in Contemporary Communication and Culture (Poscom / UFBA).     ",,,
United Arab Emirates,Arab News,Small,Participant,Best visualization (small and large newsrooms),Why Jerusalem's Al-Aqsa Mosque matters for Muslims,06/03/19,"Illustration,Infographics,Video,Arts",Adobe," Although the The Dome of the Rock is the most famous sacred place in Jerusalem, The Al Aqsa Mosque date back to 638 AD.   The infographic show the location of Al Aqsa mosque in the Jersualem city "," We had a very positive feedback for highligh the Al Aqsa Mosque details, that didn't have the attention it was deserved before. The feedback was from readers letter ", Very basic; photoshop and Illustrator only. ," Since the site is big, to know the exactly piece/construction in the area without mistake. "," The importance and location of Al-Aqsa Mosque in Jerusalem and because of politics razon, some Muslims were not allowed to visit the place. The subject call attention for this issue. ",https://www.arabnews.com/node/1503396/middle-east,https://www.youtube.com/watch?v=vd5RXBjnyjc&feature=emb_logo,https://youtu.be/vd5RXBjnyjc,,,,,DAOUD KUTTAB: Writer / Faisal Abbas; Editor in Chief/ Douglas Okasaki: senior Desiner / Simon Khalin; creative director, DAOUD KUTTAB: Writer / Faisal Abbas; Editor in Chief/ Douglas Okasaki: senior Desiner / Simon Khalin; creative director ,,,
Brazil,"Zero Hora, GaúchaZH, Grupo RBS",Big,Participant,Best visualization (small and large newsrooms),Pequenos notáveis (Little Notables),03/11/19,"Explainer,Solutions journalism,Long-form,Database,Infographics,Chart,Video,Map","JQuery,Json,Microsoft Excel,CSV,R,Python"," Education in Brazil has a history of bad results. The most recent Programme for International Student Assessment (Pisa), released in 2018, shows that two-thirds of 15-year-old students know less than the basic in Mathematics, and the country has nearly not evolved in Reading and Science in the last 10 years. But in such a disturbing scenario, where are the good examples? And what can we learn from them? Motivated by those questions, we analyzed data about every student, school, city and state in Brazil over months to find out and tell what was behind the few cities with positive results. "," Although the analysis encompassed data from Brazil as a whole, we set our focus to the state of Rio Grande do Sul, where our publication (Zero Hora / GaúchaZH) is located. We went out to visit the cities with the best results in the country's Ministry of Education own assessments and were surprised to find out that the smaller cities appeared as both good and bad highlights. After investigating, we discovered that those with the best scores in Portuguese language (including Reading and Writing) and Mathematics during Elementary Education shared a lot of the same strategies and beliefs.   Another surprise was that the schools with some of the best results were part of the public system – highly regarded as inferior to private institutions in Brazil.   So one of our objectives was, through constructive journalism – although not at all ignoring the fact that education in Brazil suffers from bad results, but trying to point out ways to overcome this –, to allow every citizen to see what worked and what did not. We tried to do this with easy to read maps and tables, presented along stories that, despite relying heavily on data, were told in a very friendly, perhaps engaging way, with data visualization that was easy to understand and still very informative.   After it was published, the article was highly regarded as an example of where good initiatives in education are, why they work and how others can take inspiration from that to apply on their own cases. The State Education Council (CEEd) praised the effort and started referring to the articles discoveries when planning new policies. We heard from a number of schools and cities that have also taken inspiration from what we showed and started working towards better education taking it as a starting point. "," Good data visualization was of uttermost importance for us from the start: we wanted to make heavy use of data, but when the time came to showcase it, everything had to be very understandable, easy to read, inviting – friendly, as we put it.   It all started with data scraping from the Ministry of Education's database, something not publicly accessible when it comes to data about every student and school (the general results are presented every year by the government, but they focus on state and nationwide numbers – we had to go deeper). We filed a freedom of information law request to do so, which took a while to be responded, but fortunately did not involve any major inconveniences.   We initially used Excel to go through data available about Rio Grande do Sul, but went to R when it came to numbers about all of Brazil.   The intention was to use interactive maps and search engines in which readers could not only see a very visual version of the information presented (as seen on maps that show how early years are doing somewhat fine, with maps mostly painted green, while the last years of Elementary School are doing bad, with maps mostly painted red), but also quickly look for their own cities of interest. And later we realized we could also use some focus on the numbers themselves and decided to create scatter plot maps where this was all very visible.   We used technologies and tools such as JQuery, Json and Python throughout the analysis and also for the final version of the project. "," The hardest part was gathering all the data: at first, because of the sheer number of entries – every student enrolled from the 1st to 9th year of Elementary School in every educational institution in Brazil, a country with a population of 209 million – and the need to figure out the scores based on classes from the same school, them from that whole year, then for all school in a state, and then of every school in the country. The project was done with a regional focus, but encompassed data from the entire country and its millions of students. That effort also means the data gathered and conveyed as this story can be used as a reference for anyone in Brazil.   In order to avoid false positives, we also analysed data from earlier years, so we could verify if the cities with the best results were actually constantly achieving the educational goals, not only having one good class during one exam, for example. The cities and schools we visited have shown good results over at least the past 6 years, as shown in three different national education assessments in this decade.   Data used was not publicly accessible, so we had to file a freedom of information law request, something that is hardly done when it comes to checking how well students are doing in the early years of education in Brazil. Finally, it was initially hard to convince all editors that the approach we decided to take was a good one: when newsrooms are usually so concerned about highlighting failures and trying to find out what is wrong, we went the other way and showed what was working, so that the article could be used as a sort of guide to those interested in achieving better results in education. "," This project was not thought about with any audience in particular coming to mind: teachers, school principals, politicians, the government in general and any reader interested in education can find their own use of the gathered material.   But it is not only that: the extensive data analyzed, presented in a friendly manner, could also be used by reporters in our newsroom and even journalists from other outlets to work on their own stories. And time and time again we saw this happening: whenever a city faced a strike from teachers, for example, we would reach out for those maps to see how well their educational system works. Whenever projects related to education would show up at any Parliament, we would check the results of Rio Grande do Sul and Brazil in general to see where there were problems, and what was working.   We have also learned of numerous teachers and schools who referred to this project since its publication when establishing their goals for a school year, seeing the examples in there and often reaching to the cities we visited to learn more about those projects. The very State Education Council (CEEd) would encourage this behaviour.   A Word document with the entire translation of the project in English is available as a project link below in Google Drive and as a backup in OneDrive. ",https://gauchazh.clicrbs.com.br/especiais/pequenos-notaveis/index.html,https://drive.google.com/open?id=1MpWYn2iA1nKcjWAao8sxRNL6z_ZWjMDO,https://1drv.ms/w/s!AjGypYDzLSJYg5dZ1I5tRIdPnF7PTw?e=d5XtdC,,,,,"Guilherme Jancowski de Avila Justino, Hermes Wiederkehr, Omar Freitas, Ticiano Osório","  Guilherme Justino  is a multimedia reporter working in print, online and radio journalism for Zero Hora, GaúchaZH and Rádio Gaúcha in Rio Grande do Sul, Brazil. Specializing in education, science and technology topics, he has won over 10 national journalism awards and was a finalist in a number of others with stories on sustainability, politics and higher education, often involving data analysis. From 2012 to 2014, he worked for Terra Networks, publishing articles in Portuguese and Spanish on the 17 national websites of what was then the largest Latin American online media company. Other than working in the newsroom, he is currently finishing his master’s degree in Communication and Information at the Federal University of Rio Grande do Sul (UFRGS) and participating in a research group focused on media studies. His research interests include information and communications technology, multimedia journalism and convergence.    Hermes Wiederkehr  is a programmer who has been working in Journalism for the past years. He specializes in data scraping and Python to create readable maps and infographics from a variety of databases in order to complement articles – and sometimes also uncover potential stories.    Omar Freitas  is a photographer and videomaker based in Porto Alegre, Rio Grande do Sul, Brazil. You can often find him taking beautiful pictures of the city for his Instagram page, @omarfreitasjunior.    Ticiano Osório  is an editor of Health, Education, Science and Culture for Zero Hora in Brazil. A fan of comics and movies, he often talks about those in his columns, published weekly on paper and two to three times a week online. ",,,
United States,The New York Times,Big,Participant,Best visualization (small and large newsrooms),The Most Detailed Map of Auto Emissions in America,10/10/19,"Explainer,Map,Environment","Personalisation,D3.js,QGIS,Python"," Coal plant smokestacks have long been the poster-children of climate change. But transportation is the largest source of planet-warming greenhouse gases in the United States today – and the bulk of those emissions come from driving in cities and suburbs. For this project, we analyzed data from Boston University’s <a href=""https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1735"">Database of Road Transportation Emissions</a> to show readers how driving-related emissions in their hometown have changed over time and how they compare to emissions from other metro areas across the country. "," Dozens of local media outlets from around the country — from Pittsburgh to Jacksonville — picked up the piece and used the dataset to report their own local stories on driving-related emissions. (See list below.)   Additionally, the map from the piece was requested and used by a representative of the Minnesota State House in a committee meeting on transportation-related emissions.  <h4> Dallas </h4> <ul>    <a href=""https://www.dallasnews.com/news/transportation/2019/10/10/auto-emissions-in-dallas-fort-worth-have-more-than-doubled-since-1990-analysis-finds/"">https://www.dallasnews.com/news/transportation/2019/10/10/auto-emissions-in-dallas-fort-worth-have-more-than-doubled-since-1990-analysis-finds</a>       <a href=""https://www.dmagazine.com/frontburner/2019/10/new-map-of-auto-emissions-in-america-shows-dallas-is-outpacing-its-peers/"">https://www.dmagazine.com/frontburner/2019/10/new-map-of-auto-emissions-in-america-shows-dallas-is-outpacing-its-peers/</a>    </ul> <h4> Des Moines </h4> <ul>    <a href=""https://whoradio.iheart.com/content/2019-10-10-new-data-shows-sharp-rise-in-des-moines-auto-emissions/"">https://whoradio.iheart.com/content/2019-10-10-new-data-shows-sharp-rise-in-des-moines-auto-emissions/</a>    </ul> <h4> Boston (interview with researcher) </h4> <ul>    <a href=""https://amp.wbur.org/earthwhile/2019/10/10/vehicle-emissions-boston-map"">https://amp.wbur.org/earthwhile/2019/10/10/vehicle-emissions-boston-map</a>    </ul> <h4> New Haven </h4> <ul>    <a href=""https://www.nhregister.com/news/article/Auto-emissions-on-the-rise-in-Connecticut-14506642.php"">https://www.nhregister.com/news/article/Auto-emissions-on-the-rise-in-Connecticut-14506642.php</a>    </ul> <h4> Philadelphia </h4> <ul>    <a href=""https://www.inquirer.com/science/climate/philadelphia-climate-change-greenhouse-gas-auto-emissions-20191011.html"">https://www.inquirer.com/science/climate/philadelphia-climate-change-greenhouse-gas-auto-emissions-20191011.html</a>    </ul>   Rochester, N.Y.   <ul>    <a href=""https://www.wxxinews.org/post/report-rochester-area-s-greenhouse-gas-emissions-decrease"">https://www.wxxinews.org/post/report-rochester-area-s-greenhouse-gas-emissions-decrease</a>    </ul>   Tampa   <ul>    <a href=""https://www.tampabay.com/news/environment/2019/10/14/vehicle-emissions-are-up-sharply-and-its-fueling-climate-change/"">https://www.tampabay.com/news/environment/2019/10/14/vehicle-emissions-are-up-sharply-and-its-fueling-climate-change/</a>    </ul>   Austin   <ul>    <a href=""https://www.kxan.com/news/local/austin/austin-vehicle-emissions-skyrocketed-178-since-1990/"">https://www.kxan.com/news/local/austin/austin-vehicle-emissions-skyrocketed-178-since-1990/</a>    </ul>   Seattle   <ul>    <a href=""https://www.king5.com/article/news/local/vehicle-emissions-rise-in-seattle-metro-areas/281-e2454541-a1db-4373-b1e2-6f7477f6a92c"">https://www.king5.com/article/news/local/vehicle-emissions-rise-in-seattle-metro-areas/281-e2454541-a1db-4373-b1e2-6f7477f6a92c</a>    </ul> <h4> Jacksonville </h4> <ul>    <a href=""https://news.wjct.org/post/new-york-times-vehicle-emissions-jacksonville-53-1990"">https://news.wjct.org/post/new-york-times-vehicle-emissions-jacksonville-53-1990</a>    </ul>","We used GDAL and rasterstats, a Python library, to process raw raster images from the Boston University research group. Mapshaper was useful in helping us trim the large rasters by various Census shapes until we found the right geography for our story. (We went through a few different analyses, first looking at metro-level, t We used QGIS to see the geospatial data on a map and test out various color scales. To analyze the numerical and per capita values, we used Python to parse raw population data and Javascript to sketch out small multiple charts and maps of different cities to find any interesting trends over time. For the final product, we made a custom slippy map, mainly using the D3.js library, sandwiching together static layers of the data and map labels along with an interactive SVG layer for users to interact with to find out more details about specific metro areas. This allowed us to keep the original map projection of the raw data so none of the pixels would be resampled. We could also add a cartographic touch with custom road shield labels to emphasize the roads where there were high emissions. We geolocated readers as they landed on the page so readers would instantly see their own metro area on the map. The article page was powered by ArchieML, a tool that links a Google Document to the article page, which allowed for easy editing and flexibility in moving components around, very useful for us during the storyboarding process. Finally, for the print product, which included a doubletruck of the map, we used QGIS to bucket our data and export individual data layers to Photoshop. This method allowed us to control the exact CYMK values we wanted to use to make sure the data was clear in","   Climate change is often a subject that feels abstract to readers. Greenhouse gas emissions are not something we encounter with our senses every day. We do not see them with the naked eye, or even smell them by nose, the way we experience other pollution. Yet, most of our actions result in these emissions.   The hardest part of this project was finding a way to tell the visual story of driving-related GHGs in a way that would connect with readers.   The high level of detail offered by the road-level emissions maps from Boston University was a big draw because it helped make the abstract idea of driving-related emissions feel less abstract.  Through the maps, readers could see emissions coming from the very roads they drive on every day, making a direct linkage back to the source: Cars. (The research team mapped emissions down to the road-segment level.)   However, relying on the detailed, road-level map data for the top visual was limiting for storytelling purposes, as these maps only show total emissions and cannot be easily translated to per-capita measurements (not as a technical issue, but rather an issue of emissions allocation). The per-capita data, however, is much more useful for comparing city by city greenhouse gas releases, and for readers who wanted to understand their own impact.    We reconciled these competing needs by using the detailed map as the base for our lead graphic, in order to provide readers with visual context, but overlaying it with information that is not directly reflected in the map in the form of a tooltip.    This top graphic provided a personalized “hook” to get people to understand the story more personally. Then we were able to use the rest of our data analysis to compare trends over time and across cities lower down.    "," A single dataviz doesn't have to do all the work, but can serve as a visual ""nut graf"" that summarizes data graphics further down in the piece. We used our top visual as a compelling hook into the rest of the piece, but were able to tell a more nuanced story further down the page once we captured a viewers attention and established them in a sense of place.   Others can also learn to be bold in their use of maps to help locate readers within the story. We built our own large slippy map geolocated to a users’ location for this piece in order to help readers understand the story through connection to their own city/town. Readers were able to intimately explore the dataset at the ""new view"" (or zoomed in scale) because it offered a more interesting story and a more personal connection to the data. ",https://www.nytimes.com/interactive/2019/10/10/climate/driving-emissions-map.html,,,,,,,"Denise Lu, Nadja Popovich","  Denise Lu  is a graphics editor at The New York Times. She makes data visualizations and maps for breaking and enterprise news.     Nadja Popovich  is a reporter and graphics editor on The New York Times’ Climate Team. She covers climate science, energy policy and the real-world impacts of our warming world. She is particularly interested in using interactivity and personalization to help readers more viscerally relate to the effects of climate change through data. ",,,
United States,ProPublica,Big,Participant,Best data-driven reporting (small and large newsrooms),Gutting the IRS,04/01/19,"Investigation,Database,Open data,News application,Map,Economy","D3.js,Microsoft Excel,CSV,R,RStudio"," For more than a year, ProPublica has been investigating the calamitous — and mostly ignored — effects of a multiyear de-funding of the IRS. In particular, ProPublica’s 2019 series “Gutting the IRS” demonstrated in multiple ways precisely how the agency's weakening has benefited the wealthy and their interests while harming the poor. A key part of the project was a stunning online graphic that allowed readers to see the audit rate of every county in the U.S. simply by moving their cursor over the map. "," ProPublica’s reporting led to pressure from Congress on the IRS to reform its auditing practices. The map was displayed during congressional hearings so members of Congress <a href=""https://www.propublica.org/article/lawmakers-to-irs-commissioner-charles-rettig-system-stacked-for-the-rich"">could grill the IRS commissioner</a> as to why residents of impoverished Tallahatchie County, Mississippi, for example, were noticeably more likely to be audited than those in wealthy Palm Beach County, Florida. Alabama Senator Doug Jones cited our findings, <a href=""https://www.propublica.org/article/alabama-senator-doug-jones-to-irs-stop-picking-on-the-south"">calling them “blatantly discriminatory,”</a> in a letter to the IRS commissioner. In a hearing around the same time,<a href=""https://www.propublica.org/article/lawmakers-to-irs-commissioner-charles-rettig-system-stacked-for-the-rich""> Oregon Senator Ron Wyden cited the same findings</a> and demanded that the commissioner come up with a plan to address the problems. Another senator commented, “The map looks like the IRS is targeting black, Hispanic and Native American populations for audit.”     In October, ProPublica followed up with an article that described the agency’s response to the senators’ demand for a plan. The article, <a href=""https://www.propublica.org/article/irs-sorry-but-its-just-easier-and-cheaper-to-audit-the-poor"">“IRS: Sorry, but it’s Just Easier and Cheaper to Audit the Poor,”</a> described how the IRS had effectively thrown up its hands. The article captured the catch-22 — Congress pointing a finger at the IRS and the IRS blaming lack of funding by Congress — this way: “the IRS has no plan and won’t have one until Congress agrees to restore the funding it slashed from the agency over the past nine years — something lawmakers have shown little inclination to do.” "," Our ""<a href=""https://projects.propublica.org/graphics/eitc-audit"">Where in The U.S. Are You Most Likely to Be Audited by the IRS?</a>"" map project relied on an estimate of the number of audits per county, which was calculated by a former IRS researcher. We decided the researcher's rates used a denominator that would be confusing for readers, so we recalculated them using a measure that would be easier to describe and understand. (And we got sign-off from the researcher that our method was sound!) We used R to clean the data and combine it with additional data from the IRS. We used Javascript to make the interactive maps. We also open-sourced the data and analysis so others could review and reproduce our methods (Github link is below). "," Our main challenge was to render a topic as dry as IRS audit rates into something that would grab attention and spark reform. To do this, we employed two approaches. We transformed the audit data into a story about actual people who lived in actual places, and we experimented with different analyses of the audit groups until we hit on the most potent description of the problem. A story that was just generally about poor people bearing the brunt of the audits turned into a story about people in the Mississippi Delta and Alabama. And our analysis allowed us to clearly say, as we wrote, ""<a href=""https://www.propublica.org/article/irs-now-audits-poor-americans-at-about-the-same-rate-as-the-top-1-percent"">It's Getting Worse: The IRS Now Audits Poor Americans at About the Same Rate as the Top 1%</a>."" Both of these techniques clearly framed the problem and eventually galvanized interest from Congress. "," It really is possible to get people interested in taxes! It just takes the right framing.   As far as design of our audit map, we decided the clearest presentation would be two separate maps, one for areas audited more commonly than average, and one for those audited less frequently. That design decision made the maps easy for people to understand quickly, and had a big impact on the success of the story. ",https://projects.propublica.org/graphics/eitc-audit,https://www.propublica.org/article/irs-now-audits-poor-americans-at-about-the-same-rate-as-the-top-1-percent,https://www.propublica.org/article/irs-sorry-but-its-just-easier-and-cheaper-to-audit-the-poor,https://www.propublica.org/article/lawmakers-to-irs-commissioner-charles-rettig-system-stacked-for-the-rich,https://github.com/propublica/auditData,,,"Paul Kiel, Hannah Fresques"," Paul Kiel covers business for ProPublica. His recent focus is <a href=""https://www.propublica.org/series/gutting-the-irs"">on the IRS</a> and its ability to administer the nation's tax laws.    Hannah Fresques is a data reporter at ProPublica. She previously worked as a data fellow at ProPublica and in education policy research with MDRC, a nonpartisan, nonprofit research organization specializing in random assignment program evaluation. She holds a master's degree in quantitative methods for social sciences from Columbia University, where her coursework focused on applied statistics, research methodology and data science. ",,,
Germany,ZEIT ONLINE,Big,Shortlist,Best visualization (small and large newsrooms),The Millions Who Left,05/02/19,"Investigation,Chart,Map,Politics","Animation,Personalisation,D3.js,QGIS,Json,CSV,R,RStudio,PostgreSQL,Python,Node.js"," The year 2019 marked the 30th anniversary of the fall of the Berlin Wall and the opening of the inner-German border. Millions of people have since left the east for western Germany in hopes of a better life, thus triggering a demographic crisis. We’ve evaluated data on every single move that has taken place ever since. For the first time, it is now possible to tell one of the least-documented stories of German post-war history.    The key visual is an animated map where each dot corresponds to a single move illustrating the historic movement in a very personal manner.     "," The story shows the force with which migration has hit most of the regions in the former East Germany and what consequences it still has in these districts today. We were able to show that areas that have experienced the greatest population loss are more susceptible to right-wing populist parties.    However, the data evaluation also shows a surprising historical watershed moment for the year 2017: For the first time, more people moved from west to east than in the other direction. For the time being, decades of outward migration has been halted – and it shows that there’s cause for hope.    Our data story was covered by media around the world, such as Voice of America in the USA or the daily Dagens Nyheter in Sweden. The German public TV station MDR reported about the data and even hosted a talk show on the topic. German local media printed reports based on our data on migration in their respective regions.    In the meantime, we have also made the data set available to scientists from Harvard and Stanford for research purposes. For example, the data gives the researchers an opportunity to examine the extent to which the expansion of the German long-term care sector in the mid-90s - after the introduction of the social long-term care insurance in reunified Germany - was made possible due to the willingness of people arriving from the former GDR to take up these jobs. This historical episode allows them to shed light on the challenges and possible solutions relating to the shortages of labour and the quality of jobs in the long-term care sector today.    "," We cleaned the data in Python and Pandas and double-checked it in R using absolute numbers of relocations per town from a second source. We then built a database in Postgres. We analysed the data in iPython Jupyter Notebooks, R, Excel and QGIS. We also used our good old-fashioned printer to have a look at a lot of small multiples of the migrations flows to inspect the data physically.   We used React to build the interactive visualisations in the article. The flow map used react-three-fibre and custom shaders to render one moving dot for every move in Germany along pre-computed paths. We calculated those paths using Force-directed edge bundling. The population change visualisation was done with d3’s force simulation to lay out the points. Line charts made use of d3’s scales and path drawing code. We also used Adobe Illustrator to finalise our static graphics.    "," We obtained the data in their raw form as unstructured Excel files by year and state. In total, the raw data compromised 288 Excel tables in different formats and without uniform columns. There were separate entries for moves from and to one town. However, these two entries were not symmetrical. We consulted with the Statistical Offices, resulting in several data corrections and the discovery of missing entries. Some of the places, that stood out most in our analysis, were in fact transit centres for the resettlement of German-Polish repatriates.   The data were also not directly comparable because district borders in Germany have changed over time. As a workaround, we used the most recent demarcations from 2017 provided by the Federal Institute for Research on Building, Urban Affairs and Spatial Development. Using the institute’s conversion keys, past moves are counted as a proportion of the population of the new district. This may result in deviations due to rounding. To show relative migration flows, we used census-adjusted population figures from the Federal Institute.   Visualising the migration flows on the animated map proved to be challenging, too. Several visualisation ideas didn’t work. So, we summarised them by Force-directed edge bundling. But we weren’t able to calculate these maps on our computers. We had to use the VR machine from the video department with a powerful graphics chip.    "," We introduced a two-eyes principle in the data cleaning and analysis process: Two team members used different sources, programming languages and approaches to compare and verify their results in the end. ",https://www.zeit.de/politik/deutschland/2019-05/east-west-exodus-migration-east-germany-demography,"https://www.zeit.de/politik/deutschland/2019-05/ost-west-wanderung-abwanderung-ostdeutschland-umzug (Original publication May 2nd, Translation: May 30th)",,,,,,"Christian Bangel, Paul Blickle, Elena Erdmann, Philip Faigle, Andreas Loos, Julian Stahnke, Julius Troeger, Sascha Venohr"," Christian Bangel (Political Correspondent), Paul Blickle (Information Designer), Elena Erdmann (Data Scientist), Philip Faigle (Special Projects Editor), Andreas Loos (Data Scientist), Julian Stahnke (Interaction Designer), Julius Troeger (Head of Visual Journalism), Sascha Venohr (Head of Data Journalism) ",,,
Singapore,Reuters,Big,Participant,Best data-driven reporting (small and large newsrooms),Wiki wars: Hong Kong's online frontline,28/11/19,"Investigation,Chart",Scraping," The Reuters graphics team analysed edit data of Wikipedia pages related to the Hong Kong protests, the city’s leader, and the police force. Anti-police and anti-government sentiment seemed to be gaining momentum so we had the idea of checking if this had made its way into articles of the open online encyclopaedia.    It had, and in a big way. We scraped all of this date to visualise the spikes in editing, show how edit wars played out between users and more seasoned editors, and also show some of the extreme comments made. "," This piece formed part of our Hong Kong graphics coverage and was used extensively during the year. It was shared widely on social media, providing a completely alternative angle to a story that was already heavily covered.   "," Data from the Wiki pages was scraped programmatically before being analysed and experimented with in R. Visualisations were then exported and styled in Adobe Illustrator before being placed on a web page using ai2html.    Animations, such as the opening summary box, were all made by replicating the ones on Wikipedia using HTML and CSS directly in the browser window. "," One of the hardest parts of this project was being sure to input the multitude of balancing voices. We needed to get comment from the police, government, Wikipedia, a Wiki editor involved in the pages, and also someone who specialises in this kind of online behaviour. There was a lot of reporting that went into this graphic.    Coming up with this fresh way to look at the HK story was also a challenge in itself. None of our Hong Kong pieces follow the typical line of news developments. We always try to create unique stories and visualisations. ", Sometimes a new and interesting idea is what makes a project special or unique. Creating something which stands out from the noise of everyone else’s coverage is a way to add value to an ongoing story. ,https://graphics.reuters.com/HONGKONG-PROTESTS-WIKIPEDIA/0100B33629V/index.html,,,,,,,"Manas Sharma, Simon Scarr"," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Puerto Rico,Radio Ambulante,Small,Participant,Innovation (small and large newsrooms),Headcount,04/02/19,"Investigation,Long-form,Podcast/radio,Audio,Health","Scraping,Microsoft Excel,Google Sheets,PostgreSQL,Python"," In the aftermath of hurricane Maria in 2017, local authorities in Puerto Rico insisted that only 64 people died. In reality, there were three thousand hurricane-related deaths. This podcast episode from Radio Ambulante digs deep into the data from the office of vital statistics in Puerto Rico. From that analysis, a portrait emerges of negligence on a massive scale by the government, hospitals and institutions that take care of older Puerto Ricans. The old, the sick and the poor, those were the people that died after the hurricane. This episode follows the story of one death after the hurricane. "," This project exposed the lies told by the Puerto Rican government regarding the death toll after Hurricane Maria.   More importantly, it investigates the causes that led to the deaths of the most vulnerable residents of the island. The reporting shows that the government's Department of Family and Welfare Services did not have communications with nursing homes and other facilities that it is tasked with regulating and overseeing.  They didn't even know where these institutions were located. This was a deadly oversight in the midst of the historic power outage that followed the hurricane. The Department of Family and Welfare Services is supposed to be the main driver of aid for institutions like foster homes and nursing homes. Their inability to identify and reach these places delayed the arrival of assistance for several weeks, and sometimes for several months. For older Puerto Ricans with chronic health conditions, the wait was too long.  Hundreds died in nursing homes as they waited for help to arrive.   Once the story landed on the Radio Ambulante feed, it generated a long-overdue conversation about disaster preparedness, government accountability and transparency in Puerto Rico. "," As a podcast that features the best of narrative journalism and non-fiction storytelling in Latin America, we wanted to tell the story of the death toll in sound.  This required an intense process of data sonification. After identifying the data set with death records, we worked with a data journalist who analyzed the statistics using scraping techniques, excel, python and other tools.  After that, we engaged in a conversation between the editors, the lead reporter, our sound designer and musicians in order to turn the data into sound. We used the tools of our trade to do this: music and Hindenburg, our mixing program. "," The most exciting part of this project was the journalistic conversation between experts in different fields who came together to tell a data-driven story based on sound.    Our lead reporter Luis Trelles worked very hard to access the death records from Puerto Rico's office of vital statistics.  Hassel Fallas, the data journalist on this project, then worked with those statistics for several months. As the team broke down and sorted over 25 thousand entries in death records, the full story of the deceased began to emerge.  Our editors Daniel Alarcón and Camila Segura led the conversation on how to turn that data analysis into a seamless part of the narrative. At that point, our sound designer Andrés Azpiri and musician Remy Lozano joined the conversation. The whole group brainstormed ideas about how to turn data points into elements of the sound design.    The result was a three minute section of the episode that features data sonification.  It was focused on the real scope of daily deaths after the hurricane, and the stark contrast with the official government death toll.  The biggest challenge was centered on how to preserve an exact relation between data points and the rhythms and beats that made up the sound design.  It was a process of trial and error. The number of beats in each draft was broken down and analyzed by the whole team to make sure that it represented the data with total accuracy.      Our social media team, led by Jorge Caraballo, then turned that part of the episode into a stand-alone ""audiogram"", a short sound piece that could be easily shared in social media to maximize its impact. "," As podcasts and investigative audio shows gain prominence, data sonification will become increasingly relevant.  There is a need to tell data-driven stories in sound without asking listeners to go look for a visualization package in the show's website.  The conversation about techniques and innovative approaches is just beginning, but there is something to be learned from the process we followed for this Radio Ambulante episode.  The discussions in our team centered around storytelling, and how to approach data journalism and sound design in a way that feels organic to an audio format.     Big complicated numbers and figures are extremely difficult to convey in radio and podcast episodes.  The abstract nature of data tends to pull the listener out of a piece. Data sonification is a way of grounding those numbers and relations with concrete sounds.  The results can be dramatic. A well thought out piece of data sonification conveys the full force of a story in a way that will not fall on deaf ears. ",https://www.dropbox.com/s/dco5vt63lt75a8j/Radio%20Ambulante%20Data%20Sonification%20for%20social%20media.mp4?dl=0,https://radioambulante.org/audio/el-conteo,https://radioambulante.org/en/translation/translation-headcount,,,,,"Luis Trelles, Andrés Azpiri, Daniel Alarcón, Camila Segura, Jorge Caraballo, Hassel Fallas, Remy Lozano"," Luis Trelles is an editor and producer with Radio Ambulante, a podcast that tells the stories of Latin America and LatinX communities in the United States in Spanish.  He is based in Puerto Rico.   Andrés Azpiri is the sound designer and engineer for Radio Ambulante.  He is based in Mexico City.   Originally from Costa Rica, Hassel Fallas is a free lance data journalist.   Daniel Alarcón is the Executive Producer for Radio Ambulante, he is based in New York City.   Camila Segura is lead editor with Radio Ambulante, she is based in Bogotá, Colombia.   Remy Lozano is a musician.  He is based in France. ",,,
Germany,ZEIT ONLINE,Big,Participant,Best news application,This is what the Bundestag is talking about (Darüber spricht der Bundestag),09/09/19,"News application,Illustration,Chart,Video,Politics","Animation,Personalisation,Scraping,D3.js,Json,Adobe,Creative Suite,Google Sheets,CSV,PostgreSQL,Python,Node.js"," Over the past 70 years, Germany’s federal parliament, the Bundestag, has met in 4,216 sessions. The stenographers working in parliament transcribed more than 200 million words over those years. But only tiny excerpts of these speeches make it into the big news, the rest is recorded and then disappears into the archives. But it provides a valuable source and dataset. We have therefore made all speeches since 1949 analysable in an interactive tool. Readers can explore for themselves when which issues were debated and the how language relating to those issues has changed over time. "," Just in time for the 70th anniversary of the Federal Republic of Germany, our tool makes it possible to look at the history of the country as if through a magnifying glass, to understand its twists and turns, its development. The curves show which debates were big and verbose, which were small. What was frequently discussed and when, what was rarely or never discussed in all these years, although it might have been important?   We discovered, for example, that in 1983, there were long debates in the Bundestag about whether fibre optic cables should be laid in large cities in order to transmit larger amounts of data. But neither the Federal Post Office nor the Federal Government succeeded in pushing through this innovation. It took more than 30 years to realise how important fibre optic technology is. The result being that Germany is known today for its very slow Internet.   Based on the news app, we created a text series Bundeswörter (<a href=""https://www.zeit.de/serie/bundeswoerter"">https://www.zeit.de/serie/bundeswoerter</a>). Other media also reported on the project. We received requests from students and researchers, including from Oxford University and the UN Security Council. We provided the researchers with our data. The project has also inspired endeavours in other countries such as Austria, were a similar project was implemented based on our idea. Furthermore, a book project with a major publishing house is in progress.   Furthermore, the project was a huge success on social media. Our hashtag #bundeswoerter was #1 on the Twitter-Trending-Topics for one day, the article was shared thousands of times – and not only the article itself but thousands of different graphs, since we included a sharing option for every state of the interactive graphic. "," The frontend part was mainly developed using React (with libraries such as react-select) and D3. For the data work, we used Python: We scraped the data from the website of the German Bundestag using the Beautiful Soup and Requests libraries. We parsed the data in Python using Regular Expressions.    In the next step, we broke this text body down into individual words and created a separate document for each word, which contains the absolute frequency as well as the frequency per year. We have standardised the document length, since the number of speeches in the Bundestag has varied considerably over the years - and would otherwise not have been comparable.   We indexed the documents using the Lucene-based open source search technology Elasticsearch. Elasticsearch not only offers fast search options for texts, but also has several features for decomposing and analysing these texts. For example, we realised our word suggestions during input with the so-called Completion Suggester. This automatic dropdown also enabled us to forego a spell check. We contained the database in a Docker Container, so that we could adjust the performance to cater a large number of readers efficiently. We have blogged in detail about the back end technology: https://blog.zeit.de/dev/reden-im-bundestag-auf-knopfdruck-skalierbar/ (German) "," The biggest challenge with our News App was to provide a perfect interface. It needed to be absolutely focused and immediately understandable and usable by everyone - but also be able to display special features of the language in an uncomplicated way. We consciously decided against using more sophisticated Natural Language Processing Methods, and to use an approach that all of our readers could understand: counting words.    During the course of the project, we consistently discussed how much we wanted to preprocess the text data. For instance, we tried lemmatisation to group similar words such as conjugated verbs. However, lemmatisation and stemming are still not perfect, especially in German. Initial tests and discussions with linguists have shown that lemmatising in German is still very error-prone and we have therefore decided to use a simple variant. Instead, we did offer the possibility of adding up all the counts for several words.   Another challenge was the sheer quantity of data: After parsing the 4,216 text documents since September 7, 1949, the date of the founding of the German Bundestag, we had about 200 million words. To allow a large number of readers to access the entire database at the same time, we had to provide a robust technical infrastructure. "," Analysing large data sets in a newsroom often results in a single article or data visualisation. Oftentimes, these stand-alone projects do not justify the amount of work that goes into the process of cleaning and analysing the data. More importantly, many stories that can be found in the data go untold. This is why we started a series of smaller articles using the tool as a starting point for investigations.   We built the news app with a focus on social media sharing options, including a back end solution for personalized sharing images. It always pays off for us if we can break down information for our readers to the point where they can find out exactly what something means to them. That's why we think of news applications as being shareable elements that can be personalised and shared via social media. In this application, our readers could share a personalised graphics and a parameterised URL leading to their own findings. Thousands did so on Facebook and Twitter.   It should also be noted, that there were no differences in hierarchy within the interdisciplinary team of 14 people. Backend developers, investigative journalists or designers were authors equally. ",https://www.zeit.de/politik/deutschland/2019-09/bundestag-jubilaeum-70-jahre-parlament-reden-woerter-sprache-wandel,Google Translate: https://translate.google.com/translate?hl=de&sl=de&tl=en&u=https%3A%2F%2Fwww.zeit.de%2Fpolitik%2Fdeutschland%2F2019-09%2Fbundestag-jubilaeum-70-jahre-parlament-reden-woerter-sprache-wandel,,,,,,"Kai Biermann, Paul Blickle, Ron Drongowski, Annick Ehmann, Elena Erdmann, Flavio Gortana, Alicia Lindhoff, Christopher Möller, Christoph Rauscher, Stephan Scheying, Michael Schlieben, Julian Stahnke, Julius Troeger, Sascha Venohr"," Kai Biermann (Investigative Journalist), Paul Blickle (Information Designer), Ron Drongowski (Backend Developer), Annick Ehmann (Information Designer), Elena Erdmann (Data Scientist and Data Journalist), Flavio Gortana (Freelance Web Developer), Alicia Lindhoff (Reporter), Christopher Möller (Freelance Web Developer), Christoph Rauscher (Design Lead), Stephan Scheying (Backend Developer), Michael Schlieben (Political Correspondent), Julian Stahnke (Interaction Designer), Julius Troeger (Head of Visual Journalism), Sascha Venohr (Head of Data Journalism) ",,,
United Kingdom,The Guardian,Big,Participant,Best visualization (small and large newsrooms),Big Polluters: MPs' climate scorecards,10/11/19,"Investigation,Infographics,Chart,Elections,Politics,Environment","Scraping,D3.js,Json,Google Sheets,CSV,Python,Node.js"," The ground-breaking Polluters Project was a Guardian investigation into the fossil fuel companies driving the world to the brink of climate breakdown and the political, financial and lobbying structures that enable them. It aimed to focus the climate debate, away from individual responsibility, to those in power. In this part of the project we investigated MPs’ climate records, combining several strands of expertise - environmental reporting, data methods, visualisation and design - to produce a visually compelling and interactive piece of investigative journalism on one of the most important issues of our time. "," Although information on MPs' interests is publicly available, the way in which it is held is almost impossible for voters to access in a meaningful way.    Our MPs' scorecard interactive is the most comprehensive picture to date on the climate records of the UK parliament's elected representatives, providing readers with relevant information and enabling them to weigh up the climate record of their own MP and other elected representatives.   Our interactive was published ahead of the 2019 General Election and, in that period, was used as a tool by members of the public, many of who shared the results on social media. We were pleased to hear from MPs who reported that they had been pressed on their climate records while on the campaign trail (although some of the MPs themselves were less than thrilled to be pressed on the issue on the doorsteps).   Although the analysis provoked a negative reaction from some MPs, others - even those with modest scores - recognised it was an important exercise that would stimulate debate. The hope is that initiatives like this will help press governments to act: as the UN says the coming decade is crucial if the world is to avoid the most catastrophic consequences of global heating.   A similar methodology has since been used by the Irish <a href=""https://notherenotanywhere.com/nhna-climate-score/"">Not Here Not Anywhere campaign</a> in the run up to the February 2020 general election.  "," In order to provide readers with a fuller picture of MP's climate records we developed complementary methodologies before visualising the resulting data:  <ul>  We analysed 10 years of MPs’ published interests - donations, shares, salaries and gifts - against a list of more than 400 search terms as associated with fossil fuel companies, petrostates and climate sceptics (as agreed by the Guardian/DeSmog). Each individual MP's full interests history was scraped using Python scraper and terms were identified using regex.    We developed a scoring system based on 16 indicative parliamentary divisions on climate chosen by the Guardian and DeSmog UK in consultation with several other organisations. The sole criteria in selecting MPs’ climate votes was whether their votes were likely to increase or reduce emissions. To carry out this part of the analysis we used the Import functions within Google sheets to quickly fetch the data and built out a scoring table using If Statements, VLookUps and Index/Match formulas.   We used an interactive histogram that: i) visualised the distribution of MPs' climate scores, and ii) showed MPs' 'climate scorecards' - their voting records alongside a summary of their published interests (donations, shares, salaries and gifts associated with fossil fuel companies, petrostates, aviation companies and climate sceptics). The core technologies used in the project were Javascript and Google Sheets. A NodeJS script fetched the data from a Google Sheet and parsed it into json files. These files were imported by a React application that rendered the graphics and the interactive. The main interactive was built using d3-force, a d3 module that creates a physics based simulation to position visual elements according to a set of criteria (in our case the individual MPs' scores). The application was deployed to AWS.  </ul>    "," There were several challenges to overcome in producing this project:  <ul>   Identifying the appropriate legislation:  a lot of thought went into identifying the most meaningful parliamentary divisions on climate. The legislative pieces were chosen by the Guardian and DeSmog in consultation with Friends of the Earth, Greenpeace, Rebecca Willis (a research associate at Lancaster University) and others. The sole criteria in selecting the climate votes was whether they were likely to increase or reduce emissions.    Developing a scoring system:  we went through several iterations of scoring systems. We had wanted to up-vote those who had put their climate interests over the party whip: however, this disadvantaged MPs who consistently voted green. In the end we went with a simple scoring method as described in our methodology, with a base number of three votes required.    Scraping:  the sheer number of terms involved in our analysis - more than 400 in all - applied to a decade of MPs’ interests was a challenge which required a lot patience and various iterations to overcome.    Mobile design:  One of the big challenges of the project was creating a design that worked on wide desktop screens and tiny mobile phones. Given the sensitive nature of the data it was essential to frame it correctly, and ensure that readers coming to it on a mobile phone did not miss out on context. We achieved this by adding mobile functionality that made the scorecards expandable. Mobile readers could therefore explore the data visualisation, while still having access to the full information on each MP, rather than a truncated mobile version.  </ul>  We overcame those challenges and the resulting piece was the most comprehensive exercise to date on the climate records of the UK’s elected representatives.  "," The main lessons which others can take away are:   <ul>   Collaboration is key:  the project could simply not have been done were it not for the combined skills of individuals in various departments as well as collaboration with the guidance of external experts.     Code for good:  this was a data-led project which would not have been possible were it not for the use of code to identify the key material that was subsequently checked and summarised by the journalists involved.     Reuse, recycle:  we invested a lot of time on the code that went into this project. However, the code used in this project has since been applied to other projects and will continue to be useful in future.    Enable your readers:  it is not always the case but, in the run up to the election, we felt that giving readers the utility to check the records of MPs and their parties was important. The beeswarm histogram linked to MPs' scorecards provided great functionally, working as it did across platforms and devices, and giving the reader an overview of the parties' scores as well as individuals' scores.   </ul>    ",https://www.theguardian.com/environment/ng-interactive/2019/oct/11/guardian-climate-score-how-did-your-mp-do,https://www.theguardian.com/environment/2019/oct/11/tory-mps-five-times-more-likely-to-vote-against-climate-action,https://www.theguardian.com/environment/2019/oct/11/mps-and-the-oil-industry-who-gave-what-to-whom,https://www.theguardian.com/environment/2019/oct/11/guardian-mp-climate-score-methodology-parliament-votes,https://www.theguardian.com/environment/2019/oct/11/holding-mps-to-account-climate-records,,,"Pamela Duncan, Antonio Voce, Jonathan Watts, Frank Hulley-Jones and Lydia Smears"," Pamela Duncan, Data Journalist:   Lydia Smears, Visual Projects Editor:   Antonio Voce, Interactive Journalist:   Jon Watts, Global Environment Editor:   Frank Hully-Jones, Designer: ",,,
Russia,RUGRAD.EU,Small,Participant,Best data-driven reporting (small and large newsrooms),PAY FOR THE EXCREMENT,29/08/19,"Investigation,Illustration,Infographics,Map,Corruption",OpenStreetMap," This is an environmental anti-corruption investigation into how recently built sewage treatment plants in the Kaliningrad region (Russia) do not clean sewage and pollute the Baltic sea. We checked the operation of all treatment facilities in the Kaliningrad region - we made laboratory analyses of the effluents that they release into reservoirs (rivers, bays and the Baltic sea), according to 8 main indicators. Some treatment plants violate environmental regulations,  while others were poorly constructed - they do not sufficiently clean the effluents. This can be regarded as corruption in construction     "," We have been conducting an investigation for about 3 years, all this time we have been encouraging the authorities to respond to environmental pollution. We monitored how the authorities respond to environmental violations - many treatment facilities were upgraded and the quality of treatment was improved. After our investigation, the Prosecutor's office issued demands to improve the work of treatment facilities, some were closed for revision. We have also demonstrated that the existing environmental control over the operation of treatment facilities is not always effective - public control is required.   Every resident of the Kaliningrad region pays for the treatment of sewage sewage treatment plants. We have shown that in some cities people are being unreasonably charged money because treatment facilities are not working. "," In the investigation, we used laboratory examinations and data processing. We evaluated the quality of cleaning on 8 indicators and for three years tracked how these indicators change - for better or worse.   We also had to carry out a lot of field work, we independently searched for points where treatment facilities dump waste into reservoirs. We were afraid for the safety of our investigation and did it in secret - we could not ask the authorities for coordinates and searched for them on our own, so as not to attract attention. Skills of urban orientation, working with maps and satellite images, working with sources, and the local population were useful here. These places were often in hard-to-reach places, we had to go through impenetrable forests, drown in swamps - we needed Hiking skills. "," The most difficult task was to find places where sewage treatment plants discharge waste water. Confirm that these are the places we are looking for. To get to them unnoticed by the personnel of treatment facilities and security. At the same time, keep your visit and the subject of the investigation a secret. The jury should know that we have been monitoring the operation of treatment facilities for three years, crawling into hard-to-reach places, drowning in swamps, hiding from security to take tests.    ", How to make an investigation with the help of examinations and laboratory tests that can confirm the quality of certain processes    ,https://rugrad.eu/projects/dirty-water-2019/,https://docs.google.com/document/d/1uvZjjQhvcrcxYyGDUljjCN7BPdvX2aBj6mYIjYx0BkE/edit?usp=sharing,,,,,,"Roman Romanovskii, Mariya Pustovaya"," Roman Romanovskii, investigative journalist, analyst Transparency International-Russia     ",,,
United States,Quartz,Big,Participant,Best data-driven reporting (small and large newsrooms),How music is changing in the streaming age,17/01/19,"Explainer,Infographics,Chart,Arts,Lifestyle,Culture","Scraping,Adobe,R,RStudio","In the 1990s, the global music industry was riding high on the back of huge CD sales. Online piracy and technical innovation led to a tumultuous period for music that ultimately led to the current streaming age. My project uses data to examine how these changes have impacted the economics of the music industry, and how it's reflected in the artistic choices of musicians and producers. One of my key insights is that the payment structure of streaming led artists to make shorter songs. This is a scoop of analysis that started with reading an analysis by the data scientist"," Streaming now accounts for an estimated 80% of all music industry revenue. Prior to this reporting, how that shift is affecting music itself was little noticed or understood. Plus, many music consumers are unaware of how music streaming payment structures work. Through data analysis and compelling graphics, I was able to write popular articles that helped people understand why music may be evolving differently in the streaming age. Our project points out that artists receive payment for songs after 30 seconds of listening, and that an artist receives no more money for a 5-minute song than a 2-minute one. I also point out that streaming revenue is distributed using a ""pro-rata"" system rather than a ""user-centric"" one (see the article ""Your Spotify and Apple Music subscriptions pay artists you never listen to"" for a technical description of the distinction). This payment structure disproportionately harms classical and jazz artists.    Our work on how streaming is changing music has been widely covered. It was made into episodes of the podcast ""Switched on Pop"" and the Austrian radio show ""The Money."" It was also covered on NPR and the BBC. "," For this project I used the statistical programming language R, Adobe Illustrator, and the chart-making tool The Atlas.   I went to find data relevant to my hypotheses to analyze. I used R to collect data on the lengths of songs from the Billboard Hot 100 and the Spotify API. Once the data made emerging trends clear, I then took this data and used the R charting library “ggplot2” to make basic versions of the visualizations I would use to show how particular artist’s songs were getting shorter over their subsequent albums. I also used R’s data analysis library “dplyr” to get summary statistics of the length of songs in each year, and how that differed across genres. To collect data from the Spotify API, I used the library “SpotifyR.”   After making basic versions of the charts in ggplot2, I took those charts and cleaned and annotated them in Adobe Illustrator. I also made several different versions of the visuals in Adobe Illustrator so that the charts looked good on different screen sizes.   For simple line charts and bar charts, I used Quartz’s data visualization tool The Atlas. "," The most difficult part of this project was coming up with a compelling way to demonstrate how streaming economics actually impacts music listeners' lives. It was very simple to show that more revenue now comes from streaming and songs are getting shorter. A simple line chart or bar chart suffices to show the trend.   But to drive these changes home and ensure they really resonated with readers, I decided to look at particular artists and how their albums changed over time. This involved thinking creatively about how to show the composition of each album in terms of song length. I chose to use a horizontal stacked bar chart in which each section represented a song. This approach would not necessarily be intuitive for all readers, so I put a lot of thought into how to annotate and color the chart in ways that would help make the visualizations more approachable.   In addition, I made visuals for the last three albums of seven different artists, which took time and care. I felt that seeing this pattern for such a large number of artists would help bring the point home.    The result is what grew into an engaging project consisting of multiple stories, each one telling a different facet of the larger narrative about artists, music companies, consumers, and platforms, all grounded in and powered by data journalism. "," Data journalism can sometimes suffer from being impersonal. Typically, data analyses are looking at averages of large groups—such as people, countries, or, in this case, songs—and finding a trend. These trends often tell us something important about the world, but by their nature, they are abstractions. One way to deal with this problem is by interviewing people who are examples of the trend or who are impacted by it. I tried another tactic for this project, which I think others can learn from.   This project was successful because I started with a thesis and then took a broad finding, ""songs are getting shorter in the streaming age,"" and then honed in on specific examples. This helped readers connect to the reporting. By looking at the last three albums of popular musicians, for example, they could see how streaming changed the music of artists they love and the music they listen to and buy. I believe that this is a good lesson for data journalists. Humanizing the data through specific data examples can make all the difference. ",https://qz.com/1519823/is-spotify-making-songs-shorter/,https://qz.com/quartzy/1438412/the-reason-why-your-favorite-pop-songs-are-getting-shorter/,https://qz.com/1660465/the-way-spotify-and-apple-music-pays-artists-isnt-fair/,https://qz.com/1692498/taylor-swifts-new-album-lover-proves-she-can-still-sell-cds/,https://qz.com/quartzy/1655137/economics-explains-why-there-are-so-many-music-festivals/article,https://qz.com/1563588/4-songs-on-when-i-get-home-may-earn-solange-no-money-from-spotify/,https://qz.com/1683609/how-the-music-industry-shifted-from-napster-to-spotify/,Dan Kopf," Dan Kopf is a senior data reporter for Quartz based in Oakland. He covers economics, music and statistics. He acts as Quartz's chief evangelist for the plentiful and thoughtful use of charts and data.   Before becoming a journalist, Dan spent a decade working in economic and statistical research and received a Masters in Economics from the London School of Economics. Dan's first job in journalism was with data-focused new site Priceonomics where he wrote on economics, demographics, and pop culture. For his collaboration with The Pudding's Russell Goldenberg, he won the <a href=""https://www.informationisbeautifulawards.com/news/259-winners-2017"">2017 Information is Beautiful award</a> in the Arts, Entertainment & Pop Culture category.    In addition to his work at Quartz, Dan writes the newsletter ""<a href=""https://goldenstatswarrior.substack.com/"">Golden Stats Warrior</a>,"" in which he uses data to explore issues in the San Francisco Bay Area. ",,,
Germany,ZEIT ONLINE,Big,Participant,Open data,The New Colours of Europe,07/02/19,"Cross-border,Open data,Map,Elections","Scraping,QGIS,Canvas,Json,CSV,R,RStudio,OpenStreetMap,Python","While the European Union holds the second largest elections in the world, with more than 400 million eligible voters, there were no detailed maps and analyses so far, because there is no pan-EU data available. We changed In our article ""The New Colours of Europe,"" we were the first to visualise the votes across the whole Union on the most-detailed level possible, showing all votes in almost 80,000 administrative units, mostly municipalities. Therefore, we collected and harmonised the results of all 28 EU countries right after the election. We also opened the raw data later to researchers and the general"," We received requests for the data from Germany’s Federal Institute on Building, Urban Affairs and Spatial Development, which is working on a study where they intend to measure the influence of socio-demographic factors on election results and the European Commission’s Joint Research Centre, which used the data in a Science for Policy report called “Voting, attitudes towards immigration and Euroscepticism – A territorial perspective” which is due to be published on 4 February 2020 (Embargo). Our detailed data was used to analyse whether high shares of migrants in a given territorial unit may be associated with a higher vote for anti-immigration parties during elections for the European Parliament. So, we created a dataset that’s not only valuable for our users but also for research purposes.  "," To collect the data, when downloading entire datasets was impossible, we wrote custom scripts in Python and NodeJS that parsed and scraped the election data from the sites of national election authorities. When downloading the data was possible, the files made available by the national election authorities often required bits of python code to transform them into our standard format. The pandas library was indispensable for this work, since it allowed us to easily manipulate tabular content. Preparing the shapefile that would serve as the backbone of the visualisation was modified in QGIS (updating administrative divisions, manually creating constituencies in Slovenia, i.e.).   The main challenge we had while developing the interactive maps was that of handling the large amount of data. To ensure, that around 80,000 districts can be loaded and displayed fast on every device, we came up with a mixture of raster tiles (created with Mapnik) and vector tiles (created with Tippecanoe). Both layers were put together using MapboxGL.  "," The hardest part was getting the data. Election data may be published on the website of election authorities, the Interior Ministry, sometimes open data portals. This often meant datasets only available in the member state’s national language (there are 24 official languages in the European Union). If you manage to download a file, it’s an Excel file or CSV. But in the case of the Netherlands, for example, they were more complicated XML-type files.   Also, figuring out the best way to format the data so as to harmonise elections that happen according to 28 different rules, resulting in 28 different datasets. There is no harmonised EU format for data, done either by national or EU authorities.   The data changes based on the type of elections. Most countries vote on list systems, each with its own twist (e.g. Luxembourg has six votes per person), but some states or regions use Single Transferable Vote, where candidates are ranked. Votes abroad are sometimes counted in a category apart, sometimes they get allocated to the voter’s hometown if done by mail, and sometimes embassy votes get added to votes in the national capital, skewing the data there. Often times, data is not geo-referenced with the relevant municipality codes that would help attach the data to the map’s shapefile. We ran into problems due to spelling variations (like accents) or language variations when a municipality is in a bilingual area and it has the national name in one dataset, and the regional name in another.    "," The main purpose of this project was to see the landscape of the European vote as it manifests itself in the second largest elections in the world (after India), to see how cities, rural areas, the different regions and nations vote, and where differences and similarities lie.  We also hoped to represent, for the first time, all EU voters as members of a single political sphere, with the vote coloured according to the European Parliament Groups, and not national parties. This, together with the interactive map helps foster a conversation between voters, who now share a common vocabulary when talking about their choices, but also have the possibility of comparing the vote in their municipalities, even if they come from diametrically opposed places in the EU.   Until this project, there had been no harmonised EU data format done either by national or EU authorities, since it was national authorities that published the data for European elections. By making the data available (http://interactive.zeit.de/2019/data-eu-elections-2019/data-eu-elections-2019.zip), we hoped to encourage others – both in the media and the research community – to use this data, which means more in-depth conversations about the European vote. A first step in seeing ourselves represented more as a European Community.    ",https://www.zeit.de/politik/ausland/2019-07/european-election-municipalities-eu-states-results-analysis-map,Original in German: https://www.zeit.de/politik/ausland/2019-07/europawahl-gemeinden-eu-mitgliedsstaaten-ergebnisse-analyse,,,,,,"Paul Blickle, Christopher Moeller, Arnold Platon, Dr. Michael Schlieben, Julius Troeger, Sascha Venohr"," Paul Blickle (Information Designer at ZEIT ONLINE, looks for stories in data and finds the best way to tell them visually), Christopher Möller (Freelance Web Developer, working on interactive projects with different newsrooms), Arnold Platon (Architecture graduate from Romania who currently works in 3D infrastructure design in France. He does various side projects as a data-visualisation freelancer), Dr. Michael Schlieben (Political Correspondent at ZEIT ONLINE, interested in power structures and new movements in Europe), Julius Tröger (Head of Visual Journalism at ZEIT ONLINE, specialised in maps and data analysis) Sascha Venohr (Head of Data Journalism at ZEIT ONLINE, works as an investigative data journalist). ",,,
Portugal,Rádio Renascença,Big,Participant,Best data-driven reporting (small and large newsrooms),Freelance Doctors. A 104 million business that allows a doctor to work 100 hours weekly in Portugal,16/06/19,"Investigation,Open data,Podcast/radio,Business,Health,Economy,Employment","Scraping,Google Sheets,CSV,R,RStudio"," The expenses of the Portuguese Public Health Service with doctors working as independent workers won’t stop increasing. In 2018, Portugal spent 104 million euros in these type of contracts — the highest amount ever registered. By collecting and categorizing all the contracts made by public hospitals in Portugal, Rádio Renascença was able to understand how this business works, who is profiting from it and what are the consequences for the Public Health Service. Among 4.987 contracts, our data team was able to find a doctor who worked 100 hours weekly for a year. "," Through<a href=""https://rr.sapo.pt/especial/154924/"" style=""text-decoration:none;"">  data analysis </a> and<a href=""https://rr.sapo.pt/especial/154918/"" style=""text-decoration:none;"">  interviewing doctors </a> representatives and hospital administrators, we showed that there are doctors doing<a href=""https://rr.sapo.pt/especial/154915/"" style=""text-decoration:none;"">  excessive amounts of work </a> in some areas in which lack specialists. We found a case of an anesthetist who was working 100 hours weekly during one year, sleeping between surgeries and not resting enough. This doctor earned 50 euros per hour, amounting to 326.000 euros in one year, almost five times more than a senior doctor at the highest level of the medical career.   We also found there are no rules to enter this independent provider system, you just have to be a doctor. As such, there are unprepared doctors, right after college, assuming the urgency service without any supervision.   Another big conclusion we came to was that many hospitals<a href=""https://rr.sapo.pt/especial/154919/"" style=""text-decoration:none;"">  don't publish their contracts </a>. Portuguese law makes it mandatory that all public contracts should be available online, but there's a loophole in the law targeting specifically these cases. This opens up questions about transparency because many of these contracts are in the millions, with big recruitment agencies, making it impossible to get a proper overview of the issue and how to report it as a whole.   Our investigation contributed to the debate about the quality of the Portuguese Public Health Service and the work conditions of medical people, bringing an important debate to the public sphere and to other publications. The Health Minister was questioned in the parliament about this investigation and assumed there were<a href=""https://rr.sapo.pt/2019/06/19/pais/ministra-reage-a-noticia-renascenca-sobre-tarefeiros-e-anuncia-150-vagas-com-incentivos/noticia/155248/"" style=""text-decoration:none;"">  ""evident problems"" </a> in the Public Health Service.   This was the first journalistic investigation in Portugal using data analysis with a 100% transparent methodology: besides writing an article explaining how the data was collected and analyzed, Renascença published the code and all the data<a href=""https://gitlab.com/Renascenca/dados/tree/master/medicos-tarefeiros"" style=""text-decoration:none;"">  in a reproducible and open-source regime </a>. "," Since the public authorities refused to deliver the list of contracts that hospitals did with private companies to hire freelance doctors, we had to be creative.   Using<a href=""http://www.base.gov.pt/Base/pt/Homepage"" style=""text-decoration:none;"">  BASE </a>, the Portuguese portal where public institutions are obligated to publish all contracts they do with private companies, we discovered there was an non-public API behind the website that allowed us to request all the contracts by their fiscal number. Using R and this non-documented API, we found a way to request all the contracts the 67 public hospitals did between 2011 e 2019 - about 263.000 of them.   We elaborated a big list of words that could immediately disqualify a contract as pertinent to freelancer doctors - words like ""blankets"" ou ""screw"" usually mean those contracts were not for doctors. Because we were afraid this methodology could have false negatives, we decided to request all contracts made between these flagged companies and public institutions. We came to a new database of 28.007 contracts that we needed to classify. Since we were afraid of misclassified categories, we used keyword evidence to classify some cases, but then we went by all the 28 thousand contracts to make sure they were not false positives/negatives. For that, some cases required to read the PDF file of the contract.   This way, we were able to get a database of 4.987 contracts that we were sure were about freelance doctors.   Having all the data requested and cleaned, we used R to do some data analysis and find outliers that were the basis of stories published. "," The hardest part of the project was the lack of access to public data - in spite of having a portal for transparency, the Portuguese authorities don’t analyze it properly nor share it with journalists. Therefore, we had to collect them and then clean the database, which took long hours to do.   After all, this long task turned out to be useful, because if we had a clean data set to work with, we wouldn’t be able to find the story of the doctor who worked 100 hours per week, for example. "," Even though automatized some parts of the process, this project showed us why the data cleaning process can be really relevant to an investigation. It was during the manual classification process that our journalist was able to find the contract of the doctor who worked 100 hours weekly for one year, bare sleeping.   Because these contracts could refer to one single doctor or 100 doctors - the BASE website requires contracts to tell how much they are going to spend, for how long, and what they are buying, but not the number of people - the case would not be classified automatically as an outlier. But while doing the data cleaning our team had to check the PDF of the contract. That was when we realized it was regarding only one person.   This project also taught us that sometimes public authorities not releasing data makes journalists find the real stories. The investigation started because we didn't knew who were the biggest companies profiting from the lack of professionals in the National Public Health Service. ",https://rr.sapo.pt/especial/154922/,https://rr.sapo.pt/especial/154915/,https://rr.sapo.pt/especial/154918/,https://rr.sapo.pt/especial/154917/,https://rr.sapo.pt/especial/154919/,https://rr.sapo.pt/especial/154924/,https://gitlab.com/Renascenca/dados/tree/master/medicos-tarefeiros,"Rui Barros, Inês Rocha","  Rui Barros  is a data journalist at Rádio Renascença, one of the oldest radio stations in Portugal, since 2016. There, he uses code and databases to build interactive features and create new data-driven stories.    Inês Rocha,  28, is an award winning multimedia journalist at Renascença. She has done some investigations in different areas, from health to internet. With more dedication to video projects, she can tell stories in all the formats: video, audio, photography or written stories. ",,,
United States,Kaiser Health News,Big,Participant,Open data,Hidden Harm,03/07/19,"Investigation,Open data,Health","Microsoft Excel,CSV"," This story exposed a number of “exemptions” the Food and Drug Administration gave to makers of life-sustaining medical devices, allowing them to quietly file injury, death and malfunction reports to intra-agency databases. The public, including researchers and doctors, would expect to find those reports in a database called MAUDE. The reporting focused on tragic injuries from surgical stapler malfunctions and revealed that in a year when about 80 stapler injuries were disclosed publicly, the agency quietly collected 10,000 malfunction reports internally. The stories spurred the FDA to end its largest exemption program and open 5.7 million records to the public. "," The investigation immediately triggered an uproar in the medical device industry — and at the FDA. The day after Christina Jewett’s first story ran, the FDA acknowledged that it was aware of “many more” reports of stapler-related malfunction reports that were not filed openly in the MAUDE database and came out with a wide-ranging plan to improve the safety of the staplers. Citing KHN’s work, device-safety experts called on the FDA to open up the hidden reports of harm. That triggered a tweet by FDA Commissioner Scott Gottlieb — linking to KHN’s story — that said the reports would be opened: “We're now prioritizing making ALL of this data available.” In early May, the FDA announced it would be shuttering the “alternative summary reporting” program altogether, and, on June 21, the FDA published its entire hidden database online, revealing 5.7 million device-related injuries or malfunctions for the first time. In a single data disclosure, the FDA revealed nearly 40% of the total medical device harm reports from the past 20 years for devices used to shock the heart and open blocked vessels. "," Jewett used extensive search strategies to suss out happenstance mentions of the exemption program and “alternative summary” reports from the well-known public database of device injuries. This gave her a road map of devices to focus on. She used Freedom of Information Act requests to try to get details, but her efforts to expedite those requests were denied repeatedly. So she used the power of basic reporting and persistence to nudge the FDA press office for more detailed information about the overall amount and specific number of reports that were, in effect, hidden in its internal database. The strength of the facts and human stories helped spur the agency to open millions more records. ", The hardest part of this project was finding the wherewithal to persevere in the face of closed doors. Experts who spent their careers fixing medical devices and investigating breakdowns didn’t know about this data. A former FDA commissioner didn’t even know. Information about “exemptions” in the public MAUDE database was spotty and scant. The FOIA requests were in a long line of backlogs and expected to take 22 months to fill. Jewett doggedly negotiated with the press office to get a minimum amount of data to show the public that we were seeing the tip of a very important iceberg. ," If your reporter’s sense tells you there’s a story there, keep digging. Learn that “no” often means “maybe” if you’re persistent. In other words, even if the experts don’t know about something, even if the gatekeepers are offering little detail, there are ways to get information. For example, Jewett found an old trade-news article referencing the number of these hidden reports. She gave the press office an easy enough ”ask”: Please update these numbers. That was one way she got key information for the final article. ",https://khn.org/news/hidden-fda-database-medical-device-injuries-malfunctions/,https://khn.org/news/device-safety-experts-to-fda-make-data-public/,https://khn.org/news/fda-to-end-program-that-hid-millions-of-reports-on-faulty-medical-devices/,https://khn.org/news/hidden-reports-masked-the-scope-of-widespread-harm-from-faulty-heart-device/,https://khn.org/news/more-than-half-of-surgical-stapler-malfunctions-went-to-hidden-fda-database/,https://khn.org/news/fda-ends-months-long-foia-battle-over-medical-device-failures-says-putting-database-online-satisfies-khn-request/,,Christina Jewett," Christina Jewett, a senior correspondent with the Kaiser Health News enterprise team, has repeatedly spurred changes in the law, arrests and other government reforms in her 12 years as an investigative reporter. She won a Barlett and Steele gold award in 2019 for a series of stories about the FDA allowing medical device companies to file millions of device-injury reports out of the public realm. Previously, she spent seven years with the Center for Investigative Reporting, where she worked with a partner and CNN on a series that uncovered widespread graft in Medicaid-funded drug rehab centers, spurring the closure of scores of centers and 11 arrests. She and colleagues won a George Polk Award for medical reporting in 2011, writing about a hospital chain that billed for an outsized rate of rare and lucrative diseases. She previously worked at ProPublica and the Sacramento Bee. She is a graduate of Indiana University and a mother of two young boys. ",,,
United States,Investigative Reporting Workshop,Small,Participant,Open data,The Accountability Project,14/06/19,"Database,Open data,News application","Scraping,Json,Microsoft Excel,CSV,R,RStudio,PostgreSQL,Python"," The Accountability Project was created as a tool for searching across many, otherwise siloed databases from one place. TAP is an important tool at a time when newsroom resources are scarce and journalists must work more quickly. Our team, part of The Investigative Reporting Workshop, standardizes and curates public data from federal, state and local governments. Formally launched in June 2019, team TAP continues to add data and features. The collection currently provides free access to more than 550 million records from more than 320 databases. "," Since launch, we have worked to facilitate stories and research that hold companies, government agencies and people in power accountable. We are working directly with newsrooms, including two nonprofit news organizations and a major metro newspaper,  to help them draw from TAP for stories on everything from dark money to insider dealing at nonprofits to tracking money from pharmaceutical companies.    Long-time data journalist the late David Donald conceived of the original idea for TAP,  seeing it as necessary for accountability journalism. “The key is the link among databases that provide the connections that allow us to hold the powerful accountable for their decisions and actions,” he wrote in his original proposal.   In building the site, we’ve developed processes for dealing with many large data sets. We developed a system of data checks and standardizations.  We’ve trained young journalists and students at American University and other schools in those methods, which they can apply in their careers.   We currently have nearly 3,000 unique users and are in the process of collecting user feedback so that we can continue to make the site even more useful to professional journalists and researchers as we continue to add data.    Note regarding mobile/desktop: TAP was designed to work on mobile and desktop. Currently, most users are accessing the site via desktop.      "," This is a full-stack web application with a number of moving parts, so we used a combination of open-source technologies to build it – more on that below.   The basic idea is simple: Acquire datasets by FOIA or download, perform minimal data cleaning and field restructuring, upload to a “data lake,” map which fields need to be text searchable, and index into full-text search. We then verify that search results are accurate and publish the data.   Another choice we’ve made is to use “human intelligence” to process and map the data sets. We tested machine classification techniques but have found highly trained humans to be more reliable at this task. At least two people review the data before it goes live.   The data is split into names and addresses, which are searched in ElasticSearch, and what we’re calling transactions, stored in PostgreSQL, which are linked to an entity in ElasticSearch.  A transaction might be a campaign expenditure or a voter registration, for example.   Addresses are a key part of the data. They are standardized with open-source implementations of U.S. Postal Service guidelines. We plan to build on this by geocoding addresses this year and enabling geographic searches. We currently include only U.S. data at the national, state and local level, but we see opportunities to apply this to data from other countries. We're excited about connecting with journalists in other countries who are interested in similar tools.   Our open-source technologies include: Django for the backend and static pages, svelte.js for client-side search and the administrative tool for mapping the data, PostgreSQL to store the “data lake” and ElasticSearch for full text search of entities. More details about the technology are in this <a href=""https://source.opennews.org/articles/were-building-new-central-resource-public-data/"" style=""text-decoration:none;""> article for Source </a>, a community hub for data journalists and news developers. "," The biggest technical challenge has been scaling and managing a corpus of data of more than half a billion records and growing our site and user base on a relatively modest budget.    But nearly every stage of this project offered unique challenges. At the onset, we needed to develop a shared vision for The Accountability Project among our team, mapping out an ambitious but manageable path for what we could realistically build with our resources and timeline. This involved identifying parts of our original plan that weren’t reasonable, managing our own expectations, and creating a clear plan for the specific datasets we wanted to make available and searchable in our database.   Many of the databases we’ve included in The Accountability Project required filing open records requests, scraping web sites or extracting data from documents. Reviewing hundreds of data sets with a scrappy crew of fellows and data journalists has meant a lot of hard work to gather, check, and add the data to the site. To help keep things organized and on track, we keep a log of every database from request to upload, including the name of the person responsible for it. We’re software agnostic, as long as users can create a reproducible workflow or script.   Our main challenge going forward is focusing on our audience strategy. We’re thinking of new opportunities and partnerships to expose more journalists to The Accountability Project and establish it as a go-to tool for reporters. "," The Accountability Project was built to help journalists find stories they can’t find any other way. Users will be able to discover relationships between people, addresses and companies, political groups, nonprofits and other entities that they wouldn’t have noticed otherwise because they weren’t previously searchable from one place.   Our experience also can serve to help others interested in taking on similar projects.   Our advice:       Research what else is available. What does your project do that is unique? Find people and organizations that you can partner with instead of recreating the wheel.       Understand and communicate your shared vision from the outset.       Plan for the long, long term. If you want your project to live for a long time, make sure you have a sustainability plan to keep it going. It likely will take longer than you anticipate to launch. We’ve watched plenty of well-intentioned search projects die a slow death from lack of funding because it’s hard to find long-term support. We’re working to lower our operating costs, but also taking steps to make sure the data we’ve gathered helps others.       Have fun. Projects like this are an incredible amount of work. If you can’t have some enjoyment doing them, your product will not be as effective.     ",https://www.publicaccountability.org/,https://investigativereportingworkshop.org/2019/07/10/introducing-the-accountability-project-a-new-resource-for-public-data/,https://source.opennews.org/articles/were-building-new-central-resource-public-data/,https://mailchi.mp/ejc.net/how-to-follow-the-money-conversations-with-data-issue-38?e=bbb646eb79,,,,"Jacob Fenton, Jennifer LaFleur, Cole Goins, Megan Gayle Crigger, Kiernan Nicholls, Dariya Tsyrenzhapova, Yanqi Xu, Charles Lewis, Lynne Perri","  Jacob Fenton  is the lead developer for IRW's<a href=""https://publicaccountability.org/"" style=""text-decoration:none;"">  Public Accountability Project </a>. He previously worked as editorial engineer at The Sunlight Foundation, as director of computer-assisted reporting at IRW and as a reporter and editor for newspapers in Pennsylvania and California. He was a 2015 Stanford JSK fellow, where he worked on structured data extraction and machine learning.    Jennifer LaFleur  is IRW's data editor and manages TAP. She previously was a senior editor at Reveal from The Center for Investigative Reporting, where she managed a team of data journalists, investigative reporters and fellows. She is the former data journalism director at ProPublica and has held similar roles at newspapers.     Cole Goins  is a journalist, facilitator and media consultant dedicated to making journalism more inclusive, collaborative and community-centered. He is engagement lead for The New School's Journalism + Design program. He previously served as director of community engagement at Reveal from the Center for Investigative Reporting and engagement editor at the Center for Public Integrity.    Megan Gayle Crigge r is an award-winning art director, designer, and front-end developer. She has worked in media for over a decade and has been creating websites since the ‘90s. She enjoys creating interactives that make the world a better place and raise awareness about important issues.     Kiernan Nicholls  is a recent graduate of American University's School of Public Affairs, where he completed Bachelor's and Master's degrees in political science. Kiernan is developing open-source data wrangling tools and reproducible practices for IRW's Accountability Project.    Dariya Tsyrenzhapova  is a former IRW data fellow where she worked on IRW's Accountability Project. She currently is in the Ph.D. program at the University of Texas. She holds both a master's and an undergraduate degree from the Missouri School of Journalism, where she studied data journalism and convergence reporting.    Yanqi Xu  is a data fellow at IRW focusing on the Accountability Project. She recently graduated from the University of Missouri, where she studied data journalism and investigative reporting. Previously, she was a digital reporting fellow at PolitiFact and worked as a research assistant for the NICAR data library.    Charles Lewis  is IRW's Executive Editor and a professor at American University. He, along with the late David Donald, conceived the idea for TAP.    Lynne Perri  is IRW's managing editor and is a journalist in residence at American University. ",,,
Mexico,El universal,Big,Co-winner,Innovation (small and large newsrooms),Zones of Silence,13/06/19,"Investigation,Long-form,Multiple-newsroom collaboration,Database,Infographics,Crime,Gun violence,Human rights","3D modelling,AI/Machine learning,CSV"," Violent organized crime is one of the biggest crises facing Mexico. Journalist avoid becoming a target, so they choose to stay quiet to save their lives. We set out to measure this silence and its impact on journalism. To do so, we used artificial intelligence to quantify and visualize news coverage and analyze the gaps in coverage across the country. To measure the degree of silence in each region of the country, we created a formula that allows us to see the evolution of this phenomenon over time. "," Something akin to a code of silence has emerged across the country. We suspected that there were entire regions where journalists were not reporting on the violence, threats, intimidation and murder that were well known to be part of daily life. This was confirmed by journalists who sought for us after the story was released, to tell us they have been facing this problems. In collaboration with them, now we are preparing a second part of this story, to focus on the patterns that lead to agressions. Hopefuly this will lead us to some kind of alert when certain couditions (of news coverage and crimen) are present in regions of our country. "," Our first step was to establish a process to determine the absence of news. We explored articles on violence to understand how they compare to the government's official registry of homicides.   In theory, each murder that occurs ought to correspond with at least one local report about the event. If we saw a divergence, or if the government's reports were suddenly very different from local news coverage, we could deduce that journalists were being silenced.   Early on, sorting through news articles seemed impossible. We knew we needed to find a news archive with the largest number of publications in Mexico possible so we could track daily coverage across the country. Google News’ vast collection of local and national news stories across Mexico was a good fit.   The effort required us to identify the difference between the number of homicides officially recorded and the news stories of those killings on Google News. This required machine learning algorithms that were able to identify the first reported story and then pinpoint where the event took place. With that information, we were able to connect reported events by media with the government's reports on homicides across more than 2400 municipalities in Mexico.   Finally, to measure the degree of silence in each region of the country, we created a formula that allows us to see the evolution of this phenomenon over time. The <a href=""http://zonas-de-silencio.eluniversal.com.mx/"" target=""_blank"">resulting data</a> shows a fascinating mix of falls or peaks in unreported deaths, which coincide with events such as the arrival of new governments or the deaths of drug dealers. Further investigation will allow us to explain these connections. "," The hardest part was creating the ""formula for silece"" to measure the degree of non reported homicides along the country. There are many variables behind the reason why there aren't as much articules as homicides in each region. So, in order to be sure the discrepancy was linked to violence and killings we had to rule out or include segments of data along the way. This was extremely hard to do with machine learning, because words in spanish that are usually used to represent this kind of coverage, are also synomyms for other things. We had to validate (manually) a lot of the inicial reports until we had a well validated sample of results. This took us half a year. Then we felt lost due to the amout of variables we had in our hands (disparity between events reported and published stories; matching stories reporting one singe event by different websites; the uncertanty of internet penetration in all parts of the country and its evolution over time within the 14 years we analyzed...). Luckly, the interdisciplinary nature of our team (with economyst, programmers, data experts, designers and journalists) helped us to find an answer that we felt was truly accurate.  "," No matter how hard it is to measure a problem, there is always a way to do it, even if its not what you thought you would find in the beggining, ",https://zonas-de-silencio.eluniversal.com.mx/,https://www.eluniversal.com.mx/nacion/sociedad/en-10-estados-guardan-silencio-sobre-homicidios,https://www.eluniversal.com.mx/nacion/sociedad/la-prensa-que-ya-no-habla-de-muertos,https://zonas-de-silencio.eluniversal.com.mx/metodologia.html,,,,"Esteban Román, Gilberto Leon, Elsa Hernandez, Miguel Garnica, Edson Arroyo, César Saavedra, Jenny Lee, Dale Markowitz, Alberto Cairo","<pre> Esteban Román is a journalist with twelve years of experience in television, print and digital media. He is a two times Emmy Award winner for his work as an investigative journalist. He has worked with The Wall Street Journal, ABC News and Univision. Román is currently Deputy editor for El Universal newspaper.</pre> Alberto Cairo is a journalist and designer, and the Knight Chair in Visual Journalism at the School of Communication of the University of Miami (UM). He is also the director of the visualization program at UM’s Center for Computational Science. He has been head of information graphics at media publications in Spain and Brazil.  ",,,
Portugal,Rádio Renascença,Big,Participant,Best news application,Who are the people that are running for MP in Portugal?,10/02/19,"Database,Open data,News application,Elections,Politics","Json,Google Sheets,R,RStudio"," On the 4th of October 2019, the Portuguese citizens went the polls to elect the 230 deputies that would represent them in the Parliament for the next 4 years term. 4.603 candidates were running for those seats, but very few citizens knew who they were.   Using public data, interviews and some crowdsourcing directly from the political parties, we were able to gather a complete picture of the 4.603 candidates, thus enabling the electorate to get to know who they were voting for, using filters such as constituency, gender, work experience or if they have children.     "," One month before the 4th of October election, all parties were already campaigning on the streets trying to convince everyone that they deserved their votes, but   <a href=""https://rr.sapo.pt/2019/09/18/legislativas-2019/administracao-interna-ainda-nao-publicou-listas-de-candidatos-e-uma-violacao-da-constituicao/noticia/164949/"" style=""text-decoration:none;""> it was impossible to know who was running </a>.   In an electoral system where people elect 230 deputies distributed among the country', twenty-two constituencies, all the 21 parties running had to present 230 names. But right up to mid-September, everybody was dependent on the parties' propaganda machine to know who were those names.   Aware that many people didn't even knew the name or the face of the 21 parties number one candidate running for their constituency, we built a news app, with faceted search capabilities, that allowed citizens to filter for constituencies, parties, name, gender, if it had already been elected and more personal information such as age, marital status, education or work experience.   The tool also allowed readers to get to know details such as their social media profiles, among other particularities.   All the data, collected from many sources, including a questionnaire sent to the parties. Even though it wasn't possible to get all the data for all the candidates, this was one of the most visited and shared pieces we did for the elections. "," This was the type of project that required technical flexibility. Since we used Google Forms to ask for the parties to give us the details about the MP candidates, we used Google Sheets and a Node.js server, so everyone working on the project could collaborate in the editing of the database and<a href=""https://frozen-dusk-79251.herokuapp.com/api?id=1i6etBqV2vGXIqQ6Kn9LY4PAch6qrpmBpxkRY9lf6aQA"" style=""text-decoration:none;"">  serve an API </a> with all the data we were able to fetch about that person.   We used R for some initial data cleaning. Because we wanted to get the most out of the people answering the questions, we created some open fields that required some cleaning. For example, we didn’t want the people to feel their profession didn’t fit on any of those categories, so we left it as an open field. We used R to unify those fields.   Using Vue.js we then created the news app that allowed our readers to easily filter the database. We also created a system that enabled direct links to any filtered state, using URL hash updates that preserve filter state and link to filtered views and to specific candidates - all client-side and static. This way, people were able to share with friends a specific filter (ex:<a href=""https://rr.sapo.pt/candidatos2019/?partido=all&genero=all&filtro_nome=&circulo=all&candidato_id=&ord_circulo=all&prev_deputado=all&idade=all&ec=all&f=all&ne=all&prof=Empres%C3%A1rio%2Fa&eleito=all"" style=""text-decoration:none;"">  all candidates that are businessman </a>) or even <a href=""https://rr.sapo.pt/candidatos2019/?partido=all&genero=all&filtro_nome=Ant%C3%B3nio+Costa&circulo=all&candidato_id=573&ord_circulo=all&prev_deputado=all&idade=all&ec=all&f=all&ne=all&prof=all&eleito=all"" style=""text-decoration:none;""> a specific card about one person </a>), that transformed one website to over tens of thousands of possible personalized social objects to be shared. "," Getting information about 4.603 candidates was not an easy task. Because the government didn’t want to release the names of all candidates, the job was a race against time. We started a collaborative effort among everyone in the newsroom to get information from the candidates that reporters knew, but also to fact-check the information provided by the survey respondents.   But the fact that we had information on 4.603 people paid off. On the elections night, we were able to produce another article, easily characterizing statistically the Portuguese parliament — average age, gender balance, and even most common names. "," The mission to collect personal and professional data for 4.603 candidates sounded like an impossible mission. But the fact that the data team ended up using Google sheets, a familiar and easy to use tool for everyone in the newsroom made it possible to effortless crowdsource the data. ",https://rr.sapo.pt/candidatos2019/?partido=all&genero=all&filtro_nome=&circulo=all&candidato_id=&ord_circulo=all&prev_deputado=all&idade=all&ec=all&f=all&ne=all&prof=all&eleito=all,https://rr.sapo.pt/2019/10/07/legislativas-2019/um-parlamento-quarentao-ainda-masculino-e-com-40-de-caras-novas/noticia/167354/,,,,,,"Rui Barros, João Antunes, Diogo Rodrigues, Gonçalo Filipe Lopes, Eunice Lourenço, Manuela Pires, Rodrigo Machado, Maria João Cunha"," This project was part of newsroom effort between data, online and politic's desks at Rádio Renascença. ",,,
United Kingdom,Nature,Big,Participant,Innovation (small and large newsrooms),Nature's co-citation network,11/06/19,"Database,Open data,Infographics,Video","Animation,3D modelling,D3.js,Three.js,Python"," As part of its 150th anniversary, Nature, the world's leading science journal, collaborated with network scientists led by Albert Laszlo Barabasi at Northeastern University in Boston. The scientists carried out a novel analysis using data on tens of millions of scientific articles. The resulting network created a powerful representation of the history of science and revealed how disciplines have arisen and become more connected over time. Nature published this analysis as a free <a href=""https://www.nature.com/collections/hddgebfbee"">package of content</a>: our 150<sup>th</sup> anniversary issue cover, a video, opinion article (explaining the analysis in depth) and a data <a href=""https://www.nature.com/immersive/d41586-019-03165-4/index.html"">interactive</a>, which inspired media stories worldwide. "," Nature press released the package as part of its 150<sup>th</sup> anniversary.  It was covered in 25 news stories around the world, including in Times Higher Education and La Republica, and another 200 stories in China including Xinhua and the People's Daily in China (a list of key outlets is below).   Key news stories  <ul>  Times Higher Education - <a href=""https://www.timeshighereducation.com/news/growth-interdisciplinarity-mapped-mark-150-years-nature"">Growth in interdisciplinarity mapped to mark 150 years of Nature</a>   Fast Company - <a href=""https://www.fastcompany.com/90427630/this-mesmerizing-image-charts-millions-of-scientific-studies-over-150-years"">This mesmerizing 3D map visualizes millions of scientific studies</a>   La Repubblica - <a href=""https://www.repubblica.it/tecnologia/2019/11/06/news/150_anni_di_nature_numero_speciale_firmato_da_un_designer_italiano_cerco_la_bellezza_nei_dati_-240422127/"">150 anni di Nature, il numero speciale del designer italiano. ""Cerco la bellezza nei dati""</a>   Il Sole 24 Ore - <a href=""https://www.infodata.ilsole24ore.com/2019/11/08/nature-lo-studio-della-scienza-raccontato-con-150-anni-di-articoli/?refresh_ce=1"">Nature, lo studio della scienza raccontato con 150 anni di articoli</a>   ANSA - <a href=""http://www.ansa.it/canale_scienza_tecnica/notizie/ricerca_istituzioni/2019/11/04/buon-compleanno-nature-da-150-anni-voce-della-scienza_d9ad7b86-472d-4015-b04b-a65aa0c1c391.html"">Buon compleanno Nature, da 150 anni voce della scienza </a>   Xinhua - <a href=""http://www.xinhuanet.com/science/2019-11/08/c_138537850.htm"">http://www.xinhuanet.com/science/2019-11/08/c_138537850.htm</a>   Folha de São Paulo - <a href=""https://www1.folha.uol.com.br/ciencia/2019/11/nos-150-anos-da-revista-nature-ciencia-ficou-mais-colaborativa-e-feminina.shtml"">Nos 150 anos da revista Nature, ciência ficou mais colaborativa e feminina</a>   Wired.it - <a href=""https://www.wired.it/scienza/lab/2019/11/07/compleanno-nature-paper-storia/"">https://www.wired.it/scienza/lab/2019/11/07/compleanno-nature-paper-storia/</a> (focuses on the N&V collection)   Corriere del Ticino - <a href=""https://www.cdt.ch/curiosita/nature-spegne-150-candeline-BA1983740"">https://www.cdt.ch/curiosita/nature-spegne-150-candeline-BA1983740</a>   People's Daily - <a href=""http://zj.people.com.cn/n2/2019/1108/c187005-33518781.html"">http://zj.people.com.cn/n2/2019/1108/c187005-33518781.html</a>  </ul>  The package of stories also received excellent traffic (an average news story for Nature might get 10-15,000 page views, for comparison).   Video <a href=""https://www.youtube.com/watch?v=GW4s58u8PZo"">: A network of science: 150 years of Nature papers</a>  89,000 unique page views   Interactive:  <a href=""https://www.nature.com/immersive/d41586-019-03165-4/index.html"">On the shoulders of giants</a>  43,000 UPVs   Comment:  <a href=""https://www.nature.com/articles/d41586-019-03308-7""> Nature 's reach: narrow work has broad impact</a>  13,500 UPVs   We received excellent feedback and engagement on social media, including these below:   <a href=""https://twitter.com/SciNaturaBree/status/1192533006591901697"">https://twitter.com/SciNaturaBree/status/1192533006591901697</a>   <a href=""https://twitter.com/BMatB/status/1192339017423114241"">https://twitter.com/BMatB/status/1192339017423114241</a>   <a href=""https://twitter.com/ftmaestre/status/1192343427280756738"">https://twitter.com/ftmaestre/status/1192343427280756738</a>   <a href=""https://twitter.com/csugimoto/status/1192436947207303168"">https://twitter.com/csugimoto/status/1192436947207303168</a> "," This visualization project had five main components, each necessitating a different set of tools and techniques.  Ultimately, we used 4 visualization tools combined with 6 custom pieces of software.   1)     Data processing : we wrote custom python code to combine and process publication data from two different data sources (Web of Science and Nature’s database) and form the labeled co-citation network.   2)     Network Layout : we used the open source network software, Gephi, to generate a 2D network layout based on a manually controlled annealing of the force-directed layout algorithm.  We then wrote custom python code to perform edge-bundling in 3D.   3)     3D Interactive Website : we wrote custom JavaScript code based on the three.js package to draw network nodes and edges as a large interactive particle system.  We also created custom device orientation camera controls (VR mode) to allow navigation via touch screen and device sensors simultaneously by using different positional and rotational information from two mock cameras to control the view.   4)     3D Rendered Static Images : we wrote custom python code to generate the initial 3D geometry and then modified the geometry procedurally in Maya to produce the final renderings. To expedite the rendering process, we employed cloud-based rendering through Zync and the Google Cloud.   5)     3D Animated Video : we wrote custom Java code with OpenGL library to render the 3D network, temporal animation, and provide detailed camera controls.   While each of these 5 components required their own unique toolsets, the creative process at each stage greatly benefited from all components.  For example, the 3D edge bundling required setting several parameters which were selected based on the visual appeal of the 3D interactive website.  Similarly, the 3D interactive website allowed us to create the storyboard for the Animated Video and select interesting camera locations for the Rendered Static Images. "," In answering the question “What does 150 years of scientific publications look like?”, this project encountered two primary challenges.  First, the data covered 150 years of technical innovation and research that represented so many more years of scientist's lives spent on the work, so we wanted to present it without reducing the magnitude of the achievements or the record.  This meant our visualization needed to both capture the scale and complexity of Nature’s publication landscape, while also providing a poetic visual storytelling capable of communicating simplified messages for the viewer.  Given Nature’s prominent role as a scientific publication venue, this task was further complicated by the need to maintain the scientific validity of our project.  Ultimately, the design emerged from many iterations of prototyping and input from a highly interdisciplinary team of designers, scientists, and editors.  Second, the magnitude of the data presented its own computational challenges: the weighted network was created by processing citation data from over 20 million publications, and we built custom visualization solutions to efficiently draw the entire network of 88,000 nodes and 239,000 edges as an interactive object.  We also took extra steps to ensure compatibility across multiple devices and browsers, as well as implementing additional accessibility features.  The computational challenges for this visualization process was pushed to the extreme when creating the high-resolution video with all the effects of lights and materials on such a large number of components, and required highly parallelized code running on multiple remote servers with multiple GPU.  "," The project was exemplary in showing how collaboration between academic data scientists and a team of journalists, editors, video editors and art editors can generate compelling content and images, as well as creating a resource which scientists and the public can use from now on to learn about science and its history.   The freely available interactive allows anyone to choose any paper from Nature's archive – including seminal papers such as Watson and Crick's discovery of the structure of DNA - and view its colourful ‘reference tree': all the papers that it referenced (a representation of the academic work on which it was built) and all the academic papers that went on to cite it (a representation of all the academic work that paper inspired).  This reveals in a compelling and original way how every discovery is built ‘on the shoulders of giants'.   As one researcher wrote to us on Twitter: ""This video is beautiful and gives one of the best introductions to how academic science works that I have seen in some time - ""the shoulder's of giants"" story revealed as an interconnected maelstrom of ideas - thank you""  <a href=""https://twitter.com/nature"">@nature</a> for telling this story. ",https://www.nature.com/immersive/d41586-019-03165-4/index.html,https://youtu.be/GW4s58u8PZo,https://www.nature.com/immersive/d42859-019-00121-0/index.html,https://www.nature.com/immersive/d41586-019-03165-4/reftree-home.html,,,,"Kelly Krause, Wesley Fernandes, Noah Baker, Alice Grishchenko, Mauro Martino, Albert-László Barabási, Alexander Gates, Qing Ke, Onur Varol"," Nature is the world’s leading international journal, publishing the finest peer-reviewed research in all fields of science and technology, alongside authoritative, thought-leading news and opinion content about research. The Nature team involved in this project was led by creative director Kelly Krause and included editors, journalists, art editors and multimedia editors. ",,,
United Kingdom,Nature,Big,Participant,Open data,Nature's co-citation network,11/06/19,"Database,Open data,Infographics,Video","Animation,3D modelling,D3.js,Three.js,Python"," As part of its 150th anniversary, Nature, the world's leading science journal, collaborated with network scientists led by Albert Laszlo Barabasi at Northeastern University in Boston. The scientists carried out a novel analysis using data on tens of millions of scientific articles. The resulting network created a powerful representation of the history of science and revealed how disciplines have arisen and become more connected over time. Nature published this analysis as a free <a href=""https://www.nature.com/collections/hddgebfbee"">package of content</a>: our 150<sup>th</sup> anniversary issue cover, a video, opinion article (explaining the analysis in depth) and a data <a href=""https://www.nature.com/immersive/d41586-019-03165-4/index.html"">interactive</a>, which inspired media stories worldwide. "," Nature press released the package as part of its 150<sup>th</sup> anniversary.  It was covered in 25 news stories around the world, including in Times Higher Education and La Republica, and another 200 stories in China including Xinhua and the People's Daily in China (a list of key outlets is below).   Key news stories  <ul>  Times Higher Education - <a href=""https://www.timeshighereducation.com/news/growth-interdisciplinarity-mapped-mark-150-years-nature"">Growth in interdisciplinarity mapped to mark 150 years of Nature</a>   Fast Company - <a href=""https://www.fastcompany.com/90427630/this-mesmerizing-image-charts-millions-of-scientific-studies-over-150-years"">This mesmerizing 3D map visualizes millions of scientific studies</a>   La Repubblica - <a href=""https://www.repubblica.it/tecnologia/2019/11/06/news/150_anni_di_nature_numero_speciale_firmato_da_un_designer_italiano_cerco_la_bellezza_nei_dati_-240422127/"">150 anni di Nature, il numero speciale del designer italiano. ""Cerco la bellezza nei dati""</a>   Il Sole 24 Ore - <a href=""https://www.infodata.ilsole24ore.com/2019/11/08/nature-lo-studio-della-scienza-raccontato-con-150-anni-di-articoli/?refresh_ce=1"">Nature, lo studio della scienza raccontato con 150 anni di articoli</a>   ANSA - <a href=""http://www.ansa.it/canale_scienza_tecnica/notizie/ricerca_istituzioni/2019/11/04/buon-compleanno-nature-da-150-anni-voce-della-scienza_d9ad7b86-472d-4015-b04b-a65aa0c1c391.html"">Buon compleanno Nature, da 150 anni voce della scienza </a>   Xinhua - <a href=""http://www.xinhuanet.com/science/2019-11/08/c_138537850.htm"">http://www.xinhuanet.com/science/2019-11/08/c_138537850.htm</a>   Folha de São Paulo - <a href=""https://www1.folha.uol.com.br/ciencia/2019/11/nos-150-anos-da-revista-nature-ciencia-ficou-mais-colaborativa-e-feminina.shtml"">Nos 150 anos da revista Nature, ciência ficou mais colaborativa e feminina</a>   Wired.it - <a href=""https://www.wired.it/scienza/lab/2019/11/07/compleanno-nature-paper-storia/"">https://www.wired.it/scienza/lab/2019/11/07/compleanno-nature-paper-storia/</a> (focuses on the N&V collection)   Corriere del Ticino - <a href=""https://www.cdt.ch/curiosita/nature-spegne-150-candeline-BA1983740"">https://www.cdt.ch/curiosita/nature-spegne-150-candeline-BA1983740</a>   People's Daily - <a href=""http://zj.people.com.cn/n2/2019/1108/c187005-33518781.html"">http://zj.people.com.cn/n2/2019/1108/c187005-33518781.html</a>  </ul>  The package of stories also received excellent traffic (an average news story for Nature might get 10-15,000 page views, for comparison).   Video <a href=""https://www.youtube.com/watch?v=GW4s58u8PZo"">: A network of science: 150 years of Nature papers</a>  89,000 unique page views   Interactive:  <a href=""https://www.nature.com/immersive/d41586-019-03165-4/index.html"">On the shoulders of giants</a>  43,000 UPVs   Comment:  <a href=""https://www.nature.com/articles/d41586-019-03308-7""> Nature 's reach: narrow work has broad impact</a>  13,500 UPVs   We received excellent feedback and engagement on social media, including these below:   <a href=""https://twitter.com/SciNaturaBree/status/1192533006591901697"">https://twitter.com/SciNaturaBree/status/1192533006591901697</a>   <a href=""https://twitter.com/BMatB/status/1192339017423114241"">https://twitter.com/BMatB/status/1192339017423114241</a>   <a href=""https://twitter.com/ftmaestre/status/1192343427280756738"">https://twitter.com/ftmaestre/status/1192343427280756738</a>   <a href=""https://twitter.com/csugimoto/status/1192436947207303168"">https://twitter.com/csugimoto/status/1192436947207303168</a> "," This visualization project had five main components, each necessitating a different set of tools and techniques.  Ultimately, we used 4 visualization tools combined with 6 custom pieces of software.   1)     Data processing : we wrote custom python code to combine and process publication data from two different data sources (Web of Science and Nature’s database) and form the labeled co-citation network.   2)     Network Layout : we used the open source network software, Gephi, to generate a 2D network layout based on a manually controlled annealing of the force-directed layout algorithm.  We then wrote custom python code to perform edge-bundling in 3D.   3)     3D Interactive Website : we wrote custom JavaScript code based on the three.js package to draw network nodes and edges as a large interactive particle system.  We also created custom device orientation camera controls (VR mode) to allow navigation via touch screen and device sensors simultaneously by using different positional and rotational information from two mock cameras to control the view.   4)     3D Rendered Static Images : we wrote custom python code to generate the initial 3D geometry and then modified the geometry procedurally in Maya to produce the final renderings. To expedite the rendering process, we employed cloud-based rendering through Zync and the Google Cloud.   5)     3D Animated Video : we wrote custom Java code with OpenGL library to render the 3D network, temporal animation, and provide detailed camera controls.   While each of these 5 components required their own unique toolsets, the creative process at each stage greatly benefited from all components.  For example, the 3D edge bundling required setting several parameters which were selected based on the visual appeal of the 3D interactive website.  Similarly, the 3D interactive website allowed us to create the storyboard for the Animated Video and select interesting camera locations for the Rendered Static Images. "," In answering the question “What does 150 years of scientific publications look like?”, this project encountered two primary challenges.  First, the data covered 150 years of technical innovation and research that represented so many more years of scientist's lives spent on the work, so we wanted to present it without reducing the magnitude of the achievements or the record.  This meant our visualization needed to both capture the scale and complexity of Nature’s publication landscape, while also providing a poetic visual storytelling capable of communicating simplified messages for the viewer.  Given Nature’s prominent role as a scientific publication venue, this task was further complicated by the need to maintain the scientific validity of our project.  Ultimately, the design emerged from many iterations of prototyping and input from a highly interdisciplinary team of designers, scientists, and editors.  Second, the magnitude of the data presented its own computational challenges: the weighted network was created by processing citation data from over 20 million publications, and we built custom visualization solutions to efficiently draw the entire network of 88,000 nodes and 239,000 edges as an interactive object.  We also took extra steps to ensure compatibility across multiple devices and browsers, as well as implementing additional accessibility features.  The computational challenges for this visualization process was pushed to the extreme when creating the high-resolution video with all the effects of lights and materials on such a large number of components, and required highly parallelized code running on multiple remote servers with multiple GPU.  "," The project was exemplary in showing how collaboration between academic data scientists and a team of journalists, editors, video editors and art editors can generate compelling content and images, as well as creating a resource which scientists and the public can use from now on to learn about science and its history.   The freely available interactive allows anyone to choose any paper from Nature's archive – including seminal papers such as Watson and Crick's discovery of the structure of DNA - and view its colourful ‘reference tree': all the papers that it referenced (a representation of the academic work on which it was built) and all the academic papers that went on to cite it (a representation of all the academic work that paper inspired).  This reveals in a compelling and original way how every discovery is built ‘on the shoulders of giants'.   As one researcher wrote to us on Twitter: ""This video is beautiful and gives one of the best introductions to how academic science works that I have seen in some time - ""the shoulder's of giants"" story revealed as an interconnected maelstrom of ideas - thank you""  <a href=""https://twitter.com/nature"">@nature</a> for telling this story. ",https://www.nature.com/immersive/d41586-019-03165-4/index.html,https://youtu.be/GW4s58u8PZo,https://www.nature.com/immersive/d42859-019-00121-0/index.html,https://www.nature.com/immersive/d41586-019-03165-4/reftree-home.html,,,,"Kelly Krause, Wesley Fernandes, Noah Baker, Alice Grishchenko, Mauro Martino, Albert-László Barabási, Alexander Gates, Qing Ke, Onur Varol"," Nature is the world’s leading international journal, publishing the finest peer-reviewed research in all fields of science and technology, alongside authoritative, thought-leading news and opinion content about research. The Nature team involved in this project was led by creative director Kelly Krause and included editors, journalists, art editors and multimedia editors. ",,,
Portugal,Rádio Renascença,Big,Shortlist,Best news application,Can I really afford to live in this place?,31/10/19,"Explainer,Long-form,News application,Illustration,Lifestyle,Economy,Employment","Animation,Scraping,Json,R,RStudio"," After a financial crisis at the beginning of the last decade, the Portuguese economy started to strive. The nice weather and low-cost flights made tourism bloom, but brought upon challenges with the gentrification of the old and biggest cities in the country. With rising prices on rents and meals, Portuguese people started to question: can I really live in this city?   With information from multiple data sources and the ability for the reader to provide personal context, we created an insightful news application that enabled people to find out if their budget crossed the minimum threshold for livelihood. "," When I moved out of my parent’s house, in 2012, I rented an apartment for 450€. This year, I saw the same place being rented out by 850€. In seven years, the rent for this flat almost doubled, even though the average national income in Portugal only increased 135€. On average, rent prices in Lisbon have increased by almost 40% in eight years. I realized that that wasn’t a single experience, almost everyone was complaining about rising prices in major cities - especially referring to housing prices.    This tool allowed readers to put themselves in context. Using input data from the user, we allied algorithmically generated text with storytelling techniques allowing people to compare themselves with the average prices in a city, with data gathered from prices of housing, salaries, water, gas, electricity, grocery, and schools.    Because much of this data was never aggregated in such a way, the work sparked a national debate around cost of living in the country. "," One of the main goals of this project was to create a 100% personalized reading experience. Every time I read an interactive feature that used input data it always felt like a ""fill in the gaps"" exercise. For this project, I wanted the reader to be tricked to believe that the article was written with his personal situation in mind and enabled him to gather actionable context about his life.   We ended up using Vue.js reactivity to be able to enable all the mathematical equations and to markup the text so that the proper blocks of text were displayed. This was all backed up by a very complex logic system so that no text in the article seemed like it was written by an algorithm.   On the data side of the story, I used R for all the web scraping from multiple websites that stored the necessary data. R was also very useful to create an API needed for the project (using the <a href=""https://www.rplumber.io/"" style=""text-decoration:none;""> plumber package </a>) and to generate the multiple JSON files that Vue.js used. "," This was the most challenging and complex data-driven project that I was involved in my career.  For example, I never considered that the Portuguese language was of such complexity. Because this concept is so rooted in my speaking habits, I never fully grasped that Portuguese has grammatical gender​. So, as I wanted the text to sound 100% natural, I had to develop ​an API​ to guess the gender from the reader’s name when possible.       Another challenge was making my logical side work together with my creative/journalistic side. The nature of the project required me to think about all possible outcomes when comparing data and personalizing text while sounding as natural as possible. I had started to write a draft on Google Docs, using a system of numbered blocks of text and colored “if” and “else” tags, but that ended up very confusing and the outcome wasn’t very natural. So, I decided to write while marking it up with the logic behind the multiple options. This made it possible to check if I was missing something immediately, but ended up being very weird to me because I had to use my “writer brain” and my “coder brain” at the same time. "," I believe that making the effort to bypass a “fill in the blanks” approach in automatically generated text and trying to generate text that doesn’t sound written by a machine is a challenge journocoders should focus more on - especially when English is not the language being used.   In my case, writing in a “if-else” logic system was always triggering the “lazy” side of my brain because I always wanted to write paragraphs like “The house prince increased by <var> in <city>” <OR> “The house prince decreased by <var> in <city>”. And even though there is nothing wrong with that kind of sentence, we know actual news doesn’t contain only that type of sentences. They provide additional context, explaining why the house prince increased in the specific region. Which means that, in your code, you have to probably write an almost custom paragraph for that specific case. This effort is what makes the automatically generated text sound like it wasn’t written by a robot, even if sometimes it is a phrase that only makes sense if four conditions are true. ",https://rr.sapo.pt/calculadora-custo-de-vida/,,,,,,,"Rui Barros, João Antunes, Rodrigo Machado, Maria João Cunha"," Rádio Renascença is a Portuguese national broadcaster, the leader in news and talk radio format.   For the past decade, Renascença has been recognized for its editorial online achievements, it has won more than two dozens of distinguished digital awards and is a pioneer in innovative digital media, being the first national radio creating its own web tv. Renascença established one of the first data desks in Portugal, now with award-nominated projects. ",,,
United States,The Arizona Republic,Big,Participant,Best data-driven reporting (small and large newsrooms),Arizona's Next Water Crisis,12/05/19,"Investigation,Explainer,Long-form,Illustration,Infographics,Video,Map,Satellite images,Elections,Politics,Agriculture,Economy","Microsoft Excel,Google Sheets,CSV,PostgreSQL,Python"," Rural Arizona is the Wild West for water. In most desert areas, water deep underground is the only source available. Yet there are no rules limiting how many wells can be drilled, or how much water can be pumped.    Big farming companies owned by out-of-state investors and foreign agriculture giants have descended on rural Arizona to drill massive wells, often shipping their crops out of the country.    An Arizona Republic investigation showed this free-for-all is draining away the water that homeowners also depend on, leaving some with dry wells and several rural areas of Arizona facing a crisis.     "," The Arizona Republic’s series is the most comprehensive examination ever done of Arizona’s groundwater, which accounts for 40 percent of the state’s water use.   The investigation found the water levels in nearly one in four wells in Arizona’s groundwater monitoring program have dropped more than 100 feet since they were drilled, a loss that scientists and water experts say is likely irrecoverable.   On top of that, the number of newly drilled wells has been accelerating, and the wells are hitting water at deeper levels. The conditions are worst in farming areas where there are no limits on pumping. And the problems have worsened with the arrival of big corporate farms and investors.    The series unmasked for the first time several of the corporate farms and investment funds that are likely the biggest water users in the state. The Republic profiled a Saudi farm using Arizona resources to ship hay back home, a 37,000-acre dairy that grows almost all its own food for its cattle while drilling wells a half-mile deep, and an investment fund that is renting out cropland even though there isn’t enough water for the local high school to water its football field.    The series has had an immediate impact. Members of the Arizona Legislature have announced six bills to strengthen groundwater rules in unregulated areas. Legislators have credited the Republic’s series for exposing the crisis in rural areas. Momentum appears to be building for reforms that would be the first substantial changes to the state’s groundwater law in 40 years, as the Republican Speaker of the House said legislation is needed after years of blocking water reform bills.     "," The investigation involved an unprecedented analysis of Arizona water-level data for more than 33,000 wells throughout Arizona, including some records going back more than 100 years, and nearly 250,000 well-drilling records.   We used a combination of MySql, Excel, Python to analyze the two datasets, which were very different in scope and detail. Much of the analysis was done in MySql. Python was used for some data work where MySql didn’t work as well, such as rank functions. Much of the MySql data work consisted of subqueries where we examined well data by groundwater basin, grouping the well data by together basin and across various time periods.    ArcGIS was used to analyze property records, which the Republic obtained from half of Arizona’s counties. We were able to use the property records and well-drilling data -- which often included LLCs -- to find the owners of the massive corporate farms and determine how much farmland they owned and where it was located.     "," The main difficulty with the data was with the 33,000 well monitoring records because some of the dataset was limited in scope. There was ample data available for some wells, with records going back more than 100 years. But for other wells, the data was spotty and sometimes there were only a few readings over decades. Also, a limiting factor was that the program was completely voluntary, meaning there was no data for hundreds of thousands of wells. We got around this by aggregating the data by groundwater basin and using five and 10 year periods to compare the basins so there was enough relevant data.   The well drilling records were problematic for a different reason. The records were part of a large relational database and because the way the legacy Oracle database was set up, we had to do extensive cleaning and data work to create the correct results. There was a main table that had the latest regulatory action and a historical table that had all the regulatory history, but without the most recent action. We needed to combine the two.    There was no obvious smoking gun in the well monitoring data. Because the well data was voluntary and sporadically measured, we had to look over long periods -- in places over decades --  to show the effect that pumping has had on aquifers. Some of the analysis involved layering the data alongside examples of large farms we had discovered through our reporting in rural areas of Arizona.    Through this unprecedented statewide analysis of groundwater data, we were able to show how unregulated pumping of groundwater is draining away the only water supply much of the state will ever have, as corporate megafarms continue to drain aquifers for quick profits.     "," This project began with a set of simple questions: How much have groundwater levels declined in Arizona in recent years? And have the water levels dropped more in unregulated parts of the state where there are no limits on pumping?    Our reporting for this series demonstrated how data work can be the backbone of a big investigative project without the numbers themselves being front-and-center in the telling of the story.    We were able to use the data as the main structure that proved the thesis of our series, but didn’t make the numbers the central focus of the story. Instead, we were able to tell the stories through the homeowners whose wells had gone dry, the farm owners who are concerned about their future and the residents who fear their towns may soon be turned into dust.    The six stories in the series, together with the data visualizations, attempted to distill very complicated science and data into concepts that were easier for readers to comprehend and understand.     ",https://www.azcentral.com/in-depth/news/local/arizona-environment/2019/12/05/unregulated-pumping-arizona-groundwater-dry-wells/2425078001/,https://www.azcentral.com/in-depth/news/local/arizona-environment/2019/12/05/salome-la-paz-county-residents-call-for-water-pumping-rules-amid-crisis/3792049002/,https://www.azcentral.com/in-depth/news/local/arizona-environment/2019/12/05/biggest-water-users-arizona-farms-keep-drilling-deeper/3937582002/,https://www.azcentral.com/in-depth/news/local/arizona-environment/2019/12/05/wells-drying-up-around-willcox-where-effort-change-groundwater-rules-failed/2357906001/,https://www.azcentral.com/in-depth/news/local/arizona-environment/2019/12/05/water-pumping-threatens-arizona-riparian-area-san-pedro-river/3937598002/,https://www.azcentral.com/in-depth/news/local/arizona-environment/2019/12/05/arizona-groundwater-rules-water-tables-declining-parts-phoenix-tucson/3949004002/,https://www.azcentral.com/videos/news/local/arizona-environment/2019/12/05/foreign-farms-pumping-la-paz-county-arizona-water-exporting-hay/3998095002/,"Rob O'Dell, Ian James, Mark Henle","  Rob O'Dell bio:    Rob O’Dell is a senior reporter at The Arizona Republic. He specializes in using data to drive investigative reporting. He was a crucial part of The Republic’s 2018 Pulitzer Prize-winning team.  He was the lead reporter and writer on the project’s main investigative story, which showed there are thousands of uncounted migrant deaths along the 2,000 mile border with Mexico.    He helps lead the Republic’s innovative four-person data team and is responsible for helping the other three reporters plan, develop, report and write enterprise and investigative stories. He has been named Arizona Journalist of the Year four times by the Associated Press, most recently in 2018. He lives in Phoenix with his wife and two daughters.     Ian James bio:    Ian James is a reporter for The Arizona Republic who focuses on water, climate change and the environment. His work has included an investigation of toxic pollution along the U.S.-Mexico border and a series revealing how groundwater depletion is affecting farming communities in the United States, India, Peru and Morocco.   He has reported extensively on water scarcity and climate change in the American West, including stories examining how climate change is shrinking the mountain snowpack and putting growing strains on the Colorado River.   Ian has won awards for his work including a Science in Society Journalism Award, the Society of Professional Journalists’ New America Award, and the Knight-Risser Prize for Western Environmental Journalism.    Mark Henle bio:     Mark Henle has been a staff photographer for the Arizona Republic since 1983. He started his photography career at the age of 14, working for his family’s bi-weekly newspaper in Marshall, Minn.    Mark and his wife, Marcy, live in Tempe, Ariz. They have three grown children and six grandchildren. ",,,
United States,Capital Public Radio,Small,Participant,Best data-driven reporting (small and large newsrooms),TahoeLand,08/08/19,"Long-form,Podcast/radio,Infographics,Chart,Audio,Environment,Business,Culture,Economy,Employment","QGIS,Microsoft Excel,Google Sheets,CSV,Python,Node.js"," TahoeLand is an eight-part podcast that investigates how climate change is impacting Lake Tahoe, an environmental jewel and major tourist attraction high in the Sierra Nevada between California and Nevada. It also further explores what lessons this region holds for the global climate crisis. This project is a collaboration between our environmental podcast team and our digital team to create a digital audio product that uses data, science and general nerdiness to take a localized look at climate change. "," This project connected the dots for the communities around Lake Tahoe about what climate change will mean for the place they love to call home. Climate change is a big, worldwide issue that it can be hard to conceptualize in your day to day life, but by marrying data about snow, lake clarity, the tourism economy and more, we displayed the daily struggles caused by climate change in this area. We even heard from the CEO of Tahoe’s leading economic development nonprofit, the Tahoe Prosperity Center, that this podcast led to a major revelation for her about the center’s work. She said that when Ezra and Emily first asked her about how climate change will impact Tahoe’s economy, she didn’t understand why we were asking her about climate change at all. But as we talked to her about the data we found correlating tourism job loss with California’s long drought, it became clear to her how a changing climate will dramatically change Tahoe’s economic identity. She said that she is now thinking on how she can help Tahoe’s residents cope with this change as her center continues to work at diversifying Tahoe’s economy.   The Pinecrest Nordic Ski Patrol is currently using the podcast episode about snow, including our data reporting on how climate change is impacting Tahoe’s snow, as a part of their avalanche training. The podcast is also now a part of the AP Environmental Science curriculum at a high school in the Tahoe area, and students at the Tahoe Expedition Academy are currently making a podcast inspired by our work. "," As we partnered with scientists and researchers on their findings, we were able to do most of our data analysis in  Microsoft Excel . We worked with the UC Davis Tahoe Environmental Research Center, the California Tahoe Conservancy and the Desert Research Institute to marry climate change projections with data on bear hibernation, fire danger, lake clarity and more. We also used  QGIS  to process GIS data.     To visualize this data, we used  Infogram,   Carto  and the  NPR Daily Graphics rig . We used Infogram to create a chart of the Tahoe Environmental Research Center’s predictions for the rain-to-snow ratio that features a snowy mountain. We used Carto to map a geodatabase showing different areas of fire risk throughout California. And we used the NPR Daily Graphics rig to create bar charts to accompany our stories on the length of bear hibernation and Tahoe’s tourism economy to visualize the changes scientists are predicting and the impacts this community has already felt.    We also worked with  NPR to use their app template  to create an extremely visual buildout for our story about the invasive mysis shrimp that are making the lake less clear. We used their template to marry video, illustrations, photos and interactive data visualizations to track the link between the prevalence of the shrimp, the disappearance of the zooplankton they eat and the declining clarity of the lake.    "," Working on this project meant working with really complicated scientific data, thus combining two already intense tasks: data reporting and science reporting. That meant that Emily and Ezra needed to understand this research well enough to boil it down into something that our audience could understand, and to portray it in an interesting way. For example, Emily spoke with the UC Davis Tahoe Environmental Research Center’s Geoff Schladow about the process climate scientists use to turn current data on global greenhouse gas emissions into different climate scenarios that allow them to predict different futures for places like Tahoe depending on how well we mitigate our emissions. While this process wasn’t reported in the podcast, understanding it was key for Emily to be able to explain what Schladow’s climate projections for the Tahoe Basin meant, and for her to answer any audience questions about these predictions.    But aside from just understanding this science, we had to make it interesting, too. That often meant taking these extremely high-level topics and talking them through extensively again until we were able to explain them in their simplest form. While our data segments in the podcast may sound laid-back and fun, they came from Ezra and Emily agonizing over the script until this science was digested into its simplest form. We had to work hard to make this science accessible, but we are proud that we were able to help our audience learn about research that otherwise may not have been available and relatable to them.    "," This project is a great example of how you can use data and research from scientists in your region to tell a localized story about climate change. Climate change is a giant concept, and it can be hard for people to wrap their minds around how their individual worlds are actually going to change over the next century. By working with some of the people doing climate research in our area, we were able to give our audience a look into the future of what could actually happen to a place they know and love rather than talking broadly about the fact that the climate is changing. Rather than just saying there will be less snow, we were able to say just how climate change would impact the amount of snow vs. rain that the Lake Tahoe region will be getting. This made this story more impactful and interesting for our audience, and painted a picture of what this global issue actually means for our community.   By looking at how we went deep on the research being done locally, others can get an idea of how they could tell a similar story on a local level for their audiences. We built close working relationships with the scientists in our area to get a full picture of what they’re studying, and of what concerns them the most in what they’re finding. Finding these local connections and making them a key part of our storytelling helped make this podcast great. ",http://www.capradio.org/tahoeland,http://www.capradio.org/news/tahoeland/2019/08/29/climate-change-is-disrupting-bear-hibernation-in-tahoe-thats-bad-news-for-bears-and-humans/,http://www.capradio.org/news/tahoeland/2019/09/12/adapt-to-whatever-mother-nature-gives-us-tahoes-tourism-economy-in-a-changing-climate/,http://www.capradio.org/news/tahoeland/2019/08/15/tahoelandshrimp/,http://www.capradio.org/news/tahoeland/2019/08/08/that-blue-hue/,http://www.capradio.org/news/tahoeland/2019/08/22/less-snow-more-rain-how-tahoes-climate-balance-could-be-shifting/,http://www.capradio.org/news/tahoeland/2019/09/19/playing-with-fire/,"Emily Zentner, Chris Hagan, Ezra David Romero, Sally Schilling, Nick Miller, Gabriela Fernandez, Renee Thompson, Katy Kidwell, Veronika Nagy"," Emily Zentner is a data reporter and digital producer at CapRadio in Sacramento, where she has worked on projects like their TahoeLand podcast and their wildfire history map. Having lived in Northern California for almost all of her life, she is passionate about telling untold stories about her state using data and visualizations.   Chris Hagan is the Senior Editor, Digital Content at CapRadio, and previously served as the station's data reporter and digital producer.    Ezra David Romero is the host of TahoeLand and CapRadio’s environment reporter.    Sally Schilling is the Podcast Director at CapRadio. She previously served as a podcast producer and general assignment reporter. ",,,
United States,Capital Public Radio,Small,Participant,Best news application,A History of California Wildfires,24/01/19,"Explainer,Database,News application,Map,Environment","QGIS,JQuery,Microsoft Excel,CSV,Python"," This is an interactive map showing the perimeters of more than 100 years of California wildfires using geodata from the California Department of Forestry and Fire Prevention (Cal Fire) and the U.S. Geological Survey. Because of how data were collected prior to 1950 and Cal Fire’s criteria for recording a fire’s perimeter, we can’t say that this represents every single fire that has burned from 1878 to 2018, but we can say that this data is the most complete record of California’s fire perimeters, according to Cal Fire. "," In addition to this map being one of our website’s most viewed pages in 2019, there was an amazing community response around the state to it. This map was shared in a number of community public safety pages, fire victim support groups and firefighter groups on Facebook, where we saw interesting conversations about the state’s history with fire. We even had a Cal Fire chief reach out to us to ask if we would be making prints of the map as he wanted one to hang in his office.   Researchers from around the country have reached out to us asking for the data that powers the map, including from the University of Chicago and our local University of California, Davis. These researchers plan to use it in studies about everything from fire science to the wildland-urban interface. We are hoping to make this data available for download on our site in the future. The map is also currently being used in the curriculum of the Los Angeles Unified School District.   Wildfire is arguably the biggest issue facing California now, and this project provided important context to the megafires that have burned in the state over the past few years. We saw comments from community members that the map was a “sobering” representation of how extensively fire has touched the state. We also heard from people who “nerded out” over what areas the fire has burned over and over, like the area around where the Camp Fire burned in 2018, and what areas haven’t been touched, like the Central Valley. Overall, from the reactions we got, we can say that this map helped people better understand California’s relationship with wildfire going back over a century rather than just in recent years. "," When Emily first downloaded this data from Cal Fire’s website, she loaded it into  QGIS  to see what these perimeters would look like on a map of California. Using QGIS, she methodically pickedthrough 20,000 rows of data to search for issues, such as our year column being a text column instead of a date column and a year typed as 2106, to get the geodata ready to visualize.   Once the data was cleaned up, Chris processed it with open source tools from  MapBox,  including their  Tippecanoe  library, to prepare it for the web. We built the page using  MapBox.js  and build tools from the  NPR App Template . The final page is hosted on  s3 .    We used  ColorBrewer  to find an accessible and appropriate color scheme for this map. We strive to have all of our data visualizations at the station be color-blindness friendly, which ColorBrewer is a great tool for.  "," This was Emily’s first major mapping project, and the first time she tried to clean a database of this size, which posed a few challenges along the way. She knew that creating this map was an important public service, but had to learn along the way how to create it and make it usable for our audience.   The dataset behind this map is large, featuring over 20,000 entries and almost 20 fields when we first got it, which is part of why it was important to create a map like this in the first place. The sheer number of wildfires that have burned in California over the past century is hard to grasp, and we created this map to solve this problem. But that sheer number caused some issues along the way for this relatively new mapmaker.   Emily had to learn how to clean a large database, and was eventually able to cut the number of columns in the database down by 75 percent. But it was still a lot to deal with as she searched for any year entries or fire names that looked irregular, a task she took on in QGIS and Excel. A year later, this is definitely not how she would choose to clean a database.   The reasons that this map was a challenge to make are the same reasons displaying it is an important public service. Cal Fire’s geodatabase of fire perimeters is huge and unwieldy for our audience members to ever try to sift through. By making this information accessible in a visual format, we’ve made it possible for our community to understand and discuss this information. "," This project is a testament to the power of maps to add important context to breaking news events. Our community members were able to browse this map and learn about the history of fires in areas that had recently burned, such as Paradise and Redding, to find out new things about the wildfire news that has consumed California for the past few years. It was fascinating to watch our audience discover new things through this map, like that the Camp Fire was fairly small, though incredibly destructive, and that certain areas have not burned like the rest of the state has.   This data was hiding in plain sight on the Cal Fire website, but no one had taken the step to make it accessible to the public before we created this map. By using data that was already publicly and freely available to create an interactive map, we made it possible for our audience to sift through it and learn about their state in the wake of a devastating fire season.   This project is also a great example for other journalists of how important it is to talk to the person who manages the database that you’re using. Through long conversations with Cal Fire’s Dave Passovoy, we were able to openly and plainly explain what this map does — and does not — show. This meant that when our community members had questions for us about why certain fires they remember were not included, we were able to explain to them that, while this database is incredibly extensive, it is not a 100% complete database of every fire that has burned in this time period. Our conversations with Passovoy were a key part of making this project accurate and informative for our audience. ",http://projects.capradio.org/california-fire-history/#5.71/38.819/-122.249,http://www.capradio.org/articles/2019/01/24/sierra-nevada-foothills-residents-consider-how-to-escape-if-the-next-camp-fire-ignites-in-their-community/,https://source.opennews.org/articles/how-we-mapped-more-100-years-california-wildfire-h/,,,,,"Emily Zentner, Chris Hagan, Sally Schilling"," Emily Zentner is a data reporter and digital producer at Capital Public Radio in Sacramento, where she has worked on projects like their TahoeLand podcast and their wildfire history map. Having lived in Northern California for almost all of her life, she is passionate about telling untold stories about her state using data and visualizations.   Chris Hagan is the Senior Editor, Digital Content at CapRadio, and previously served as the station's data reporter and digital producer.    Sally Schilling is the Podcast Director at CapRadio. She previously served as the station’s podcast producer and general assignment reporter. ",,,
Colombia,"Datasketch, Infraestructura Visible",Small,Participant,Best data-driven reporting (small and large newsrooms),Regalías (Royalties),10/01/19,"Investigation,Database,Open data,Infographics,Chart,Map,Politics,Economy","Adobe,Creative Suite,CSV,R,RStudio"," Royalties are the payments made to the Colombian government by oil and mining companies that allow them to exploit non-renewable natural resources in Colombia, such as oil deposits and carbon mines. This money is administered by the General System of Royalties (GSR), which has to distribute it among the municipalities, departments and regions of the country. Unfortunately this topic is somewhat opaque for the layman. In this data-journalism special, anyone can know how royalties are obtained, distributed and invested across the country with beautiful and understandable visualizations. The data used in the visualizations is available for download, as open data. "," Although royalties constitute a big part of how huge (and costly) public projects are funded, not many Colombians know what these are, how they are obtained, distributed or invested. This lack of awareness about the way royalties work, as well as how intricate and complicated the General System of Royalties (GSR) is, has made corruption a common problem in the projects financed by the GSR. This data-journalism special allows citizens to learn the most important aspects of the royalties system in Colombia and gain insights on how this money is distributed across the country. The special was released in the midst of the debate around a new law that could change the structure of distribution of these royalties, which means that this project provides tools for citizens to participate in the discussion with data-based arguments. This way, the debate will not involve only politicians, but also well-informed citizens. "," The investigation started by reviewing the legislation that has ruled the General System of Royalties (GSR), which is not readily available in a single source, but rather spread across different public institutions. It required a lot of work to simplify the language around this system and summarize its most important aspects so that anyone can truly understand how the SGR works. This resulted in a series of questions about the way royalties have been distributed and invested in Colombian territory, which could be answered through data. The dataset used for the different visualizations was downloaded from the National Planning Department and had to be cleaned and tidied with R using R Studio. The story was written after visualizing this data and answering the preliminary questions. The design was made in Adobe Illustrator and then built into a website using plain Javascript, HTML and CSS. "," The most challenging aspect of this project was to understand the legislation of the General System of Royalties (GSR) and its implications on the development of the poorest regions in Colombia. The way royalties are distributed is complicated and involves a lot of bureaucratic processes and organizations. Conducting this investigation allowed us to understand the debate around the distribution of royalties, that basically involves choosing between investing in the poorest regions in Colombia or investing in the regions in which the oil or mining exploitation actually happens –exploitation that definitely takes a toll on the natural resources available to these villages–. The data available was truly important in terms of the insights it provides about the cost of the projects funded with royalties, the most funded economic sectors (i.e. transportation, science & technology, among others) and the structure of the GSR (which explains in part the corruption within this system). This data (and this project) is fundamental in the debate around the new law that will rule the GSR, because it provides a base for conducting an analysis of the efficiency of the GSR distribution and its investments. "," Other journalists can appreciate the storytelling in this project. A very complex topic such as the royalties system in Colombia can be boring and difficult to understand, especially to non-experts. Engagement is a real challenge with this type of projects. However, the structure of the special, the language used, the webpage design and the diagrams and visualizations were key in having great feedback from experts and non-experts. ",https://regalias.infraestructuravisible.org/,,,,,,,"Mariana Villamizar Rodríguez, Ana Hernández Andrade, Cristian Saavedra, Cristina Ángel"," Mariana graduated as a designer and as a systems engineer, as well. She works in Datasketch and did the research, storytelling, design and web development of this project. Ana is a part-time mathematician at Datasketch; she was in charge of the data cleaning, analysis and visualization. Cristian and Cristina are both coordinators of the Infraestructura Visible project and graduated from Civil Engineering. They obtained and analyzed the main dataset used in the project, curated the contents and gave guidance on the general approach of the investigation. ",,,
Canada,CBC/Radio-Canada,Big,Participant,Best data-driven reporting (small and large newsrooms),Who's really behind Canada's most active Airbnb host accounts?,30/04/19,"Investigation,Long-form,Multiple-newsroom collaboration,Infographics,Business","Scraping,D3.js,QGIS,Google Sheets,Python,Node.js","  With questions swirling worldwide about the impact of short-term rental apps on the real estate market, CBC/Radio-Canada wanted to know: who’s really behind Canada’s most active Airbnb accounts, and are they legit? We scraped over 32,000 listings to find out.      Our investigation revealed many of Canada’s most prolific “mega-hosts” are fronts for multimillion-dollar companies and Airbnb is thriving in zones prohibited to short-term rentals. Our stories also questioned the company’s own failsafe mechanisms by showing how easily one apparently fraudulent host was able to build a complex web of accounts to hide a trail of bad reviews.  ","  After CBC’s initial two stories aired, Quebec’s Tourism Minister Caroline Proulx acknowledged that the law in Canada's most Airbnb-dense province lacked clarity and would be changed. Our story had also found that Revenue Québec, which was responsible for monitoring Airbnb listings since June 2018, has not issued a single fine — only warnings. The Revenue Ministry also assured CBC/Radio-Canada that fines would be issued from this point on.          After CBC aired its third story, about the prolific but likely swindling host account ""AJ"" and his eight other aliases, Airbnb shut down all nine accounts permanently, citing violations of its terms around truthfulness and potential fraudulent activity. Dozens of guests who had already booked were to be offered complete or partial reimbursements. It is believed to be the most significant action Airbnb has ever taken against a Canadian host.   ","  We began by coding a complex Node script to scrape more than 32,000 entire apartment, condo and house listings posted in 16 Canadian cities on one day in April 2019. Then we used a Python script to clean, structure and analyze the tens of thousands of ads we’d scraped. That allowed us to group properties with the host who managed them, leading us to Canada's most prolific ""mega-hosts"".         We produced a tip sheet of individuals who were each operating up to 270 listings in various Canadian cities — and then used traditional investigative reporting techniques to find out who they really were. Using tools like reverse image search engines, we found that while many had personable-sounding profiles, they were using fake headshots and were in fact owners or employees of multimillion-dollar companies.          Our second story used census data and GIS technology to calculate the concentration of Airbnb listings in each Canadian city and neighbourhood. We asked each city for an official shapefile and breakdown of their total number of private dwellings by neighbourhood - and used QGIS software to calculate what percentage of these properties were listed on Airbnb.         For our third story, we zeroed in on one of Canada’s most prolific hosts: ""AJ"" in Montreal, and his seemingly fraudulent behaviour. We scraped hundreds of reviews connected to his 90-plus properties listings and were able to track down former guests who told us they'd had horrible experiences. After analyzing patterns in reviews and listings, we found ""AJ"" wasn't operating alone. We discovered he ran eight other accounts on Airbnb under other names, all using fake profile pictures, that rented out the same properties and even gave each other glowing reviews.         To share our findings, we created a map with D3js and animated it using the scrolly telling technique.   ","  Telling this story was both a challenge on the data science and the investigative front. There was no way to directly download the Airbnb data. It took a significant amount of time for our team to understand how Airbnb’s data was structured and develop a robust code that would connect to the website’s API and collect listings advertised in 16 Canadian cities - meaning 16 slightly different scripts, with lots of requests and time. For our spatial analysis, the main challenge was to match the Airbnb listings we collected with shapefiles and customized census data from a half dozen cities.           Uncovering the true human behind each “mega-host” account was equally as challenging. We quickly discovered that, despite Airbnb’s ID verification, hosts could still use an alias and a stock photo. We were able to track down some hosts by comparing landmarks visible in their listings photos against Google Maps, then scouring property records; one of the hosts was unmasked after we noticed a well-known church steeple in one of his photos. We pieced together “AJ”’s web of aliases by assiduously tracking down two dozen of his guests based solely on their profiles -- a major challenge on Airbnb since profiles often only have a first name and a hometown. To find these people, we took to social media to look for comparable headshots, used databases of professional organizations like teachers or doctors, and often just guessed, luckily, what their email addresses might be. Contacting them proved invaluable because they sent us their communications with one or another of the AJ aliases and all the information from the listings they had booked, all of which enabled us to say definitively that the properties and the operator behind them were one and the same.  ","  From Barcelona to San Francisco, cities around the world have been struggling to better understand the impact of short-term rentals on their neighborhoods and economy. In many places, they’ve had to vote on regulations and navigate lawsuits while dealing with short-term rental companies like Airbnb who consistently <a href=""https://www.vox.com/2018/8/24/17779208/airbnb-suing-new-york-city-user-data-hosts-privacy-brian-chesky"">refuse to release their raw data</a> and prefer sharing <a href=""https://www.nytimes.com/2018/05/04/nyregion/airbnb-new-york-report-errors.html"">carefully chosen aggregated figures</a> rather than being completely transparent and held accountable for their actions and their host’s actions. Many cities are well-intended, but lack the technology and resources to even know how many rentals are, in fact, illegal or leased over the maximum of days prescribed.     We designed this project and methodology so it could be duplicated by any newsroom in any city or country, big or small, and hope news outlets around the world will take advantage of that. We used census data and QGIS which is a free software as well.      Another great lesson is how valuable traditional investigative techniques are when powered by data-driven findings. Without the valuable background information our investigative partner Zach Dubinsky dug up on these mega-hosts, our stories would have only been another Airbnb explainer with general numbers and a few interactive maps. He spent hours looking for patterns in reviews we’d scraped, speaking with countless disgruntled tourists and digging through property records to find the real life humans and victims behind the data, which enabled us to bring our stories to life in broadcast media. The stories generated an outpour of tips from Canadians who felt they’d also been scammed by Airbnb hosts, including AJ.    ",https://ici.radio-canada.ca/info/2019/04/airbnb-annonces-location-logements-plateforme-montreal-canada/index-en.html,https://ici.radio-canada.ca/info/2019/04/airbnb-annonces-location-logements-plateforme-montreal-canada/,https://www.cbc.ca/news/canada/montreal/airbnb-montreal-aj-host-suspended-accounts-1.5252233?fbclid=IwAR0WY_ZyKKD4BQcMHRYcIglZ3FaOV9QnWZooflUTKwpVXfV6fWj1Lfh64kw,https://www.youtube.com/watch?v=6G34P11Nr3o&feature=youtu.be,https://www.youtube.com/watch?v=mnjhUmXlWSc&feature=youtu.be,,,"Naël Shiab & Valérie Ouellet (Data Reporters), Zach Dubinsky (Investigative Reporter), Philippe Tardif & Francis Lamontagne (Web Designers), Vincent Maisonneuve, Romain Schué, Lina Forero (Additional Reporting).","  Naël Shiab is an award-winning data reporter working for Radio-Canada in Montreal. He chases and writes data-driven stories in innovative formats. His recent reporting includes an in-depth look at women’s representation in federal politics and an interactive take on climate change in Canada.      Valérie Ouellet is an award-winning broadcast reporter based in Toronto, Canada. As CBC News’ Senior Data Journalist, she uses her data science skills to find and report on exclusive national stories. She contributed to the International Consortium of Investigative Journalists’ (ICIJ) “Implant Files” (2018) and “ Paradise Papers” (2017) investigations. Recently, she reported on the rise of Airbnb mega-hosts, uncovered large-scale ticket scalping strategies and led a national data-driven project on student violence.      Zach Dubinsky is an investigative journalist. His reporting on offshore tax havens (including the Paradise Papers and Panama Papers), political corruption and organized crime has won multiple national and international awards.  ",,,
Brazil,Fiquem Sabendo,Small,Participant,Open data,Newsletter Don't LAI to ME,14/01/19,"Investigation,Explainer,Breaking news,Open data,Crowdsourcing,Human rights","RStudio,PostgreSQL,Python","The first brazilian newsletter to open government data using multiple Freedom of Information Law (the brazilian FOIA) requests. In just one year our impact on the national and regional media was huge: exclusive data we published was mentioned more than 400 times in newspaper from over 21 brazilian states on TV, local and national newspapers, magazines and journalistic websites. Our mail goal is to spread data journalism not only for those who already use it, but mostly for small and medium newsrooms and researchers. One of our most recent publications revealed, for the first time in 50 years, the list"," Exclusive data we published was mentioned more than 400 times in news outlets from over 21 brazilian states on TV, local and national newspapers, magazines and journalistic websites. (BBC Brasil, El Pais, Folha de S. Paulo, O Estado de S. Paulo, O Globo, Correio Braziliense, Zero Hora, HBO Brasil and many others). We've gathered a community of over 4,000 subscribers in one year - journalists, lawyers, researchers and public officers from every brazilian state -  and pressed the government to publish hundreds of new databases online. We are proud to say that small local newspapers and websites from all regions of the country are using our data to write stories about their own community. The most impactful database was published after we denounced on justice that the federal government never published salaries received by pensionists (in Brazil there are a lot of laws that give you to right to receive a salary for your whole life just because your parents were public officers or military). More than 300,000 salaries were published online after the Tribunal de Contas da União (a federal legislative court) accepted our claim. The data revealed that even sons of military accused of torturing people during the brazilian Military Regime still receive pensions. The federal government paid over R$ 590 billion on these pensions in the last 6 years (over U$S 120 billions).   We also made public more than 500 documents that were kept in secret by the federal government, even after the official seal was out of date. Those documents revealed, for example, how the military spied on activists during the 2013 protests in Brazil against Dilma Roussef and against the World Cup in 2014.   Our work was mentioned as source by GJIN and CGU (Brazil's federal agency to prevent corruption). "," We focus on using and automatizing Freedom of Information Law websites, to get more information faster and from all the brazilian states, so we have new data that can be used be journalists and researchers from all over the country. We are now using a system to send many requests at the same time, so we can ask just one question for all  the ministries from the federal government, for example.    But our focus is to ask many questions, find out the data that is still hidden and pursue it using the law and administrative appeals.  "," The hardest part is that we are in this project, for the most part of our time, as a volunteer group of 4 journalists and one lawyer. We all have our own jobs and schedules, but we decided to share what we've got with other coleagues and spread technicques about how to get information from the government. Our newsletters are published on mondays, so it took a lot of our holidays and weekends to prepare the material and make it easy to access and understand.    We also need to ""translate"" the data we get for our audience, since many journalists are not used to read databases or send FOIA requests.  "," - How to send massive FOIA requests and make it useful for national and regional use;    - How to make impact with a project on many different newsrooms   - How to automatize requests   - How to organize a flow of sending, receiving and putting the request on different categories   - How to gather data from different databased and create one single product ",https://fiquemsabendo.substack.com/,https://ijnet.org/en/story/don%E2%80%99t-lai-me-newsletter-navigate-brazils-access-information-law,https://fiquemsabendo.com.br/,https://knightcenter.utexas.edu/blog/00-20614-brazilian-journalists-create-task-force-obtain-documents-previously-kept-secret-public,,,,"Luiz Fernando Toledo, Maria Vitória Ramos, Léo Arcoverde, Fabiana Cambricoli and Bruno Morassutti"," I work as a data news producer and reporter for CNN Brasil. I write about public policies and education, for both local and national stories.  My stories have been published in the most important journalistic websites in Brazil, such as Estadão, G1, UOL and TV Globo. I have also helped to find data and documents to support journalists from The Washington Post and ProPublica. I am the youngest journalist to become director of the Brazilian Association of Investigative Journalism (Abraji). I'm pursuing a masters degree in Public Administration at Fundação Getulio Vargas (FGV).  One of my most important pieces, an investigation about misleading stories using Facebook pages during the brazilian elections for O Estado de São Paulo newspaper, was mentioned by many international newsrooms - published by Reuters and then republished by The New York Times,  Le Figaro and others. The story made more than 65 pages to be deleted, with millions of followers. It revealed a web of companies and politicians using the social network to spread false content about candidates.   I was a visiting fellow of the International Center for Journalists (ICFJ) in Washington D.C. for two months in 2018. During that time I studied entrepreneurship in digital journalism with a scholarship granted by the American government and had the opportunity to work at ProPublica, an independent investigative journalism  outlet. I worked there with research for projects related to the Freedom of Information Act. Working there was my inspiration to create Don't LAI to Me newsletter.  I have also created a project with Jeduca (Brazilian Education Writters Association) and visited more than 15 universities from different parts of Brazil to teach how to get and use data from the government. In seven years as journalist I won 10 journalist prizes, such as Claudio Weber Abramo de Jornalismo de Dados (Open Knowledge Brasil), Estado (Estadão newspaper), ANPR (Federal Prosecutor's Association) and Allianz-Ayrton Senna (Public Education). I also won in 2019 the main scholarship in Brazil for journalists to study in the US, granted by the Brazilian NGO Instituto Ling (Visionary Journalist Program or Jornalista de Visão). ",,,
Argentina,socios.red,Small,Shortlist,Best news application,Socios.red,25/08/19,"Investigation,Solutions journalism,Database,Open data,News application,Politics,Corruption","Json,CSV,Python"," <a href=""https://socios.red"">Socios.red</a> is a tool that exposes, in a simple and clear way, relationships that exist between different Argentine companies or NGOs, people who have or held positions as authorities in any of them; and various state agencies and political parties.   Socios.red combines information from a million registered companies in the City of Buenos Aires, 3 million people associated with them, and they are associated with thousands of contributions from citizens and companies to political parties, purchases from the national state and from the city ​​of Buenos Aires and generates an agile way to understand the relationships between all these data.     "," Since it was launched, socios.red was used by dozens of journalists that could understand different relationships and identify which ones deserve to be investigated more thoroughly. Socios.red was developed to facilitate and democratize the access and visualization of these data and that anyone interested without technical knowledge can use them.   Well known national media used our data tool to very important works, for example this investigation that was on TV and then on the page of <a href=""https://www.pagina12.com.ar/132187-casa-matriz"">Página 12</a> and publiched by a lot of other media, as <a href=""https://www.minutouno.com/notas/3082612-gobierno-sa-festival-incompatibilidades-269-funcionarios"">Minutouno</a>. Also <a href=""https://chequeado.com/el-explicador/apagon-verdades-y-falsedades-sobre-una-ex-empresa-de-macri-mencionada-en-whatsapp/"">Chequeado.com</a>, fact checking leader, uses our tool to support their investigations.   The main impact is that everyone can acces information than otherwise would be imposible to understand.     "," We can divide our work in 3 main steps:     We get the data from Open Data portals. The first step is to be aware of what data is available and how can we combine it.  Then we have to process it, clean it and make it ""match"", because every source not always identify people or companies in the same way. For this work, we use most of all Jupyter Notebooks.   Then, we have to build the graph, and we use ArangoDB to connect node.   After that, it comes the web interface. It is built on vue.js and Python.     We also talk a lot with journalists, to make our tool better. "," If think every step of our work has it difficult part. If I have to chose one, I would say that it is a hughe work to organize and clean all the date to make it match. Every dependency publish data with different standards and formats, and every month may be changes.    Then, there is the whole programing stuff to make it available online for everyone. It is a lot of work also. And it is very important to focus on users needs.   Besides, one of the most difficult task is to carry on all this project. At the beggining, we won a prize of human rosources to accelerate our idea in an event organized by the <a href=""https://www.opengovpartnership.org/"">Open Government Partnership</a>, and a few months later, Google News Labs gave as credit to use on Cloud services. But besides that, more than 10 people participated in socios.red project. "," I think that people could learn on how to use technological resources to make open data available for everyone. Build tools like socios.red is important because there are millons of stories to be descovered and published by journalists of different backgrounds an ideologies. Also, others can learn that governments may publish data, but to achieve accountability, sometimes we need to move one step forward as civil society and media. ",https://socios.red,https://docs.google.com/presentation/d/1Q1sJ5rF4ukO__Ck7DpELr82YCX3xkmyo8We-XqVGQ0I/edit?usp=sharing,http://noticias.unsam.edu.ar/wp-content/uploads/2016/12/Puerta-Giratoria-en-Argentina-Analisis-del-gabinete-nacional-actual-y-la-experiencia-internacional-comparada.pdf,,,,,"Nicolás Grossman, Matías Battocchia, Lisandro Espejo, Alfredo Ramirez, Bruno Salerno, Damián Silvani, Martín Sarsale, Franco Bellomo y Valeria Tiffenberg"," Nicholas Grossman, Degree in Sociology (UBA), Master in Sociology International Politics (Untref) and journalist specialized in  Open Data. He has worked on issues related to Open Data, cities and different urban dynamics for more than six years in <a href=""https://www.storybench.org/argentine-real-estate-platform-building-visualizations-latin-american-newsrooms/"">Properati</a>. Based on his Open Data experience, last year, he published online Socios.red   Matías Battocchia, Physics student (UBA) and Machine Learning specialist.     ",,,
United States,Numlock News,Small,Participant,Open data,Numlock News,01/02/19,"Explainer,Breaking news,Open data,News application,Arts,Lifestyle,Business,Culture,Women,Economy","Microsoft Excel,Google Sheets"," Numlock News is a daily morning newsletter that scours the web to highlight the best and most compelling data journalism out there. Created by Walt Hickey, Numlock takes the perspective that data journalism is most effective when it's delivered with a conversational, funny and engaging verve. Started just over a year ago, tens of thousands of readers subscribe to this daily digest of thrilling stats in the news. Numlock highlights not only traditional forms of data journalism from established shops but also opens up traditional reporting — culture, news or lifestyle — to unlock the data in the everyday. "," Numlock seeks to not only relentlessy highlight outstanding work from the best data journaists working today — be they at major periodicals, local newspapers or indpendent shops — but also to redefine in the reader’s mind what data journalism is: not a separate priesthood existing outside of the realm of reporting, but rather a vein of journalism that has existed a long time and only recently been elevated to the position it deserves. Whether it’s a technically intensive interactive or a few numbers from the Mayor’s office in paragraph nine, Numlock seeks to bring those numbers front and center and with a funny, inviting tone encourage readers to look a little deeper in the news they consume.     Numlock has tens of thousands of subscribers, over 50 percent of whom read it every day and over a thousand of whom pay to support it. Numlock delivers data journalism that would normally thrive in a nice and delivers it to new audiences — students and those who otherwise would not be interested in reading up on quantitative reporting — and by delivering it in a snappy fashion, broadens the field to people who might not consider themselves someone who can be a data journalist.    Numlock also includes a book club spinoff as well as an annual awards supplement, using the annual Academy Awards to highlight data journalism techniques and advantages to an audience unaccustomed to having quantitative reporting in cultural fields. "," Numlock uses fairly rudimentary statistical techniques, and often endeavours to keep things simple. In reality, too often the technique found wanting in the field of data journalism is the persuasive, compelling writing that encourages people to read it, the kind of conversational tone that convinces skeptical readers that this field can be for them.    Data journalism is important, but it's not just manipulation and facility with data: the journalism part, the compelling people to consume the work, is a critically undervalued portion of it, and there is little point to open data unless there are people who feel compelled or inspired to actually use it.  "," Covering over 35 stories per week, five days a week, plus a special interview with a journalist each Sunday requires a degree of discipline. Still the committment to covering diverse, international and compelling news from a variety of sources and digging deep to elevate the data journalism buried within can be rewarding, and the effort is appreciated by readers.    Thanks for your consideration, I realize this entry may be off the beaten path of what you'd typically entertain from this kind of category but I do hope this vein of data journalism becomes more prominent moving forward. Thank you for your time.  "," That it's crictical to remember the importance of readers, and to relelntlessly push ourselves to expand our audiences beyond the traditional — highly educated, typically male — audience for data journalism. By consistently making work accessible to passers-by, that's how best we serve readers and underscore the role of journalism. Opening up data is one thing, posting material to GitHub is important, showing one's work matters, and unlocking information and making it available is critical. But all that's for nothing if data journalism remains an exclusive club, one that you need to have the correct degrees for, or an understanding of this code language or access to that software.    Data journalists have a more powerful tool at their disposal: It's one thing to make data open, and that is commendable, but the step beyond opening the door to data journalism is  convincing them to want to walk through it,  and that's only possible by seeking out new audiences, young audiences, skeptical audiences, and inspiring and electrifying them daily. At its core, that is the fundamental goal of Numlock, an on-ramp to data journalism and a platform with which our most exciting voices and best work can be presented to new readers on their terms. ",http://numlock.news,https://numlock.substack.com/p/numlock-news-january-23-2018,https://numlock.substack.com/p/numlock-news-november-19-2019-hydrogen,https://numlock.substack.com/p/numlock-news-september-26-2019-abominable,https://numlock.substack.com/p/numlock-news-march-29-2019,http://awards.news,https://numlock.substack.com/p/numlock-sunday-sarah-gilman-on-invasive,"Walt Hickey, edited by Maureen McNabb"," Walt Hickey is a data journalist who focuses on culture. He started at Business Insider in 2012 and went on to work as FiveThirtyEight's chief culture writer from 2013-2018. In May of 2018, Walt left FiveThirtyEight to found Numlock, an independent daily morning newsletter that focuses on the best data journalism each morning. It's grown to tens of thousands of readers. Walt now serves as the Senior Editor for Data at Insider.    Maureen McNabb is a freelance writer and editor working out of Richmond. She has a passion for video journalism and her work has appeared in Richmond Magazine. McNabb is the editor of Numlock News. ",,,
Australia,ABC News (Australia),Big,Participant,Best visualization (small and large newsrooms),What you'd spend to prevent climate change — and what you could get with your money,17/12/19,"Explainer,Solutions journalism,Multiple-newsroom collaboration,Crowdsourcing,Illustration,Infographics,Chart,Environment","Animation,D3.js"," The ABC’s Australia Talks National Survey asked 54,000 Australians about their lives and attitudes in one of the largest surveys of its type. One question was how much more Australians would personally spend each year to help prevent climate change. When we added together their responses we had a kitty of around $4 billion — this story visualises different ways we could spend that money and transform Australia, as mapped out by four experts. Using a swarm of dots to represent the funds, we created a digestible, highly visual piece that was empowering for the audience. "," The story was read by three quarters of a million Australians and we were inundated with requests from the audience as to how they could contribute to or start a climate fund.   Many responses were from financially stretched Australians who were nonetheless inspired to help:   ""I am on a disability pension and I will find $200 to contribute. Can these people mentioned in your article get this happening? Can they organise a summit, or a round-table, or something to start this discussion in very serious ways. The government isn't going to do it. So, can we bypass them and get Australians to take back ownership of this country and help save this planet.""   The piece was also praised by the audience for its design:   “The most beautifully designed graphic and text. I think it’s by far the best experience I’ve ever had reading content on a mobile device. Whoever designed that has absolutely nailed it! The way the text and graphics interact while scrolling down is sumptuous—simple, beautiful, informative. Just outstanding design!”   Several Australians told us that after reading our piece they were looking into the practicalities of setting up a fund like this that would operate independently of government. "," The Australia Talks National Survey of 54,000 Australians was an epic undertaking, two years in the making, conducted in conjunction with social scientists and data scientists at Vox Pop Labs. We started by crowd-sourcing areas of concern for Australians through focus groups and interviews with thousands of Australians from across the political and sociodemographic spectrum. With an academic advisory panel, we then designed over 500 questions and statements that tested the attitudes and behaviours of Australians. After 18 months of crowd-sourcing and survey design, these 500 questions were then put to 54,000 Australians in July 2019. We spent a month in August analysing the data with help from data journalists at ABC and Vox Pop Labs’ data analysts.    We then commissioned more than 100 pieces of broadcast and digital content based on the data from content teams across the ABC that were distributed across all platforms. The data told us climate change was a priority for Australians so we decided to make a high-end digital piece using the question from the survey about what Australians would personally spend on climate mitigation.   Developers Simon Elvery and Colin Gourlay coded the story's interactions from scratch using D3. They used Web Workers to perform the large amounts of computation required for the graphics layout, which allowed us to render smooth visual updates as readers scrolled through the story. A challenge was making the swarms of dots change shape while retaining relative size to each other. Another was coding the swarms to best fit the displays on mobile and desktop, across variations in individual users’ devices. We achieved both these things, and the piece was served to substantial audiences both on our own platforms and on third-party apps (eg. Facebook, Apple News). "," The hardest part of this project was getting the copy to work with the graphics. Keeping the central construct of the $4 billion kitty throughout the piece was challenging — for example, we couldn’t represent amounts of money below $4 million as they would be less than one dot in size. We certainly couldn’t represent the amounts that people were willing to spend personally (eg. the $200 average).   It was also challenging to keep the graphics from becoming too 'samey' while sticking to the central concept so the piece would be visually coherent. For this reason, Colin Gourlay developed the ability for the clusters to take a different shape (eg. the sun, a battery). However it was hard to gauge the size of clusters once they were in a different shape to the circular kitty we were used to seeing. It took time to figure out how to make the dots fill a shape that was scaled to accurately represent its volume relative to other clusters.   It was also challenging to code the transitions from one big cluster to smaller clusters, as money was allocated to different things. We spent a month refining the coding and layout of this story, which was fiddly and time-consuming work. While the final product looks simple, it took a lot of revision to get there. As is so often the case with data journalism, achieving a simple and easy-to-understand final product takes significant work behind the scenes. "," Doing something about climate change is often framed as a massive challenge that humanity has yet to solve. This sort of thinking can give audiences a sense of powerlessness. We wanted to create a piece of content that showed that not only do solutions exist, but also there is a strong desire from the public to be a part of these solutions. By focusing on solutions, we turned climate change from a topic that readers are reluctant to engage with into one they found accessible and even empowering.   This story also shows the power of relating an abstract and insurmountable issue like climate change to people’s daily lives. By breaking it down to a figure — $200 — that felt tangible to people, climate action became something relatable and non-threatening. In Australia, conservative politicians and media have framed climate action as vastly expensive and “economy wrecking”. This piece countered this narrative by showing how a little bit can go a long way.   This story came out of the Australia Talks National Survey, which is dialogue journalism at its best. The survey was an epic listening exercise, one of the largest surveys of its type ever conducted. From the survey, we learnt that climate change was the most commonly cited problem facing Australians personally. This gave us the impetus to commission a range of top-tier digital stories on this subject.   This piece was an attempt to reflect Australians’ attitudes back to them. Rather than saying “you must do more to prevent climate change” in a finger-wagging fashion, we said “here’s what you’ve told us you want to do”. This two-way, listening-and-learning approach gave the piece a licence to propose solutions. ",https://www.abc.net.au/news/2019-12-17/what-youd-spend-to-halt-climate-change-and-what-you-could-get/11784704,https://www.abc.net.au/news/2019-10-06/australia-talks-explained/11570332,https://www.abc.net.au/australiatalks,,,,,"Nick Kilvert (reporter), Tim Leslie (digital producer), Annika Blau (digital producer), Colin Gourlay (developer), Simon Elvery (developer), Cristen Tilley (editor), Jonathan Webb (editor)"," The ABC News Story Lab is a team of 11 journalists, developers and designers using innovative digital storytelling to transform the Australian Broadcasting Corporation (ABC) from a traditional broadcaster to a digital leader. This story was a collaboration between Story Lab and the ABC’s Science unit. Environment reporter Nick Kilvert worked with Story Lab digital producers Tim Leslie and Annika Blau and developers Colin Gourlay and Simon Elvery to bring the data to life in a digestible and engaging way.    Story Lab's work encompasses immersive editorial experiences, visual journalism, data journalism and interactive news applications, all with the goal of pushing the boundaries of the best ways to tell powerful stories online.   Story Lab (along with the ABC’s Digital Story Innovations team) took out the top award of Most Outstanding Outfit for excellence at the 2019 Information is Beautiful Awards. Its work has also been recognised in the Society for News Design awards. ",,,
Australia,News Corp Australia,Big,Participant,Innovation (small and large newsrooms),RMIT University Data Journalism Pilot,09/08/19,"Investigation,Multiple-newsroom collaboration,Open data,Health","D3.js,Json,R,RStudio,Python","  In 2019, News Corp Australia and RMIT University trialled a unique and innovative data journalism internship program. Under the supervision of senior News Corp journalists, journalism and data science students used public data sets to find significant stories that were then published using News Corp’s one-to-many model direct to their websites and newspapers.       The collaboration produced a number of articles, such as the featured story 'Dosed Up and Dying', published last year in newspapers and online.         The published work and positive feedback from students and industry are evidence of the pilot program’s success and it will be extended this year.      "," This initiative generated articles, including 'Dosed up and Dying', which raised public awareness of prescription opioid use in Australia and provided, for the first time, suburb-level profiles of prescription rates, correlated with many other datasets.     It was significant for industry, allowing News Corp Australia to help develop crucial and necessary skills in journalism students and their own staff. The initiative brought new talent into the newsroom and helped introduce data science students to a career path they may not have previously seriously considered. This in turn potentially enriches the industry further.    For RMIT, journalism program educators are on the cutting edge of industry requirements and use this information to better equip the next generation of journalists. It provides students with experience in cross-disciplinary collaboration and the internships give students an invaluable insight into and experience in using these highly sought-after skills while working with senior journalists.    RMIT’s Associate Dean of Computer Science and IT Professor Lawrence Cavedon, welcomed  the opportunity for students to intern with such a large organisation and work with high-quality journalists, and students from another discipline. “These internships provided incredibly valuable experience, and one different and arguably richer than many other internship opportunities.”      'Dosed up and Dying' was the first step in an initiative that will have far-reaching implications for the development of both journalism and data science students at RMIT. "," The team, Hamish Lindsay (journalist) and Aaron Shankar (data scientist), downloaded readily available opioid prescription data from the Australian Institute of Health and Welfare and in subsequent weeks discovered systemic higher prescription rates in regional communities.  The suburb level prescription data used conformed to the Australian Bureau of Statistics SA3 standard, which enabled correlation of many other external datasets.  The team first reclassified every suburb whether it was located in a State capital or a regional population centre.   Shankar said the RMIT pilot found correlations “between prescription opioid rates in an area and the cancer incidence rates, socioeconomic status, arthritis and osteoarthritis rates and the median age of the population”.    “Regression and dominance analysis was conducted in order to gauge the relative amounts that these variables contribute to prescription opioid rates. This resulted in a number of potentially problematic areas to be highlighted including overprescribed areas and areas with a potential lack of access to adequate healthcare.   “The Python programming language was predominately used for data pre-processing, analysis, and visualisations tasks, while R Studio was utilised for a regression analysis. Simple spreadsheet tasks were completed in Microsoft Excel.”  Shankar and Lindsay produced dozens of visualisations to explore the data using the Seaborn library, swarm plots most effectively capturing the underlying story of difference.  Shankr also subsequently produced a research report, “Data Journalism Pilot - An Exploration Into Data Science in Journalism”, in which he recommended Australian media organisations do more to incorporate data journalism into newsroom workflows.   The pilot produced granular insights for every town and suburb in Australia, including relationships between:   Opioid prescription rates, net prescriptions, scripts per minute;   Median income;   Unemployment rates;   Remoteness;   Socio-economic quintiles;   Cancer rates and incidence;   Mortality rates;   Age distribution;   Musculoskeletal Diseases;   Arthritis numbers and ratios; and  Aged care beds, ratios per 100,000.           "," News Corp Australia Newsroom Operations Manager Peter Judd said the biggest challenge was “selling” in the concept of student hybrid data journalism teams to traditional newsrooms - and getting them to trust the results.  The RMIT pilot is effectively a disrupter, a cross-disciplinary approach where the schools of data science and journalism collaborate to tell stories embedded in data, then work with a network of senior newsroom journalists to make the story public.  The reality is that these skills do not exist in many newsrooms, so editors are cautious about “going out on a limb” in a leap of faith on something they struggle to validate.   This is part of a bigger problem where journalists, who pride themselves on speaking truth to power and holding institutions to account, are powerless to do so if they can’t interrogate the information themselves, but rely on the analysis of others.  The reality is that journalists will not become data scientists and data scientists will not become storytellers by confirming their narrative and talking to people.  In our pilot workflow, the data scientist finds the story lead, the outlier, the anomaly in the data, and the journalist then prosecutes the investigation, armed with unprecedented insights that often surpass the knowledge of experts in the community.  The issue of trust was also compounded by traditional community wariness of student journalism, making it much harder to secure the interviews and the time necessary to check the story leads on the gorund.  Overall, the challenge rested with merging the needs of the country’s biggest media organisation and the senior journalists with the academic requirements for students. It also required a complete rethink of the university's more usual Work Integrated Learning process and a commitment to genuine collaboration.     "," This project demonstrates the mutual benefits of strategic industry and interdisciplinary partnerships.     While journalism schools use many techniques to teach data journalism, research continues to call on educators to look at more inventive ways to cultivate skills.     The current literature shows there is much work being done by universities to bridge the gulf between journalism schools and computer science departments through the creation of research centres, developing joint degrees and co-teaching.     Some have also successfully used project-based assessments and industry collaboration to develop data journalism skills.    But the literature, including Weiss & Retis-Rivas, also calls for more research into innovative pedagogical practice.     Given universities are facing challenges producing graduates with data journalism skills, this conceptually innovative project strategically uses Work Integrated Learning as a vehicle to address current and future industry needs.     Journalism and data science academics consistently collaborated, working closely to nominate and select candidates - monitoring the process through feedback and assessments.         This has been combined with the extraordinary commitment of News Corp Australia to work with students to develop important stories along with the skillsets of participants with demonstrable results.      News Corp Australia Newsroom Operations Manager, Peter Judd has worked on-site with student teams based at RMIT.     In 2020, apart from increased student numbers, additional opportunities potentially include on the ground research and reporting.     This pilot has great significance broadly as a potentially useful tool that could be adopted by other universities who team with industry partners.     RMIT and News Corp Australia would be delighted to share details of this pilot with the international data journalism community.     ",https://www.dropbox.com/sh/qiun0c3r6uacrhl/AADyY4bEP9DWROpo2gYeuJhCa?dl=0,https://www.heraldsun.com.au/news/victoria/victorians-in-regional-centres-perscribed-strong-drugs-at-record-rate/news-story/6b4533691f9a6931ca3d0113eb34f7f3,,,,Surry Hills,NSW,"Hamish Lindsay, Aaron Shankar, Peter Judd, Sonja Heydeman","<ul>     Sonja Heydeman is a lecturer in the Graduate Diploma in Journalism at RMIT University. She's a journalist, broadcaster, editor, producer with more than 25 years of industry experience.          Peter Judd is  the Newsroom Operations Manager, News Corp Australia. He also leads News Corp’s data journalism team, codes in Python and has been a newspaper editor for more than 20 years.          Hamish Lindsay: RMIT Journalism post graduate student. University of Melbourne graduate with four years of content creation experience in games and tech journalism.          Aaron Shankar: RMIT Masters of Statistics and Operations Research, University of Melbourne Bachelor of Science, Mechanical systems.          Mandy Squires: Senior News Corp journalist currently working exclusively for the Sunday Herald Sun, Mandy Squires reports on a wide range of subjects and issues. A former News Corp National Network investigative reporter and correspondent, Mandy is also a former chief of staff of the Geelong Advertiser.     </ul>",,,
Colombia,"Datasketch, Transparencia por Colombia",Small,Participant,Best data-driven reporting (small and large newsrooms),Elecciones y Contratos (Elections and contracts),12/11/19,"Investigation,Explainer,Long-form,Multiple-newsroom collaboration,Database,Open data,News application,Infographics,Chart,Elections,Politics,Corruption,Money-laundering,Economy","Personalisation,Scraping,D3.js,Json,Adobe,Microsoft Excel,Google Sheets,CSV,R,RStudio,Node.js",Eleccions and contracts is a long form investigative and data journalism report with an additional news app to understand the relationship of political campaign financing and public contracts in Colombia. It was developed by Datasketch and Transparencia por Colombia as an effort to unveil possible corruption schemes in politics in Colombia and the incorporation of private citizens and companies in such schemes with the use of large volume open data. The special report features different stories and a web app to explore the public data. From death political donors to a big data app to help other newsrooms in Colombia.," Political campaign financing has many intricacies, and Colombia is no exception, several laws norms regulate political financing but unfortunately they are not always enforced because of lack of access to proper tools that handle big data. This is particular relevant because campaign financing data makes more sense to analyze in the context of other datasets. In our report we used hundred thousands of records from campaign financing and 8 million public contracts to explore their relationship through a series of stories and an interactive web app to explore the bulky datasets. The stories tell how we discovered irregular ""anonymous"" donors (we found death people being reported as financiers or citizens with social assistance financing candidates with millions) but most importantly we created a web app for other journalists and researchers to explore.        The special report was released in the midst of a series of debates around recent elections and how different government entities are using data to track corruption. One of the most important points of the project is that it put the issue of quality data access to debate among high level officials in Colombia to incorporate new technologies to track corruption. It also offered a new way to create communities of data savvy journalists to use these tools to research corruption topics in a more intuitive way. For instance, in the web app one can explore interesting connections such as that of a company who was sanctioned by corruption with around 100 million dollars, whose CEO is donated to the elected president in Colombia who later appointed the wife of the CEO as the equivalent of the secretary of state. "," The investigation started with the support of multiple civil society organizations for a pilot project developed jointly with the ministry of technologies in Colombia. After one year of data cleaning, organizing information and crossing multiple databases, including a custom built repository with the history of the most important corruption incidents in Colombia, the team at Datasketch and Transparencia por Colombia managed to put in a digestible format the complexity of the issue. We used data from multiple sources, from excel sheets, data collected by hand and database dumps from open contracting data, totaling more than 9 million registries that were compiled and organized using different R scripts that would be used to answer a series of questions proposed by law experts and journalists in multiple design thinking exercises. Special care with data cleaning algorithms and custom scrappers was done. Visualizations were built with custom R code to implement javascript visualizations in multiple libraries (d3, vis.js, highcharter, datatable) as well as an interactive web app with multiple javascript components using R and the Shiny web framework for data driven applications. Final report results was created using a static site generator called Hugo and code is hosted freely and open sourced on github. The design was made in Adobe Illustrator and then built into the website and web app. "," The most challenging aspects of this project were: first to understand the legislation of campaign financing and its implications the recent elections (presidential and congress) as different rules apply and second the data access and cleaning, that included very high volumes of data and a lot revision from the team.  The data available was truly important in terms of the insights only when it was crossed with multiple databases that came in very different formats (plain texts, excel sheets, database dumps, web services). Additionally, the exercises of design thinking we did with multiple stakeholders to finally end with an intuitive, yet useful design was fundamental as we covered individual stories, but also a web app for others to reuse the data we collected and prepared for other organizations. "," Other journalists can appreciate the storytelling in this project but also that many times it is necessary to implement a project as complex as this in partnership with multiple organizations. A very complex topic such as campaign financing can be boring and difficult to understand, especially to non-experts so bridging the gap between the stories and a news app can be very powerful. Finally, we are big advocates of using R in data journalism, it helps cover all of the data needs a small newsroom. From data access or scrapping, to data cleaning and even building the full interactive sites and web apps to host the project, all possible with R. ",http://especiales.datasketch.co/elecciones-y-contratos/,http://www.monitorciudadano.co/elecciones-contratos/campanas,http://www.monitorciudadano.co/elecciones-contratos,http://www.monitorciudadano.co/elecciones-contratos/descargas,,,,"Juan Pablo Marín Díaz, Mariana Villamizar Rodríguez, Camila Achuri, Juliana Galvis, David Daza, Sebastián Botero, Sandra Martínez, Camilo Peña, Ángela Rodríguez, Sergio Rocha"," Juan Pablo Marín Díaz and Camila Achuri are data scientists and Datasketch. Mariana is a designer and as a systems engineer at Datasketch. David Daza (Datasketch) and Sebastián Botero (Transparencia por Colombia) and Data and software Engineers. Cualitative research, law and in depth reporting was done by Sandra Martínez, Camilo Peña and Sergio Rocha (political scientists and political systems researchers at Transparencia por Colombia) and Juliana Galvis (political scientist at Datasketch) ",,,
Australia,ABC News (Australia),Big,Participant,Best visualization (small and large newsrooms),How does your income compare to everyone else's?,21/05/19,"Explainer,Quiz/game,News application,Crowdsourcing,Infographics,Chart,Map","Personalisation,D3.js"," ""How does your income compare to everyone else’s"" is an interactive tool which offers users a surprising look at their own position on the income spectrum.   Users are asked to guess where they sit and then enter their weekly income. The tool reveals whether or not they were correct — most were not, in line with recent research.   After establishing the reader’s position, the interactive takes them on a visual journey which compares them to the very richest Australians — the 3.84 per cent who earn above $A156,000 a year, or $A3,000 a week, before tax. "," Over a million people viewed the piece across ABC platforms, and it was widely shared on social media. In an environment where many Australians likely felt hard-done-by without wage increases for many years, the tool provided them with a perspective that they were unlikely to see otherwise.   Recent research shows that people are terrible at estimating where they sit in terms of money relative to others. Poorer people tend to overestimate their relative worth, whilst richer people tend to underestimate. This is related to our psychological tendency to see our own position as average, and our inability to see the bigger picture.   This was backed up by the data we collected when people used our tool. Only 11 per cent of respondents correctly guessed within the range of their income bracket. The vast majority of people underestimated their position — in short, they were relatively richer than they thought they were. We were able to produce another piece of original work using this data, which told an interesting story about our audience.   The most telling thing about the success of the project is that the data which underpins it (the 2016 census) had been available for over a year for anyone to look at. All we did was take the data and put it in an easily digestible format which told the reader a surprising personal story which was well-visualised and easy to understand.   In the months after our story, many other news outlets published similar explainers, both in Australia (eg. <a href=""https://www.smh.com.au/politics/federal/think-you-re-well-off-this-is-how-you-compare-to-everyone-else-20190712-p526qb.html"">Sydney Morning Herald</a>) and overseas (eg. <a href=""https://www.nytimes.com/interactive/2019/08/01/upshot/are-you-rich.html"">New York Times</a>). ","<ul>   Data analysis : We used the Australian Bureau of Statistics’ Census Table Builder to extract all of the data from the 2016 census, including cross referencing income data with demographic data. From here, we used simple Excel spreadsheets to tease out the most interesting findings.    Personalisation : We asked users for only three simple inputs — their weekly take-home pay (after tax, to keep it simple), their postcode or suburb, and a guess of their position. From these simple inputs, we were able to tell a completely personalised story which had national significance.    Scrollyteller design : We utilised the user's scrolling to step them through the geographical data, allowing words and visualisations to be presented in a perfectly complementary design.    Data visualisation : Simple but tried and true techniques — choropleth maps and dumbbell charts — with a personal touch.  </ul>"," The team had endless discussions about the design of the tool, how it would function, and how it would present its results. Deciding how to tell the geographical story was especially fraught — Australia is an enormous country with a geographically spread population. The question was, how could we make the national picture relevant to them?   The centrepiece of our visualisation was a choropleth map of Australia, colour coded with the proportion of high income earners in each Local Government Area of Australia. The problem we had here was a common one with choropleths of Australia — the big picture often obscures data in smaller areas.   To get around this, we built a ‘scrollyteller’ interactive which steps readers through the relevant information to them, moving them and zooming around the choropleth and highlighting details. The user enters their postcode or LGA, and is presented with the statistics and rankings for their area and their state, as well as the top and bottom LGAs for the measure. As the reader proceeds, data relevant to the statistics they are reading is highlighted; irrelevant areas melt away. It allowed us to show both the national and personal picture all at once, and counter the psychological predisposition towards the smaller picture in favour of the bigger one. ","<ul>   The power of personalisation : Legacy news organisations often focus on the bigger picture because technology in the past did not offer opportunities to personalise. People love to be told a surprising story about their own situation, and how they compare to others — technology and new ways of telling stories can facilitate this. You just need to think outside the box.    Just because data can be accessed, doesn’t mean it’s accessible : The underlying data was available for more than a year online for anyone to access. If people wanted to, they could search through it to find out where they sat. But who has the time for that, or even the knowledge that it’s possible? We simply took the legwork out of the task and presented it well. The learning here is that data doesn’t always need to be exclusive or embargoed, it just needs to be interesting.    As users access your data, they’re also creating data : If you’re asking users to enter data, be prepared to capture it as it can create good exclusive follows. Using the data collected from our readers, we were able to generate another story, read by another 280,000 people about how people are terrible at estimating their relative income wealth. This in turn drove even more people back to the original story.  </ul>",https://www.abc.net.au/news/2019-05-21/income-calculator-comparison-australia/9301378,https://www.abc.net.au/news/2019-07-02/income-inequality-australians-dont-know-their-place/11161124,,,,,,"Reporting: Matt Martino, Design: Ben Spraggon, Development: Joshua Byrd, Editor: Matt Liddy, Editor: Cristen Tilley"," This story is a product of the ABC News Story Lab. Story Lab is a team of 11 journalists, developers and designers using innovative digital storytelling to transform the Australian Broadcasting Corporation (ABC) from a traditional broadcaster to a digital leader.   Story Lab's work encompasses immersive editorial experiences, visual journalism, data journalism and interactive news applications, all with the goal of pushing the boundaries of the best ways to tell powerful stories online.   Story Lab (along with the ABC’s Digital Story Innovations team) took out the top award of Most Outstanding Outfit for excellence at the 2019 Information is Beautiful Awards. Its work has also been recognised in the Society for News Design awards. ",,,
United States,Texas Tribune,Small,Participant,Best data-driven reporting (small and large newsrooms),Texas police can seize money and property with little transparency. So we got the data ourselves.,06/07/19,"Investigation,Multiple-newsroom collaboration,Database,Fact-checking,Illustration,Infographics,Chart,Map,Crime","QGIS,Json,PostgreSQL,Python", Civil asset forfeiture allows law enforcement to seize money and property from someone without necessarily charging them with a crime. A Texas Tribune project analyzed 560 cases from four Texas counties to shine a light on how the controversial practice is used across the state. ," This investigative project was built on exhaustive data-collecting in four Texas counties to find out how Texas law enforcement uses a powerful tool: civil asset forfeiture. We decided to focus on four counties that represented different areas of the state: urban, mid-sized, rural and border. The data came from public records covering more than 500 civil and criminal cases from 2016.   Law enforcement officials often claim that civil asset forfeiture helps them go after big criminal organizations. In reality, officers often seize small amounts of cash or property from regular people — typically as a result of traffic stops. Twenty percent of seizures did not result in a criminal charge, and about 40% of the citizens who lost property were never convicted of any crime. And the majority did not challenge the seizure, often because they couldn’t afford to do so and were not entitled to a court-appointed lawyer.   We have not seen any direct impact in terms of changes by law enforcement or state lawmakers. However, as stated, our findings challenge the dominant narrative — that civil asset forfeiture targets big-time criminals rather than ordinary people. A sole legislative hearing on this topic was held in April, before the article was published.. Legislation to regulate civil asset forfeiture and prevent abuse was introduced in the 2019 legislative session but failed; it is possible, especially with continued attention, that the topic will reemerge when the Legislature meets again in 2021. "," Because Texas law enforcement agencies don’t make information on civil asset forfeiture cases readily available, this was a very data-heavy project. Only one of the four counties we examined made those case files available online; the other three kept only hard copies in their county clerk’s offices. We had to create our own database from hundreds of individual court cases and travel to several parts of Texas because records were not available electronically, and they were not free. We then had to clean up the data and resolve missing or incomplete items before starting our reporting.   The developer on this project used the data to create a Django database, which he used to collaborate with reporters and analyze the data. We built charts that tell the story based on that data. This project was published with the Tribune’s open-source <a href=""https://github.com/texastribune/data-visuals-create"">development kit</a>, which is built in node. Our art team created illustrations and assisted with page design.   In addition to our published story, the Tribune hosted two public panel discussions on civil asset forfeiture — in June and September — that featured journalists, a county prosecutor, a sheriff and advocates for reforming the system.  "," The hardest part of this project was reporting and gathering the data as explained above. The lack of transparency we encountered makes it difficult for journalists, watchdogs and concerned people to learn the truth about how civil asset forfeiture is being used. The fact that we were only able to gather this information with extensive travel and organize it using specialized skills shows that, in the age of easily accessible electronic records, Texas counties have a long way to go in making civil asset forfeiture data accessible. "," Texas has 254 counties, so we chose the four counties that we reported from very carefully to represent different facets of this law enforcement practice in the state.   We regret not using a database immediately to gather the information — we lost time by translating spreadsheets into the data.   We have not found any other project that has created a database from scratch to reveal how this tool is used by law enforcement.   Additionally, other reporting has relied mainly on statewide asset forfeiture statistics collected by the federal government. Those numbers don’t reveal important details like how often forfeitures are connected to criminal charges. County-level statistics can provide important detail. ",https://apps.texastribune.org/features/2019/texas-civil-asset-forfeiture-counties-harris-webb-reeves-smith/,,,,,,,"Jolie McCullough, Acacia Coronado and Chris Essig", Jolie McCullough reports on criminal justice issues and policy for The Texas Tribune.   Acacia Coronado was a Tribune investigative reporting fellow.   Chris Essig builds data visualizations and news apps at the Texas Tribune.  ,,,
United States,Texas Tribune,Small,Shortlist,Best visualization (small and large newsrooms),Is Texas really going purple? Our Heat Index shows how competitive your district was — and is.,06/04/19,"Explainer,Illustration,Infographics,Chart,Map,Elections,Politics","Personalisation,D3.js,Json,Adobe,Creative Suite,Google Sheets,Python,Node.js", We set out to explore whether the 2018 election results marked a new trend toward more competitive general elections or a one-time swerve away from the steady quarter-century pattern of Republican dominance in Texas. ," The Tribune used voting data to explore whether the 2018 elections marked a new trend toward more competitive general elections or a one-time swerve away from the steady quarter-century pattern of Republican dominance. We created our own “Heat Index” to clearly visualize which districts were competitive — and which weren’t. Users can also look up an address and see that district’s competitiveness in different races over time. This project received tremendous feedback from readers, both political insiders and otherwise, who said they appreciated what the analysis told them about which way Texas could vote next. "," This project was built with d3.js and the Tribune’s open-source <a href=""https://github.com/texastribune/data-visuals-create"">development kit</a>, which is built in node. We also used an in-house <a href=""https://github.com/rdmurphy/scroller"">scrolling library</a> for the narrative and district lookup tool for the customization. The development process involved lots of iteration and experiments with visualization techniques. The geography of the districts doesn’t matter nearly as much as how they have voted, so we decided to display that change over time as clusters of circles.     "," This project was developed by an intern at a time of enormous change for our organization — the team that she was part of was temporarily reduced from four to two full-time staffers, and they were both busy covering a legislative session. But, we knew that this story, which involved visualizing a data index developed by Ross Ramsey, our in-house expert on Texas politics, was important. As a publication dedicated to statewide reporting, we were the only organization that was going to tell the story. A rigorous editing process led us to a simple and clear visualization that readers greatly appreciated. "," Interns are full team members who can do great work, given the opportunity! And, while a map may be geographic and a traditional choice, it isn’t the best way to visualize change in political districts. ",https://apps.texastribune.org/features/2019/texas-turn-blue-voting-pattern-history/,,,,,,,Shiying Cheng and Ross Ramsey," Shiying Cheng studied Computational Data Journalism for her Post-Baccalaureate at Columbia Graduate School of Journalism and Political Science for her Bachelor’s at Colorado College. She previously interned at The Wall Street Journal.   Ross Ramsey is executive editor and co-founder of The Texas Tribune, the only member-supported, digital-first, nonpartisan media organization that informs Texans — and engages with them — about public policy, politics, government and statewide issues. He writes regular columns on politics, government and public policy. ",,,
Taiwan,CommonWealth Magazine,Big,Participant,Best news application,"Guide to 2020 Taiwan Presidential Election: Interactive Maps, Polling Results, and Bot-generated Voter Analysis",30/12/19,"Breaking news,Database,Open data,News application,Infographics,Chart,Video,Map,Elections,Politics","AI/Machine learning,Personalisation,D3.js,QGIS,JQuery,Json,Google Sheets,CSV,Python,Node.js"," 2020 election played a crucial role in Taiwan's democracy. The race between Presidential Tsai and KMT candidate Han Kuo-Yu reflected people's decision to vote for pro-China or pro-America stance.   The project covers four sections, including Presidential Election Interactive Map, Bot-generated Voter Analysis, Policy Watch, and Polling Results Explained to provide comprehensive election reportage, to provide comprehensive data analysis: Which constituencies elected pro-China candidate? These constituencies were leaning towards the KMT party (blue) or DDP party (green)? What about the voter demographics? "," Since published in December 2019, the project generated 3.6 million pageviews within two weeks, bounce rate was lower than 20%, and pages per session reached 2.6 pages. Those numbers suggested that readers had engaged highly with innovative news content.   Among other things, Bot-generated Voter Analysis compiles all the demographic data (such as age, education, median income, and gender ratio) in 22 cities/counties, 368 towns, and 7761 villages as well as cross-compares election data of party vote share and voter turnout. It totally produced 8151 short pieces of data analysis and generated more than 2 million pageviews within 24 hours after poll results released.          <div id=""gtx-trans"" style=""position: absolute; left: 106px; top: 132px;""> <div class=""gtx-trans-icon"">    "," Data source: Central Election Commission, Ministry of Finance, Ministry of the Interior, Central Weather Bureau (all the data come from government's open data)   Data processing: php, python, nodejs, QGIS (for shp files)   Visual design: d3.js, jQuery   Infra: Google Cloud Platform   To give journalists easy access to maps, the developer created an additional function: users can key in ""Konami Code"" (↑↑↓↓←→←→BA) in the desktop version of Presidential Election Interactive Map to download SVG files and then key in ""gg"" to close the function.   More information：http://bit.ly/2RXKJSK "," During 2020 election, Taiwan saw a surge of disinformation and fake news. The public lost trust in news outlet, even show obvious hostility to them. The hardest part of the project was that how we can rebuild trust between media and readers.   We decided to hand over the right of news analysis and interpretation to readers. This was the first time we mainly focused on data-driven interactive map and bot-generated analysis rather than journalists reporting since Commonwealth Magazine was founded 38 years ago. By means of data visualization and bot-generated analysis, readers were capable of perceiving and interpreting the implication of those hard data.    With the help of CrowdTangle, we observed that readers would apply these content and share their own thoughts about election results on Facebook, Instagram, and Twitter. For example, Han Kuo-Yu Supporters rushed to explain in which constituencies KMT had lost. Tsai Ing-wen supporters surprisingly found that their neighbors didn't vote for Han Kuo-Yu. "," With interactive maps and bot-generated data analysis, readers could recognize immediately the changes of pan-blue and pan-green vote share and voter demographics; moreover, they were allowed to identify the shifting constituencies in 2020 election with one click.   2020 election also included legislative election. Policy Watch section scraped 78-thousand social media posts from 433 candidates' social media accounts (Facebook, Instagram, and YouTube). We selected 318 keywords based on 15 critical issues (such as same-sex marriage, energy, and cross-strait relations) in hope of presenting candidates' arguments and stance on these issues and providing guiding reference to voters. ",https://web.cw.com.tw/2020-taiwan-presidential-election/index.html,https://storage.googleapis.com/www-cw-com-tw/ckeditor/202002/ckeditor-5e38e1b665e14.jpg,https://storage.googleapis.com/www-cw-com-tw/ckeditor/202002/ckeditor-5e38e3680947d.jpg,http://bit.ly/2RXKJSK,,,,"Yu-Hsin Lee, Wei-Han Chi, Yi-Wen Lin, Chia-Yu Peng, Ya-Wen Pai, Shih-Yen Chen, Chen-Hua Chen, Li-Hsun Tsai, Kai-Ting Tsao"," CommonWealth Magazine was founded in 1981 and has actively invested in digital transformation in these five years. This project consists of four journalists and editors, one front-end developers, two back-end developer, and two web designers. ",,,
United States,Texas Tribune,Small,Participant,Best visualization (small and large newsrooms),Most migrants cross at the Texas border. Here's how the flow of people intersects with Trump's policies.,09/10/19,"Explainer,Cross-border,Open data,News application,Infographics,Chart,Map,Politics,Immigration,Human rights","Scraping,D3.js,QGIS,Adobe,Creative Suite,Google Sheets,Python,Node.js"," Since President Donald Trump took office in 2017, his administration has tried to curb migration at the southern border. Most migrants cross into Texas — here's how the flow of people intersects with Trump’s ever-shifting policies. The Tribune continues to update the tracker monthly with the latest available data. "," This project was designed during the summer of 2019, when a huge surge in migration sent federal officials scrambling to change U.S. policies and stem the flow. We set out to use data to measure the impact of those policies, with a focus on Texas, where the majority of people cross the border. We also wanted to improve our workflow for publishing a regular graphic breaking down migration levels in stories, such as <a href=""https://www.texastribune.org/2019/10/18/trump-asylum-policy-immigration-impact/"" style=""text-decoration:none;""> this one </a>. Reader reaction to the project was positive; many appreciated how the data visualizations gave them extra context in understanding a complex and fast-moving issue. "," This project is powered by a Python scraper that fetches new data from U.S. Customs and Border Protection each month. The Texas border map was created with ai2html, and the data changes to reflect monthly border crossing totals for each sector. The two charts were made with d3.js, which makes regular updates easy. The entire project is built on the Texas Tribune’s open source development environment, which is based in node. "," Collaboration with researchers who study this data and reporters and editors to understand how to best contextualize the flow of migrants was essential. The data also contained critical flaws: Specifically, the Border Patrol reports numbers of “inadmissibles,” people presenting themselves at ports of entry, in some parts of the data but not in others. In order to present the most complete picture of migration, including a breakdown of children arriving alone and families arriving together, we had to use data with and without inadmissibles and describe each chart correctly.   "," Building and regularly updating a tracker graphic is not easy, but it’s worthwhile for topics that newsrooms cover frequently. We implemented a scraper and built this graphic with regular updates in mind, which means spending more development time with tools like d3.js. It’s also easy for anyone on the Tribune’s data visuals team to run an update. This graphic includes an embeddable version, which saves us time making the same migration chart each month when our border reporter writes about the latest numbers.   Mandi Cai tells stories with code and graphics as part of the data visuals team. Previously, she created dashboards for scientists at BioBright, a Boston-based biotechnology company, and visualized defense data for Defense Footprint, a project contextualizing the United States' international military presence. She graduated from Brown University in 2017 with a concentration in neuroscience, focusing on the intersection of cognitive science and design.       Connie Hanzhang Jin, a recent graduate of the University of North Carolina, was the Tribune’s data visuals fellow in 2019. At the Tribune, they covered migration and water policy. Connie now works at NPR. Previously, they worked as a design assistant at Carolina Union Communications and Creative Services and was an intern for Clarkston Consulting and Chapel Hill/Durham Magazine. In 2018, Connie traveled to Puerto Rico to work as the web developer for the multimedia documentary storytelling project ""Aftermath: Puerto Rico Rebuilds After Maria,"" which won a student award from the Online News Association. ",https://apps.texastribune.org/features/2019/migrant-texas-border-trump-policies/,https://www.texastribune.org/2019/10/18/trump-asylum-policy-immigration-impact/,,,,,,Mandi Cai and Connie Hanzhang Jin," Mandi Cai tells stories with code and graphics as part of the data visuals team. Previously, she created dashboards for scientists at BioBright, a Boston-based biotechnology company, and visualized defense data for Defense Footprint, a project contextualizing the United States' international military presence. She graduated from Brown University in 2017 with a concentration in neuroscience, focusing on the intersection of cognitive science and design.       Connie Hanzhang Jin, a recent graduate of the University of North Carolina, was the Tribune’s data visuals fellow in 2019. At the Tribune, they covered migration and water policy. Connie now works at NPR. Previously, they worked as a design assistant at Carolina Union Communications and Creative Services and was an intern for Clarkston Consulting and Chapel Hill/Durham Magazine. In 2018, Connie traveled to Puerto Rico to work as the web developer for the multimedia documentary storytelling project ""Aftermath: Puerto Rico Rebuilds After Maria,"" which won a student award from the Online News Association. ",,,
United States,Texas Tribune,Small,Participant,Best visualization (small and large newsrooms),"Texas had seven mass shootings over 10 years. Meanwhile, gun control has loosened statewide.",11/12/19,"Explainer,Infographics,Chart,Elections,Politics,Crime,Gun violence","D3.js,Google Sheets,Node.js"," Texas saw two mass shootings within a month in 2019, and the Tribune set out to explore what steps the state has taken to prevent the next tragedy. This timeline combines 10 years of Texas Tribune-University of Texas poll results, data on shooting incidents (including fatalities) and significant related actions taken by lawmakers. "," The headlines repeat themselves:    <a href=""https://www.texastribune.org/2019/08/05/hispanics-terrorized-after-el-paso-shooting-and-racist-manifesto/"" style=""text-decoration:none;""> A racist manifesto and a shooter terrorize Hispanics in El Paso and beyond </a>.    <a href=""https://www.texastribune.org/2019/08/06/texas-gun-laws/"" style=""text-decoration:none;""> Here's everything you need to know about Texas gun laws </a>.    <a href=""https://www.texastribune.org/2019/08/07/trump-considers-red-flag-laws-texas-lawmakers-have-blocked/"" style=""text-decoration:none;""> President Trump is talking about red-flag laws. Texas lawmakers have blocked those bills in the past. </a>    <a href=""https://www.texastribune.org/2019/09/01/texas-rep-matt-schaefer-says-no-gun-restrictions-twitter-thread/"" style=""text-decoration:none;""> After West Texas shooting, Texas House Rep. says ""NO"" to gun restrictions </a>.*   Texas saw two mass shootings within a month in 2019, and the Tribune set out to explore what steps the state has taken to prevent the next tragedy. This timeline combines 10 years of poll results, data on shooting incidents (including fatalities) and significant related actions taken by lawmakers. Polls consistently show that Texans are divided about gun control — with 40% to 50% saying they want stricter gun laws — while all but one of the laws passed over the past decade by the state’s Republican-controlled Legislature have expanded where guns are allowed, who can have a firearm in schools and the right to openly carry guns.   This project increased the transparency around the major shooting incidents and changes in gun laws. Should another mass shooting occur in Texas, the Tribune will be able to update and provide this information.    *Source: The Texas Tribune headlines "," With this project, we sought to combine and contextualize three related sets of information over a decade: news reports on mass shootings, records of gun-related legislation that passed and statewide poll results. This means that the greatest challenges were related to design — it’s a timeline, plus much more. The project incorporates d3.js and html bar charts. It’s built upon the Tribune’s open-source <a href=""https://github.com/texastribune/data-visuals-create"" style=""text-decoration:none;""> development kit </a>, which is built in node. "," This project required a huge amount of research and reporting. The reporters reached out to lawmakers and advocacy organizations involved in each piece of gun legislation to learn about their motivations and what they think of the outcomes.  The three sets of information we used were inconsistent, and led to design challenges when there were years with less data or no data. It also challenged us to think about how the end user could best navigate the piece.  "," Sometimes a simple timeline is the best design option, and projects like this are mainly exercises in sourcing and research.    Most gun policy comes from the federal level, so we tried our best to focus on things that are in the purview of the state of Texas and left out federal laws. Other outlets may want to do the same for their state. ",https://apps.texastribune.org/features/2019/texas-10-years-of-mass-shootings-timeline/,https://texaspolitics.utexas.edu/polling-data-archive,,,,,,Mandi Cai and Stacy Fernández," Mandi Cai tells stories with code and graphics as part of the data visuals team. Previously, she created dashboards for scientists at BioBright, a Boston-based biotechnology company, and visualized defense data for Defense Footprint, a project contextualizing the United States' international military presence. She graduated from Brown University in 2017 with a concentration in neuroscience, focusing on the intersection of cognitive science and design.       Stacy Fernández is The Texas Tribune’s breaking news reporter. When she isn’t breaking stories, Stacy covers the gun beat and leads the Tribune’s reader-driven explainer series, Texplainer. First introduced to the Lone Star State as an intern at The Dallas Morning News, she was also a News21 fellow, reporter for NPR’s Next Generation Radio Project and Buffalo News intern. Stacy graduated from Syracuse University with dual degrees in magazine journalism and Latino-Latin American studies.  ",,,
United States,American Civil Liberties Union,Small,Participant,Best visualization (small and large newsrooms),Wealth-Based Driver's License Suspensions: Driven to Poverty,14/11/19,"Explainer,Chart,Video,Map,Women,Human rights","D3.js,Adobe,CSV,R,RStudio","This piece visually presents the stories of two women whose driver's licenses were suspended due to being unable to afford their traffic fines and fees. The clients' experiences expose how South Carolina's punitive, wealth-based driver's license suspension system makes it harder for people who are already financially struggling pay their debts and care for themselves and their families. Wealth-based driver's license suspension ris a racially skewed legal system where people who cannot afford traffic tickets are punished more harshly than people who have the resources to pay. We illustrate the burden our clients face in their daily lives and fight"," It is the ACLU's stance that wealth-based driver’s license suspensions violate the U.S. Constitution -- the Fourteenth Amendment rights to due process and equal protection of the law require that one's inability to pay is considered before being punished for the nonpayment of a court fine. Additionally, due to longstanding racial disparities in poverty and wealth, Black South Carolinians are disproportionately harmed. Black people make up 48% of those with driver’s license suspensions for failure to pay traffic tickets while making up only 27% of South Carolina’s population. The ACLU sued South Carolina and North Carolina in two lawsuits: <a href=""https://www.aclu.org/cases/white-v-shwedo"">White v. Shwedo</a> and <a href=""https://www.aclu.org/cases/johnson-v-jessup"">Johnson v. Jessup</a>. These lawsuits seek to end the suspension of driver's licenses without access to a proper hearing and without first determining that individuals can afford to pay. We are currently waiting to hear back from the courts. Through this interactive visual story, we hoped to illuminate just how burdensome everyday and necessary activities, like caring for a child or driving to work, might be for the many thousands impacted by wealth-based license suspensions in South Carolina, and we sought to visualize just  why  our legal stance on this is so important.      "," We used R to analyze the overall impact of South Carolina's policy of automatically suspending driver's licenses for unpaid traffic tickets, as well as the racial disparities in who this policy affects. We used d3.js, scrollama.js, Vue and Adobe Illustrator to design and create the interactive graphics that tell our clients' stories.   Our process involved iterating on mock-ups of the two visualizations (using Figma) and building (and re-building) the interactivity between design feedback sessions. We were lucky to have access to a photographer who was able to capture even more of our clients' stories through portraits and videos.  "," Our teams are fairly new to the ACLU, and this was the first collaborative (and published) effort between the Analytics and Product Technology teams. This was also our first interactive ""scrollytelling"" piece on ""graphics.aclu.org"" and was created shortly after a website redesign. Creating visualizations without an existing system in place (and with a new technology stack!), was a challenge at times. For some of us, this was our first time using the Vue framework combined with d3.js.    Additionally, deciding on how to visualize  burden  in the right way for each client was a process. Each client had a separate and distinct story despite both having their driver's licenses suspended because they couldn't afford their traffic tickets. Janice, for instance, spent much more time and money getting to work and to simple errands every day. At first, we thought about showing a map of the  almost impossible  public transit paths she takes now compared to her old driving routes, but we didn't feel that comparison in map form effectively showed a burden. We ultimately landed on using the length of lines and the number of stop points to both show how complex and long her commute has become. Emily's world, on the other hand, simply got smaller without the use of a car -- she could no longer get to certain locations and had to give up work, family visits, and her children's extracurricular activities. After many iterations, we agreed that the effect of removing locations on a map one after another would visually represent just how much smaller her world has become.     Though challenging at times, we are proud of the way we illuminated two individual stories to bring light to a much larger problem.   "," There are almost always multiple ways to visualize similar effects.  We  learned how important it is to brainstorm at least three ways to visualize something before getting too deep into one. Lastly, it can be easy to overlook individual stories behind large amounts of data, but it's often important to bring these stories to the forefront; we hope that others are inspired by our use of ""small data"" to visually represent the burden endured by just two people in a (hopefully) powerful and unique way.  ",https://graphics.aclu.org/south_carolina/,,,,,,,"Sophie Beiers, Rhonda Friberg, Brooke Watson, Nusrat Choudhury and Allen Tan"," For nearly 100 years, the ACLU has been fighting for our individual rights and liberties guaranteed by the Constitution. The team that built this project is a scrappy group of analysts, designers, engineers (and one lawyer!) that found our way to the ACLU in 2018 because we believe in its mission and believe that data and journalism have critical roles to play in that mission moving forward. Our team is passionate about bringing quantitative evidence to identify injustices and demonstrate disparate impacts on communities, both to bolster our lawyers' legal arguments and to bring these issues to the forefront.     ",,,
United States,The Washington Post,Big,Participant,Best data-driven reporting (small and large newsrooms),2C: Beyond the Limit,13/08/19,"Explainer,Environment","Animation,3D modelling,D3.js,QGIS,Adobe,Creative Suite,R,RStudio"," The Washington Post’s  2C: Beyond the Limit  series was grounded on a simple idea: let’s tell the story of what is, not what might be or could be.    It fundamentally reshaped the climate debate by showing that extreme warming is no longer a worry for the future – it is, in fact, here, with daunting consequences. Our analysis showed that ten percent of the planet has already warmed by 2 degrees Celsius (3.6 degrees Fahrenheit), a critical threshold for global warming.  "," The Post data analysis identified the globe’s fastest warming places based on an internationally recognized threshold. We built a framework and then let the analysis itself determine the stories. The stories have themselves generated widespread coverage in regions across the globe affected by extreme warming, and have shifted the way top policymakers and scientists approach the issue.     Senior environmental officials in South America approached Uruguay’s Omar Defeo and Argentina’s Alberto Piola after they were quoted in our story about the hot spot identified off Uruguay’s coast. The Uruguay-Argentina Rio de la Plata Commission, which sets offshore policy for this region, has held two workshops on the topic and Piola has presented to the group. Andrew Mueller, general manager of the Colorado River District, instructed his entire staff and board to read our initial piece after it identified several of the counties under his jurisdiction as ones that have already warmed 2 degrees C since the late 1800s.     Outlets across the country and the globe either reprinted our story verbatim or wrote their own versions, using our findings. This included small and regional papers in California, Colorado, Florida, Maryland, New Jersey and Texas.      The piece on Alaska led the Anchorage Daily News’ homepage for an entire day.   Our stories on hot zones were translated into languages ranging from Spanish to Ukrainian, and reprinted in foreign outlets across multiple continents, including some of the biggest papers in Argentina and Uruguay. (https://tsn.ua/nauka_it/porig-nezvorotnih-zmin-temperatura-v-ukrayini-za-100-rokiv-zbilshilas-na-dva-gradusi-i-bilshe-1409682.html and https://thebabel.com.ua/news/35547-serednya-temperatura-povitrya-v-ukrajini-za-ostanni-sto-rokiv-zrosla-na-dva-gradusi-i-prodovzhuye-rosti and https://www.9news.com.au/world/qatar-installs-air-conditioning--to-deal-with-heat-and-prepare-for-world-cup-2022/d46d0100-0195-4960-ba36-fea195409f76)     The series’ initial article garnered nearly 1 million page views, and was widely shared by readers, presidential candidates, climate activists and professors. Our audience stayed immersed in the stories for long periods of time, reading several stories for nearly three minutes. "," The data analysis was performed in R using the Raster package maintained by Robert Hijmans. R was used to read raw data, calculate means, find the area experiencing certain temperature increases and perform linear regressions, as well as output data in a format suitable for mapping. The maps were created using QGIS and Adobe Illustrator. Interactive graphics were programmed in JavaScript using D3.js. "," Learning how to work with massive global temperature datasets to produce scientifically sound results, selecting the methodology to represent temperature change, and finding clear language to explain it to our readers were some of the biggest challenges we faced along the way.    We decided we needed to write detailed methodology sections at the bottom of the articles (<a href=""https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-america/#methodology"">1</a> and <a href=""https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-world/#methodology"">2</a>), and to integrate discussions of data quality and uncertainty directly into the stories when necessary. They added transparency to the data that was the backbone of this series, and they set it apart. This mindset in handling of data culminated in our graphics-focused, profoundly explanatory, piece ""How we know that global warming is real,"" which used a three-dimensional model, multiple animations, and an archival 18th century ship's logbook to demonstrate how global temperature data has emerged over time and how it has been used.   We performed thousands of linear regressions for this series, mastered raster GIS and spatial data, and mapped gridded temperature estimates at resolutions as fine as five square miles. We located every official land and ocean temperature report going back to 1701. Much of this drove us along a steeply inclining learning curve for our climate reporters and national editors alike.     "," The ability to bridge two worlds - data and journalism - is no small task. This was a project of impressive scope and ambition, and the presentation of the data managed to convey both magnitude and immediacy, and demonstrated that extreme climate change is already a life-altering reality across 10 percent of the Earth’s surface. Our work was scientifically advanced, but the results were simple to understand.   Something to note and keep in mind: While the data lies at the heart of our 2C series, it is the places and the people who live in them that reveal the damage climate change is wreaking in the hot spots. Our reporters captured these altered places and lives with profound grace and clarity.  ",https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-america/,https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-world/,https://www.washingtonpost.com/graphics/2019/national/climate-environment/thermometers-climate-change/,https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-alaska/,https://www.washingtonpost.com/graphics/2019/world/climate-environment/climate-change-qatar-air-conditioning-outdoors/,https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-siberia/,https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-california/,"Brady Dennis, Juliet Eilperin, Darryl Fears, Chris Mooney, Steven Mufson, John Muyskens, Harry Stevens, Monica Ulmanu, Trish Wilson"," The team that worked on this series was formed by reporters on the graphics desk (John Muyskens and Harry Stevens) and journalists on the climate and environment desk (Chris Mooney, Brady Dennis, Juliet Eilperin, Darryl Fears, Chris Mooney and Steven Mufson). Editing by Monica Ulmanu and Trish Wilson. ",,,
Australia,ABC News,Big,Participant,Best data-driven reporting (small and large newsrooms),The rise of red zones of risk,23/10/19,"Explainer,Long-form,Database,Infographics,Chart,Video,Map,Environment,Business,Economy","Animation,Personalisation,D3.js,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,CSV,Node.js"," This project shows how changing climate risks (such as bushfire, flood, subsidence, inundation and extreme wind) are set to impact the cost and availability of insurance between now and the year 2100.    Based on an analysis of every address in Australia (15 million), the project visualises the projected impact for almost every suburb in Australia in numerous ways.   It also includes an interactive search that allows users to explore and compare the data for their local area.    "," By putting proprietary data into the hands of the public, this story empowered a general audience to participate in a debate that directly affects every resident of Australia, but that had previously been dominated by a handful of powerful insurance companies and big banks.   It injected a level of transparency and accountability into debates about rising insurance premiums, and enabled “ordinary Australians” to understand and explore the numbers and calculations that drive wider business decisions about their homes and neighbourhoods     By building an interactive tool that gave users the opportunity to “drill down” into detailed, 80-year projections of climate risk for their local area, this story opened up the world of climate risk, insurance and property prices to an entirely new audience.   This information is critically important not only to banks and insurance companies faced with mounting costs, but governments and homeowners making critical decisions about which parts of Australia will be safe to live in an increasingly unstable future.   It's importance has also been highlighted by the recent unprecedented bushfire season, which has destroyed numerous properties and again focused attention on the urgent question of how to insure against extreme weather disasters. "," Maps of Australia pose significant problems for data visualisation because we have densely-populated coasts and a vast, sparsely-populated desert interior. This means the “zoomed out” view of the country is visually dominated by areas with almost no inhabitants, while the tiny regions that are the most important to the most people are next to impossible to see.   Due to this problem, our team built a plug-in to step users through a video of an SVG-based choropleth map. This “guided tour” allowed us to highlight Australia’s worst-affected areas even if they were geographically small.   We also created a tool that allowed users to:  1. Search the dataset for personalised projections for their local area, and   2. Compare their local area to the projections for neighbouring areas.    This required setting up a database in Firebase to perform a three-way match with the user’s suburb or postcode and their larger geographic region (what most users think of as their “neighbourhood”).     Each suburb contained either partially or fully within this larger “neighbourhood” was displayed on an interactive line chart. Users could use this line chart as both a visual comparison tool or exploration tool to find detailed projections for other suburbs either in their “neighbourhood” or elsewhere in Australia.   The graphs associated with the suburb search were built using the D3 library. A custom React wrapper was used to group neighbouring suburbs. With nearly 11,000 suburbs included in the dataset, finding solutions for animating and exploring the visualisation required creative solutions that didn’t crash the browser.   Data organisation, cleaning and blending was done in Excel and Tableau Prep. Analysis and “proof of concept” visualisations (including the maps, scaterrplots and area charts) were done in Tableau Desktop.    "," At the heart of the story was a proprietary dataset intended for technical users in the insurance, banking and risk analysis industries. The hardest part of the project was working out how to distil this vast, multi-dimensional, technical dataset into a compelling narrative for a lay audience.   It took weeks of discussion and further analysis to come up with a “headline” measure that would immediately convey the importance of the data whilst still being meaningful and accurate.   This was achieved through a process of brainstorming what we thought readers would want to know from the data, then working out if we had the variables/calculations to provide an answer. For example:  <ul>  The key question: “How much could my insurance premium rise?” could be answered by calculated the percentage change in the average risk fraction across all five hazards   “Is my house likely to become uninsurable?” could be answered by examining the number and percentage of addresses in a suburb that exceed the 1% threshold for average risk fraction across all hazards   “What are the climate risks in my local area?” could be answered through a breakdown of average or total risk cost for each of the five hazards  </ul>  Once we had identified the what readers may want to know and the relevant measure to answer their questions, we brainstormed potential visuals for presenting the data, listing the strengths and weaknesses for each, including whether interactivity would help or hinder the readers’ search for relevant information (see attached “Visuals list”).    This process enabled us to decide on a combination of interactive and static graphics, which first gave readers a “big picture” understanding of the issue, before allowing them to “drill down” into the data to find insights relevant to their specific location.    "," This story is an outstanding example of data-driven explanatory reporting. It showcases multiple ways of visualising and explaining complex, technical data to a general audience, including:       Making data explorable, so they can find areas relevant and interesting to them       Re-framing potentially dry, “boring” concepts like insurance calculations and risk analysis in ways that immediately convey their importance to individual readers       Using different visual forms (e.g. maps, scatterplots, area charts, line charts, radar charts, etc.) to explore different “stories” within the data; and choosing the appropriate visual form for each story       Creative approaches to handling vast amounts of data       Reaching users where they are (in our case, optimising all our content for users on mobile phones. These users now make up more than half our entire audience.)     ",https://www.abc.net.au/news/2019-10-23/the-suburbs-facing-rising-insurance-costs-from-climate-risk/11624108,,,,,,,"Inga Ting, Nathanael Scott, Alexander Palmer, Michael Slezak"," Inga Ting is a data journalist with the ABC’s Digital Story Innovations team.   Nathanael Scott is a developer with the ABC’s Digital Story Innovations team.   Alexander Palmer is a designer with the ABC’s Digital Story Innovations team.   Michael Slezak is the ABC’s national science, technology and environment reporter. ",,,
Australia,Australian Broadcasting Corporation,Big,Participant,Innovation (small and large newsrooms),Australia Talks,10/06/19,"Explainer,Cross-border,Multiple-newsroom collaboration,Quiz/game,News application,Podcast/radio,Crowdsourcing,Infographics,Chart,Video,Audio","AR,Personalisation,D3.js,Microsoft Excel,R,Python"," With Australia Talks, the ABC created a first-of-its-kind initiative that delivered unprecedented insights into the thoughts, feelings and everyday lives of Australians.   Partnering with Vox Pop Labs, the ABC created a National Survey with a representative sample of 54,000 Australians. The data informed:  <ul>    a <a href=""https://australiatalks.abc.net.au/"">personalised, multilingual news application</a> that let Australians see how they compared with everyone else;       a live TV special that explored insightful, surprising findings;       <a href=""https://search-beta.abc.net.au/#/?query=australia%20talks&page=1&configure%5BgetRankingInfo%5D=true&configure%5BclickAnalytics%5D=true&configure%5Banalytics%5D=true&configure%5BenablePersonalization%5D=true&configure%5BfacetFilters%5D="">hundreds of stories</a> across broadcast and digital;        community engagement events across Australia and a <a href=""https://www.abc.net.au/life/nominate-your-kindness-hero/11676496"">social impact campaign</a>.    </ul>  Australia Talks sparked important conversations about climate change, mental health, national identity, sex, and more. "," Australia Talks aimed to reach out to the Australian community and start conversations about issues that matter. An impact study found that 30 per cent of Australians engaged with the project.  <ul>    The <a href=""https://australiatalks.abc.net.au/"">Australia Talks news application</a> was completed by more than 450,000 people.       The National Survey informed more than 90 online articles, with 15 million pageviews and 41 million engaged minutes.       More than 40 featured broadcast stories were created for key TV and radio programs.       ABC accounts shared more than 750 social posts, creating more than 2 million engagements and 1.1 million video views.    </ul>         The survey    The National Survey was the first study of its kind — a huge sample size of 54,000 across almost 500 questions. The data (aggregated and anonymised) is going to be made publicly available in 2020.          Audience focus    The project led to targeted content offerings for: Culturally and Linguistically Diverse people, 18 to 34-year-olds and those living in the suburban corridors of Australia’s biggest cities. Content was translated into Simplified Chinese, Bahasa Indonesian, Arabic and Vietnamese to reach those audiences.          Community Engagement and social impact campaign    Dozens of community engagement events including outside broadcasts and community meetings were held, providing important opportunities for content teams to talk with local communities about what matters and help inform story commissioning.          Innovative content    As part of the initiative, teams were invited to pitch innovative content ideas for funding. The content created from this small fund was some of the most engaging from the project, and these experiments have had an ongoing impact on the way journalism is conducted at the ABC. Key funded projects included <a href=""https://www.abc.net.au/news/2019-10-09/tarneit-suburb-on-melbourne-fringe-feeling-growing-pains/11537562"">community embeds</a> for news reporting teams, a <a href=""https://www.facebook.com/72924719987/posts/10160295389869988/?substory_index=0"">series of cartoons</a> tailored to speak to CALD Australians' experiences, <a href=""https://www.facebook.com/abcnews.au/videos/vb.72924719987/425268581469157/?type=2&theater"">social videos for Chinese Australians</a>, and powerful <a href=""https://www.abc.net.au/news/2019-10-27/australia-talks-tara-schultz-drugs-abuse-welfare-quarantine/11639602"">first-person opinion pieces</a>. ","  Survey    The Australia Talks National Survey of 54,000 Australians was an epic undertaking, two years in the making, conducted in conjunction with social scientists and data scientists at Vox Pop Labs.   We started by crowd-sourcing areas of concern for Australians through open-text questions put to thousands of Australians from across the political and sociodemographic spectrum. With an academic advisory panel, we then designed 500 questions that tested the attitudes and behaviours of Australians.   The massive sample size provided sufficient statistical power to permit granular inferences into differences across demographics such as sex, age, and geography. Pre- and post-stratification techniques were applied to the sample in order to ensure that inferences derived from the survey were representative of the population and sub-populations of interest.        Data analysis    Techniques such as raking and multi-level regression and poststratification were used in order to model the data so as to render findings representative of the Australian population. Weights were derived using the Australian census estimates for sex, age, education level, language spoken at home, geographic region, state, and 2019 Australian federal election vote.   Data analysis and cleaning was conducted using a wide array of tools, including R, python, Tableau and Excel.        News application    We created a personalised, interactive tool that let 450,000 Australians answer a subset of the survey questions, so they could find out exactly how they compared to the rest of the nation. Each individual received one of 574 sextillion possible personalised results pages — all available in English, Simplified Chinese, Vietnamese and Arabic. The application was built using the latest web technologies, which allowed for a sophisticated, seamless user experience. The tool's results pages used a range of data visualisation and storytelling techniques to help the user understand their own identity in the larger narrative of the Australian community. "," This project was the first of its kind for the ABC: developing an entirely new 496-question survey, an engaging digital tool which serves personalised results with 574 sextillion result combinations available in four languages and using these results to inform engaging and meaningful journalism for multiple audiences.      Ensuring that the survey maintained research rigour but framed questions in a way that was interesting and meaningful for a mass audience was an involved and challenging process.   Partnering with University of Melbourne and a core group of seven academic advisors from universities across the country for the National Survey added rigour and credibility to the survey but also further levels of complexity.       Developing a cross-platform, multi-team content strategy with associated community engagement and social impact campaigns was new and complex. Our storytelling teams in digital, radio and television needed data and insights specific for their audience but we had to ensure that the overarching content proposition as a whole made sense to the audience all the while holding back some of the most interesting insights for the live show meant there were many competing priorities to juggle. The eight-week campaign was grouped around 14 topics (e.g. gender, discrimination, cost of living, climate change) to manage this with data and insights released to teams along with a master planning document covering all content planned for the campaign — more than 90 digital stories, 40 across radio and television with dozens of flow radio programming spots across the country. ","  1. The power of listening to and engaging with your community    Australia Talks involved extensive community engagement — from its earliest stages through to completion. The survey was developed using an array of crowdsourcing techniques to ensure it tackled topics of interest and relevance to people from a diverse range of communities.   The ABC also held dozens of community events across the country — from our big cities to tiny towns — ensuring people from all walks of life understood the goals and findings. That was further extended with an impact campaign that took the issue which most united people in the survey (the need for respect), and invited people to share their own experiences.        2. How to make data stories entertaining as well as informative    The Australia Talks National Survey consisted of nearly 500 questions, presented to a total of 54,000 people. The ABC's Factual and Entertainment teams were set the unenviable challenge of turning those dry numbers into 90 minutes of live television for a mass audience.   They created a show that was informative, surprising, insightful, funny — and challenging. The program attracted a younger, unique audience for the ABC, and held them across the 90 minutes with a rich combination of data visualisation mixed with real human stories. The approach provides much that data storytellers around the world could take their cues from.        3. How partnering across disciplines can extend your journalism    The ABC has a long history of delivering important journalism. But journalists are not trained in the latest methods of social science or public opinion research.   To develop such a sophisticated project, it was necessary to marry the ABC's journalistic prowess with specialists in social science and data science. Australia Talks proves that the combination of journalism with social science can deliver powerful results. ",https://australiatalks.abc.net.au/ (online tool page),https://australiatalks.abc.net.au/results/2ba047c5-e507-42cb-add5-71a723645701 [indicative results page],https://vimeo.com/388880381 (password: Australiatalks) [Australia Talks live show TX 18 November 2019],https://www.abc.net.au/news/2019-11-15/sex-dating-and-prejudice-why-we-are-a-nation-sharply-divided/11694038 [digital story using data],https://www.abc.net.au/news/2019-12-17/what-youd-spend-to-halt-climate-change-and-what-you-could-get/11784704 [digital story using data],https://www.abc.net.au/news/about/backstory/digital/2019-10-24/australia-talks-your-questions-answered/11608434 [explainer],https://www.abc.net.au/life/annabel-crabb-and-leigh-sales-share-their-kindness-heroes/11704014 [kindness campaign],Australia Broadcasting Corporation and Vox Pop Labs,"  Australian Broadcasting Corporation  is Australia's national public broadcaster. The ABC provides Australian stories and conversations across radio, television, online and mobile services throughout metropolitan and regional Australia and overseas through ABC Australia and Radio Australia. The ABC provides informative, entertaining and educational services that reflect the breadth of our nation.    Vox Pop Labs  is a <a href=""https://bcorporation.net/about-b-corps"">B Corp</a> operated by academics and based at McMaster University in Canada. Vox Pop Labs specialises in the application of digital technology and data science to foster democratic participation and civic engagement.    At the ABC, the Australia Talks project was led by Matthew Liddy (editor, ABC News Story Lab), Julie Hanna (managing editor, Factual), Nick Hayden (managing editor, Entertainment) and Natasha Banks (projects lead, Content Ideas Lab). The Vox Pop Labs team was led by founder and CEO Clifton van der Linden. They worked with teams across the ABC, academic partners at the University of Melbourne as well as a multi-institutional panel of academic advisors to deliver Australia Talks. ",,,
Australia,ABC News (Australia),Big,Participant,Best visualization (small and large newsrooms),See how global warming has changed the world since your childhood,12/06/19,"Explainer,Long-form,Illustration,Infographics,Chart,Politics,Environment","Animation,D3.js"," ""See how global warming has changed the world since your childhood"" is an interactive story that gives the audience a personalised look at the change in temperatures in their lifetime, how this change has already impacted the world around them, and what the future could hold. By asking the audience to select the year they’re born, we framed the impact of climate change in the context of their own life. This makes the story immediately relatable and emphasising the reality that climate change is not just a problem affecting the future — but also the present. "," The story was incredibly successful when published, quickly reaching a large local and international audience. It resonated with readers on a deeply personal level, and is an excellent example of how personal and emotive data visualisation can be.   After the piece was published it was <a href=""https://climatefeedback.org/evaluation/abc-article-effectively-illustrates-important-climate-trends-for-australian-readers-tim-leslie-joshua-byrd-nathan-hoad/"">reviewed by four scientists for Climate Feedback</a>, who estimated its overall scientific credibility to be 'very high'.   On social media it has been shared widely by readers, who have spoken about how it transformed their understanding of climate change, and by expert science communicators, who praised how effective it is in conveying the key concepts and consequences of climate change.   “I wish that I could convince every politician in the world to take five minutes for this interactive. It shows how climate change has impacted the world since your childhood, and peeks into the future.” The Planetary Security Initiative (Netherlands Ministry of Foreign Affairs) - Twitter   “This is the most important thing to read as we enter the 2020s. Shocking... plus if we don’t address climate change now, by 2070, Sydney will routinely reach temperatures in the 50s.” Dr Kate Devitt - Twitter   “This is some of the most powerful, informative, effective journalism about climate change and what can still be done that I’ve ever seen. As the fires rage and Canberra doubles down on batshit crazy I’m showing this to every single person I know… This really was like a bright shining beacon in the smoke haze that is our absolutely soul draining news feed.” Sophie Black - Twitter   Two months on it has amassed over 950,000 pageviews and continues to be read and shared by audiences. Published against the backdrop of the bushfire crisis in Australia, it provided a compelling explanation of the very effects that are devastating the country before our eyes. "," The story's visualisation techniques build on the work of climate scientist Ed Hawkins, who with his 'warming stripes' showing temperature, has helped powerfully and simply convey the increase in global temperatures.   At the heart of the story is a dataset of changes in temperature — historical temperature anomalies in Australia, and future projections of Australian temperature anomalies under different emissions scenarios.   We take that data and use a range of visualisations — a temporal heatmap for the stripes that then transitions into a bar chart — to create a timeline of the reader’s past and possible future.   By annotating and customising the range of the visualisations, we place the reader at the heart of this narrative, in a deliberate strategy to force them to interrogate the common statement that ‘we had this weather when I was a child’.   It is dynamically generated by referencing a file that contains annual temperature anomaly against the reader’s year of birth. We use a JavaScript library called D3 to draw and transform the coloured bars on the page that represent the temperature anomaly.   This is driven by a custom-developed format, which ties the transitions of the visualisation to the user's scroll position, so that they appear just as the relevant text is on screen.   For the historical data we used the Australian Bureau of Meteorology’s climate variability & change time series graphs. For projections we turned to Dr James Goldie, a climate knowledge broker from the <a href=""https://monash.edu/mcccrh"">Monash Climate Change Communicaton Research Hub</a> and the <a href=""https://climateextremes.org/"">ARC Centre of Excellence in Climate Extremes</a>.    Dr Goldie used the <a href=""https://climexp.knmi.nl/start.cgi"">KNMI Data Explorer</a> to run multi-model means (32 models, one member per model, masked to Australia) of different temperature projections based on the Intergovernmental Panel on Climate Change projections for best and worst-case scenarios. "," The challenge for our story was to shift the audience’s understanding of climate change from an abstract future problem to something that is deeply personal and is already impacting their lives.   The challenge with personalisation is to be able to create a story that speaks to an individual, without overloading the amount of information they need to give. We navigated this by asking for a single piece of information — their year of birth — and then used that to personalise the story.   But we also wanted to highlight the impact on the people our readers share their lives with.    By using a six-year-old, and a child born today, we were able to give audiences something to project their own experiences on without having to gather more personal information. This strategy was born out on social media, where many people referenced their children when sharing the article.   When reporting on an issue as contentious as climate change it is vital that the scientific interpretation be accurate. To achieve this we collaborated with the Monash University Climate Change Communications, commissioning a custom calculation of modelling of future projections.    The story had an extensive fact-check process that resulted in a 10-page document, and our interpretations and visualisations were pored over by a team of climate scientists at the ARC Centre of Excellence in Climate Extremes. This resulted in a piece of work that has both impressed climate scientists and communicators, but also become a resource for audiences to help share their understanding with other people — something that is evident in the patterns of sharing on social media.   The key to this success is the work that went into ensuring the science was clearly articulated, and the framing that kept this science at the heart of the narrative. "," Often reporting on climate change is viewed as a challenge of informing the audience, but the success of this project shows how important marrying the hard science of climate change to the daily lives of our audience is in creating stories that help them connect. Our audience feedback was couched in terms of hope and fear, and offered an emotional outlet for readers while further informing them of the realities of climate change.   This story also offers an example of how, with the right framing and context, scientific data can be explained to the audience in a simple and effective way. In particular it shows the value in taking the time to explain a visualisation to the audience, and then returning to that same visualisation to make multiple editorial points and tell a complex story.   It also is a good example of creating a personal story that leaves space for the audience to fill with their own experiences and narratives. Too often with personalisation it is easy to fall into the trap of covering for all eventualities, which can weigh down the narrative and complicate the story. By putting the work in to think through how the audience would engage with the story, and who they were, we were able to use techniques (such as the comparison to a six year old) that would offer them a context they could apply to their own lives without needing to ask for more information. ",https://www.abc.net.au/news/2019-12-06/how-climate-change-has-impacted-your-life/11766018,,,,,,,"Tim Leslie (Reporter/Producer), Nathan Hoad (Designer/Developer), Joshua Byrd (Developer), Ben Spraggon (Designer), Emma Machan (Illustrator), Cristen Tilley (Editor), Matt Liddy (Editor)"," The ABC News Story Lab is a team of 12 journalists, developers and designers using innovative digital storytelling to transform the Australian Broadcasting Corporation (ABC) from a traditional broadcaster to a digital leader.   Story Lab's work encompasses immersive editorial experiences, visual journalism, data journalism and interactive news applications, all with the goal of pushing the boundaries of the best ways to tell powerful stories online.   Story Lab (along with the ABC’s Digital Story Innovations team) took out the top award of Most Outstanding Outfit for excellence at the 2019 Information is Beautiful Awards. Its work has also been recognised in the Society for News Design awards. ",,,
Australia,ABC News,Big,Participant,Best visualization (small and large newsrooms),"Rich school, poor school: Australia's great education divide",13/08/19,"Investigation,Explainer,Database,Crowdsourcing,Infographics,Chart,Map","Animation,Personalisation,Scraping,D3.js,QGIS,Canvas,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,OpenStreetMap,Node.js"," This project used exclusive income and spending data for every school in Australia to show for the first time exactly how big the divide between rich and poor schools has grown.    By ranking schools according to income, then analysing each school’s capital spend over five years, we were able to illustrate the enormous wealth gap between public and private schools.   We combined this first-of-its-kind analysis with powerful visual storytelling techniques to explicitly connect the data with the reality most readers directly experience via schools in their neighbourhoods.     ","The topic of school resourcing is widely seen as dry, complicated and impenetrable, with both ""sides"" of the debate tending to rely on well-worn, emotive arguments, rather than facts and figures. This project injected credible facts and figures into one of Australia's most protracted and polarising debates. The combination of ground-breaking data analysis and compelling visuals allowed the story to achieve a degree of cut-through rarely managed by stories about this topic. At the core of this investigation is My School data – one of Australia's most tightly-held datasets on school resourcing. Numerous organisations have tried to obtain this data and requests for the data have only ever been granted to a handful of researchers under strict conditions not to further disseminate the data. ABC News is the first media outlet to successfully scrape the full dataset. To verify the data, we worked closely with a number of academic researchers who were familiar with it. Our analysis is the first of its kind, and the most comprehensive and detailed to date. It compares the individual finances of nearly every school in Australia, and then analyses this data to reveal the powerful connection between sector (Government, Catholic and Independent) and school funding. Due to distorted spending patterns pre-2012, this was the first year such an analysis was possible. This story put a closely-guarded government dataset into the hands of the public, allowing them to ""drill down"" into the data and see, for the first time, exactly how their school's income and expenditure compares. This stirred vital debate about transparency and accountability in school As one expert says in the story: ""The Federal Government had much of this information but it had never been made public before… And now you can see how little gets spent on government schools compared to non-government"," For the opening visualisation, we combed annual reports, school newsletters, fundraising brochures, P&C bulletins, campaign videos, architectural plans and development applications for dozens of schools   The technical challenge of visualising 8,500 dots – particularly for our large mobile phone readership – required creative design and development responses. In particular, we chose not to allow users to interact with the visualisation, instead curating a “guided” visualisation that highlighted specific schools that are either well-recognised or representative in the dataset.   We used online survey tool Screendoor to crowdsource data on the capital funding needs of schools across Australia. Data included photos and personal accounts from dozens of school principals, P&C committees, teachers, students, former students and parents. This proved invaluable for reaching out to under-resourced schools, which are often prevented from speaking to media by the education departments or non-government sector authorities   We also built a searchable database to allow users to drill down into the data and find “personalised” information for their school. This included an interactive map, that allowed users to compare public and private funding and capital expenditure for any given school and its neighbours across the nine-year dataset.   Data was scraped using Chrome browser extension webscraper.io. Data organisation, cleaning and blending was done in Excel and Tableau Prep. Analysis and “proof of concept” visualisations were done in Tableau Desktop.    "," The first challenge of this project was collecting the data. The second was finding the most rigorous way to use it. The third – and most difficult challenge – was working out how to transform a vast and detailed (but also potentially “dry” and boring) financial dataset into an arresting story that would cut across the deep ideological divide on school funding.   Our visual language had two main aims:     To help readers make the explicit connection between the data and their local schools   To show how each of the 8,500 schools fit into the bigger picture; a story previously untold because the only data previously available were summary statistics.      We initially built the data with svg using D3.js, but due to the large volume of schools represented, we opted to flatten the bubbles in the visualisation to ease the load on browsers. This ultimately led to a more streamlined, guided experience.    The intention of opening with a monolithic ‘beeswarm’ scroll was to invoke sense of scroll fatigue for the user, physically articulating the staggering gap between the top and lower income schools.    To counteract the considerable length of the visualisation, we highlighted breadcrumbs of information along the way, showcasing promotional material, crowdsourced photos, GIFs, 3D fly-throughs, time-lapse videos and satellite imagery. This was accompanied by an overview of the school’s spending, including income, capital expenditure, and government capital expenditure.   The final product was a unique story that could not have been adequately told in any other way.    "," This story showcases how data-driven techniques can be applied across the entire spectrum of news-gathering and storytelling to propel a ground-breaking investigation.   The skills and techniques other could potentially learn from this project include:       Making data explorable, so users can find information that is relevant and interesting to their personal situation       Connecting the personal and the impersonal – that is how to link “impersonal” data to the personal (and often emotional) reality of users’ lives and choices (such as where to send your child to school)       When does interactivity serve the audience (and when should you abandon it)?        The pros and cons of crowdsourcing data       Scraping, organising and verifying large datasets       Visualising a complex, detailed or technical analysis for a general audience       Creative approaches to handling vast amounts of data       Reaching users where they are (in our case, optimising all our content for users on mobile phones. These users now make up more than half our entire audience.)         ",https://www.abc.net.au/news/2019-08-13/rich-school-poor-school-australias-great-education-divide/11383384,,,,,,,"Inga Ting, Nathanael Scott, Alexander Palmer", Inga Ting is a data journalist with the ABC’s Digital Story Innovations team.   Nathanael Scott is a developer with the ABC’s Digital Story Innovations team.   Alexander Palmer is a designer with the ABC’s Digital Story Innovations team.     ,,,
Armenia,Investigative Journalists NGO,Small,Participant,Open data,Making Armenian Politically Exposed Persons' Money Public: an Online Database of Armenian PEPs' Assets,22/07/19,"Investigation,Solutions journalism,Database,Open data,Corruption",Node.js," The official asset disclosure system of politically exposed people (PEP) in Armenia was a website of thousands of disorganized, unsearchable, non-machine-readable files where finding information, combining and drawing comparisons was challenging not only for taxpayers but also for journalists. This project has created a website called  data.hetq.am  which is an organized, searchable, downloadable and interactive database of PEPs’ asset and income declarations which is a useful tool to increase the public interest towards accountable governance in Armenia. Additionally, we mapped PEPs' affiliates by combining voters registry, companies registry data, as well as information from social networks.  "," Data.hetq.am contains almost 2500 profiles of PEPs from Armenia’s legislative (206), executive (214) and judiciary (576) powers as well as their affiliates (1494). We mainly receive feedback from politically exposed people included in the database.    In addition to the database, the website has an articles section where stories about PEPs’ suspicious assets are published. Data.hetq.am has received feedback and clarifications from the officials about whom the articles were published. For example, in an article about Bagrat Adamyan, the Deputy Head of the General Department of Special Investigation Service and Election Crimes Investigation Department, it was described that he bought an apartment worth 50,000 USD when his declared funds did not exceed 10,000 USD. After the publication of the article Bagrat Adamyan contacted data.hetq.am and clarified that there was a mistake in the asset and income declaration form, and he purchased the apartment on a mortgage loan.   Data.hetq.am has received feedback from the journalistic community as well. On September 27, 2019, during the local “Tvapatum (Digistory) – Stories About Change 2019” awards ceremony Honorable Mention went to data.hetq.am for covering the assets of Armenian government officials and persons linked to them. "," We developed scripts to scrape the information from PDFs and non-machine-readable files of the official asset disclosure website (ethics.am) and convert it into open data. Meanwhile, we used programming languages and web design techniques to publish the database in visually appealing ways. Each official has their profile at data.hetq.am, with their biography, affiliate map, cash schedule, movable and immovable property, as well as information on securities, loans, real estate, and income.    At the top of the website's home page, there is a search box and classification filters. We can go to a specific official’s profile by writing the name in the search box. If we are interested not only in one person but in the trends, we need to use filters. There are four filters: it’s possible to filter down the information by year, by income, by connection count, by cash, by ascending or descending order. All features on the website are reusable which means that our fellow journalists can simply copy the embed code and place it on their websites. "," Facebook Graph Search was a great help for the team to quickly and accurately indicate PEPs’ affiliated people. However, in June 2019 Graph Search was withdrawn by Facebook. After that, twice as much time is needed to get the same findings because most of the operations previously done by the algorithm are now being done manually.   Another limitation was the incompleteness of officially published Asset and Income Declaration Forms that prevented the integration of a statistical formula calculating the transparency rate of PEPs’ assets. It was planned to develop a formula to count the transparency rate of officials based on their asset and income declarations. The formula could work if complete, accurate and enough amount of data was inserted. Unfortunately, some of the public officials failed to submit complete asset declarations as a result of which the team decided not to calculate PEPs’ transparency rate. "," By creation and promotion of  data.hetq.am  project, we put the idea of open data and PEP accountability into the discussion in a country where most of the state institutions do not keep, organize and analyze reliable data. We have received numerous questions from politicians asking why do we collect and publish data about them. We patiently explained to them the importance of open data in establishing accountable governance.    Besides, the team regularly presents  data.hetq.am  and the idea of open data in secondary schools, colleges, as well as in different professional meetings with fellow journalists, international organizations and policymakers.  Data.hetq.am  has been presented during one of the Organized Crime and Corruption Reporting Project's (OCCRP) conferences, also at CFI Media Development Forum in Paris.  ",https://data.hetq.am/,,,,,,,"Ani Hovhannisyan, Samson Martirosyan, Artak Kolyan, Tigran Baghdasaryan"," Ani Hovhannisyan is a master’s candidate in Journalism at West Virginia University.  She is a Fulbright scholar and interned at International Center for Journalists in the Muskie Internship  Program.    Ani is from Yerevan, Armenia where she works as a data journalist team lead and a project manager at Investigative Journalists NGO that publishes Hetq.am Online News Portal. She covered issues related to healthcare and education.     ",,,
China,The Paper,Big,Participant,Best data-driven reporting (small and large newsrooms),How Chinese Seafood Giant Zoneco Lies about Its Scallops,17/12/19,"Investigation,Explainer,Business",Microsoft Excel," Zoneco is one of the most famous aquaculture companies in China, not because their products are of great qualities, but rather due to a series of scandals it has mired in. When we talk about this company, we always know that something is wrong there. But not many of us can exactly point out the problems. And this is where our project comes in handy. "," This is one of the projects of a series of coverage about Zoneco by the Paper. Shortly after the publishment, CSRC sent out an inquiry to Zoneco, which requires the company to explain the problems that pointed out by us.  "," With the help of previous media coverage, CSRC(China Securities Regulatory Committee) investigative reports, and Zoneco's financial reports, we have the ability to finally understand what Zoneco did behind the scene and explain it to readers in the most intuitive way. First, we showcase the Zoneco's stock chart from 2007 to 2019, with annotations of many significant events that happened during the years, such as mass die-off in October 2014 because of a cold water mass, and other repeated scallop crop failures in recent years.    After that, we explain how Zoneco plays with the numbers to polish its financial report. On the chart, we highlight how normal scallops crops count and how Zoneco reduces its actual catchment area to make its profit number look beautiful. Then, we demonstrate how Zoneco lies about its spot surveys in the sea--where its ships have been and where they haven't. By reading all these interactive graphics, even readers who haven't read any financial reports before would fully understand the secrets behind Zoneco's numbers.  "," Our goal is to point out factual and logical flaws of Zoneco's reports, and restore the reality under the flawed numbers.    The hardest part is to sift through hunderds of CSRC(China Securities Regulatory Committee) investigative reports, and Zoneco's financial reports, and find out the numbers that tells a consistent story, and point out Zoneco's lies with its own numbers. "," Zoneco's scandals were like TV dramas over the years. Most readers can spot the absurdity of Zoneco's claims: scallops starved to death, or scallops escaped. But it's hard to understand what actually happened.   By reading all these interactive graphics, even readers who haven't read any financial reports before would fully understand the secrets behind Zoneco's numbers.  ",http://h5.thepaper.cn/html/zt/2019/12/zzd/index.html,,,,,,,"Kong Jiaxing, Du Haiyan, Wang Yasai, JiangXin‘er, Hang Shengjiang, Li Yuequn, Lu Yan"," The Paper data news team focuses on data-driven stories and explanatory storytelling. The team consists of 20 people: half are data journalists, and half are graphics designers of all sorts, from information design to 3D modeling.   We cover stories on tight deadlines; We measure policy effects by data; We stretch ways to get data suitable for our stories; We try to push the envelope and use suitable storytelling for valuble stories. We also collaborate constantly with our English-language sister publication, Sixth Tone, and published some of our projects in bilingual. ",,,
Austria,Addendum.org,Small,Participant,Best data-driven reporting (small and large newsrooms),How accessible are our lakes?,15/07/19,"Investigation,Infographics,Chart,Map,Satellite images,Lifestyle","Sensor,D3.js,QGIS,R,RStudio"," Private versus public access to lakeshores is a hot topic in Austria. It's relevant both for residents close to the lakes and for the tourism sector. There are vast differences in public access to lakes and regular discussions on the topic, but no one had taken a data-based look at the issue and made numerical comparisons. We wanted to answer the question: how does the share of publicly-accessible shore compare between similar lakes? ", Local newspapers reported on our story and new public discussions were ignited. , We walked Austria's 11 largest lakes – as close as possible to the shore – and used a GPS app to mark the beginning and end of publicly-accessible areas. We then cleaned up the geodata using QGIS and visualized it using an Instagram-Story-inspired mobile-first format. We also researched the regulations regarding public access and compared them across regions. ," The data collection was a boots on the ground effort, and quite straining in parts, since some lakes are in quite mountainous regions.  It was also the team's first project where we collected our own geodata, so getting that process up and running was new territory for us. "," Collecting the data missing for an ongoing public discussion can be worth it, especially if it's geodata for locally relevant topics. ",https://www.addendum.org/seezugang/welche-seen-zugaenglich-sind/,https://www.addendum.org/seezugang/welche-seen-zugaenglich-sind/,,,,,,"Danijel Beljan, Stefanie Braunisch, Gerald Gartner, Markus Hametner"," The Addendum data team produces personalized, interactive journalism in Vienna, Austria. ",,,
France,Contexte,Small,Participant,Best data-driven reporting (small and large newsrooms),European election: reality check on political promises,05/09/19,"Explainer,Long-form,Database,Fact-checking,Infographics,Elections,Politics","Json,Google Sheets"," During the 2019 European elections, French candidates made a lot of campaign promises, arousing skepticism among EU experts. Indeed, many of the policy measures they put forward were out of touch with the functioning of the institutions. Some of them were utterly impossible to implement given the treaties in effect.    Contexte is a digital-only news outlet reporting on public policies at the French and European levels. The six journalists of the Brussels bureau gathered 420+ measures found in the platforms published by the eight lists of candidates leading the polls. They reviewed them one by one, and assessed their feasibility. "," The result of this unprecedented evaluation was published by batch in the weeks leading up to the election. <a href=""https://twitter.com/i/moments/edit/1224383318592053248"">On Twitter</a> and other social platforms, the story drew attention of various media pundits, social science researchers, EU operatives, elected officials and other political junkies.    Many readers pointed out a worrying trend: 21% of the measures were rated ""extremely difficult"" to implement, 15% ""already in effect or already scheduled"", 11% ""out of scope"", meaning that the EU is not currently in charge of their field.    Heartened by the positive feedback, Contexte's team decided to lift the paywall a few days before the election. A total of 13,000+ unique visitors spent on average 4m 31s on the page. Rounded up, the short comments published for each measure form a 23,300+ words long text corpus.   A few weeks into the project, a TV news team reached us to reuse the data we produced. We sat down with them to explain our methodology, and the thoroughness of our analysis encouraged them to confront the top tier candidates with our conclusions. They had to explain on camera why so much of their agenda was unfit to the position of member of European Parliament they were running for. <a href=""https://www.francetvinfo.fr/replay-jt/france-2/20-heures/video-europeennes-2019-des-propositions-hors-sujet-dans-les-programmes_3453981.html"">The story aired on ""L'œil du 20 heures"",</a> the fact-checking segment of France 2's popular evening news.    This project also strengthened the reputation of Contexte as an expert and independent source of information on public policies. It validated our ambition to report on the European election with a European perspective, where so many French media still keep the focus on national political issues. "," We used <a href=""https://www.dropbox.com/s/t12e3sq4qaznbum/Capture%20d%E2%80%99%C3%A9cran%202020-02-03%20%C3%A0%2018.39.12.png?dl=0"">a Google Sheet</a> to gather, classify and comment the 420+ measures, broken down to 9 main categories : institutions, energy, transportation, digital, military, budget, immigration, social and economy.   EU reporters from our four editorial sections were asked to explore each proposal and assess them, using a preset list of 7 ratings: ""extremely difficult"" to implement (red), ""already in effect or already scheduled"" (red), ""difficult"" (orange), ""too vague"" (orange), ""realistic"" (green), ""out of scope"" (grey) or ""unevaluated"" (grey).    This evaluation grid was fined tuned during the early stages of the project, to take the feedback of the journalists into account.    The content of this growing database was made available in a JSON feed through a Google Apps Script, and a data visualisation was designed using HTML, CSS and Javascript to be embedded in a story. Once the desktop version finalized, the layout was adapted to produce a mobile version. "," The sheer volume of data to process was the biggest challenge: at the end of the day, we decided to limit our assessments to the topics our reporters are most familiar with. We also selected the parties that actually had a chance to get more than 5% of the vote according to the polls – in France, that is the threshold they have to reach in order to have candidates elected at the European Parliament.    We also had to make sure that the rating process remained consistent across the team. Neither the merit of the proposed course of action, nor its effectiveness or its ideological premises were at stake: the only criteria to use was its potential to be implemented. The assessment has to be justified by the existing EU legal framework, including the treaties signed by member states, and the current political landscape in Brussels.   From a design point of view, we had to come up with a powerful navigation system that enables the user to find quickly the measures put forward by a given candidate or on a given topic. We created a natural language form and implemented it in a sticky story header. We also had to communicate the rating in a straightforward manner, and therefore adopted a “traffic light” color coding: green is “good”, orange is “so so”, red is “bad”. "," During each election cycle, many news outlets rush to publish stories and infographics that strive to summarize the party platforms and compare them to one another. But in a era of fake news and political polarization, we believe that journalists can take these projects a step further.    By taking a stand on the feasibility of proposed public policy, we can enable the voters to make an educated choice in the voting booth. Evaluating a party proposal requires a solid expertise in its field, which make it a perfect job for trade media outlets such as Contexte’s editions.    Regarding the process, a crucial decision was to keep all our data well structured in a database since day 1 of the project. To manage it, we chose Google Sheet, a tool versatile and easy to use. We also mashed up up the data with other datasets we manage, as the list of EU national parties we created for another project and open sourced on Github. <a href=""https://github.com/Contexte/open-data/tree/master/eu-political-parties"">https://github.com/Contexte/open-data/tree/master/eu-political-parties</a> ",https://www.contexte.com/article/pouvoirs/europeennes-on-a-passe-a-la-moulinette-les-programmes-des-candidats_99880.html,https://translate.google.fr/translate?sl=fr&tl=en&u=https%3A%2F%2Fwww.contexte.com%2Fapps%2F2019%2F04-programmes-europeennes%2Fstory.html,https://www.lemonde.fr/les-decodeurs/article/2019/05/24/europeennes-otan-frontieres-immigres-de-nombreuses-propositions-sont-irrealisables_5466897_4355770.html,,,,,"Yann Guégan and Jean-Sébastien Lefebvre, with Sabine Blanc, Jean Comte, Hortense Goulard, Laura Mercier, Guénaël Pépin, Fanny Roux and Isabelle Smets","  Yann Guégan  is a journalist in charge of editorial innovation inside Contexte’s newsroom in Paris. He designs advanced web scrapers, interactive infographics and data visualizations and internal tools for the reporters. He also contributes <a href=""http://dansmonlabo.com/"">in his blog</a> to the current conversation about the future of news, and is a proud founding member of the Conseil de déontologie journalistique et de médiation (CDJM), France’s long awaited journalism self-regulatory body.   He’s a renowned data driven journalism trainer, hired for courses, workshops and conferences in France and abroad. A former freelance copy editor for various print outlets, he started working online in 2007, and discovered a whole new world, where journalists can enter a conversation with their readers. Since then, he kept on broadening his set of skills, learning data driven journalism techniques, webdesign tricks, user experience processes or useful programming languages.    Jean-Sébastien Lefebvre  is a seasoned EU reporter, head of Contexte’s newsroom in Brussels and editor of the Pouvoirs (“Powers”) section. A former freelance, he was a contributor to national news outlets as Slate.fr, L’Express or Rue89 when he joined the team that founded Contexte in 2013. He teaches journalism at the Institut pratique de journalisme (IPJ), a Parisian J-school where he created a seminar on EU news. He’s also a skilled photographer: wide-format prints of his work are on display at the Parisian newsroom.   Prior to becoming a keen observer of the Brussels’ European bubble, he was hired in 2008 at the headquarter of CafeBabel, the first multilingual participatory magazine made by and for young people across Europe. He started his carreer as a reporter for le Courrier de l’Ouest, a local newspaper. ",,,
Brazil,Fogo Cruzado,Small,Shortlist,Open data,Fogo Cruzado,26/04/19,"Investigation,Solutions journalism,Database,Open data,Mobile App,Map,Politics,Crime,Gun violence,Human rights","Personalisation,Google Sheets,CSV,R,OpenStreetMap"," Fogo Cruzado (""Crossfire"") is a Brazilian open data platform about gun violence that aims to democratize information in debates about public security and human rights. In response to overwhelming interest from media and university researchers, in 2019 we developed a new API that allows any user to search the platform for the data they need and to make applications with our data.    The collaborative platform maps shootings and shootouts in the metropolitan regions of Rio de Janeiro and Recife. Its integration with a mobile application allows rapid alerts to be sent to users about potential high-risk situations. ","There is no democracy without active participation by civil society. In a country with alarming homicide rates - around sixty thousand killings per year - and a dangerous upsurge in public security practices that endorse abuses and excesses by state agents, initiatives such as Fogo Cruzado are increasingly relevant in order to guarantee the population access to high quality information This project breaks with a monopoly of the narrative on gun violence - currently enjoyed by the government - in order to guide evidence-based discussions for the construction of efficient public policies. In the long run, the project serves as a channel for engaging the population in debate and for producing of knowledge about security, demonstrating that it is possible to build reliable data and qualified interventions from civil society. Fogo Cruzado also aims to encourage a culture of journalism and citizen-driven public security reform, modernizing and democratizing security organs that still bear traces of Brazil's authoritarian past, when the actions of security forces could not be questioned. In 2019, more than 300 articles published in the Brazilian press used Fogo Cruzado's data. The foreign press also used it as a source in 41 stories. Our many requests for data include being contacted in 2019 by 22 researchers and we also gave 10 interviews for academic papers. To address this, in 2019 we created the API, where any user can search for the data they need and make their own applications. Today, the API has 213 registered users. Our data has also been used by politicians to push for better policies. In addition to being cited in speeches, last year, a study we did on shootings around schools was used as a basis for a city councilman's proposed law that would allow school principals to close schools in the event"," The Fogo Cruzado app uses Google Maps technology. When a person registers  a gunfight / gunshot in the app, that information goes to our data management system. It arrives in the system georeferenced, which allows the crossing of our database with any other that is also georeferenced.   The prototype is a hybrid software for APP developed in the Phonegap / Cordova programming language for Android and IOS platforms, integrated with a web service developed in PHP with MySQL database. The geolocation uses the Google Maps API and the notification features a Google Firebase API.   In addition to receiving notifications from users directly via the app, journalists receive direct information from partners who work on the spot. In this case, only trusted sources are considered -- sources with whom there is a previous relationship, such as local community organizations and locally engaged residents. The team also adds information collected via the press and law enforcement authorities to the databases. When notification of a gunfight / gunshot arrives, it is not automatically published on the map and on social media. Immediately, the team checks the notification with scripts and filters developed to aggregate information on social networks about firearm shots in the metropolitan region of Rio de Janeiro and Recife. This allows the comparison of further information about the same firearm gunfight / gunshot. After this verification, the notification is posted on the networks and the incident is publicly registered on the website and app "," This project was born on a Google spreadsheet in 2015, when Rio de Janeiro was getting ready to host the Olympics and the Brazilian media was painting an optimistic picture of the city. Those who live and work here knew that the situation was not as good as described. There were many shootouts in Rio, but when I searched for information to write about the situation, I could not find it.   I began tracking the gun violence myself, thinking about what to do with the information. There, an application was born, together with a map, that aggregates and makes information available.   Fogo Cruzado is a live project, done in real time, with great demand for its data. Turning the idea for the project into methodology, communication strategies, trainings, information networks and methods for sharing the results has not the most difficult part of the project. The greatest challenge is sustainability of the project -- even as it has received many compliments, awards and honorable mentions.   Today, our team consists of 12 people operating in 2 states. There are always requests for operations in other areas, but when it comes to financing, things change.   Having ideas is easy. Executing ideas is an exercise in perseverance and faith. "," Fogo Cruzado was born out of a government information vacuum. It is a strategic vacuum, since whoever holds the information also holds the narrative about the situation and can sell the solution they want.   Developing a matology to create information is not easy, but it is ready and open to anyone who wants to replicate it. FC is open source, its data is open and its methodology is free to be adapted and adapted to other demands. There is a collective of activists in the Amazon studying our work to map conflicts, fires and other local peculiarities. In Manaus, a researcher is adapting the technology to map traffic accidents.   Citizens, journalists, researchers, and activists can and should use technology to improve their work and not depend on governments to guide their work. Technology is an ally and gives us the freedom to produce data, stories, narratives and press for change. ",https://api.fogocruzado.org.br/,https://fogocruzado.org.br/,https://github.com/voltdatalab/crossfire,,,,,Maria Cecilia de Oliveira Rosa," I'm a journalist with a postgraduate degree in Crime and Public Security and Public Administration with an emphasis on Social Management. I've studied drug policies, HIV and human rights at the Intercambios Asociación Civil and reporting on drug trafficking at the University of Texas at Austin and participated in the Open Society Latin American Advocacy Fellowship Program on Drug Policy Reform in London.   I was formerly a consultant for Amnesty International, where I worked as a researcher and led the development of the Fogo Cruzado armed violence data platform, now managed by the Update Institute. I was also a communications consultant for LEAP Brasil and a communication advisor for PRVL (Program for the Reduction of Lethal Violence against Adolescents and Youth), an initiative of the Favelas Observatory carried out in conjunction with UNICEF and the Brazilian President's Human Rights Secretariat. I coordinated the communications team at the Redes da Maré NGO, where we published the  Maré de Notícias newspaper. ",,,
Finland,Helsingin Sanomat,Big,Participant,Best data-driven reporting (small and large newsrooms),How Do the Rich Really Think?,09/01/19,"Investigation,Long-form,Multiple-newsroom collaboration,Database,Illustration,Infographics,Economy","Animation,Personalisation,D3.js,Canvas,Creative Suite"," This article tells the results of a large academic study on the richest 0.1 % in Finland. Tax data, basis of the research, on the rich was based on Helsingin Sanomat's tax archives. For the article HS's data team created illustrations and interative graphics. The use of animations is subtle yet effective. The story combined academic qualitative research material to the qualitative tax data collected througout the years by Helsingin Sanomat. "," The article is based on a large amount of interviews of Finland's richest. The research was the first of its kind in Finland. First the article tells who these people are: who they are, where they live, how much they earn etc. Reader could also compare their yearly salary with the rich. The results of the interviews are told thematically: what they think of themselves, others, taxes, and politics. The insights on the rich are rarely heard, so the story created totally new discussion around the wealthy minority in Finland. The story had wide readership: 294 095 visits, 443 304 page views and it was the 24th read story of the whole year. "," Tax data from years 1999-2017 was gathered and analyzed. Data was visualized using d3.js, both on svg and on canvas. ", The most challenge was how to combine vast academic material which included interviews of the wealthy people and the qualitative data from almost 20 years to work together in a narrative. We managed with structuring the text to data part and then having the quotes from the really rich people to follow the data. Like they were explaining themselves after their wealth was examined in numbers. ," With the help of subtle visuals - data visualizations, interactive comparison and illustrations - we were able to produce a coherent narrative from the vast material. We succeeded in combining both academic and journalistic material to a higly informative and also entertaining story, that was the talk of the country for weeks. ",https://dynamic.hs.fi/2019/promille/,,,,,,,"Tuomo Pietiläinen, Uolevi Holmberg, Elisa Bestetti, Tuomas Jääskeläinen, Minna Nalbantoglu and Juhani Saarinen."," Tuomo Pietiläinen, text. Pietiläinen is staff investigative reporter.    Uolevi Holmberg, illustration. Uolevi Holmberg is staff motion graphic designer.   Elisa Bestetti, code. Elisa Bestetti is staff designer.    Tuomas Jääskeläinen, Minna Nalbantoglu ja Juhani Saarinen, production. Jääskeläinen, Nalbantoglu and Saarinen are staff journalists working with design, economics and data. That was their pilot project working over different departments. ",,,
Russia,BBC News Russian,Big,Participant,Best data-driven reporting (small and large newsrooms),Why Chinese farmers have crossed border into Russia's Far East,11/01/19,"Investigation,Explainer,Long-form,Cross-border,Documentary,Database,OSINT,Podcast/radio,Infographics,Video,Map,Satellite images,Audio,Business,Agriculture,Immigration,Economy,Employment","Scraping,Json,Adobe,Microsoft Excel,Google Sheets,CSV,Python","BBC Russian calculated that Chinese companies either owned or leased at least 350,000 hectares (3,500 sq km) of Far Eastern land in Russia. We went to the places where they have largest territories and discovered how they build relationships with locals, how their life and business is organised. For this project we've made a database of more than 20,000 agricultural land plots in 5 regions of Russian Far East that border China. Ownership structure was analised. The actual territory owned or rented by could be higher, the BBC has learned.Such data were not publically available and was collected from several", The text was translated to several languages within BBC and was widely quoted in Russian media.   Total page views on BBC News Russian were nearly one hundred thousand. It is a large number for Russian market. Total page views of English translation were nearly three hundred thousand. With all the translations total impact of the text could reach approximately half a million page views.    After the publication we got a reaction from Chinese Foreign Ministry. The Ministry's spokesman commented on the issue at the press conference.     ," Russian land registry data is publicly available but getting it requires a lot of manual work.   We used several ways to reduce manual work and build a list of land plots to be analysed. Data scraping was one of the main instruments.    Also, we used mouse movement emulation to request data from official database (we did it via 1 stream not to overload the server).   Besides we dealt with a wide range of different databases for instance database of deals with state owned land, Russian companies’ databases, Russian land registry, database of companies who hire foreign workers. "," The full list of Russian land plots doesn't exist. One can find only outdated lists of land plots in Russian Far East.    Actually we created our own list by combining old avaliable lists and finding new land plots by scraping data from cadastral map. From this list we've chosen agricultural plots by scraping the data. Afterwards we requested ownership data for more than 20,000 land plots from official registry.   As a result we got a database of more than 20,000 land plot registration data for Russian Far East. We analysed that data to find out land plots owned or leased by Chinese companies. It is a way to make one of the first estimates of Chinese presence in agricultural sector of Russian Far East.   Our list cannot be considered as a full list of Chinese owned land although it's the largest database available.     ", The presence of Chinese farmers in Russian Far East is highly debated issue in Russian media and society. We can see a lot of rumours and unconfirmed data. We even asked Russian official on the figures. However we haven't got any hard data covering all the Far East region.So we got our own estimate.   Using this precise data with addresses we were able to visit Chinese businessmen on land. Combining data research and field reporting we could build a multidimensional image of Chinese business presence in Russia.         ,https://www.bbc.co.uk/news/world-europe-50185006,https://www.bbc.com/russian/features-49978027,https://www.youtube.com/watch?v=8magQAEAK5g,https://www.bbc.com/russian/av/media-50103811,,,,"Text and data analisys: Andrey Zakharov, Anastasia Napalkova. Editors: Alexandra Zaytseva, Georgy Neyaskin. Design: Olesya Volkova. Photo and video: Kiril Glazkov. Producer: Ekaterina Venediktova"," Andrey Zakharov is an investigating journalist of BBC News Russian. Previously he worked for RBC magazine and for major St Peterburg's media Fontanka. He speciales in OSINT and business investigations.    Anastasia Napalkova is a data journalist of BBC News Russian. She worked for Russian major media RBC and major news paper Kommersant. Anastasia specialized in analysing property and registry data, government procurement data. ",,,
Ukraine,TEXTY.org.ua,Small,Winner,Best news application,HOT DISINFO FROM RUSSIA (Topic radar),08/07/19,"Explainer,News application,Politics","AI/Machine learning,Scraping,D3.js,PostgreSQL,Python"," TEXTY developed the data acquisition&analysis platform  and dashboard tool <a href=""https://topic-radar.texty.org.ua/"">https://topic-radar.texty.org.ua</a> which shows an overall dynamics of topics of Russian disinformation in manipulative news. We are doing an NLP on thousands of news per week to detect manipulative ones, group them by topics and meta-topics to show on interactive dashboard.   We also publish weekly reviews   (21 so far), based on the results of analysis.   In addition we developed ""Fakecrunch"" add-on based on the same platform (for Chrome and Firefox).  It automatically signals to users about manipulative content and could be used to collect suggestions about possible low quality/fake/manipulative news items. "," The project is aimed to track the content and intensity of Russian disinformation narratives and manipulative information in online media. It raises awareness of government bodies, civil society organizations, journalists and experts on major disinformation themes that are being pushed by Russia at any given week. Just one example: Dmytro Kuleba, Deputy prime minister of Ukraine, mentioned this project as an illustration of the huge level of Russian disinformation flowing to Ukraine.    This quantitative approach allows us to overview and to zoom-in, from top to bottom, of the vast propaganda landscape and to track topics in different periods of time.   Starting from May 2019, <a href=""http://texty.org.ua/mod/archive/tag.php?tag=disinfomonitor"">21 weekly reviews</a>, based on the project, were published. Each review illustrated key narratives of manipulations, which our application determined. Average audience engagement for each publication on texty.org.ua was about 8,000 users.    Other media used to share our reviews, as well as some bloggers and influencers. Also we got positive feedback and mentions of this news application from international experts, for example Andreas Umland (German), Lenka Vichova (Czech republic). In words of Maciej Piotrowski, from Instytut Wolności in Warsaw, Poland: "" Useful information. Sometimes we share it in our materials in Instytut Wolności, sometimes used for analysis. Longtime tracking is useful to see the full picture. ""   After many requests about additional features we decided to develop version 2 of the application. It will be published in April 2020 (approximate date) and we’ve freezed data updates until the new version arrives. "," Data was downloaded from sites' RSS feeds or links on their Facebook pages. Preprocessed data about news items stored in PostgreSQL. Each text was prepared for analysis: tokenized (divided into language units — words and punctuation marks), lemmatized for topic modeling. Custom Python scripts were used to obtain (Scrapy), process and store data.    Each news item was then evaluated by an improved version of our manipulative news classifier ( ULMFiT based model for Russian and Ukrainian languages, created by TEXTY back in 2018, programmed in Pytorch/fast.ai). This model is available from our github. It estimates the likelihood that the news contains emotional manipulation and/or false argumentation.   Selected manipulative news, ~3,000 pieces per week on average, was broken down into topics by automatic topic modeling (<a href=""https://en.wikipedia.org/wiki/Non-negative_matrix_factorization"">NMF</a> algorithm). We edited the resulting news clusters manually: combined similar topics, discarded irrelevant or overly general clusters.    Each subtopic in our news application is also illustrated by a sample of titles from news which belong to it to let new readers know what it is about. "," For our best knowledge, this is the first such tool & whole pipeline for Russian and Ukrainian languages. The main challenge was to retrieve accurate topics and track them over time. Topic modelling was made using NMF, an unsupervised method of clusterization. Results are less accurate compared to supervised learning, when the model is trained using humal labels. But we cannot train topic classifier since we do not know all the topics in news and cannot easily update supervised model if the news agenda changes. So we have to keep using unsupervised NMF solution. Topics for the week are reviewed by analysts, as well as improved by rules to fix possible errors of unsupervised topic modelling. A lot of manual work is the hard part of this project.   Because we detect topics in weekly samples of news, we have to aggregate them for dashboard to track topics for longer periods. We addressed this challenge by hierarchical NMF, namely clusterized weekly clusters. Meta-topics in the dashboard were first clusterized and reviewed by analysts so that each weekly topic relates to one meta-topic on the dashboard. Aggregation of clusters from different models is not well-studied and a great part of it is done manually. "," Long-term tracking of disinformation makes it possible to see what topics are most important for the Russian authorities, who is the biggest irritant to them, and what they plan to do in the future in Ukraine. One of the conclusions of our analysts is evidence that there are entire array of manipulative news from Russia which can be logically combined under the umbrella name of “Failed state” (related to Ukraine). The purpose of this campaign is obvious: it aims to create an image of Ukraine as a non-state, an artificial state entity that arose against historical logic.   We are considering the dashboard as a usable tool for further research by analysts, and Fakecrunch add-on as a usable tool for online-readers in their everyday ""life"". Other journalists got the source for their materials. General public got evidence-based tool for media literacy and for self-control in social media.   Lenka Vichova, Czech Republic: "" Many of these messages enter not only the information field of Ukraine, but also to Czech and Slovak media sphere. So it is core to know and be prepared.  I use your reviews, when work on my own analytical articles and also in comments for Czech and Slovak media. "" ",https://topic-radar.texty.org.ua,http://texty.org.ua/mod/archive/tag.php?tag=disinfomonitor,https://fgz.texty.org/,,,,,"Nadiia Romanenko, Nadja Kelm, Anatoliy Bondarenko, Yuliia Dukach","  Nadiia Romanenko  is an analyst and data journalist at Texty.org.ua, where she started as an intern in 2015. Her scope of interest is studying manipulative news and Russian disinformation in Ukrainian online media using natural language processing and deep learning. She believes in a quantitative approach to study manipulative news because it is a mass phenomenon, human-only monitoring of media cannot deal with huge amounts of news. In data journalism creates mostly interactive visualisations using D3 and Vue.js.   Graduated Faculty of Sociology of Taras Shevchenko National University of Kiev, having completed exchange studies at the Department of Sociology at Lund University.   Selected projects:<a href=""http://texty.org.ua/d/fb-trolls/index_eng.html"">The Trolls Network</a> — analysis of pseudo-patriotic trolls community in Ukrainian Facebook   <a href=""http://texty.org.ua/d/2018/mnews/eng"">We’ve Got Bad News</a> — ranking of manipulative news websites and longread about junk news ecosystem    <a href=""https://topic-radar.texty.org/"">Hot Disinfo from Russia</a> — daily monitoring of topics of Russian disinformation    Nadja Kelm:  designer in Texty.org.ua, Kyiv, Ukraine. Author of the course about design in data visualization for School of Infographic (Internews Ukraine), winner of the national competition The best book design 2019 (the Arsenal Book Festival).    Anatoliy Bondarenko:  Co-founder and head of data journalism in Texty.org.ua, Kyiv, Ukraine. Physicist by education. Author of the course about data visualization for Prometheus (Ukrainian MOOC platform) and visiting lecturer in UCU, Lviv (“Practical introduction to data journalism”).    Yuliia Dukach:  data journalist in Texty.org.ua, Kyiv, Ukraine. PhD in sociology, lecturer in the Igor Sikorsky Kyiv Polytechnic Institute (department of sociology and law). ",,,
China,The Paper,Big,Participant,Best data-driven reporting (small and large newsrooms),China has FOIA too. Here's what information people applied for and why they got rejected.,04/04/19,"Investigation,Explainer,Database",Microsoft Excel," Regulation of the People's Republic of China on the Disclosure of Government Information, was enacted 11 years ago, back in May 2008. Stunning as it sounds, we are curious about the effect of this law. How often do citizens take advantage of it? What kind of information do people care most about? Why does the government reject to make the information public? Or why does the court decide to support the applicants? Hence we turned to the OpenLaw, a dataset of Chinese court case records, and analyzed 5,000 out of more than 13,000 case records related to the regulation. "," After publishing, the project was republished by sveral stakeholders, getting attention from experts in the open data area. For the general audience, the project was viewed more than 200,000 times on The Paper platform. "," Our data is from OpenLaw, a dataset of Chinese court case records. The team spent two months decoding the case records into a database for analysis, containing more than 40 variables. "," 1) There are no public records of how people acquire information from government and what kind of requests they get. To able to address this issue, we use the court case records, to find out why people couldn't get through with regular requests and had to resort to lawsuits. We found these records can help us get a glimpse of the very extreme usage of the law. The team spent two months decoding the case records into a database for analysis.   2) Freedom of information law, is a distant topic to the general audience. We start the project by telling a story of a retired doctor who sued the local government several times to acquire the document that could prove a medicine a hospital used during an eye treatment she received was unapproved by the State Drug Adminstration. Therefore, the readers can understand the usage of the law can be very personal and useful.   3) We tried to show not only what the data tells, but also what the findings mean. To understand our findings, we interviewed professors and lawyers to enhance the interpretation of the topic. For example, we found out the kind of information Chinese citizens apply for the most are land, houses, and everything related. Through Professor Xiao Weibin's interpretation, we understood it shows the usage of the law is still at a very low level. This type of use doesn't contribute to the information disclosure or open data spirit; since personal property information, even successfully acquired after lawsuit, has few reuse value, and therefore is often unuseful for public interest. "," Our data analysis unveils a very contradictory fact: even when a citizen wins the lawsuit, he or she may still not be able to get the information wanted. Because the law prioritized at ensuring government's lawful acts, which means responding to data requests within required time, and giving reasons when rejecting the requests. But when the government didn't make enough efforts, the court has no certain obligation to investigate.    We also found out the kind of information Chinese citizens apply for the most are land, houses, and everything related. It shows the usage of the law is still at a very low level. This type of use doesn't contribute to the information disclosure or open data spirit; since personal property information, even successfully acquired after lawsuit, has few reuse value, and therefore is often unuseful for public interest. ",https://www.thepaper.cn/newsDetail_forward_3248133,https://shimo.im/docs/RhwqchVvYVCdtcv9,,,,,,"Zou Manyun, Wang Yasai, Lu Yan"," The Paper data news team focuses on data-driven stories and explanatory storytelling. The team consists of 20 people: half are data journalists, and half are graphics designers of all sorts, from information design to 3D modeling.   We cover stories on tight deadlines; We measure policy effects by data; We stretch ways to get data suitable for our stories; We try to push the envelope and use suitable storytelling for valuble stories. We also collaborate constantly with our English-language sister publication, Sixth Tone, and published some of our projects in bilingual. ",,,
China,The Paper,Big,Participant,Best data-driven reporting (small and large newsrooms),Who benefits the most from China's new tax cuts?,01/07/19,"Solutions journalism,Quiz/game,Database,Economy","Personalisation,Python"," China’s new personal income tax cuts came into force on Jan.1, 2019. This time, the government not only raises the tax exemption threshold but also changes the tax rates and adds tax deduction items. Facing all these new policies, we are wondering what kind of people being affected the most, or in other words, who benefits the most? Does the new tax cuts policy really work out as what the policymakers planned? "," The average staying time on this project is more than four minutes, which doubles the average staying time for our other projects. It shows our personalization storytelling approach helps readers digest the effects of the new tax cuts policy. "," To answer the question, we use a census dataset named CHIP to simulate the tax cuts impact on different income groups. The CHIP dataset is produced by the China Institute for Income Distribution. The institute takes sampling surveys on different income groups across various provinces in China, which mirrors the income distribution around the nation. The dataset records interviewees’ jobs, income, the status of assets, family members, education background and more.   We used ai2html for SEO purposes. "," 1) The income tax cut was a trendy topic for several months after it was announced. However, other than explaining the new rules, and interviewing experts, the readers had few access to understand the effects of this new policy. We chose to measure the effects of the new tax cut policy, providing a very important aspect of the topic.   2) To be able to measure the effects, we used the CHIP database from the China Institute for Income Distribution in Beijing Normal Unversity. The dataset records interviewees' jobs, income, the status of assets, family members, education background and more. The institute takes sampling surveys on different income groups across various provinces in China, which mirrors the income distribution around the nation, so that it is meaningful to use this dataset to estimate the tax cut effects on the working population in China. We interviewed professors in this topic area to supervise our analysis, and explained the sample and methodology in details in the project.   3) As for the storytelling part, we used a personalisation approach to start, asking users to input their monthly income, and calculating their tax cuts and ranking the results in the sampling group, so that the story feels more related to them. Then we unfolded the story, mearsured the effects in terms of different income and education levels, social security statuses, employer categories, and etc.   4) We further questions the tax cut impact on income equality, which is the ultimate goal of any tax reforms. "," The new tax-exempt group has different characteristics compared to the old group. In the new tax-exempt group, we can find people with higher education degrees, better jobs, and social security benefits; while the old group earn only pocket cash and seldom has any social benefits. However, unlike tax cuts in the US, the rich still get more tax cuts than the poor because the principle stands- the higher income you earned, the more tax cuts you get. Hence, if people are divided into groups based on their jobs, the employees in the foreign company and the private company can save more money from the tax cut while self-employed taxpayers won’t be affected much. Similarly, people with graduates degrees will certainly save up more money than people with primary school degrees.   We discover that middle-to-high income groups benefit from the new tax policy the most. By that, we mean people who at least earn 15,000 yuan per month. For people who earn more than 30,000 yuan monthly, the new tax policy won’t affect them at all because the applied tax rates stay the same.   we use the Gini index to measure how the new tax policy improves China’s income equality. Unfortunately, it doesn’t change much. You can tell from the last chart on the bottom that the curve stays the same before and after the new tax cuts. ",http://image.thepaper.cn/html/zt/2019/01/tax/index.html,https://shimo.im/docs/x9yQ9W6k9GjpHJK3,,,,,,"Zou Manyun, Wang Yasai, Kong Jiaxing, Du Haiyan, Lu Yan"," The Paper data news team focuses on data-driven stories and explanatory storytelling. The team consists of 20 people: half are data journalists, and half are graphics designers of all sorts, from information design to 3D modeling.   We cover stories on tight deadlines; We measure policy effects by data; We stretch ways to get data suitable for our stories; We try to push the envelope and use suitable storytelling for valuble stories. We also collaborate constantly with our English-language sister publication, Sixth Tone, and published some of our projects in bilingual. ",,,
China,The Paper,Big,Participant,Best visualization (small and large newsrooms),Less Fireworks for New Year: Does it Improve Air Quality?,02/03/19,"Explainer,Environment","Adobe,Microsoft Excel"," China has a centuries-long history of putting fireworks on Chinese New Year’s Eve. However, as air pollution attracts more attention, fireworks became a target of criticism because they produce much smoke, dust, and hazardous gases like sulfur dioxide. What impact does fireworks have on air quality? What kinds of policies do cities have concerning fireworks, and are these policies effective? ", The project was awarded in short list at Information Is Beautiful Awards. ," 1) Banning fireworks has become a common practice for the 50 most populated municipalities in China. During 2017 Chinese New Year, 22 out of the 50 municipalities banned fireworks; and in 2018, this policy is extended to 11 more municipalities. Most of them with the ban only bans fireworks in downtown areas, allowing fireworks in suburban and rural areas. Only three cities carries out a complete ban within their administrative boundary.   To rule out most confounding factors and solely see the impact of fireworks on air quality, we applied a scholarly index by researchers from Institute of Atmospheric Physics, Chinese Academy of Sciences. This index divides the hourly concentration of PM2.5 by the hourly concentration of carbon monoxide. The smaller the ratio, the less pollution caused by fireworks. We got the air quality data from National Urban Air quality release platform, and calculated the data using the method mentioning above.   2) The visulization was plotted using Processing. "," 1) It is almost common sense that firework ban is for the benefit of air quality, even though there were repeated discussions of protecting fireworks as holiday cultures every year. We tried to address the issue by using data.   2) The visualization is in the form of firework, demonstrating the data clearly, while stressing the topic.   3) We tailored the visualization on different platforms. For the mobile version, the data is demonstrated in a side-by-side format. And municipalities are arranged by policy types, since that’s the key variable for the storytelling. For the PC interactive version and poster version, we maximize the capacity of larger screen. We located each pair of data vertically, and the difference between 2017 and 2018 can be compaired. We arranged municipalities horizontally, and the readers and see all the 50 municipalities’ data at one glimpse. They were first catgorized by policy types, and the users can regroup the data by the air quality rankings of 2017 and 2018. The difference between two years can also be highlighted by clicking on a compare button. "," Comparisons between air conditions during 2017 and 2018 Chinese New Year are made for all 50 municipalities, It’s rather clear that this ban has a positive influence on air quality, because when compared with air conditons during 2017 Chinese New Year, the municipalities that started this ban in 2018 experienced better air quality during 2018 Chinese New Year.   In addition to a general comparison between 2017 and 2018, the choice of polar bar chart helps readers to identify the most popular times to put fireworks if fireworks are at least partially allowed in a municipality. Usually, people start to put fireworks after the midnight of New Year’s Eve.   As metioned above, only three municipalities banned fireworks completely; observing the spatial differences within a muncipality that has a partial ban also says something about the environmental consequences of fireworks. Beijing is used as an example here. After midnight of 2018 New Year’s Eve, suburban areas turned around the table to surpassed downtown areas in terms of PM2.5 concentration.   Visually, polar chart charts are intended to mimic the shape of firework display. It also corresponds to the fact that with more fireworks, the air is more polluted. We used yellow polar bar chart showing the 2017 data and the pink one showing the 2018 data. ",http://image.thepaper.cn/html/zt/2019/02/fireworks/index.html,https://www.thepaper.cn/newsDetail_forward_2941170,http://projects.thepaper.cn/thepaper-cases/839studio/yh_full.jpg,,,,,"Yasai Wang, Liangxian Chen, Jingran Zhang"," The Paper data news team focuses on data-driven stories and explanatory storytelling. The team consists of 20 people: half are data journalists, and half are graphics designers of all sorts, from information design to 3D modeling.   We cover stories on tight deadlines; We measure policy effects by data; We stretch ways to get data suitable for our stories; We try to push the envelope and use suitable storytelling for valuble stories. We also collaborate constantly with our English-language sister publication, Sixth Tone, and published some of our projects in bilingual. ",,,
China,The Paper,Big,Participant,Best visualization (small and large newsrooms),We Visualized the Rules of Shanghai's First Compulsory Waste Sorting,28/06/19,"Explainer,Database,Environment","Animation,Python"," On the 1st of July, Shanghai started enforcing its compulsory domestic waste management, which is the country's very first to start the regulation. The city's regulation requires people to sort trash into four categories - dry garbage, wet garbage (kitchen waste), recyclables and hazardous waste.    Although some resources have already listed out things belong to four categories, it is still hard for residents to get the gist of the waste sorting because sometimes. Therefore, instead of text, we decided to visualize it.  "," The project was widely shared by our users, and it was viewed more than 200,000 times on our own platform. ", 1) We scraped a list of all 2055 items from the local government's public database.   2) We made the visualization using P5.js and D3.js , The whole project was finished within five days. The tight schedule force us to make each step as precise and efficient as possible. ," Under the new rule, different parts or usages of items could lead to different waste sorting. For instance, glass bottles of milk are recyclables waste while the same containers of medicine are hazardous. We scraped a list of all 2055 items from the local government's public database. To make things clear, we applied five colors to four categories of waste mentioned above and non-domestic wastes. Then, we colored 2055 things and grouped them into 47 queries.    Each ball of the visualization represents one query group of items, like packing materials, cups, pens, metals, and more. The colors of the ball are a mixture of the color of every single thing of the query group. The more evidence of one color, the more likely the query group belong to a waste category. For example, the ball of electronics is almost covered by blue, so we learned that most electronics are recyclable. Seeing the visualization, readers can instantly get the whole picture of waste sorting. If they are especially interested in one query group, they can click on the ball and check the attribute of every item.  ",http://projects.thepaper.cn/thepaper-cases/839studio/ljfl/index.html,,,,,,,"Kong Jiaxing, Du Haiyan, Wang Yasai, Zhang Yijun, Xu Xueqing, Zhang Zehong, Lu Yan"," The Paper data news team focuses on data-driven stories and explanatory storytelling. The team consists of 20 people: half are data journalists, and half are graphics designers of all sorts, from information design to 3D modeling.   We cover stories on tight deadlines; We measure policy effects by data; We stretch ways to get data suitable for our stories; We try to push the envelope and use suitable storytelling for valuble stories. We also collaborate constantly with our English-language sister publication, Sixth Tone, and published some of our projects in bilingual. ",,,
China,The Paper,Big,Participant,Innovation (small and large newsrooms),The Paper,15/10/19,"Explainer,Chart,Human rights",Python," Many people think the blind experience complete darkness in their daily life. But actually, this is a misunderstanding. On October 15th, the International Day of the Blind, we built a project to demonstrate how blind people actually see the world.  "," The project was viewed more than 180,000 times on our own platform. ", We used Baidu audio API for the screen reading function in the project. ," 1) To pick a topic to present the experience of blind people is tricky. We devided the topic into three parts, how they see, how they ""read"", and how they live. We were able to match very creative materials for each part of the storytelling.    2) To be able to present the experience as accurate as possible, we consulted with experts to visualization different levels of blindness correctly at the top of the project; and we also asked a volunteer that specialized at creating screening reading text for blind people to help us write the descriptions of our graphics in the project.   3) From the technique part, the screen reading part took us a lot of time in testing, because the function has to work on all devices, and we want the experience to simulate the blind's experience on smart phone. "," In honor of the topic, we prioritized reading accessibility when designing the webpage. Readers with visual impairments can use the screen reader, for which we have specifically created content that works well, such as infographic explanatory text, operating instructions, and more. All these details are not very common in other interactive projects.    To illustrate the blind's experience, we also designed a screen reader simulation for those who can see. If readers click the yellow button with a horn icon on the right side of the top, they will enter the simulation mode where readers can only click through paragraph by paragraph, instead of swiping through page by page. Also, since blind people's brains can rewire themselves, the reading speed would be way faster than what normal people would expect.    Additionally, we interviewed two doctors and pulled data from VizWiz to showcase how the blind actually see in their daily life and what the reasons are. Despite the disability, the blind can also lead a wonderful life. After analyzing 1966 posts from one of the most popular web forum for the blind, we presented what blind people talk about each day. Not much different from those who can see. They also talk about the world cup, love affairs, and complain about their boss.  ",http://projects.thepaper.cn/thepaper-cases/839studio/blind/index.html,,,,,,,"Kong Jiaxing, Du Haiyan, Wang Yasai, Chen Liangxian, Lu Yan"," The Paper data news team focuses on data-driven stories and explanatory storytelling. The team consists of 20 people: half are data journalists, and half are graphics designers of all sorts, from information design to 3D modeling.   We cover stories on tight deadlines; We measure policy effects by data; We stretch ways to get data suitable for our stories; We try to push the envelope and use suitable storytelling for valuble stories. We also collaborate constantly with our English-language sister publication, Sixth Tone, and published some of our projects in bilingual. ",,,
United Kingdom,JPIMedia (project shared across all titles),Big,Participant,Best data-driven reporting (small and large newsrooms),Here's why special needs children are battling to get into mainstream schools,19/08/19,"Multiple-newsroom collaboration,Open data,Chart,Human rights","Microsoft Excel,Google Sheets"," A child’s first day at school is always an emotional time but when your child has special educational needs (SEN) just finding a place in a school can be a challenge. My investigation into SEN children in mainstream schools used publicly available data from the Department of Education (England and Northern Ireland departments), the Scottish Government and the Welsh Government to analyse inclusion rates across mainstream schools in each UK nation.     "," My analysis revealed there has been a decline in the number of SEN children attending mainstream schools in England, despite laws introduced six years ago meaning children with SEN should go to mainstream schools where possible. Commenting on the findings a leading disability charity said pupils are being “forced” into special schools. However, in Scotland I discovered there had been a sharp rise in the overall number of children registered as having Additional Support Needs (ASN) in mainstream schools since 2012. A pupil support assistant (PSA) from Edinburgh told me the system was still playing catch up and PSAs were often struggling to work with highly vulnerable children due to a lack of support and training. Inclusion figures in Northern Ireland and Wales remained relatively steady over the same period though. This story was shared across all JPIMedia titles and used in over 60 local and regional newspapers, including i News, Yorkshire Post and the Sheffield Star. ", I used Microsoft Excel for analysing the data and Google Sheets for sharing my findings with our titles. , The hardest part of this project was cleaning the data to deliver it in a straight forward way to help our reporters who are less familiar with datasets understand their top lines. This meant they could then use a simplified version of the governments' datasets. ," I hope this project/story would have helped people think more about interpreating the human stories behind numbers. While we had two very different tales in Scotland and England it wasn't until we spoke to case studies directly that we could better understand what the numbers were telling us. For example, Scottish schools are becoming more inclusive, but at what cost? To better understand what this meant for schools we spoke to a PSA who explained that despite schools becoming more inclusive, the system is still playing catch up to accomodate potentially vulnerable children.  ",https://www.thestar.co.uk/news/read-this/heres-why-special-needs-children-are-battling-get-mainstream-schools-491043,https://inews.co.uk/news/education/government-segregation-special-needs-children-mainstream-schools-495476,https://www.holdthefrontpage.co.uk/2019/news/regional-publishers-probe-raises-serious-questions-about-special-needs-law/,,,,,"Aimee Stanton, Claire Wilde", Aimee Stanton - Edinburgh based data reporter in the JPIMedia Data Unit.   Claire Wilde - Leeds based data and investigations editor for JPIMedia ,,,
Ukraine,TEXTY.org.ua,Small,Participant,Innovation (small and large newsrooms),Fakecrunch (AI algorithms),22/11/19,"Database,News application,Fact-checking,Crowdsourcing","AI/Machine learning,Scraping,Json,PostgreSQL,Python"," <a href=""https://fgz.texty.org/"">Fakecrunch</a> (Фейкогриз) is a browser add-on and Telegram-bot which promotes media literacy. It warns about manipulative news, as well as lets users report about dis- and misinformation.   Under the hood, Fakecrunch leverages the power of AI to label manipulative content. Its algorithm is based on a language model classifier trained on news labeled by trusted professional news editors. "," A lot of mentions in Ukrainian media, the total user count (after two months) is about 700, most of them are journalists.   Up to date, we processed about 35 000 requests to check news items. "," The manipulative news database is build using <a href=""https://arxiv.org/abs/1801.06146"">ULMFIT</a>-based AI classifier. Every week we crawl more than 80 000 news from Ukrainian and some Russian websites using <a href=""https://scrapy.org/"">Scrapy</a>. Each piece of news is pre-processed — language detection, tokenization, conversion of tokens to the sequence of token-ids, filtering out news unrelated to politics, economics, and society (such as sports, celebrities, lifestyle). Relevant news items are checked with ULMFIT classifier implemented in fast.ai Python library, initially trained on Wikipedia corpus, fine-tuned on manually labeled news corpus. News classified as manipulative with high confidence are loaded into Fakecrunch database (PostgreSQL). The classifier algorithm is available on <a href=""https://github.com/texty/manipulative_news_methodology"">GitHub</a>.    Backend for Fakecrunch is built using Django. It links the database of automatically detected and human-labeled manipulative news with Fakecrunch applications, add-ons, and Telegram bot. We use social authentication to sign up users in order to avoid misuse and distinguish trustworthy users. Also, we have developed an interface for the moderation of users' labels.   Fakecrunch <a href=""https://addons.mozilla.org/en-US/firefox/addon/fakecrunch/"">Firefox</a> and <a href=""https://chrome.google.com/webstore/detail/%D1%84%D0%B5%D0%B9%D0%BA%D0%BE%D0%B3%D1%80%D0%B8%D0%B7/aogjlhjkkcihgimccdodibjmpkpmgpad"">Chrome</a> extensions are built using WebExtensions API, a JavaScript API for building browser add-ons. Telegram bot is implemented in plain Django with Telegram Bot API. "," For our best knowledge, this is the first such tool & whole pipeline for Russian and Ukrainian languages. Texty was first asked about such a tool to warn about manipulative news at the end of 2018 when we developed an AI classifier for the detection of manipulative news. The accuracy of the algorithm at the time did not allow us to use it for the classification of individual pieces. Instead, we were able to estimate the average manipulativeness of content on some website. Recently, after improving the accuracy of classifier we decided to apply it for the labeling of individual news items. To address the problem of false-positive labels (when a non-manipulative item is marked as manipulative) we have increased the threshold of classifier (at a price of an increasing number of false-negatives). Also, all AI-classified news are labeled as “suspicious”, not as outright manipulative. Users are informed whether a piece of news was labeled by classifier or by a human.   Another challenge was the risk of misuse of Fakecrunch. First of all, only signed-up users can send reports about manipulative news. The login is implemented as social authentication with a minimum of permissions to protect user privacy and make login as comfortable as possible. We moderate all user labels and are developing automated ways of moderation, such as user ranking and agreement with classifier.   Technically the project was challenging in putting the whole NLP pipeline (crawling — pre-processing — classification) into a real-time application. It was made possible by the optimization of news crawling and data processing scripts to add known manipulations to Fakecrunch within one hour after publication. The variety of products (add-ons, bot, and backend, real-time data processing) was difficult to handle in itself and required discipline in the project’s code and development process. "," The project shows how a research data journalism project was turned into a tangible interface. Texty started work on automated detection of manipulative news more than two years ago. Since then we were constantly improving algorithms and delivery of findings, trying to reach more audience and promote media literacy through our work.   In addition to journalists, an application attracted a more diverse audience. ",https://fgz.texty.org/,github.com/texty/manipulative_news_methodology,,,,,,"Nadiia Romanenko, Nadja Kelm (design)","  Nadiia Romanenko  is an analyst and data journalist at Texty.org.ua, where she started as an intern in 2015. Her scope of interest is studying manipulative news and Russian disinformation in Ukrainian online media using natural language processing and deep learning. She believes in a quantitative approach to study manipulative news because it is a mass phenomenon, human-only monitoring of media cannot deal with huge amounts of news. In data journalism creates mostly interactive visualisations using D3 and Vue.js.   Graduated Faculty of Sociology of Taras Shevchenko National University of Kiev, having completed exchange studies at the Department of Sociology at Lund University.   Selected projects:<a href=""http://texty.org.ua/d/fb-trolls/index_eng.html"">The Trolls Network</a> — analysis of pseudo-patriotic trolls community in Ukrainian Facebook   <a href=""http://texty.org.ua/d/2018/mnews/eng"">We've Got Bad News</a> — ranking of manipulative news websites and longread about junk news ecosystem    <a href=""https://topic-radar.texty.org/"">Hot Disinfo from Russia</a> — daily monitoring of topics of Russian disinformation    Nadja Kelm:  designer in Texty.org.ua, Kyiv, Ukraine. Author of the course about design in data visualization for School of Infographic (Internews Ukraine), winner of the national competition The best book design 2019 (the Arsenal Book Festival). ",,,
Netherlands,"The Correspondent, De Correspondent",Big,Participant,Best data-driven reporting (small and large newsrooms),Europe spends billions stopping migration. Good luck figuring out where the money actually goes,12/09/19,"Investigation,Explainer,Solutions journalism,Cross-border,Multiple-newsroom collaboration,Open data,Crowdsourcing,Illustration,Infographics,Immigration","Scraping,Canvas,Microsoft Excel,Google Sheets,Python","  How much money goes from Europe to Nigeria to prevent migration?  It took over five months of research to answer what should have been a simple question. In this cross-border project, journalists Giacomo Zandonini (Italy), Ajibola Amzat (Nigeria) and Maite Vermeulen (the Netherlands) scoured databases, visited countless agencies, and ploughed through an endless stream of expenditure reports. The main finding was a complete lack of transpency in how the money is being spent.  "," As The/De Correspondent is a member-driven platform, the investigation started with a callout asking our members to share their expertise and knowledge in the sector, this has helped the three journalist to define the scope of the research.    The article tries to unveil how much the EU is spending on a particular policy area, where that money goes, and the effects it has. The lack of transparency and sources makes a story in itself.  The research the three journalist carried out to try and fill this gap created a unique database that can certainly demonstrate what European migration money is spent on (and what is not) – for example, border control rather than creating jobs locally. The database is available for everyone to access <a href=""https://docs.google.com/spreadsheets/d/1EqCgrDtDw2x82maASEJZxPzyURZXjkQdueeYLJFVQBA/edit#gid=321560865"">here.</a>   After publication, the three journalist were invited to talk about their investigation at the European Parliament. The MEPs who attended the talk are now trying to set up a new financial working group specifically for migration. Also, Dutch parliamentarians have expressed their interest and asked official parliamentary questions to our ministers about our findings.             "," For the data processing, Excel, Python and the pandas library were used. For the interactive graphic, a lot of sketching as well as the final graphic were made using Adobe Illustrator. Because this graphic was over 18000 pixels wide, we then used an Ai->Canvas plugin to translate the graphic to a JavaScript script that renders an HTML Canvas element. To navigate through this huge Canvas element, we wrote a JavaScript library that generates two smaller canvases per step from the big one by defining the x & y coordinates of each step and the width. While you scroll through the publication, these canvases constantly get created and deleted, since your browser can’t hold all of them in memory at the same time. For the scroll interaction that triggers each step when appropriate, we used a “scrollytelling” tool we’ve build and used a couple of times before. "," The research took five months that we spent investigating online databases, in conversations with embassies in Nigeria, with management organisations in Brussels, and appeals to the Government Information (Public Access) Act in various European countries, in Nigeria and the European Commission. We spoke to researchers and officials who already knew what we found out: they called European migration financing 'minimally transparent', 'unclear' and 'chaotic'. But above all, we saw how problematic it is that there is no clear overview of European migration budgets. And how that painfully typifies European migration policy.   This project was not only complicated in terms of content, but also quite a challenge behind the scenes. The story was written and edited in two languages ​​(the article was published in both English and Dutch). The project involved a core team of nine people working across three countries that had to visualize something that seemed impossible to visualize, and to compile a database that excels in incompleteness. "," From a journalistic prespective, this project proves how you can tell a story about missing data. The visualisations and infographic in this story expemplify the complexity of the research carried out by the journalists and not just the final conclusions that can be drawn from the data. The main big interactive graphic inflicts a sense of ""messiness"". This was done deliberately. The dataset isn’t a complete overview of all migration related funding going to Nigeria from the EU, so we didn’t want the graphic to have the more scientific infographic style that we often use in other publications. It was meant to convey the feeling of sketching along with the authors as they build their database and tell you about their struggle and findings.    The project shows how data-driven reporting can be a storytelling tool as well as a powerful tool of transparency. ",https://thecorrespondent.com/154/europe-spends-billions-stopping-migration-good-luck-figuring-out-where-the-money-actually-goes/12918991162-143114fb,https://thecorrespondent.com/150/a-breakdown-of-europes-eur1-5bn-migration-spending-in-nigeria/12583432950-df740aa9,https://docs.google.com/spreadsheets/d/1EqCgrDtDw2x82maASEJZxPzyURZXjkQdueeYLJFVQBA/edit#gid=321560865,,https://thecorrespondent.com/166/how-the-eu-created-a-crisis-in-africa-and-started-a-migration-cartel/13925665798-00d4e32a,https://thecorrespondent.com/172/the-eu-bypasses-public-tenders-in-africa-this-is-what-the-eus-financial-watchdog-has-to-say/14429003116-c16d40f9,https://thecorrespondent.com/76/tracking-the-european-unions-migration-millions/6375606028-a3babfff,"Maite Vermeulen, Giacomo Zandonini, Ajibola Amzat, Reinier Tromp, Leon De Korte, Heleen Emanuel, Afonso Gonsalves","  Maite Vermeulen  is one of the founding correspondents of De Correspondent, reporting stories across the globe, about development, humanitarian aid, international relations, conflicts and most recently, migration.    Giacomo Zandonini  is freelance journalist, working at the intersection of migrations, human trafficking, European policies. Rome-based, but mostly travelling through West Africa.    Ajibola Amzat  is a Nigerian investigative journalist. He has years of experience uncovering major corruption scandals in Nigeria.    Reinier Tromp  has been a full-time data journalist at De Correspondent since April 2019. He studied political science, philosophy and artificial intelligence and worked as a researcher at the Hogeschool van Amsterdam.    Leon De Korte  is the editorial designer at De Correspondent. He graduated as an illustrator at the Willem de Kooning Academy in Rotterdam. He then worked at design agencies and as a journalism designer at the KRO-NCRV. He is currently following a part-time master 'geographical information science' at the VU in Amsterdam.   As an art academy dropout,  Heleen Emanuel  has rolled into the profession because of a love for computers and affinity with code. After working as a freelance developer for a number of years, she missed having colleagues, working towards a goal, and working on long-term projects. That is why in 2016 she started as a front-end developer at design agency Momkai. When De Correspondent set up a data editorial in the beginning of 2019, she joined it.    Afonso Gonsalves  is the editorial designer at The Correspondent. He has art directed and designed campaigns, books, underwear, stop motion films, multimedia installations, broadcast environments, music videos, and visual identities.      ",,,
United States,FiveThirtyEight,Small,Shortlist,Innovation (small and large newsrooms),RAPTOR,10/10/19,"Database,Open data,Chart,Sports","Scraping,D3.js,Json,Adobe,Creative Suite,Google Sheets,CSV,PostgreSQL,Python,Node.js"," RAPTOR, which stands for Robust Algorithm (using) Player Tracking (and) On/Off Ratings, is FiveThirtyEight’s new NBA statistic. We’re pretty excited about it. In addition to being a statistic that we bake in house to fuel our data-driven coverage of the NBA, RAPTOR fulfills two long-standing goals of ours:  <ul>  First, we wanted to create a publicly available statistic that takes advantage of modern NBA data, specifically player tracking and play-by-play data that isn’t available in traditional box scores.   Second, and relatedly, we wanted a statistic that better reflects how modern NBA teams actually evaluate players.  </ul>"," RAPTOR allows us to cover the NBA in new and more nuanced ways than ever. It allows us to compare players in the context of the modern NBA while accounting for remarkable subtleties in how different players play, such as how much they control the ball and how they affect their team on the court. It powers our reporting, fuels our forecasts, and allows us to make more nuanced comparisons between current players and those of the past. This new metric is the analytical engine behind FiveThirtyEight's rigorous NBA coverage, empowering:  <ul>  Interactive <a href=""https://projects.fivethirtyeight.com/2020-nba-predictions/"">2019-20 NBA Predictions</a> that forecast which teams will win specific games, make the playoffs, and bring home the Larry O'Brien trophy. These forecasts update after every game and depth chart revision to give the most accurate picture of today's NBA landscape.   A fancy visual interactive <a href=""https://projects.fivethirtyeight.com/2020-nba-player-ratings/"">player ratings leaderboard</a> that answers the question: ""Who are the best offensive, defensive, and overall players in the NBA this season?""   Interactive <a href=""https://projects.fivethirtyeight.com/2020-nba-player-projections/"">player projections</a> that identify similar players throughout NBA history and uses them to develop a probabilistic forecast of what a current NBA player's future might look like.   <a href=""https://fivethirtyeight.com/features/russell-westbrook-or-chris-paul/"">NBA Stat Battles</a> that pit sportswriter against sportswriter to answer some of the biggest NBA barroom debates about which player is better.   Reporting on players, teams, and trends throughout the NBA season, like <a href=""https://fivethirtyeight.com/features/luka-doncic-is-a-star-its-time-to-take-his-team-seriously-too/"">this piece</a> on the star-level performance of Luka Dončić.   Pretty much the rest of <a href=""https://fivethirtyeight.com/tag/nba/"">NBA reporting</a> FiveThirtyEight produces  </ul>"," RAPTOR's technical infrastructure is complicated and intensive, pulling data from multiple sources and building interactive front-end visuals. The project's back-end scaffolding uses a host of different tools and technologies, including:  <ul>  Amazon EC2 PostgreSQL database   API calls and cron jobs build in Ruby on Rails   Two STATA statistical models   Google sheets that bring in injuries and suspensions   Python/Monte Carlo simulation forecast model  </ul>  These tools and techniques are set up in a daily process that scrapes data from ~1,500 URLs across multiple sources, organizes these data into structured data inputs, runs multiple statistical models, stores results of those models in a SQL database, generates data outputs, and deploys builds of fancy front-end interactives that pick up these data outputs. Our front-end interactives use node builds (based in gulp) that use ArchieML and d3 amongst a host of other libraries and techniques to build visual representations of player rankings, team ratings, and season forecasts that update every day – or whenever a game is played or a new team depth chart revision is released. "," A project of this scale and complexity comes with a host of challenges, both in data analysis and front-end visualization. The data we use to power the player ratings in this project come from an incredibly detailed and nuanced statistical process. RAPTOR collects, parses, and analyzes thousands of pieces of publicly available data to evaluate NBA players in a statistically rigorous way – an inherently complex, complicated task when player value is often hard to concretely define and even harder to measure. This process parses an incredible volume of data pretty frequently, repeating every day, along with every time an NBA game goes final or when a team releases an updated depth chart. It also manages the complexities around modeling the real world of the NBA, incorporating injuries and suspensions to reflect a team's current rotation in real games. Nate and Jay were able to manage these complexities to build reliable, consistent, and surprisingly efficient scripts and processes to parse an impressive amount of data, crunch those numbers with sophisticated techniques, and spit out cleanly formatted data.    After parsing and processing the data, the design and development aspects of RAPTOR provided other challenges that our team navigated well. There were so many different analyses, forecasts, and ratings we needed to share with our users, along with explaining exactly how RAPTOR works. Our main player rating dashboard had to visually show our assessment of the top players, while still allowing users to explore the whole league and see stats for any player. The main RAPTOR dashboard manages these challenges well, providing an overall leaderboard that shows the top players four different ways (WAR, overall, offensive, and defensive ratings) while also giving a fully interactive scatterplot and heatmap-based table. "," RAPTOR provides some great opportunities to learn about how we took complex statistical modeling and visualized the results in an intuitive, understandable way. RAPTOR is a really ambitious project that covers web scraping, data aggregation, back-end web builds, statistical modeling, and interactive visualization. Taking a closer look at this project would give plenty of valuable takeaways to both back-end data crunchers and front-end designers. The visual design of the project was ambitious in its approach to condense complex information into concise ratings, providing both high-level takeaways and more expansive exploration. Exploring some of the visual design and development decisions our team made in this project would show interesting nuanced conversations we had around ""smaller"" issues like scaling the upper and lower bounds of our beeswarm charts (along with when we kept them consistent with each other, and when we didn't) along with high level decisions about what we wanted the takeaways of this dashboard to be – and how we approached showing those takeaways from a visual context.   A deep dive into the end-to-end data pipeline and analysis process would also provide great learning opportunities, both in our approach and technical execution. RAPTOR's data process touches a number of different coding languages, infrastructures and softwares that come together in an automated process that feeds data into our front-end visualizations. It includes inputs from web scraping, manual tweaks to incorporate injuries and connection with Slack that allows our team to be proactively involved while still being hands-off. The impressive volume of data flowing into and out of RAPTOR on a daily basis would make for an interesting and informative case study for data journalists. ",https://projects.fivethirtyeight.com/2020-nba-player-ratings/,https://fivethirtyeight.com/features/introducing-raptor-our-new-metric-for-the-modern-nba/,https://fivethirtyeight.com/features/how-our-raptor-metric-works/,https://projects.fivethirtyeight.com/2020-nba-predictions,https://fivethirtyeight.com/features/lebron-or-mj-raptor-picks-the-best-nba-players-of-the-past-40-years/,https://fivethirtyeight.com/features/the-best-nba-players-of-the-last-6-seasons/,https://github.com/fivethirtyeight/data/tree/master/nba-raptor,"Nate Silver, Jay Boice, Ryan Best, Ella Koeze", Nate Silver is the founder and editor in chief of FiveThirtyEight and the author of “The Signal and the Noise: Why So Many Predictions Fail — But Some Don’t.”  Jay Boice is a computational journalist for FiveThirtyEight. He was previously deputy data editor at The Huffington Post.  Ryan Best is a visual journalist for FiveThirtyEight.  Ella Koeze is a visual journalist for FiveThirtyEight. ,,,
Brazil,Agência Pública,Small,Participant,Best data-driven reporting (small and large newsrooms),Negros são mais condenados por tráfico e com menos drogas em São Paulo,05/06/19,"Investigation,Long-form,Database,Infographics,Crime,Human rights","Scraping,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,PostgreSQL,Python"," The investigation “<a href=""https://apublica.us8.list-manage.com/track/click?u=47bdda836f3b890e13c9f416d&id=fe97c41e04&e=2182ef3aa2"">In São Paulo, black people are convicted for drug trafficking more often and with less drugs</a>”, published in may 2019, sought to examine the institutional racism still present in the brazilian judiciary system. Through the analysis of thousand of drug sentences in São Paulo during the year of 2017, the piece revealed that, proportionally, most of the convicted people were black, and that black people were convicted with smaller quantities of drugs than their white counterparts. "," This data-driven investigation unveiled the currency and some of the consequences and extent of racism in the brazilian judiciary system. The piece was republished by 45 brazilian websites, including some of the most important news outlets in the country, such as UOL, Exame and El País Brasil.   The reporting was also mentioned in several brazilian articles regarding racism, the judiciary and the war on drugs, published in outlets such as Vice, Carta Capital, Alma Preta and Almanaque SOS. In some of these articles the data was used to to deny false statements made by Bolsonaro that racism is rare/doesn’t exist in Brazil.   Emicida, one of the most renowned brazilian rappers, quoted the data revealed by the investigation during a debate at a TV show. "," This piece was supported by a program developed by the journalists, using Python programming language, to scrape information and documents from the Court of Justice of São Paulo website. The program was also used to process and analyse the data and documents. The reporters also used a SQL database system; KNIME platform, to process and classify text files; and Tableau software, to analyse and produce visual data. At last, we designed didactic infographics with Agência Pública’s visual identity to easily present the investigation results to the readers. "," The biggest challenge was filtering the information gathered by the scrapping program we developed, since we obtained 2,5 million digitized sentences from the Court of Justice of São Paulo. This stage of the project allowed us to analyse the data based on our initial question: are black people and people of color more frequently sentenced than white people? There were sentences for all sorts of crimes: theft, homicide, murder in attempted robbery, domestic violence, rape, torture, etc. But we only extracted drug trafficking-related cases that went to trial in 2017; there were 24,000 sentences in the whole State of São Paulo and 4,000 in the city of São Paulo. We searched for the defendants characteristics, like race, in the case records and digitized documents - defendant information sheets, forensic reports, release orders - those 4,000 sentences. Then we cross-referenced the qualitative values of the sentences (acquittal x conviction) and the skin color of the defendants. "," We managed to show in numbers, through the analysis of a massive number of sentences, what many supposed but weren’t able to prove: the extent of the institutionalized racism that prevails inside the Court of Justice of São Paulo, the largest judiciary department in the world. ",https://apublica.org/2019/05/negros-sao-mais-condenados-por-trafico-e-com-menos-drogas-em-sao-paulo,,,,,,,"Thiago Domenici, Iuri Barcelos, Bruno Fonseca"," Thiago Domenici is an editor and reporter at Agência Pública with 17 years of experience with journalism and communications. Thiago has worked in another independent média outlets such as Caros Amigos and Retrato do Brasil. He is co-author of two non-fiction books: “Brasil Direitos Humanos”, published in 2008, and  “Ni pan ni circo – historias de hambre en América Latina”, published in 2016. Thiago has recieved several awards for his investigative work.    Iuri Barcelos is a reporter and audiovisual producer. He has worked at Agência Pública, at the Department of Human Rights and Citizenship of São Paulo and worked in the production of several documentaries.    Bruno Fonseca is a multimedia journalist at Agência Pública with a Master in Social Communication and experience in data and investigative reporting, infographics, and animation. My stories have won some national journalism awards, such as Prêmio Petrobras, Republica, Direitos Humanos, and Ministério Público de Journalism. I have also worked as a freelance reporter, fact-checker, journalism college professor, hyperlocal journalist, and TV editor. ",,,
Bosnia and Herzegovina,The Center for Investigative Reporting in Sarajevo,Small,Participant,Open data,Lenient Sanctions for Judges' and Prosecutors' Mistakes,16/05/19,"Investigation,Database,Open data,Fact-checking,Illustration,Infographics,Video","Adobe,Microsoft Excel"," After years of waiting for and coaxing judicial representatives into making asset declarations public, the Center for Investigative Reporting in Sarajevo (CIN) has set up a database of biographies and records on income, real properties, vehicles, savings, loans, shares and court proceedings of 29 judges and prosecutors.   CIN has also published a second database of disciplinary sanctions taken against judges and prosecutors while publishing two stories about those judiciary officials who kept evidence in their offices or failed to write verdicts and send convicts to prison. "," The publishing of our two databases and related stories have had three important consequences for the general public in Bosnia and Herzegovina and for the judicial community in particular.       First off, the Bosnian public for the first time got a chance to access information about judiciary officials in a single place in a clear, searchable and in-dept manner. For example, the database features names of sanctioned judges and prosecutors who have repeatedly violated laws and court procedures over the past nine and half years.   Several hundred court cases were now exposed where judges and prosecutors took no action while citizens waited helplessly or criminals got off scot-free because some judges and prosecutors kept these files hidden in their drawers.       Secondly, the representatives from the Bosnian judicial oversight body have published new guidelines for judges and prosecutors in order to improve their work. Also, non-governmental organizations started monitoring disciplinary hearings, while Bosnian journalists began using the database as a research tool their stories.     Third, the project galvanized the public on the issue of transparency of judges and prosecutors’ asset declarations and their financial statements for work done outside of their office. As a consequence of this, some members of judiciary started publishing their asset declarations on their own after years of denial.  "," During the work on this project, reporters have sent over 100 requests for access to information, collected over 1,000 documents from courts and prosecutor’s offices and interviewed 50 judicial officials and citizens who were wronged because of judges and prosecutors’ errors. "," During reporting, reporters encountered numerous difficulties: the judicial oversight body, courts and prosecutors’ offices didn’t want to share this information. Some of the most powerful judiciary officials warned reporters that publishing of the database might have an adverse effect on judges and prosecutors’ integrity. Other judicial institutions tried to delay complying with the law deadline or outright refused to provide information that reporters had a right to access.   The project was published at the time when state prosecutors put pressure on Bosnian journalists to not publish their findings, while some of our colleagues were also interrogated by the prosecutors ","     Reporters and civil society organizations may learn how to collect records that government agencies refuse to make publicly available. The general public wants this type of information to be transparent so that it can monitor the work of judicial institutions and also learn more about the right of public to know and how it can act if they are not happy with the work of the judges and prosecutors. The project will also help reporters and organizations in the countries that don’t have transparent judiciary to learn how to get similar information. However, the most important thing is that such a project shows the power and significant of investigative reporting as a method to make government institutions more transparent and accountable.  ",https://www.cin.ba/en/sudije-i-tuzioci-funkcija-javna-imovina-tajna/,https://www.cin.ba/en/blage-kazne-za-greske-sudija-i-tuzilaca/,https://www.cin.ba/en/vstv-godinama-tolerisao-nerad-sudije/,,,,,"Merima Hrnjica, Selma Učanbarlić","  Merima Hrnjica    Merima Hrnjica joined CIN in 2011, after four years as a journalist and editor for the Balkan Regional Investigative Network ((BIRN). Prior to BIRN, she worked as a reporter and deputy editor of the cultural-info magazine Echo, an assistant on the TV show Pošteno Govoreći on BHT and a volunteer on the student eFM radio. She holds a journalism degree from the School of Political Sciences in Sarajevo and in 2011 she participated in a Reuters scholarship program on economic and political reporting.    Selma Učanbarlić    Sarajevo-born Selma Učanbarlić joined CIN in Nov. 2015. She worked as a journalist and court reporter for the Balkan Investigative Regional Network in BiH and the regional broadcasting service InfobiroTV. She received her B.A. in Communicology from the School of Political Sciences in Sarajevo in 2009. ",,,
United States,"ProPublica, Wired",Big,Shortlist,Innovation (small and large newsrooms),"Aggression Detectors: The Unproven, Invasive Surveillance Technology Schools Are Using to Monitor Students",25/06/19,"Investigation,Explainer,Database,Crowdsourcing,Chart,Video,Audio,Crime,Economy","AI/Machine learning,Sensor,CSV,Python"," In response to mass shootings, some schools installed devices with algorithms that purport to identify aggressive voices before violence erupts. Our data analysis found this technology unreliable.   Our reverse engineering found that while the device tended to infer aggression from strained voices, it had troubling blind spots. The device raised false alarms for benign sounds like laughing, coughing and cheering, yet high-pitched screaming often failed to trigger an alarm.   This confirmed our reporting, which found instances where the algorithm often reported false alarms, yet failed to trigger during a dangerous situation at a New Jersey hospital. "," The story raised questions about the effectiveness of electronic surveillance devices installed in the name of safety. It also examined the rise of uninterpretable and unappealable black-box algorithms that pass judgment on people without their consent or knowledge.   After our story, the ACLU, <a href=""https://www.aclu.org/blog/privacy-technology/surveillance-technologies/bogus-aggression-detectors-are-audio-recording"">citing our investigation</a>, urged school districts and state legislatures to ban surveillance technologies such as facial and voice surveillance, as well as social media monitoring in schools. "," We purchased the device to reverse-engineer and test it.  We rewired its programming so we could feed it any sound clip of our choosing, and then played gigabytes of sound files for the algorithm and measured and collected its prediction for each.    The result was a database of public voices and sounds. The database consisted of a snippet of sound, its spectrum or ""fingerprint"" (generated using our data pipeline) and the algorithm’s prediction. We then used the millions of rows in this database to reverse-engineer the algorithm and analyze where it could be flawed.   After this preliminary testing, we ran several real-world experiments to confirm our suspicions.  We recorded the voices of high school students in real-world situations, collected the algorithm’s predictions and analyzed them.   All of the programming done for the data analysis and machine learning was done in Jupyter Notebooks in Python. We used scientific computing packages such as scipy, numpy and pandas for the data transformation and analysis, scikit-learn, statsmodels and xgboost for machine learning and reverse engineering, librosa for sound analysis, and matplotlib and seaborn for data visualization. ","We overcame numerous technical challenges in order to peer inside a black-box algorithm and understand how it passes incorrect judgments about ordinary people. Previous ProPublica investigations analyzed machine-learning algorithms that make predictions from structured, tabular data, but we had not yet seen any investigative reporting that analyzed machine learning on unstructured data, such as video or audio. We had to be innovative in how to collect and record the algorithmic output of the device. We modified the programming of the device so that we could test it with our own, custom-written software. The result was a multi-million row database of sound clips and their purported ""aggression"" s that allowed us to reverse-engineer the algorithm Raw sound data is also difficult to analyze. But we used techniques from signal analysis and processing rarely used in data journalism, such as Fourier transforms, to derive a spectrum — essentially a fingerprint — for individual snippets of sound. We studied academic papers in audio analysis and interviewed researchers in the field to arrive at a set of data features that we could derive and analyze from a sound spectrum. Finally, we also overcame a number of technical challenges in our field testing. We reproduced an in-school setup as closely as possible in our experiments. We recorded our sound using microphones bought from the company and relied on company manuals to set up their location, distance and height. When we brought our findings to the company, they disagreed with our conclusions, citing a audio engineering phenomenon known as ""clipping"" — when a microphone becomes overwhelmed by too much noise, distorting the sound and potentially throwing off the algorithm's readings. We only found clipping in a small subset of our data, but to be sure we re-recorded the sound, controlling for clipping. Our results remained the"," Our project showed that with the right tools and expertise, extensive reporting and advice from experts, it is possible to peer inside the black box of a machine learning algorithm. Journalists are often the first to report on misuses of flawed technology, and with machine algorithms becoming more widely used, it's essential to help the public understand the impacts of machine-assisted decision making at scale.   We also wrote an extensive methodology to help other reporters and researchers do similar investigations in the future. We also put a lot of thought into how to best present to our readers a complex, technical topic such as machine learning. The integration of audio and video into our story helped give readers a sense of the device’s flaws that could not be conveyed only by words. ",https://features.propublica.org/aggression-detector/the-unproven-invasive-surveillance-technology-schools-are-using-to-monitor-students/,https://projects.propublica.org/graphics/aggression-detector-data-analysis,https://youtu.be/WUL_Kk5EiNw,https://youtu.be/lsW6ROCTWIg,,,,"Jack Gillum, Jeff Kao"," Jack Gillum is a senior reporter at ProPublica covering technology, specializing in how algorithms, big data and social media platforms affect people’s daily lives and civil rights. He joined ProPublica in July 2018.   Gillum came to ProPublica from The Washington Post, where he was part of the investigative team that dug into mismanaged taxpayer funds and troubled relief efforts in Puerto Rico. Prior to the Post, Gillum was an investigative reporter at The Associated Press, where he broke stories on the existence and location of Hillary Clinton’s private email server, as well as a U.S.-backed “Cuban Twitter” program that secretly mined data for political purposes. At the AP, he also covered two presidential races and the world of campaign finance.   Gillum began his career as a business reporter and database specialist at the Arizona Daily Star in Tucson, his hometown. He is a graduate of Columbia University's graduate school of journalism and Santa Clara University in California. He lives in Washington, D.C.   Jeff Kao is a computational journalist at ProPublica who uses data science to cover technology and artificial intelligence. He used natural language processing techniques to uncover 1.3 million fake comments submitted to the FCC in its proceeding repealing net neutrality. This work was cited in the Washington Post, Fortune Magazine and engadget, among other publications, and by members of the U.S. Senate. He has appeared as a data scientist in the New York Times and on the WNYC program Science Friday.   Kao previously worked as a machine learning engineer at Atrium LTS, where he developed natural language processing systems for legal services. He holds a law degree from Columbia Law School, where he was the editor-in-chief of the Columbia Science and Technology Law Review, and a bachelor’s degree in systems design engineering from the University of Waterloo. ",,,
Uruguay,la diaria,Big,Participant,Innovation (small and large newsrooms),la diaria datos - open/collaborative exit polls,27/10/19,"Breaking news,Open data,Crowdsourcing,Elections,Politics","R,RStudio"," 2019 elections in Uruguay were particularly unpredictable with new political parties, eroded trust in polls due to large differences with results in previous elections, and no clear favourites for winning with very small margins in polls leading to the election.   In that context, la diaria (a cooperative owned, subscription-based newspaper) decided to leverage it's heavily active community of subscribers to crowdsource real-time exit poll data through an app, gathering precise, high quality data, that thanks to a collaboration with the University of the Republic (UDELAR) allowed to create an indepent exit poll and live, real-time coverage of the election night. "," First and foremost, this allowed la diaria to have first hand, independent access to real-time data of the highest quality, in a context of distrust of traditional exit polls. And to be perfectly clear about the quality, this crowdsourced, technology-aided effort was beyond successful, achieving the closest results to the actual vote compared to evey other exit poll (Link 3).   It also enabled deeper analysis of voter behaviour, leading to insights such as detecting that the left-wing candidate was able to capture 27% of the far-right vote in the runoff vote. A larger percentage than the one captured from center-right and right wing parties (Link 5).   The availability of real-time data also allowed to form partnerships with other media to explore new formats. The exit polls results were available through the newspaper's website and social media presence, but also through live TV coverage in partnership with TevéCiudad (local government TV channel) and live radio coverage through the University's UniRadio. These transmissions included political commentary from la diaria's journalists, and professors from the University's Faculty of Information and Communication (Link 7).   For la diaria, the biggest impact might have been the real-world proof of the potential of it's community of subscribers and allied organizations, to create powerful and impactful reporting. Data and technology have been part of the newspaper strategy for many years, as well as community, which has been fundamental for sustaining the project, and a source of much of the quality content published by la diaria. But for the first time, this success has proven that the community, aided by technology can help achieve a feat as surprising as beating opinion polling companies at their own game. "," The statistic sample was randomly selected among electoral circuits, it includes three strata shaped on available open data from 2014 elections; one with 3.000 circuits where the incumbent party had majority, one with 3.000 circuits where the sum of opposition parties had majority, and one with 1.000 circuits with new voters.   On every one of the 150 electoral circuits randomly drawn from the sample, a volunteer la diaria subscriber was present when the ballot box was opened, and uploaded online the first 50 opened votes in reral-time, through a purpose built app developed by la diaria's IT department. This level of coordination with volunteers and the drafting among subscribers was only possible thanks to the open source CRM also developed by the IT department and the in-house call center.   All data is available as open data (Links 1,7), including data by circuit (circuitosLD.csv), individual votes (votosLD.csv), sample information (Muestra_ld_con_st_y_orden.csv) and two files with the partial results obtained through the evening, (porcentajeAcumLD.txt y totAcumLD.txt). The Rstudio project (LD.Rproj) and Rmd (reporteLD.Rmd) are also open and results fully replicable using R and Rstudio. "," This project couldn't have existed without a vibrant community that has been growing, sharing and learning over 14 years since the foundation of la diaria (through community crowdfunding, incidentally). This community and this project have allowed for a new paradigm of collective building of the newspaper and reporting, evolving beyond financial support, beyond being a prime source of experts for consultation in the newsroom, beyond generating a substantial percentage of the published content, and into allowing the paper to punch way above its weight, with a country-wide, real time collective effort involving over 200 people, that would have been logistically and financially impossible without the combined support of community and technology.   We believe it should be selected because it proves beyond any doubt that not only independent journalism is possible, but it can also aim much higher than traditional reporting, and develop complex, logistically challenging initiatives that allow complete freedom from corporate sources of information, through strong communities, open source principles, and collaboration with key partners. "," Hopefully, this helps demonstrate that innovation in the newsroom does not come exclusively from the use of sophisticated technologies, but also from exploiting the power of collaboration with the same people that are interested in what journalism has to say. In this particular case, innovation also comes to disrupt a long-standing status quo, where corporate players have had dominance for decades, with an absolutely opaque operation.   This has also been possible with very limited resources, which we hope positively impacts it's chances of replicability in other countries and contexts.   And most importantly for replicability, all components of this project are open sourced and available for others to use or improve on. From the software used to collect data, and the analysis done with it, to the CRM used to coordinate volunteers. ",https://ladiaria.com.uy/articulo/2019/12/la-diaria-open-data/,https://ladiaria.com.uy/articulo/2020/1/la-diaria-datos-libero-su-trabajo-de-proyeccion-de-escrutinio-sobre-el-balotaje/,https://ladiaria.com.uy/articulo/2019/11/la-metodologia-detras-de-la-proyeccion-de-escrutinio-de-la-diaria/,https://ladiaria.com.uy/articulo/2019/10/escrutinio-entre-nosotros/,https://ladiaria.com.uy/articulo/2019/11/daniel-martinez-recogio-mas-adhesiones-entre-votantes-de-cabildo-abierto-que-del-partido-colorado/,https://drive.google.com/file/d/1_FyEXmt09JVnjm-WnsxaMBYQdqUN7NpV/view,https://youtu.be/Xc5Uyvvx5Bs?t=1424,"Natalia da Silva, Juan José Goyeneche, Ana Coimbra, Lucas Silva, Ana Tuduri"," Juan José Goyeneche   Ph.D. in Statistics, Iowa State University. Master in Mathematical Statistics, Centro Interamericano de Enseñanza de Estadística (CIENES), University of Chile. Public Accountant. Administration graduate. Associate Researcher, Centro de Investigaciones Económicas (CINVE). Former Direction Chief, Instituto de Estadística, Facultad de Ciencias Económicas y de Administración, Universidad de la República. Professor of Statistics.   Natalia da Silva   Assistant Professor in the Department of Statistics at the Universidad de la República in Montevideo (UDELAR-IESTA). Ph.D. in Statistics at Iowa State University since July 2017 working with Di Cook and Heike Hofmann. Interested in supervised learning methods, prediction, exploratory data analysis, statistical graphics, reproducible research and meta-analysis. Co-founder of R-Ladies-Ames and R-Ladies-Montevideo. Working in different initiatives to get a stronger and bigger R community in Latin America.   Lucas Silva   Graduate in Communications at the Universidad de la República. Works in la diaria newspaper since its foundation, in march 2006. This newspaper is the second largest by sales in Uruguay and one of its defining characteristics is its business model: 80% of income in brought in by subscriptions.  In la diaria he was editor of Economics and of Politics, and since 2014 he occupies the position of Journalism Director. He is also professor of Political Journalism in Universidad de Montevideo and collaborates as opinion columnist in the spanish version of The Washington Post.   Ana Coimbra   Graduate in Statistics at the Universidad de la República in Montevideo (UDELAR-IESTA).Master in University Teaching at the Universidad de la República. Working in the Faculty of Medicine (UDELAR) researching evaluation and medical education, and at the Faculty of Economic Sciences and Administration (UDELAR), on the design and implementation of sampling techniques, and monitoring/data analytics for Plan Ceibal.   Ana Tuduri   Ana Tuduri, is a lawyer graduated in the Faculty of Law of the Universidad de la República Oriental del Uruguay (UDELAR). Her research is centered in privacy, surveillance, intellectual property and digital rights, as well as open data and access to public information. She is currently working with la diaria in a data journalism project related to water resources in Uruguay. ",,,
Spain,RTVE.es,Small,Participant,Innovation (small and large newsrooms),A thousand women murdered,19/06/19,"Investigation,Documentary,Database,Open data,Fact-checking,Illustration,Infographics,Chart,Video,Map,Audio,Women,Crime,Human rights","Scraping,Microsoft Excel,Google Sheets,CSV,Node.js"," The website rtve.es/milmujeres is an interactive initiative created after intense research work to document who were the women murdered due to gender-based violence in Spain and how they used to live before being killed by their partners since the official records exist (January 1st, 2003).  The objective of the project was to make the victims visible and to document their lives from a new perspective, avoiding to portrait women’s deaths, but putting the focus on the sorrows of the murderers. "," The website impact of the has reached 45.000 unique users and 70.000 website visits so far. It has also transmitted on air to more than 1.530.000 people. The social media impact was huge, more than 140.000 impressions of the first tweet and more than 45.000 reproductions of the first video of the project. It has won the Silver and People's Lovie Awards. It has also been awarded with one of the most prestigious laurels in Spain about gender violence: the  ""Journalism against machismo"" award of the Ministry of the Presidency, Relations with the Courts and Equality of the Government of Spain. It also received the IV Journalism against Gender Violence Awards Fundación Grupo Norte.   Therefore, as a part of the initiative users who have known the murdered women can participate in this tribute to the victims and collaborate in the documentation work of the project by sending a message through whatsapp. The response was unprecedented. There was a flood of new messages, providing new data, congratulating for the initiative, or sending us photos and audios explaining how the women murdered was like.   The news of the one thousand murdered was frontpage of all the media and our company broadcast features about our project in TV and Radio news programs. It was a chance to offer quality content and complete the story with all the feedback that we got from the relatives.  "," More than 20 people, including journalists, developers, designers, data analyzers and tv directors, have worked together to create one of the most ambitious database about the violence against women in Spain, describing life details of all the official victims in our country.    The archive of RTVE, the official records of the Government about this kind of violence, the local newspapers and the judicial sentences were just some of the sources used to develop a database of the project.    The data was extracted from an extensive database in which we crossed both information from the Spanish Delegation for Gender Violence and records from the Spanish Women's Institute (all those cases prior to 2006 come from there), as well as other reference sources such as ibasque.com or femicidio.net. It was used some data analysis and visualisation tools such as Flourish for creating the graphics and chronologies. The design and format of the website is and original creation from the LAB Rtve.es and the website development is also created from scratch with code.    Some of the technologies used are React, Node JS and Excel. "," The hardest part was to recollect all the details and personal info about the victims, and to  develop the biggest, updated and more complete database about gender violence in Spain,  better even than the official one.    From LAB RTVE.es, the innovation department developed this project with two goals: help prevent this kind of violence, show that there is not a specific profile of women who is victim to it, and repair the memory of the victims by giving the families the chance to pay homage to them. For that reason, this is an interactive project that aims to explain who these women were and how they lived before being victims of gender violence in Spain, with the help of the audience by trying to break the silence around a type of crime that can´t be considered private violence anymore.     It is also an innovative project because it has been implemented by user generated content . The feedback of the relatives of the victims was essential for the project and that's why we opened a phone number to get in touch with them by whatsapp messages and voice notes. The call to action was pushed by off line TV and Radio shows and it let us to get a lot of quality feedback to improve our database, after we verificate the information we got through the whatsapp.    The other big goal was to talk about what happens after the crime . The perpetrators who took their lives do not fit any profile, they do not share any social or economic traits. Therefore, we underline their names, their prison sentence and the financial compensation they had to pay to their families. ","   People can get conscious, raise awareness and help in prevention of this gender-based violence. In addition, finding statistical patterns to this social scourge by reviewing the data by community, province, age of the victims, as well as the origin of the aggressors can bring a new way of understanding this crime with more context and from a different perspective.  ",https://lab.rtve.es/mil-mujeres-asesinadas/,https://readymag.com/labrtves/1000m/,https://lab.rtve.es/mil-mujeres-asesinadas/datos/,https://lab.rtve.es/mil-mujeres-asesinadas/016/,,,,"Alicia G. Montano, Miriam Hernanz, Estefanía de Antonio, Esther Pérez-Amat, Daniel Borrego Escot, Jessica Martín, Beatriz Gálvez, Raquel Navarro, Cristina Pérez, José Ángel Carpio, César Peña, Ana Ortas, Víctor Peña, Pelayo Prieto, Marcos Martín"," An intense work of documentation in which a team of almost 20 professionals: journalists, documentalists, audiovisual producers, web developers and graphic designers have worked together make this project real and humanize each single case. All these professionals comes from the LAB RTVE.es, the innovative department of the Spanish Public Television and the newsroom of the website Rtve.es. ",,,
United Kingdom,Tortoise,Small,Participant,Best data-driven reporting (small and large newsrooms),Big little lies,12/11/19,"Fact-checking,Infographics,Chart,Elections,Politics,Brexit","Scraping,D3.js,Google Sheets,CSV,R,RStudio"," We decided to track and visualise all the ‘untruths’ told by parties and politicians during the UK general election in 2019.   Using the UK's leading fact-checking organisations as our source, we began tracking untruths from the start of the election (6 November) up to two days before polls opened (10 December).   The result is an interactive timeline of each untruth which readers can tap to discover the claim made, who made it, on what platform and the fact-checker’s verdict. We also gave each untruth a ‘severity’ score so that we could assess where the ‘worst’ untruths were coming from.    "," The piece – and screenshots of the graphics – went viral on Twitter the day before the election. Tortoise’s tweets about the article received around 800,000 impressions in total. The evening the piece was published, it featured as a top UK news item on Twitter, before being shown on TV show, BBC Outside Source. "," We began by getting a list of all the URLs from the fact-checking site’s base URLs (eg “fullfact.org/election-2019”) using ‘xml-sitemaps.com’. We then converted these xml files to csv, scraped the web pages using rvest to get key information (like date published) and filtered out any articles that were published before the election began. The URLs and dates we were left with was essentially a list of all the lies we would assess. We then described, categorised and scored each one manually in a Google spreadsheet. To create the visualization, we used JavaScript (D3) and Flourish. "," Working out a way to systematically measure lying.   The first challenge here was the most fundamental: establishing what actually counts as an “untruth”. We ended up using a broad definition to cover any statement, manipulation or misrepresentation where politicians strayed from a known truth, ranging from misleading remarks to outright lies.   The next challenge was to fairly assess the severity of each lie. We came up with a scoring system that took into account both the significance of the original claim being made (For example, is this about a relatively trivial matter, such as a post-Olympic baby boom, or a major electoral issue that will likely affect how people vote, such as spending on the NHS?) and the untruthfulness of the lie. If the claim was untrue simply because no one knows the real answer we gave it a ‘1’. If it’s just factually wrong we gave it ‘2’. And if there’s a strong reason to believe this was a more deliberate act to deceive or distort the truth then we gave it a ‘3’. We then multiplied the significance score by the untruthfulness score to get an overall severity score.    "," You shouldn’t be put off from trying to measure something that might seem ‘unmeasurable’ – the key thing is come up with a systematic criteria, sense-check it with others and be upfront about the methodology with the reader. ",https://members.tortoisemedia.com/?article=page-111193&edition=com.tortoisemedia.tortoise.timelinetoday_tortoise_today,https://twitter.com/caitlinmoran/status/1204722900965744644,https://twitter.com/DeborahMeaden/status/1204751841797562368,https://twitter.com/HPIAndyCowper/status/1204710227242897408,,,,"Matthew d'Ancona, Basia Cummings, Ella Hollowood, Chris Newell","  Matthew d’Ancona  Editor and partner, Tortoise. Previously editor of The Spectator.    Basia Cummings  Editor, Tortoise. Previously at Huffington Post and The Guardian.    Ella Hollowood  Data journalist, Tortoise. Previously at Beyond Words Studio and Information is Beautiful.    Chris Newell  Data visualization designer, Tortoise. Previously at The Telegraph and The Guardian. ",,,
United Kingdom,Tortoise,Small,Participant,Best data-driven reporting (small and large newsrooms),The political climate,27/11/19,"Illustration,Infographics,Elections,Politics,Environment","D3.js,Google Sheets"," How have the UK political parties responded to the increasing urgency of the climate crisis in their manifestos? This was the question that Giles Whittell sought to answer in ‘The political climate’, written for Tortoise during the UK election in 2019. Together with designer, Chris Newell, he showed exactly how often environmental issues were mentioned in each of the main parties’ manifestos, from where it got first mentioned to where (and if) it got a full treatment. The article also compared parties on their climate commitments, from spending to power storage and the number of trees they would plant. ", The piece – and screenshots of the graphics – were widely shared on Twitter and featured in the J++ Stockholm newsletter.  ," Google spreadsheets for research and analysis. JavaScript, D3 and Flourish for data visualization. ", Finding ways to compare like with like on subjects where each party cherry-picks from the data to seem more concerned than they really are. For example money earmarked for climate mitigation could be specifically for carbon reductions or part of a massive reorganisation of the economy.  ," Sometimes the best data-driven reporting is the simplest and even better if it can take readers straight to the original source. This piece – among others – inspired us at Tortoise to start a new series called ‘Primary sources', where we take a systematic look at a primary source that's making the news. Since looking at party manifestos in ‘The political climate', we have covered the collected work of Dominic Cummings (https://members.tortoisemedia.com/2020/01/24/200124-primary-sources-dominic-cummings/content.html) and the meaning of ‘Sussexit' (https://members.tortoisemedia.com/2020/01/10/harry-and-meghan-annotated/content.html). ",https://members.tortoisemedia.com/2019/11/27/climate-election/content.html,https://twitter.com/pollycurtis/status/1199590442775568389,https://twitter.com/basialcummings/status/1199604625357332480,,,,,"Giles Whittell, Chris Newell", Giles Whittell is a partner and editor at Tortoise. He is also a published author and has worked for The Times as Correspondent in Russia and the United States.    Chris Newell is data visualization designer at Tortoise.    ,,,
United Kingdom,Tortoise Intelligence,Small,Participant,Best data-driven reporting (small and large newsrooms),Global AI Index,12/03/19,"Explainer,Long-form,Cross-border,Database,Infographics,Business,Women,Immigration,Economy,Employment","Scraping,D3.js,Google Sheets,CSV,R,RStudio,Python"," Is the world ready for AI? Who’s winning the global AI race?   As governments and industries try to keep pace, our flagship index is the first to benchmark countries on AI innovation and implementation.   The Global AI Index analyses how 54 countries are driving and adapting to AI’s accelerating development across 100 indicators covering themes of talent, infrastructure, operating environment, research, development, government strategy and commercial ventures. Each has been weighted for importance after consultation with experts across the field.    "," The impact of the project has been to provide a framework, data and a forum for policymakers, academics, the business community and the wider public for understanding what is driving the global AI race. Since creating the Index, we’ve entered a data-sharing partnership with the UK’s national institute for data science and artificial intelligence, the Alan Turing Institute. We’ve presented our unique indicators on online coding communities to representatives from the Department for Business, Energy and Industry. We’ve held ThinkIns - Tortoise’s open editorial conference format - with members of the public to showcase our findings and walkthrough specific areas of AI, particularly ethical issues.    In terms of press coverage, the Global AI Index was covered by BBC News, The Independent, The Telegraph, South China Morning Post, Le Canada, Abacus News, Towards Data Science, IT Technology News and in the MIT Technology Review newsletter. Head of Tortoise Intelligence, Alexandra Mousavizadeh, was also interviewed by the BBC World Service.    "," For data collection, the Global AI Index firstly relied on web scraping. For instance, we created a unique dataset on the world of AI academia by scraping journal websites for published research papers. Often we would gather only the university affiliation of the authors so we embarked on a large-scale geocoding process to associate these authors with particular countries.   We also relied on a wide range of APIs for the Global AI Index. We tapped into code-hosting platform GitHub’s API to gather data on how developers across the world are building and using AI-enabled software. We examined the way communities were being formed around AI via the API of events platform Meetup.   To help analyse particularly large databases, we used Google BigQuery. This tool was critical for, for instance, filtering massive databases on patents filed internationally to find out who has been behind the development of AI technology.    The entirety of the Global AI Index’s rankings and findings were visualised through data visualisation tool Flourish. This included a bespoke template developed by the Flourish team that allowed us to visualise clusters of nations based on the way they are developing and applying AI technology. "," The hardest part of the project was overcoming Western bias within our data sources. We wanted the Index to be truly global and not just a measure of AI activity in Europe and the US. But some of our sources represented other nations poorly - for instance, our use of business information platform Crunchbase means we have likely underestimated non-Western AI commercial activity. Going forward, we are building on the first iteration of the Global AI Index by improving our source selection.  "," One of the main lessons of the Index is that the US is the undisputed leader in AI development. The western superpower scored almost twice as highly as second-placed China, thanks to the quality of its research, talent and private funding. America was ahead on the majority of key metrics – and by a significant margin.    However, on current growth experts predict China will overtake the US in just five to 10 years. China is the fastest growing AI country, our Index finds, beating the UK specifically on metrics ranging from code contributions to research papers in the past two years. Last year, 85 per cent of all facial recognition patents were filed in China,  we found, as the communist country tightened its grip on the controversial technology. Beijing has already been condemned for using facial recognition to track and profile ethnic Muslims in its western region.   Britain is in third place thanks to a vibrant AI talent pool and an excellent academic reputation. This country has spawned hugely successful AI companies such as DeepMind, a startup founded in 2010 which was bought by Google four years later for $500 million. Britain has been held back, however, by one of the slowest patent application processes in any of the 51 countries. Other countries are snapping at its heels.    ",https://www.tortoisemedia.com/intelligence/ai,https://members.tortoisemedia.com/?article=page-108299&edition=com.tortoisemedia.tortoise.timelinetoday_tortoise_today,https://members.tortoisemedia.com/2019/12/03/global-ai-index-2-countries/content.html,https://members.tortoisemedia.com/2019/12/04/ai-in-the-uk/content.html,https://members.tortoisemedia.com/2019/12/05/the-global-ai-index-part-5-the-new-world-order/content.html,https://members.tortoisemedia.com/2019/12/05/global-ai-audio/content.html,,"James Harding, Alexandra Mousavizadeh, Andrew Haynes, Alex Clark, Luke Gbedemah, Alice Thwaite, Alexi Mostrous, Chris Newell, Ella Hollowood."," James Harding is editor and co-founder at Tortoise. Before Tortoise he was Director of BBC News and Editor in Chief of The Times.   Alexandra Mousavizadeh is director of Tortoise Intelligence and has more than 20 years’ experience in ratings and indices. Before this, she was Assistant Vice President for the Africa sovereign ratings portfolio at Moody’s, and was the founder of the Global Disinformation Index.   Andrew Haynes, Alex Clark, Luke Gbedemah, Alice Thwaite and Ella Hollowood are members of the Tortoise Intelligence team.   Alexi Mostrous is editor and partner at Tortoise. Before Tortoise he was Head of Investigations at The Times.   Chris Newell is data visualization designer at Tortoise.    ",,,
United Kingdom,Tortoise Intelligence,Small,Participant,Best data-driven reporting (small and large newsrooms),The Responsibility100 Index (Beta),23/09/19,"Database,Open data,News application,Infographics,Chart,Environment,Money-laundering,Business,Women,Health,Economy,Employment,Human rights","Scraping,D3.js,Google Sheets,CSV,R,RStudio,Python"," There is an accountability gap for Britain’s most powerful companies. Annual reports often declare a commitment to sustainability goals, but rarely are these pledges followed up. The Responsibility100 Index is designed to close this gap and keep companies honest.   It’s also a first. Previous attempts to measure corporate responsibility have used proprietary data, or direct company surveys, but the Responsibility100 uses only publicly available data. And unlike previous attempts that measure only what a company says, this one directly compares a company’s commitments – in the form of their ‘talk’ score – with their actions – their ‘walk’ score.    "," The Responsibility100 has inspired a race to the top on sustainability reporting. The first step on the road to solutions – for the climate crisis, for equality and sustainability – is knowing the extent of the problem. The Responsibility100 is a landmark project in terms of accountability, providing comprehensive data across multiple issues for public use.    We’ve built the index to incentivise transparency. When a company fails to report data for a particular indicator, they are treated as if they had the worst possible datapoint. For instance, though around half of the FTSE100 report their renewable energy use, those that don’t are assumed to use 0%. But this assumption means a company can never lose points by disclosing data – instead, they are likely to move further up the ranking by becoming more transparent.   It’s this incentive structure that means the Responsibility100 has already encouraged changes in the way the FTSE 100 report their data. A major supermarket is now considering publishing their waste-to-landfill figure in response to the Index. Another company has told us that though it already pays a living wage it is now seeking accreditation to increase their Index performance.    ","   To build Responsibility100, we first developed a methodology for a team of data journalists and scientists to manually examine the FTSE 100’s annual reports in a consistent way. The team relied on a pre-agreed collection template when reading sustainability material, awarding points for particular indicators when finding sufficient evidence.   However, we also developed automated tools to collect data. For example, to create a first estimate of the gender proportions of the FTSE 100’s boards of directors, we wrote a Python script that used the Companies House API to download each company’s register of directors and then guessed each director’s gender based on their name. Likewise, to get a sense of how often the FTSE 100 were being taken to a tribunal by employees, we used the web scraping framework Selenium to gather cases available from the gov.uk website.   One particular technological challenge during data collection was to account for the large number of subsidiaries that each of the FTSE 100 has significant control in. We mined the Companies House ‘People with significant control snapshot’ as well as undertaking tables in annual reports to create a parent-subsidiary network for each FTSE 100. In total, we found the FTSE 100 had majority control in around 20,000 distinct companies in 2018. Having this data meant we could not only search for, for instance, the employment tribunal cases involving the FTSE 100 but also any case involving one of their subsidiaries.   To visualise our investigation, designer Chris Newell created custom templates using the Flourish SDK, including a full ranking table that showcased each of the FTSE 100’s strengths and weaknesses.  Another highlight of Chris’s work was a dumbbell chart to demonstrate which of the FTSE 100 had the biggest gap between their ‘talk’ on sustainability and their ‘walk’.     "," The hardest part of the project was the technical side of data collection. At best, we faced well-populated data tables provided by companies that were nonetheless contained in impenetrable formats like PDFs. Examples included gathering scope 1, 2 and 3 emissions values as well as the addresses of subsidiaries for checking whether they were in tax havens.    At worst, we had to discard indicators that seemed obvious choices for assessing corporate responsibility but proved unfeasible in practice. For instance, though we could gather the number of meetings the FTSE 100 were having with UK ministers and EU commissioners, we were unable to tell what had been discussed in those meetings. This meant we couldn’t assess whether the company was making a positive contribution on sustainability issues and so we stopped short of including this data in the final ranking.   However, while time-consuming, the benefit of the manual approach we were often forced to adopt has been to lay the foundations of an AI algorithm capable of extracting this data instead. As we've been going through annual reports we’ve been noting down page numbers, extracting text and taking screenshots - building a training set with an eye to automation. We hope to deploy such a model by the final quarterly update of the Responsibility100 in 2020 as we continue amassing training data in the meantime.  "," The first lesson to take away from Responsibility100 is that more transparency is needed in corporate sustainability reporting. While there are examples of good progress - for instance, the UK government’s required publishing of gender pay gap data - in other areas we faced huge gaps. Responsibility100 shines a particularly bright light on issues of data availability because, in being an index, by definition we require like-for-like, quantifiable data to fairly compare companies.    But another lesson from Responsibility100 is that having complete data on a company only gets you part of the way there on judging their sustainability efforts. In building the index and deciding on weights, we grappled with issues such as: if you are in an inherently environmentally damaging industry like mining or energy, but committed to sustainability in word and deed, perhaps you should be able to outperform a tech or media company with a much smaller environmental footprint that’s doing little to improve the prospects for people or the planet.    However, the lesson is not that these comparisons between companies shouldn’t be made -  rather, Responsibility100 shows us that though no two companies face exactly the same sustainability demands, all can be assessed on whether they’re meeting a bare minimum as well as on how well they’re trying to meet their own distinct duties.     ",https://members.tortoisemedia.com/2019/09/23/responsibility-100-day-one/content.html,https://members.tortoisemedia.com/2019/09/24/responsibility-price-of-doing-business-190924/content.html,https://members.tortoisemedia.com/2019/09/26/responsibility100-climate/content.html,https://members.tortoisemedia.com/2019/09/27/responsibility100-index-talk-v-walk/content.html,https://members.tortoisemedia.com/2019/10/01/readout-responsibility-100/content.html,https://www.thetimes.co.uk/article/the-responsible-way-to-rank-firms-is-by-how-much-harm-they-cause-5tl7pl90w,,"James Harding, Alexandra Mousavizadeh, Andrew Haynes, Alex Clark, Luke Gbedemah, Alexi Mostrous, Chris Newell"," James Harding is editor and co-founder at Tortoise. Before Tortoise he was Director of BBC News and Editor in Chief of The Times newspaper.   Alexandra Mousavizadeh is director of Tortoise Intelligence and has more than 20 years’ experience in ratings and indices. Before this, she was Assistant Vice President for the Africa sovereign ratings portfolio at Moody’s, and was the founder of the Global Disinformation Index.    Andrew Haynes, Alex Clark and Luke Gbedemah are data scientists and researchers in the Tortoise Intelligence team.   Alexi Mostrous is editor and partner at Tortoise. Before Tortoise he was Head of Investigations at The Times.   Chris Newell is data visualization designer at Tortoise. ",,,
United States,The Intercept,Big,Participant,Open data,The Condemned,12/03/19,"Investigation,OSINT,Crime","Google Sheets,CSV"," The project is a data set of every death sentence handed down in the U.S. since July 3, 1976 – the year capital punishment was reauthorized by the U.S. Supreme Court – by the country’s 29 active death penalty states and the federal government. In all there are 7,335 individual entries. The data set is accompanied by four written pieces that provide context for the project, including explaining the Supreme Court’s historic death penalty decisions and how the responses among different states led us to where we are today. "," The final data set reveals the U.S. experiment with capital punishment is a failed public policy. We’re told that the death penalty is necessary to punish the worst of the worst. But what our data set shows is that the single largest group within the data are individuals who are no longer on death row, but for some reason other than execution. This group is comprised of 3,135 individual entries -- the majority of these people have been resentenced to a lesser term in prison; hundreds have been freed; at least 132 were exonerated of the charges against them. And although the project was only very recently released, it has already had considerable impact. Lawyers have reached out to say that they will be using the data in litigation and related activities; news outlets have used the data to inform their own stories on the death penalty. And researchers are using our data to enhance their own work. "," During data gathering, research, and analysis phases of the project the team used a mix of off the shelf software and custom project specific software. Google Sheets, Microsoft Excel, and Open Refine are some of the off the shelf tools used to manage the data compilation, cleaning and analysis. Custom software was also written by the research team to scrape, standardize and clean data. The team also built tools to OCR records that were provided in non-digital formats. The research team also built additional tooling to prepare the data for publishing and to push it Github. "," The hardest part of this project was collecting the data. Despite the gravity of the government’s responsibility to ensure capital punishment is carried out in a manner that aligns with statute, we found that the state of record keeping on individuals sentenced to die was abysmal in all but a handful of states. It took nearly three years to collect accurate information on the individuals contained in the data set, to correct errors and inconsistencies contained therein, and to fill in gaps in the data, a process aided by a network of attorney sources who are involved in capital litigation and through extensive online research. The result is a first-of-its-kind dataset that reveals details about every individual sentenced to death in active death penalty jurisdictions. "," Broadly speaking, people can get a fuller picture of the way the death penalty has been applied in their state and why it is a failed policy. They can also learn more about the individuals who have been condemned to die in this country -- their names, ages, race, and any other aspects of their cases they might want to research on their own. Above all, The Intercept’s death-row dataset was designed as a tool and a living document, available on Github, that we will continue to update as new death sentences are meted out, old convictions are overturned, and executions continue. ",https://theintercept.com/series/the-condemned/,https://theintercept.com/2019/12/03/death-penalty-race-texas/,https://theintercept.com/2019/12/03/death-penalty-abolition/,https://theintercept.com/2019/12/03/death-penalty-reform-prosecutors/,https://theintercept.com/2019/12/03/death-penalty-capital-punishment-data/,,,"Liliana Segura, Jordan Smith, Akil Harris"," LIliana Segura is an award-winning journalist covering criminal justice. She is based in Nashville, Tennessee. Jordan Smith is an award-winning, Texas-based investigative reporter with more than 20 years of experience covering criminal justice. Akil Harris is a Senior Research Engineer at First Look Media. ",,,
Nigeria,Vanguard Newspaper,Big,Participant,Best data-driven reporting (small and large newsrooms),Investors may lose N1.8bn Monthly to VAT,30/09/19,"Explainer,Database,Open data,News application,Chart,Business,Economy","Adobe,Microsoft Excel"," Following the dwindling tax revenue in the country (the tax administration agency - Federal Inland Revenue Service - has missed its revenue target in the last three years from 2015 to 2018), the Federal Government introduced various taxes in 2019 and one of them was the reinstatement of five percent Value Added Tax (VAT) on all equities transactions.   Additionally, the government announced an increase in the VAT rate by 50 percent to 7.5 percent. The project, therefore, explored the impact of the VAT increase on equities investors and the entire equities market, which has been struggling with low investors' appetite. "," The report created excitement among the domestic retail investors and was seen as a call to action by the investors, who believed at that time that the federal government may rescind its decision to jerk up VAT on equities transaction by another 50 percent or even, possibly, revert to statusquo by effecting total removal of VAT on equities.   Unfortunately, typical of the federal government of Nigeria, the decision has been taken and so, the government was bent on increasing VAT due to its dwindling revenue and the need to also fund the 2020 budget, and so, it would not go back on its decision as it just signed the Finance Bill into law. The government had budgeted N10.59 trillion for 2020 out of which, N292.6 billion or 2.8 percent of the total revenue is expected to come from tax revenue while 4.4 percent (N463.95 billion) of the total budget is expected to come from stamp duty collections. ", I basically used Microsoft excel to record the data I used. I also use it in plotting the charts.  ," The process of calculating the Value Added Tax reinstatement and additional 2.5 percent increase for both the buy and sell side transaction was quite demanding as I took special care to ensure that all calculations were thoroughly done to avoid error. The calculations were done on the commissions charged by the brokerage firms as well as the apex regulatory body in the market – the Securities and Exchange Commission and the Nigerian Stock Exchange. Infact, for all the commissions chargeable before the reinstatement of VAT, I calculated and reflected the changes before going ahead to calculate what that increment would cost investors in monetary terms. This was done after I also calculated the average daily transaction value.       "," A lot of information are hidden in data and public interest stories that could make a difference in policy formulation could be written just by thinking through and interrogating some numbers/figures and what their implications are. This is where developmental journalism and agenda setting come in as they are necessary to sway government policies for public interest. Agenda setting is one thing I set out to do with this report.   Therefore, as far as this project is concerned, journalists should understand that no matter how difficult a government in power is, we can still do stories that address the needs and interest of the public. ",https://www.vanguardngr.com/2019/09/investors-may-lose-n1-8bn-monthly-to-vat/,https://www.vanguardngr.com/2019/02/commercial-paper-market-booms-over-credit-crunch/,https://www.vanguardngr.com/2019/06/q119-investors-stake-in-mutual-funds-rises-29-7/,https://www.vanguardngr.com/2019/10/foreign-portfolio-investment-in-equities-shrinks-by-34/,https://www.vanguardngr.com/2019/04/how-low-yield-high-npl-took-toll-on-tier-1-banks-performance/,https://www.vanguardngr.com/2019/11/how-illicit-financial-flows-drain-cash-from-maternal-care-budget/,https://www.vanguardngr.com/2019/07/cbns-new-lending-policy-puts-7-banks-under-pressure/,Nkiruka Nnorom,"  Basic information    My name is Nkiruka Nnorom (Nee Anene) from Awka in Anambra State, Nigeria.   My primary education was at Ekwulobia in Aguata Local Government Area of Anambra State where I passed out with distinction in the First School Leaving Certificate Examination.   Thereafter, I proceeded to Ekwulobia Girls High School, also in Anambra State where I graduated with eight credits and a pass in mathematics.   I gained admission into the famous University of Calabar in Cross Rivers State, Nigeria, three years after in 1997 where I studied English and Literary Studies, graduating with second class upper division.   I am married with two children.    My Career as a journalist    I started my career as a journalist in 2006 with Leadership Newspapers Limited as a capital market reporter. My job in Leadership Newspapers was my first exposure to journalism though I did not study Journalism as a course in the university. This also marked my first major exposure in the corporate world. My core duties while in Leadership Newspaper were news gathering, reportage and conducting of interviews.   Before this time, I worked as a classroom teacher in a secondary school in Lagos State after my one year compulsory national youth service.   I was one of the staff affected in 2010 restructuring in Leadership Newspaper. Following my exit from the organisation, I joined Newsstar Newspaper, again, as a finance reporter.   On January 2012, I joined Vanguard Newspaper as a finance/capital market reporter, a position I still hold to-date. Like in other media organisations where I previously worked, I am saddled with the responsibility of reporting, reviewing and conducting interviews not just on capital market activities, but in the entire business community.   Since becoming a journalist, I avail myself of every opportunity to build capacity. I have taken advantage of trainings by the Nigeria’s Securities and Exchange Commission (SEC), the Nigerian Stock Exchange (NSE) and those organised by the umbrella body of capital market reporters in Nigeria – Capital Market Correspondents Association of Nigeria (CAMCAN) including those organised by private entities to improve myself.   I have also undertaken certification training by School of Media and Communication of Pan-Atlantic University (Solution & Data Journalism) and PricewaterhouseCoopers (Data Journalism).   Over the years, I have distinguished myself in my field of reportage and this has earned me a number of awards and recognitions, the most recent being 2017 PWC Reporter of the Year in capital market category. ",,,
United States,ProPublica,Big,Shortlist,Best data-driven reporting (small and large newsrooms),Opportunity Zones,19/06/19,"Investigation,Database,Map,Politics,Corruption,Business,Economy","D3.js,QGIS,CSV,PostgreSQL,PostGIS,Python"," President Donald Trump’s tax code overhaul contained a provision to help the poor called “opportunity zones.” Our reporting shows that while the benefits to the poor have yet to materialize, some have already reaped the rewards: the wealthy and politically connected.   We used records requests, data analysis and mapping to find areas that never should have been qualified for the program, but were allowed in by a flawed implementation of the law by the U.S. Treasury Department. Wealthy developers in those areas then lobbied governors to include their projects in the program. "," Our articles, along with those of other outlets, led to Congressional calls for investigations into the designation process, as well as proposed reforms to make the program more transparent and to eliminate potential abuses by investors.   Citing ProPublica’s reporting:  <ul>  Senator Ron Wyden introduced a bill that would disqualify hundreds of areas from the original legislation because they were not poor, including the area in Baltimore that ProPublica identified.   Senator Cory Booker, a vocal backer of the program and co-sponsor of the original legislation, and Representatives Emanuel Cleaver and Ron Kind called on the Treasury’s inspector general to review the program.   House Ways and Means Committee Chairman Richard Neal, Senators Wyden and Booker and Representative John Lewis requested the Government Accountability Office review the program and “identify any Opportunity Zones that do not meet the statutory criteria and explain how and why they were designated.”   Senator Bernie Sanders called for the program to be abolished altogether.   Representative Rashida Tlaib sent a letter to several House committee leaders to investigate political influence in the opportunity zone designation process. Tlaib also introduced legislation to repeal the program.  </ul>"," We first noticed several Census tracts that were included in the Opportunity Zone program even though they seemed to be too rich to qualify. To confirm our suspicions, we recreated the Treasury Department’s analysis entirely.   We used Python to obtain Census data on all 74,000 tracts in every state in the U.S. from the Census API. We obtained their shapes from the Census FTP site and we got the map files for the program from a request to the agency. We loaded the data into a Postgres server and then did the spatial analysis with queries in PostGIS. We used QGIS to visualize the results.   Our analysis resulted in a handful of unqualified tracts, including ones in Detroit and Baltimore. We then focused our reporting on those tracts and the real estate interests who stood to benefit. For the first story, we acquired shapefiles of parcels in Baltimore in order to determine how much of the census tract the developer owned. We got data on tract recommendations via a request to the City of Baltimore.   We then filed a series of FOI requests to state agencies around the country to get insight into the lobbying during the high-stakes selection process for the tax break. Once we started publishing stories, we received tips that led to our reporting on the superyacht marina in Florida. "," This project centered on an abstruse — but highly consequential — intersection of the tax code, state-level lobbying, and technical mapping decisions.   We had to consult with corporate tax lawyers, GIS mapping experts, and local city officials around the country.   The stories also required penetrating the business dealings of billionaires in three states, with no access or cooperation offered by the subjects. Public records requests to state and city agencies allowed us to show what the real estate interests were up to, and explain how it didn’t align with the lofty goals of the opportunity zone tax break. "," This was not a traditional “data” story. We didn’t make sweeping statements about the world based on a database’s summary statistics. Instead, we based our reporting on the half-dozen or so outliers our analysis found.   Our data analysis was not tacked on at the end of our reporting – it was the impetus for it. After completing our analysis, we filed over a dozen public records requests at every level of government involved in the opportunity zones processes in Maryland, Michigan, Florida and other states. This included requests for emails and data to city agencies in Baltimore, Detroit, West Palm Beach and Tampa, as well as state agencies and the governor’s office in Maryland, Michigan and Florida, and FOIAs with the U.S. Treasury Department.   At the end of the day, did it involve the toolkit of the data journalist? Yes. We had to code, run queries, make maps and crunch numbers. But data didn’t tell the story; it found the story. It showed us where to file requests and start digging. It guided us along the way, helping us uncover explicit lobbying, influence and intervention by people in power. And it told us where to start asking questions. Traditional reporting eventually confirmed our suspicions: a federal agency so badly mismanaged the implementation of a program meant to benefit the poor, it benefited the very, very rich instead. ",https://www.propublica.org/article/trump-inc-podcast-one-trump-tax-cut-meant-to-help-the-poor-a-billionaire-ended-up-winning-big,https://www.propublica.org/article/how-a-tax-break-to-help-the-poor-went-to-nba-owner-dan-gilbert,https://www.propublica.org/article/superyacht-marina-west-palm-beach-opportunity-zone-trump-tax-break-to-help-the-poor-went-to-a-rich-gop-donor,https://www.propublica.org/article/billionaires-keep-benefiting-from-a-tax-break-to-help-the-poor-now-congress-wants-to-investigate,,,,"Jeff Ernsthausen, Justin Elliott, Kyle Edwards"," Jeff Ernsthausen is a data reporter at ProPublica. He previously worked on the investigative team at the Atlanta-Journal Constitution, where he investigated sexual abuse by physicians nation-wide, police misconduct in Georgia and evictions in metro Atlanta. Prior to his career in journalism, he studied history and economics and worked as a financial and economic analyst at the Federal Reserve.   Justin Elliott has been a reporter with ProPublica since 2012, where he has covered money and influence in the Obama and Trump administrations, the American Red Cross, and TurboTax maker Intuit. He has produced stories for outlets including the New York Times and National Public Radio, and his work has spurred congressional investigations and changes to federal legislation.   Kyle Edwards is a Lorana Sullivan senior business reporting fellow. ",,,
Nigeria,Vanguard Newspaper,Big,Participant,Best data-driven reporting (small and large newsrooms),Investors May lose N1.8bn Monthly to VAT,30/09/19,"Explainer,Database,Open data,News application,Chart,Business,Economy","Adobe,Microsoft Excel"," Following the dwindling tax revenue in the country (the tax administration agency - Federal Inland Revenue Service - has missed its revenue target in the last three years from 2015 to 2018), the Federal Government introduced various taxes in 2019 and one of them was the reinstatement of five percent Value Added Tax (VAT) on all equities transactions.   Additionally, the government announced an increase in the VAT rate by 50 percent to 7.5 percent. The project, therefore, explored the impact of the VAT increase on equities investors and the entire equities market, which has been struggling with low investors' appetite.       "," The report created excitement among the domestic retail investors and was seen as a call to action by the investors, who believed at that time that the federal government may rescind its decision to jerk up VAT on equities transaction by another 50 percent or even, possibly, revert to statusquo by effecting total removal of VAT on equities.   Unfortunately, typical of the federal government of Nigeria, the decision has been taken and so, the government was bent on increasing VAT due to its dwindling revenue and the need to also fund the 2020 budget, and so, it would not go back on its decision as it just signed the Finance Bill into law. The government had budgeted N10.59 trillion for 2020 out of which, N292.6 billion or 2.8 percent of the total revenue is expected to come from tax revenue while 4.4 percent (N463.95 billion) of the total budget is expected to come from stamp duty collections. ", I basically used Microsoft excel to record the data I used. I also use it in plotting the charts.  ," The process of calculating the Value Added Tax reinstatement and additional 2.5 percent increase for both the buy and sell side transaction was quite demanding as I took special care to ensure that all calculations were thoroughly done to avoid error. The calculations were done on the commissions charged by the brokerage firms as well as the apex regulatory body in the market – the Securities and Exchange Commission and the Nigerian Stock Exchange. Infact, for all the commissions chargeable before the reinstatement of VAT, I calculated and reflected the changes before going ahead to calculate what that increment would cost investors in monetary terms. This was done after I also calculated the average daily transaction value. "," A lot of information are hidden in data and public interest stories that could make a difference in policy formulation could be written just by thinking through and interrogating some numbers/figures and what their implications are. This is where developmental journalism and agenda setting come in as they are necessary to sway government policies for public interest. Agenda setting is one thing I set out to do with this report.   Therefore, as far as this project is concerned, journalists should understand that no matter how difficult a government in power is, we can still do stories that address the needs and interest of the public. ",https://www.vanguardngr.com/2019/09/investors-may-lose-n1-8bn-monthly-to-vat/,https://www.vanguardngr.com/2019/02/commercial-paper-market-booms-over-credit-crunch/,https://www.vanguardngr.com/2019/06/q119-investors-stake-in-mutual-funds-rises-29-7/,https://www.vanguardngr.com/2019/10/foreign-portfolio-investment-in-equities-shrinks-by-34/,https://www.vanguardngr.com/2019/04/how-low-yield-high-npl-took-toll-on-tier-1-banks-performance/,https://www.vanguardngr.com/2019/11/how-illicit-financial-flows-drain-cash-from-maternal-care-budget/,https://www.vanguardngr.com/2019/07/cbns-new-lending-policy-puts-7-banks-under-pressure/,Nkiruka Nnorom,"  Basic information    My name is Nkiruka Nnorom (Nee Anene) from Awka in Anambra State, Nigeria.   My primary education was at Ekwulobia in Aguata Local Government Area of Anambra State where I passed out with distinction in the First School Leaving Certificate Examination.   Thereafter, I proceeded to Ekwulobia Girls High School, also in Anambra State where I graduated with eight credits and a pass in mathematics.   I gained admission into the famous University of Calabar in Cross Rivers State, Nigeria, three years after in 1997 where I studied English and Literary Studies, graduating with second class upper division.   I am married with two children.    My Career as a journalist    I started my career as a journalist in 2006 with Leadership Newspapers Limited as a capital market reporter. My job in Leadership Newspapers was my first exposure to journalism though I did not study Journalism as a course in the university. This also marked my first major exposure in the corporate world. My core duties while in Leadership Newspaper were news gathering, reportage and conducting of interviews.   Before this time, I worked as a classroom teacher in a secondary school in Lagos State after my one year compulsory national youth service.   I was one of the staff affected in 2010 restructuring in Leadership Newspaper. Following my exit from the organisation, I joined Newsstar Newspaper, again, as a finance reporter.   On January 2012, I joined Vanguard Newspaper as a finance/capital market reporter, a position I still hold to-date. Like in other media organisations where I previously worked, I am saddled with the responsibility of reporting, reviewing and conducting interviews not just on capital market activities, but in the entire business community.   Since becoming a journalist, I avail myself of every opportunity to build capacity. I have taken advantage of trainings by the Nigeria’s Securities and Exchange Commission (SEC), the Nigerian Stock Exchange (NSE) and those organised by the umbrella body of capital market reporters in Nigeria – Capital Market Correspondents Association of Nigeria (CAMCAN) including those organised by private entities to improve myself.   I have also undertaken certification training by School of Media and Communication of Pan-Atlantic University (Solution & Data Journalism) and PricewaterhouseCoopers (Data Journalism).   Over the years, I have distinguished myself in my field of reportage and this has earned me a number of awards and recognitions, the most recent being 2017 PWC Reporter of the Year in capital market category.     ",,,
Brazil,"Agência Pública, Repórter Brasil",Small,Participant,Innovation (small and large newsrooms),Robotox,14/05/19,"Multiple-newsroom collaboration,Politics,Environment,Agriculture","Scraping,Json"," Robotox is a Twitter bot that tweets whenever new pesticides have been authorised by the Brazilian government, for use in the agriculture sector.Robotox is a Twitter bot that tweets whenever new pesticides have been authorised by the Brazilian government, for use in the agriculture sector. "," Significant. Because information on the authorisation of new pesticides is buried deep in daily state journals, this information was hard to access. By making this information easily and publicly available, Robotox has provided much transparency on what pesticides have been given state authorisation.   The Twitter account has around 17.500 followers. The lauch of Robotox was reported by some media outlets, such as Outras Palavras, Hypeness and Open Knowledge Brasil. Also, it was mentioned in some articles, one of them <a href=""https://brasil.elpais.com/brasil/2019/06/19/opinion/1560948146_966466.html"">written by Eliane Brum</a>, one of the most important environmental journalists in Brazil.   A radio show broadcasted on the radio of the Federal University of Minas Gerais mentioned Robotox in an episode about how science is looking for alternatives to reduce the use of pesticides. "," Robotox reads the daily state journal and looks for texts indicating the authorisation of new pesticides. When new pesticides have been authorised, all new pesticides are extracted, ordered and queued up for being sent out as tweets.   The underlying database is a MySQL database, the scripting is server-based and done in PHP. Tweets are sent out via the Twitter API. "," The most difficult part of developing Robotox was to interpret the texts of the daily state journals to abstract information on the authorisation of new pesticides. This requires a type of natural language processing, which is technically difficult. ", The Brazilian state appears to authorise the use of a painfully large amount of poisons to manipulate our food production. ,https://twitter.com/orobotox,https://apublica.org/2019/05/conheca-o-robotox-um-robo-que-tuita-sempre-que-o-governo-federal-libera-um-registro-de-novo-agrotoxico/,https://portrasdoalimento.info/2019/05/14/conheca-o-robotox-um-robo-que-tuita-sempre-que-o-governo-federal-libera-um-novo-agrotoxico/,,,,,"Natalia Viana, Babak Fakhamzadeh, Pedro Grigori, Bruno Fonseca","  Babak Fakhamzadeh  was working in ICT4D before it had a name (2001), never really left it, and knows how to throw together a pretty mean combination of a wide array of programming languages, both frontend and backend. In 2016, together with Agência Pública, he won the Prêmio Jornalístico Vladimir Herzog de Anistia e Direitos Humanos. In 2017, work of his was selected to represent Brazil for the UN World Summit Award, going on to become a winner. In the same year, other works were nominated for the Gabriel García Márquez award and the Prêmio Petrobras de Jornalismo. He created what was probably the first mobile phone based city tour (2004) as well as what probably was the first QR-code assisted smartphone-based city tour (2009).    Bruno Fonseca  is a multimedia journalist at Agência Pública with a Master in Social Communication and experience in data and investigative reporting, infographics, and animation. His stories have won some national journalism awards, such as Prêmio Petrobras, Republica, Direitos Humanos, and Ministério Público de Journalism. He has also worked as a freelance reporter, fact-checker, journalism college professor, hyperlocal journalist, and TV editor.    Natalia Viana  is a co-founder and co-director of Brazilian investigative journalism <a href=""https://apublica.org/""> Agência Pública, </a> founded in 2011 by women reporters. She has covered stories such as indigenous people being massacred in Colombia and human rights violations by the authoritarian regime in Angola and their relations with the Brazilian company Odebrecht. She is the author and co-author of four books about human rights violations, including the e-book The Bishop His Sharks, on the impeachment of Fernando Lugo in Paraguay in 2012. As a reporter and editor, she has won several journalism awards, including the <a href=""https://alchetron.com/Vladimir-Herzog-Award""> Vladimir Herzog Human Rights Award (2005/2016), </a> the Comunique-se Award (2016/2017), the Women's Trophy Press Award (2011/2013) and the  <a href=""https://knightcenter.utexas.edu/blog/00-17531-non-traditional-media-latin-america-and-spain-win-big-gabriel-garcia-marquez-festival"">Gabriel García Márquez award (2016).</a>  Natalia is a member of the Board of the Gabriel Garcia Marquez Foundation in Colombia and was recognized as a social entrepreneur by the organization Ashoka. She is also a member of the International Consortium of Investigative Journalists (ICIJ).    Pedro Grigori  is a journalist graduated from the Catholic University of Brasília. He is Agência Pública's correspondente in Brasilia. As a reporter, Pedro is part of the Project ""Por trás do Alimento"", a partnership between Agência Pública and Repórter Brasil to investigate the use of pesticides in Brazil. He has worked for two years at Correio Braziliense. ",,,
United States,The Hechinger Report,Small,Shortlist,Best news application,Tuition Tracker,30/10/19,"News application,Chart","D3.js,JQuery,Json,R,Python"," TuitionTracker is a tool that projects the net costs for prospective students enrolling in college and allows them to compare the value and outcomes of 3,891 colleges against one another. Unlike other web apps offering historical data to students, TuitionTracker uses a financial projection technique to estimate the tuition to be charged by each institution in the coming year and the expected net cost (which is often much lower). This means that students enrolling in 2020-21 would know the cost of tuition in 2020-21, not be forced to imagine the price based on what it cost in 2017-18. "," The data produced for this tool went on to power a number of impactful stories. The Hechinger Report produced two of these stories, based on issues relating to the cost of American universities. The <a href=""https://hechingerreport.org/how-louisianas-richest-students-go-to-college-on-the-backs-of-the-poor/"">first story</a>, reported and written by Emmanuel Felton, focused on Nicholls State University and how that Louisiana college tried to offset cuts in state funding by increasing tuition on low-income students and their families. This became the basis for an on-air story from Soledad O’Brien for her nationally televised show Matter of Fact.    The <a href=""https://hechingerreport.org/university-of-chicago-projected-to-be-the-first-u-s-university-to-charge-100000-a-year/"">second piece</a>, reported and written by Pete D’Amato, examined skyrocketing tuition prices at elite universities and how the “sticker shock” scares off low-income and minority students from applying, despite the fact that they are likely to receive institutional funding that drops their attendance costs close to zero. Focusing on the University of Chicago and the 2025 milestone when the school is expected to break $100,000 for a year of attendance, D’Amato traced rising tuition prices back to certain accounting tweaks in the 1980s at some small liberal arts schools, which created the byzantine and opaque process of paying for college that we have now. The University of Chicago milestone spurred a piece on the cost of college in The Atlantic, and data on expected cost increases generated state-level stories for public radio stations in Iowa and Vermont.    Importantly, the <a href=""https://www.tuitiontracker.org/"">tool itself</a> has been used by many students, as well as the parents and guardians of college-bound students, to learn about the disparity between advertised tuition and the real cost of attendance. It allows comparisons for five different levels of family income. Counselors working with high schoolers have contacted Hechinger Report to say they recommend the tool to prospective college students. "," The web application itself is built off of an earlier version, created in partnership with the Education Writers Association and The Dallas Morning News. Various components were overhauled or developed from scratch for the new version.    The process of generating the data that powers the tool was the most difficult and sensitive part of the development process. Over a period of months, Pete D’Amato worked with federal education data using the R statistical package to determine the best predictors of year-on-year changes for college costs. Performing regression analysis, he compared the historical net prices for colleges to a range of variables, from advertised tuition to incomes of students enrolled to U.S. gross domestic product. In the end, he determined the most closely correlated statistic would be advertised tuition, and so he pegged the actual cost to the estimated tuition for the upcoming school year. This data was generated using the compound annual growth rate over the past ten years for each institution.    The tool uses Python to extract all the data from a master CSV file and place it into separate json files for each individual institution. The data is then read from the json files using D3.js and displayed using D3 or HTML with CSS animations. A small component — the comparison tool, which allows users to compare multiple schools by graduation rate or by actual cost — is built in React.js and deployed using Webpack.   Tuition Tracker includes data from 3,891 American colleges and universities.  "," The most difficult part of this project was figuring out how to best project costs for college-bound students accurately using available historical data. It was important to spend the time working with R to decide which variables were most correlated with each other, then testing the formula used to project prices against fresh data. "," As people demand more transparency about institutions such as colleges, and more data becomes available to the wider public, journalists should strive to present this data in the most digestible way to audiences. This requires thinking about the situational context of users and the way that they consume and interpret data. It is thus not sufficient to show historical data on to audiences and hope they will be able to apply the historical data to the future. In the college affordability example, a parent who is presented historical data on college costs without instructions on how to project them forward may take the topline data point — the last year price data was released — and assume this is the price to be advertised for them and their child, when this figure is likely to be thousands or tens of thousands of dollars off the mark. Adding to its usefulness for consumers, the tool also allows users to compare graduation rates, acceptance rates and the percentage of students who pay the full sticker price at any of the 3,891 colleges.    Journalists creating data visualization tools that are consumer-focused need to develop skills to better interpret such data, whether through machine learning, statistical analysis or other methods, in order to present a final product that requires less guesswork on the part of users and allows for more accurate estimation.    ",https://www.tuitiontracker.org/,https://hechingerreport.org/how-louisianas-richest-students-go-to-college-on-the-backs-of-the-poor/,https://hechingerreport.org/university-of-chicago-projected-to-be-the-first-u-s-university-to-charge-100000-a-year/,,,,,Pete D'Amato," Pete D’Amato is the data visualization developer at The Hechinger Report, the independent, nonprofit newsroom devoted to covering education. He started his journalism career as a writer and photographer in Latin America, and his work has appeared in Bloomberg, Forbes, Americas Quarterly, Vice and Sports Illustrated. He began learning to code during journalism school as a way to enhance his written stories but has turned to focus on how interactivity and data visualization can allow audiences to get a more informative and personalized news experience. Before joining The Hechinger Report, he was the web editor at Crain’s New York Business.  ",,,
United States,FiveThirtyEight,Small,Participant,Best visualization (small and large newsrooms),The Moderate Middle Is A Myth,24/09/19,"Infographics,Chart,Elections,Politics",R," This story is a visual tour of the murky middle of the electorate. Pundits and politicians love to talk about ""moderate"" or ""independent"" voters, but this story, using data from the Democracy Fund Voter Study Group, shows that those groups of voters are not an ideologically coherent voting bloc. It starts with a venn diagram showing the overlap of these groups, and then uses the venn diagram as a recurring way finder throughout the piece when diving deeper into each one.  "," The story was widely read and addressed a clear blind-spot in media coverage of elections and polling, in which conversations are framed around appealing to ""moderate"" or ""independent"" voters. The piece, which contains original analysis, has been cited by other reporters and academics who work in elections and polling.  "," The analysis and charting was done using R and the ggplot charting package, except for the venn diagram. The graphics were then finished in Adobe Illustrator. The most important technique used in creating this piece, however, was close collaboration between the graphics editor and author. The intitial drafts had the venn diagram at the bottom, but as the graphics editor, I suggested a visually-led strucuture and the author was very open to reconfiguruing the story along that outline. The end result was more narratively cohesive, and much easier for the reader to follow. "," There were two hardest parts to this this visualization. The first was to demostrate how these groups are related but distinct from each other. The venn diagram, while not the best at showing relative size, ended up being an effective way to guide readers through the contours of this group of people. The second challenge was how to show a  lack  of trend and still make the visualizations compelling. In this instance. In this case the repetition of ""no trend"" made the series of scatterplots more effective. "," This piece serves as a corrective to popular narrative about moderate, independent and undecided voters. From a visual perspective, it's instructive as a way to guide readers using visualization as a frame narrative and using repetive visual motifs to make a point.  ",https://fivethirtyeight.com/features/the-moderate-middle-is-a-myth/,,,,,,,"Ella Koeze, Lee Drutman"," Lee Drutman is a senior fellow in the Political Reform program at New America. He’s the author of the forthcoming book, “Breaking the Two-Party Doom Loop: The Case for Multiparty Democracy in America.”  Ella Koeze is a visual journalist at FiveThirtyEight. ",,,
Colombia,"Rutas del Conflicto, Consejo de Redacción",Small,Shortlist,Open data,The post-conflict numbers,02/10/19,"Investigation,Long-form,Multiple-newsroom collaboration,Database,Open data,Infographics,Video,Map,Politics,Corruption","Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV"," The post-conflict numbers is a collaborative journalistic project that aims to find out what has happened with the money destined to the implementation of the Colombian Peace Agreement. This project promotes a permanent, and well informed, citizen oversight of the public spending in post-conflict. Journalists and developers worked jointly to gather information of contracts signed by governmental agencies regarding post-conflict, build a database that contained all the information collected and present it through interactive maps to show where those investments have been undertaken. "," Using the information contained in the database built by journalists of Rutas del Conflicto, reporters from several Colombian regions have published 74 articles included in this project. These pieces are multimedia reports with infographics, videos, images, maps, etc., all uploaded in the same web site. We are also working on a printed publication containing the main 6 special reports to ensure that communities from regions where the Peace Agreement is being implemented have access to the findings concerning them according to their region.   Since this is a collaborative project we do not only count with Rutas del Conflicto, Consejo de Redacción and Colombiacheck (the media outlet of Consejo de Redacción), but also <a href=""http://VerdadAbierta.com"">VerdadAbierta.com</a> as well as 6 more regional media outlets. All the articles, data visualizations, videos, maps and images have been replicated in all of the participants websites and social media.   Colombian government announced in 2017 he creation of a platform to centralize the information about the implementation of the Peace Agreement, called SIIPO. Such platform hasn’t been released as of January 2020. Under such circumstances, The post-conflict numbers stands as the only database that contains the information of hundreds of public contracts derived from the implementation of the Peace Agreement, allowing for citizen overseeing of public spending regarding post-conflict.  "," For gathering the information, we used both formal and informal requests of information, using ""derechos de petición"", an official request of information protected by Colombian constitution. In some cases, journalists had to recur to judges in order for governmental institutions to answer those requests. Since institutions responded with very brief information, we contrasted it with public databases that contain public procuring contracts, that are public under the law but are also very opaque and hard to use for non specialized citizens.    After getting the information of hundreds of contracts, we created a database containing data that had to be standardized since its sources were often scanned images and pdf documents. We used Excel and Open Refine to work with the data collected. We used HTML to build the tool and Hypertext Preprocessor. The platform was made on DRUPAL. "," Corruption is what colombians see as their biggest problem nowadays. Information about public spending is not always accessible and transparent. The dispositions contained in the Peace Agreement need to be viable if Colombia is to have a successful transition to peace. However, the implementation of those dispositions faces a great challenge as corruption at all levels threatens its viability. Getting the information needed for this project was difficult since we were denied repeatedly the access to it and had to resort to judicial instances. Once we had the information, we needed to extract the data to be presented in this project. That data was found often in PDFs or in paper so we had to review a lot of paperwork in order to extract the relevant information. Once we did, we also needed to find a way to present the data in comprehensible ways for it to be of use to other journalists and community leaders. Even though the Colombian government announced the creation of a platform that would contain the information about the contracts signed to implement the Peace Agreement and its dispositions, it hasn't been put to use yet. Therefore, citizens could follow the implementation using only the information the government chose to promote officially. This tool provides more relevant data about those contracts, making it easy for users to know what was the object of the contract, what institution signed it, who is the contractor, how much money is invested in each contract and in which areas of the country is the contract being executed. "," Others can learn what we did while creating this project. We learned that there are several legal tools in colombian legislation that allowed us to have some access, though scarce and opaque, to information about public spending, or any other public information. We also learned how public procuring should be done to abide by the law and how it is actually done at some levels. We learned that money destined to improving the rural roads sometimes got lost in the process. We learned about the loopholes in Colombian public procuring laws that allowed for an incresed opacity that raise questions about the implementation of the Peace Agreement not being transparent enough. We learned that two years after the Peace Agreement being signed, little was being invested in crucial matters for the success of the peace stabilization process, such as rural healthcare, crop substitution and roads improvement.   Finally, we learned that journalism is a joint effort and working collaborativelly with several media was crucial to bring together knowledge, expertise and forward thinking.  ",https://colombiacheck.com/especiales/numeros-del-posconflicto/node/754,,,,,,,"David Riaño, Daniela Aguirre, Silvia Corredor, Pilar Puentes, Alejandro Ballesteros, Oscar Parra, Ginna Morelo, Claudia Mejía, Diana Salinas, Juan Diego Restrepo, Dora Montero, Camilo Amaya, Laura Gracia, Andrés García"," David Riaño: Journalist and specialist in International Human Rights Law and International Humanitarian Law. Has been recognized with academic excellence scholarships throughout his entire career by Universidad del Rosario and with the Ernst-Reuter-Gesellschaft at the Freie Universität Berlin.   Daniela Aguirre: Journalist graduated from Universidad del Rosario. She has worked for 4 years as an investigative journalist for Rutas del Conflicto.    Silvia Corredor: Journalist and anthropologist working for Rutas del Conflicto since 2017. She has investigated land ownership conflicts and the situation of endangered social leaders in Colombia.   Pilar Puentes: Journalist in Rutas del Conflicto since 2018. She has followed closely the social leaders situation in Colombia   Alejandro Ballesteros: Journalist and developer of Rutas del Conflicto. In charge of the layout and construction of the web portals of the media. Developer with knowledge in HTML, CSS, JS, PHP languages ​​and management of libraries like Boostrap, Leafleat and D3. In addition to design skills and  development of multimedia pieces.   Ginna Morelo: Journalist and former president of Consejo de Redacción, an organization that unites investigation journalists in Colombia and promotes investigative journalism. She was recognized as Journalist of the Year by the Simón Bolívar Journalism Awards in 2018.   Oscar Parra: Director of Rutas del Conflicto and journalism professor at Universidad del Rosario. ",,,
Peru,Convoca.pe,Small,Shortlist,Best data-driven reporting (small and large newsrooms),Espacios públicos de Lima entregados a empresas equivalen a 79 campos de fútbol. Headline in English: Privatized public spaces in Lima equals to 79 football fields,05/08/19,"Investigation,Explainer,Solutions journalism,Long-form,Database,Mobile App,Infographics,Chart,Map,Satellite images,Audio,Politics,Environment,Corruption,Human rights","JQuery,Json,Microsoft Excel,Google Sheets,CSV,Node.js"," In Lima, capital city of Peru, parks, beaches and other public spaces have been privatized by private companies, by the equivalent of 79 football fields. Through public information requests, I accessed official documents from 8 municipalities that proved that using concession contracts many private companies were in fact obtaining possession of intangible and non-transferable public spaces, between 1994 and 2018. This derived in various social conflicts, which affect thousands of citizens in the designated districts, as well as irregularities in contracts that reveal cases of alleged corruption. "," The research had a great impact on Lima’s political life. Many local governments decided to review the contracts and, in many cases, those reviews ended in complaints to the General Comptroller of the Republic, whom intervened and recommend the cancelation of the contracts due to the irregularities found.   Meanwhile, the academic community, specially of the architecture field, urban planning and social sciences, pronounced on these facts and began investigations on the legal gaps and irregularities found,  taking this research as a source. Currently, the Peruvian Congress is holding a debate about a public spaces bill, in order to correct the legal gap that has allowed this kind of contracts to occur for decades.   Many local media took interest on the matter, especially on the findings, considering that the case was shown to the public during local elections. Also, it has great impact on the architecture and urban planning faculties, where professors are using the report as an example for their students.   It is important to mention that the topic has gain importance during the last years but this is the first report that contains a detail chronicle of the unappropriated use of public spaces by private parties involving local authorities.   This report was finalist for the National Journalism Prize of Peru in 2019, in the category 'Journalistic Innovation'. "," The web application 'City in Concession' (http://ciudadenconcesion.convoca.pe/aplicativo.html) that complements the report (http://ciudadenconcesion.convoca.pe/el-espacípublico-entregado-a-empresas. html) is the result of a great technological and research work that shows the user how local governments deliver public areas to private companies against the opinion of citizens, by using legal loops.   For research, we built a database on Google Spreadsheets with the obtained documentation from contracts between municipalities and private companies that agreed to the privatization of public space. This database includes the coordinates of each public space obtained with the Google Street Maps tool. In addition, to determine the magnitude of each affected area, the polygons of the free Google Earth tool has been used to obtain the approximate measurements of the privatized areas in square meters. This data is included in the data set.   A digital tool was developed with the information obtained, in order to show the situation of each privatized public space. Google Maps API was used to geolocate the affected parks beaches and stadiums through coordinate.   In addition, aerial photographs of the Peruvian Air Force and satellite photos of Google Earth were used to create an image gallery. Also, the Swiper slider tool was used in order to show the evolution of the impact of public space privatization trough the time.   Highcharts for Javascript was used to show the statistical findings, creating interactive charts to make the visualization of the report more accessible in the web application. Images, graphics and maps are included with specific information on each one of the 25 privatized areas showed on the study. "," One of the most important difficulties in the process of investigation is the lack of transparency of the local authorities, specially about the concession contracts celebrated with private companies. The information is not shown on transparency portals or institutional webs. Because of that, in order to obtain each one of the documents that proves the irregular use of the public spaces, we need to send special written request.   In Peru, contracts between local governments and private companies in relation to public spaces, such as parks, beaches and stadiums, have been executed under different legal regulations, so they are not standardized. Therefore, we built a database to organize the most important information I found about the contracts, council agreements and addenda, which I obtained after the requests.   Another difficulty we had was identifying the size of the privatized public spaces that, in most cases, are not found in the contracts. To overcome these problems, we carry out the measurements using the free Google Earth tool to obtain the approximate measurements of the privatized areas in square meters. With the obtained information, a digital tool was designed in order to show the situation of each privatized public space. "," This research allowed different approaches. By reading the paper and discovering the findings in the interactive visualization, citizens can learn how municipalities use various legal mechanisms to privatize public spaces and how to identify possible irregularities in these types of contracts. This investigation also turns relevant for lawyers and judges, and local authorities in order to avoid the social conflicts described.   Journalists can find in this research an interesting technique to build a database, based on physical documents that can be obtain through transparency mechanisms. Also, the research provide important  methodology to analyze databases and obtain periodic impact results.   Also, developers can learn to use Google Maps APIs to geolocate areas through coordinates found in Google Earth. They will also learn how to use Highcharts for Javascript optimally for its ease in configuring graphics in a web application. The research can also inspire the develop of new visualization forms using the digital tools provided, and also other investigations regarding public spaces. It is important to consider that the search engine registered 25 infrastructure projects in public spaces and include maps, aerial and satellite images and records of each project.     ",http://ciudadenconcesion.convoca.pe/el-espacio-publico-entregado-a-empresas.html,https://www.youtube.com/watch?v=PSjBvSOVFoY,https://www.actualidadambiental.pe/en-8-distritos-empresas-controlan-espacios-publicos-equivalentes-a-79-campos-de-futbol/,https://la.network/cuando-los-espacios-publicos-si-tienen-quien-los-defienda/,http://ocupatucalle.com/wp-content/uploads/2019/08/LibroIntervenciones_Final_baja3.pdf,https://puntoedu.pucp.edu.pe/noticias/premios-nacionales-de-periodismo-2019/,,"Research, reporting and data analysis: Luis Enrique Pérez Pinto, Web development and application: Anthony Atauqui and Elvis Rivera, Report editor: Milagros Salazar Herrera, Design: Jackeline Cárdenas"," Luis Enrique Pérez Pinto was born in Lima, Peru on July 20, 1988. At the early age of 14 years old, he becomes school journalist by El Comercio, the most important local newspaper. He began his career while studying journalism at Jaime Bausate y Meza University and continues his studies as Master in Research, Data and Visualization Journalism at the International University of La Rioja (Spain). He currently works as a reporter in Convoca.pe; were he has been investigating corruption cases, privatization of public spaces, urban conflict and irregularities in local administrations. ",,,
United States,Deseret News,Big,Participant,Best data-driven reporting (small and large newsrooms),Fatal courts,17/09/19,"Investigation,Long-form,Database,News application,Fact-checking,Map,Crime","Microsoft Excel,Google Sheets"," The four-month investigation into how family courts put children at risk by Gillian Friedman, with further data analysis by Saul Marquez, exploring how family courts fail to protect children, leading to abuse and even homicide. The Center for Judicial Excellence shared data files with the Deseret News — helpful, but since the data came from an advocacy group, it had to be vetted for accuracy and to assess for bias. In-house data team member Saul and reporter Gillian carefully  fact checked each data point. The data was also verified using other references, including death records, obituaries and news stories.        "," It's really too soon to say how much impact the project will have as state legislatures are just beginning to do their work and that informs policy. But we heard from dozens of people who read the story and then shared their own experiences with famiy court, some good and some bad. It's clear the story raised some awareness of a very serious issue. The story was also widely shared on social media. The Deseret News also received hundreds of messages from parents around the world that said the article was the first time tehy ever saw a news story that reflected their experience with family court. "," The data was provided to us in an Excel spreadsheet that was incomplete, sometimes inaccurage and contained a lot of misspelled names, which made it hard to verify the deaths, though we eventually did -- and corrected errors. It took two weeks to do that, using news stories and court documents. To populate the map, we integrated in a program called Datawrapper. We also had to manually stack the data on the map to avoid too uch overlap that could make it unreadable.     "," The amount of time spent going through each name, looking for news sources to back them up and then placing each child individually on the map was very tedious, which made it difficult. But the hardest thing was probably the emotional toll of reading the stories of all those children. Said said he googled just about every name and found news stories about nearly every death and reading the details hundreds and hundreds of times was a sad experience. ""I'm grateful to have done it, though. I feel more aware and I've thought about those children a lot since,"" he said. "," Not all data-driven projects are going to be high tech and use super computer power. Some, like this one, are going to rely on shoe leather and a degree of tedious old school tracking along with the ability to use some helpful tech programs. But the stories are important to tell and challenging, which is great.     ",https://www.deseret.com/2019/9/17/20805882/fatal-family-court-parental-rights-custody-battles-child-deaths-harm-center-for-judicial-excellence,,,,,,,"Gilllian Friedman, Saul Marquez"," Gillian Friedman writes for the Deseret News In-Depth Team.  She graduated from Whitman College in 2016 with a B.A. in Race and Ethnic Studies. She previously worked as a journalist at KIRO News Radio in Seattle and KSL News Radio here in Salt Lake City.    Saul Marquez is an associate product manager at the Deseret News where he previously contributed to the Home Page and Arts and Entertainment teams. Originally from Arizona, Saul moved to Utah in 2014 to study at Brigham Young University. He graduated in 2018 with a degree in Communications (emphasis News Media) and minors in French and Writing and Rhetoric. ",,,
Mexico,"EMEEQUIS, CONNECTAS",Small,Participant,Best data-driven reporting (small and large newsrooms),To Kill One's Child: Every two days a child is killed in Mexico by a parent,15/10/19,"Investigation,Solutions journalism,Database,Open data,Illustration,Infographics,Chart,Video,Map,Audio,Politics,Health,Crime,Human rights","Adobe,Microsoft Excel,CSV"," <a href=""https://matarunhijo.m-x.com.mx/""> #MatarAUnHijo </a> (To Kill One's Child) approaches the end point of the chain of victims of child abuse in Mexico:  death .   The cases registered in this research found how the lack of interest in the three levels of government have had fatal repercussions on the minors' lives. Between 2012 and 2017,  almost 2,600 children under 15 were killed . The 42% of them by a  family member,  in their  homes  or as a result of  child abuse .   EMEEQUIS developed a database of homicides and created a methodology which revealed that fathers, mothers and step fathers are the main aggressors. "," This is the first journalistic research that approaches an integral vision of the causes, ways and consequences of child abuse in Mexico. The three displayed cases answer the questions:  who, how and why Mexican families kill.    The report was widely commented on radio stations, national television, and retaken on web portals reaching a wider audience. In addition, the methodology created was replicated in the state of <a href=""https://www.diarioelmundo.com.mx/index.php/2019/10/19/asesinados-41-ninos-en-veracruz-por-sus-familiares/"">Veracruz</a> to discuss this problem at the local level.   This caused a first reaction from the government. The  president of Mexico,  Andrés Manuel López Obrador,  spoke about the feature in his matutine conference,  promising to reinforce with greater budget the fight against child abuse.   One month after publication, the  Senate approved the prohibition of corporal punishment  as an educational method. Also, members of the Chamber of Deputies,  began to discuss legislation to fight against child sexual abuse.  <a href=""https://www.m-x.com.mx/entrevistas/la-violencia-no-esta-en-las-mochilas-sino-en-las-casas"">Lorena Villavicencio</a>, legislator,  called on the Secretaries of Public Education, Health and Government to collaborate in the reduction and attention of cases of abused children.    The four texts that make up the research were consolidated as the most read of EMEEQUIS, with an average reading of almost six minutes per article, this is an indicator that one of the main objectives was met:  to reach Mexican homes.    Nevertheless,  the greatest achievement was to break the silence on a hidden and private topic, based on data and a completely new methodology.  These entries are also a tool that calls society and governments to assume the commitment to protect children. "," An  open database  of the National Institute of Statistics and Geography (INEGI, as in its acronym in Spanish) containing the total deaths registered annually in Mexico was used.  This database feeds on the death certificates  provided by the Civil Registry, the Forensic Medical Services and the statistical notebooks generated by the Agencies of the Public Ministry.   This data was eligible because  it’s the only database that contains specific characteristics of the deaths recorded in Mexico,  which allowed us to establish patterns of behavior around child abuse. We defined a five-year analysis period between 2012 and 2017 to portray patterns of violence committed against minors.   Prior to the data analysis, interviews were conducted with doctors, NGOs and specialists in child abuse topics, these  interviews were the starting point for establishing the analysis methodology.  ""Most battered children are killed in their homes"" said one of the specialists. As of that, we defined three analysis categories. First, the cases in which  the aggressor was identified and it had some kinship with the victim .  Second,  deaths that were recorded inside a house.  Third, d eceases in which death' cause was directly mistreatment, neglect or abandonment.    In order to establish the study population, UNICEF's definition of childhood were retaken, which considers that children under 15 are children.   Under these parameters five databases were downloaded with information on mortality, one for each year of study, in  .csv format.  Microsoft Excel was used for data processing. After making the corresponding filters, we used  pivot tables  to perform data crossings and thus detail the place of death, cause of death, victim's gender, who the aggressors are and how they kill, as well as locate the hot spots in states and municipalities from Mexico. "," Talking about child abuse in Mexico is not easy, because  it's a crime that occurs in private,  by the people who are supposed to take care and protect the children. In addition, the  disinterest of the authorities,  have suppressed the opportunity to build official statistics on the subject, despite being considered by specialists as a "" social disease "".   The lack of reliable statistics was the main obstacle, there are no data that allow to know the seriousness of the problem even on the end point of the chain of violence: death.  The creation of a methodology that allows to establish the severity of a problem that costs lives was fundamental  and required a lot of previous fieldwork to define the correct indicators.   However,  the most difficult part was to get the voices that could show the human part of the data that we had already processed.  This included the work to persuade the families of Diana Mia and Landon Yahir, as well Paloma, the woman who murdered her two daughters, to talk about their stories and let them realized the importance of telling their stories to raise awareness about the problem. This investigation wouldn't have had the desired impact without those voices. "," This in-depth report is a reminder that  investigative journalism is about revealing something that has not been said,  either because of an explicit interest in hiding it or, as in this case, because of a general lack of interest in addressing a problem that affects a certain sector of the population.   This project  shows the importance of generating new research methodologies  that allow us to reveal those realities that are not being talked about, so that they can be put on the public agenda and decision makers can attend to the people affected.   Another thing that can be learned from this project is that  the absence of official data on a problem shouldn’t be seen as an obstacle: there will always be ways to narrate what happens, in this case with the use of open databases.  Journalism is also about creating those new techniques that help highlight those situations that, as in the case of To Kill One's Child, cost lives. ",https://matarunhijo.m-x.com.mx/,https://m-x.com.mx/investigaciones/every-two-days-a-child-is-killed-in-mexico-by-a-parent-,https://m-x.com.mx/investigaciones/matar-a-un-hijo-en-mexico-cada-dos-dias-muere-un-menor-a-manos-de-sus-padres,https://www.youtube.com/watch?v=tM0dEA2Cbc8&t=370s,https://www.youtube.com/watch?v=1E9yK1Bdx-I,,,Alejandra Crail," Alejandra Crail is an investigative journalist with over nine years of experience in digital and printed media in México. Throughout her career she has collaborated with news media such as El Universal, Más Por Más, Chilango, Vice News and currently with EMEEQUIS.    She  works with databases and public information access systems,  tools that she has used to develop various researches on corruption, use and diversion of public resources, human rights violations, feminism and gender violence, among other topics.    In 2018 she obtained the second place in the German Journalism Prize with the article:  La red fachada que se beneficia de los Moreno Valle  (The facade network that benefits from the Moreno Valle) published in República 32.   Her works also include  La poesía de la derrota  (The Poetry of Defeat), about the companies that earned millions of pesos in the 2018 presidential campaign, which she published in Vice News; Also the article  Plata y Plomo: la nómina dispareja de Felipe Calderón  (Silver and Lead: Felipe Calderón's Uneven Payroll), which portrays the salary increases to the high commands of the Mexican Army just before the War on Drugs were declared, documented in EMEEQUIS. ",,,
Mexico,"EMEEQUIS, CONNECTAS",Small,Participant,Best data-driven reporting (small and large newsrooms),To Kill One's Child: Every two days a child is killed in Mexico by a parent,15/10/19,"Investigation,Solutions journalism,Database,Open data,Illustration,Infographics,Chart,Video,Map,Audio,Politics,Health,Crime,Human rights","Adobe,Microsoft Excel,CSV","  #MatarAUnHijo  (To Kill One's Child) approaches the end point of the chain of victims of child abuse in Mexico:  death .   The cases registered in this research found how the lack of interest in the three levels of government have had fatal repercussions on the minors’ lives. Between 2012 and 2017,  almost 2,600 children under 15 were killed . The 42% of them by a  family member,  in their  homes  or as a result of  child abuse .   EMEEQUIS developed a database of homicides and created a methodology which revealed that fathers, mothers and step fathers are the main aggressors. "," This is the first journalistic research that approaches an integral vision of the causes, ways and consequences of child abuse in Mexico. The three displayed cases answer the questions:  who, how and why Mexican families kill.    The report was widely commented on radio stations, national television, and retaken on web portals reaching a wider audience. In addition, the methodology created was replicated in the state of Veracruz to discuss this problem at the local level.   This caused a first reaction from the government. The  president of Mexico,  Andrés Manuel López Obrador,  spoke about the feature in his matutine conference,  promising to reinforce with greater budget the fight against child abuse.   One month after publication, the  Senate approved the prohibition of corporal punishment  as an educational method. Also, members of the Chamber of Deputies,  began to discuss legislation to fight against child sexual abuse.  Lorena Villavicencio, legislator,  called on the Secretaries of Public Education, Health and Government to collaborate in the reduction and attention of cases of abused children.    The four texts that make up the research were consolidated as the most read of EMEEQUIS, with an average reading of almost six minutes per article, this is an indicator that one of the main objectives was met:  to reach Mexican homes.    Nevertheless,  the greatest achievement was to break the silence on a hidden and private topic, based on data and a completely new methodology.  These entries are also a tool that calls society and governments to assume the commitment to protect children. "," An  open database  of the National Institute of Statistics and Geography (INEGI, as in its acronym in Spanish) containing the total deaths registered annually in Mexico was used.  This database feeds on the death certificates  provided by the Civil Registry, the Forensic Medical Services and the statistical notebooks generated by the Agencies of the Public Ministry.   This data was eligible because  it’s the only database that contains specific characteristics of the deaths recorded in Mexico,  which allowed us to establish patterns of behavior around child abuse. We defined a five-year analysis period between 2012 and 2017 to portray patterns of violence committed against minors.   Prior to the data analysis, interviews were conducted with doctors, NGOs and specialists in child abuse topics, these  interviews were the starting point for establishing the analysis methodology.  ""Most battered children are killed in their homes"" said one of the specialists. As of that, we defined three analysis categories. First, the cases in which  the aggressor was identified and it had some kinship with the victim .  Second,  deaths that were recorded inside a house.  Third, d eceases in which death' cause was directly mistreatment, neglect or abandonment.    In order to establish the study population, UNICEF's definition of childhood were retaken, which considers that children under 15 are children.   Under these parameters five databases were downloaded with information on mortality, one for each year of study, in  .csv format.  Microsoft Excel was used for data processing. After making the corresponding filters, we used  pivot tables  to perform data crossings and thus detail the place of death, cause of death, victim's gender, who the aggressors are and how they kill, as well as locate the hot spots in states and municipalities from Mexico. "," Talking about child abuse in Mexico is not easy, because  it's a crime that occurs in private,  by the people who are supposed to take care and protect the children. In addition, the  disinterest of the authorities,  have suppressed the opportunity to build official statistics on the subject, despite being considered by specialists as a "" social disease "".   The lack of reliable statistics was the main obstacle, there are no data that allow to know the seriousness of the problem even on the end point of the chain of violence: death.  The creation of a methodology that allows to establish the severity of a problem that costs lives was fundamental  and required a lot of previous fieldwork to define the correct indicators.   However,  the most difficult part was to get the voices that could show the human part of the data that we had already processed.  This included the work to persuade the families of Diana Mia and Landon Yahir, as well Paloma, the woman who murdered her two daughters, to talk about their stories and let them realized the importance of telling their stories to raise awareness about the problem. This investigation wouldn't have had the desired impact without those voices. "," This in-depth report is a reminder that  investigative journalism is about revealing something that has not been said,  either because of an explicit interest in hiding it or, as in this case, because of a general lack of interest in addressing a problem that affects a certain sector of the population.   This project  shows the importance of generating new research methodologies  that allow us to reveal those realities that are not being talked about, so that they can be put on the public agenda and decision makers can attend to the people affected.   Another thing that can be learned from this project is that  the absence of official data on a problem shouldn’t be seen as an obstacle: there will always be ways to narrate what happens, in this case with the use of open databases.  Journalism is also about creating those new techniques that help highlight those situations that, as in the case of To Kill One's Child, cost lives. ",https://matarunhijo.m-x.com.mx/,https://m-x.com.mx/investigaciones/every-two-days-a-child-is-killed-in-mexico-by-a-parent-,https://m-x.com.mx/investigaciones/matar-a-un-hijo-en-mexico-cada-dos-dias-muere-un-menor-a-manos-de-sus-padres,https://www.youtube.com/watch?v=tM0dEA2Cbc8&t=370s,https://www.youtube.com/watch?v=1E9yK1Bdx-I,,,Alejandra Crail," Alejandra Crail is an investigative journalist with over nine years of experience in digital and printed media in México. Throughout her career she has collaborated with news media such as El Universal, Más Por Más, Chilango, Vice News and currently with EMEEQUIS.    She  works with databases and public information access systems,  tools that she has used to develop various researches on corruption, use and diversion of public resources, human rights violations, feminism and gender violence, among other topics.    In 2018 she obtained the second place in the German Journalism Prize with the article:  La red fachada que se beneficia de los Moreno Valle  (The facade network that benefits from the Moreno Valle) published in República 32.   Her works also include  La poesía de la derrota  (The Poetry of Defeat), about the companies that earned millions of pesos in the 2018 presidential campaign, which she published in Vice News; Also the article  Plata y Plomo: la nómina dispareja de Felipe Calderón  (Silver and Lead: Felipe Calderón's Uneven Payroll), which portrays the salary increases to the high commands of the Mexican Army just before the War on Drugs were declared, documented in EMEEQUIS. ",,,
Kenya,The Elephant,Small,Participant,Best news application,"Ebola: Truth,  Facts,  Lies",11/01/19,"Explainer,Cross-border,Quiz/game,News application,Fact-checking,Mobile App,Illustration,Health","Animation,Personalisation,Microsoft Excel,Google Sheets","Ebola Story is an interactive storytelling game about the challenges of Ebola response in East Africa. The goal was to create an entry point to a story so often told in pieces and easily dismissed by audiences. We often hear that people in DRC and East Africa feel the response is ineffective, while those internationally don't understand the layers of complexity of the response efforts. The goal was to show readers the challenges - poor infrastructure and access, security risks, mistrust of the population, challenges diagnosing cases and with contact tracing - through their own choice of narrative and a"," This project garnered a large audience for the Elephant and was successful at driving conversation about the response efforts. In particular, readers highlighted how they were unaware of certain issues, such as burial approaches or the frequency of attacks against responders. It also drove a different audience to the Elephant's site, allowing them to grow their audience base regionally and internationally. It also fostered connections between the journalists across borders, and the animator, with whom they hope to collaborate again. "," We worked very hard to design a storytelling method that would address the key issues of Ebola response, while still engaging the readers, and work well on many mobile formats. Our story was in part inspired by the Financial Times uber game, but we applied lessons learned and our regional context to that idea to develop this one.   We had initially planned to have a developer manually create the site, but he was unable to meet our deadlines. Instead, journalist Carolyn Thompson used an online template and tool to create the pathways that allowed the export of an html file. We then hired a different developer, Gabriel Soare, to add custom code to the basic structure to make the site more attractive and responsive.   We also had an illustrator design images to go with the story narrative, and used Photoshop to create graphics for the data. This was in part to simplify the process and meet our deadline, and also to create a consistent and responsive approach for all the data.   For the data processing, Thompson used Excel and Google sheets. "," We encountered several challenges throughout the development of the site. In particular, the original planned developer was unable to deliver within our deadline, and disappeared without having done any work. We were able to find an open source online template that journalist Carolyn Thompson used, adding custom code where needed.   When there were some specific elements we wanted that were beyond our in-house coding knowledge, we hired developer Gabriel Soare, an expert in CSS and Javascript who was able to design custom code to make the story more visually appealing. He also helped us with the process of getting the project hosted on the website.   It took much longer than expected to develop the story routes, due to the desire to make the story clear, readable, and yet comprehensive. In the end, the project has 138 passages (pages of information) and 187 links between them.   We also connected with an animator who created illustrations to help tell the story. She initially didn't think she could meet the deadline, but we decided to reduce the complexity of the graphics in favour of publishing with strong visuals.   Due to the high level of time involved in laying out those pathways and links, we decided not to translate the project on our first publication. However, we still plan to release French and Swahili versions to ensure we can target readers in the Democratic Republic of Congo and other affected areas. ","We worked very hard to design a storytelling method that would address the key issues of Ebola response, while still engaging the readers, and work well on many mobile formats. Our story was in part inspired by the Financial Times uber game, but we applied lessons learned and our regional context to that idea to develop this one. Initially we had envisioned the story with many more charts and maps and data points, but as we designed the project we realized that it was more important to focus on audience and engagement to ensure people made it through the experience. We erred away from using too much data, as we decided the narrative was the more important focus. We still included some data updates as section breaks and to help situate the story in time. We decided to provide two streams of storylines: one that travels through time from the perspective of an international responder, and another that experiences the response through the perspective of a citizen who risks becoming infected. This allows the reader to follow their area of interest, or to experience different versions of the story by trying multiple routes. We were also able to enlist an animator who created illustrations to go with the story. We thought a lot about how those illustrations should represent the story . especially in choosing to use bright colours and beautiful imagery to resemble a game and ensure we highlighted that daily life continues even amid a complex spread of disease, hoping to counter stereotypes of . We also had reporting gathered on the ground by a Congolese journalist to add to the story, but later decided to reserve the videos until the last section, to avoid interrupting the story flow and bring readers back to reality after the game",ebolastory.com,,,,,,,"Concept, reporting and writing: Carolyn Thompson, Muyisa Kamathe Seros, Juliet Atellah, and Halima Gikandi Illustrator: Kayee Au, PixelStory.co Editor: Stellar Murumba Developer: Gabriel Soare"," Carolyn Thompson is a freelance data journalist based in Nairobi, Kenya.   Muyisa Kamathe Seros is a freelance journalist based in Beni, Democratic Republic of Congo.   Juliet Atellah is data editor at The Elephant in Nairobi, Kenya.   Halima Gikandi is a journalist at The World radio program in Boston, USA   Kayee Au is the founder of PixelStory.co based in Cairo, Egypt.   Stellar Murumba is a data journalist based in Nairobi, Kenya.   Gabriel Soare is a freelance web designer and developer based in Bucharest, Romania. ",,,
Brazil,TV Globo,Big,Participant,Best visualization (small and large newsrooms),"In Brazil, at least 43 PMs are dismissed each day due to psychiatric disorders",15/09/19,"Investigation,Solutions journalism,Long-form,Database,Open data,Video,Politics,Health,Gun violence,Human rights","Microsoft Excel,Google Sheets,CSV,PostgreSQL"," In Brazil,at least 43 military police officers are removed from work every day for psychiatric disorders. A worrying fact, especially in a profession whose mission is to protect the citizen.   Permanent alert, stress, risky situations, confrontation, fear.And if all this accumulated over the years turns into a disease? What if this is all your profession? How a police officer is seen and treated by the Corporation when he has some kind of psychological distress?   Fantástico collected data from all the Brazilian military police, who works directly with conflict, to find out about their mental health. And the result is worrying. "," This was Fantástico´s first news report using data journalism, all made by us. It took around five months to get all data information needed for analyzing the current scenario of the matter described previously. Unfortunately, Brazil has differences in terms of compliance with the law and responses to access to public security information requests.   Once this first step was mostly completed, we took a couple months more to finish producing it all before airing, on September 15<sup>th</sup>.   The Institute for Applied Economic Research (Ipea), CICV and FBSP, three of the most important foundations/institutions that uses public security data for their surveys, asked me for sharing the results of my research, so it helps doing future researches about the matter. Indeed, Ipea is starting a study about police victimization.   Finally, police officers (especially the ones from the lowest ranks) and health professionals from the military corporations were encouraged to continue their work and denounce the prejudices and damage that the lack of psychological support brings to police officers and, consequently, to society.   It was also important to reveal the taboo subject within the militaries, as well as to denounce the prejudice suffered by many police officers who seek psychological assistance in the workplace.   We also show the initiatives of some states in Brazil, which maintain activities designed for the military police mental health assistance. "," Initially, I sent several requests through the Law of Access to Information (LAI) to all twenty-seven federative units in Brazil. It took a few months until I could have enough data to analyze the mental health reality inside the military police in the country; Then, I created a database in Excel and tried to insist for most of data I could get from the states, once they didn´t answer all I've asked in the  first replies; I did some data crossing on Excel for analyzing the scenario and finding some headlines (like the one in the first phrase from the first question).   Once this is a scarcely explored subject I had to keep insisting, also by phone, for data and informations that I had already requested through LAI. For the support images, and super-produced takes, I negotiated with Rio de Janeiro, São Paulo, Bahia e Santa Catarina policies.    The tunnel metaphor was also explored to talk about the disorders suffered by these professionals "," I highlight the difficulty of obtaining data on the subject in the public security of the states, which keeps a highly relevant issue as a taboo in the military, especially due to the conditions in which these police work, and taking into account the violence of the country added to the type of work of these public security professionals. As well as the prejudice on the subject in the military, which make it difficult for these police officers to seek psychological and psychiatric support services. So, too, the numbers of suicides, psychiatric leave, and police killings only increase. ","<pre> About the process,we can learn that the journalist needs to have mastery over the data he´s collecting,and the support of statisticians or data scientists,who can help in the mathematical part, without the journalist losing sight of the analytical character. Doing the process, alone, from start to final moments(when my editor Thiago helped me a lot, including the many times we've checked all data before airing the story) made me realize how fragile are the data provided by the state public security, the lack of a standard of transparency and information on the issues related to the mental health of these professionals who are exposed daily to situations of stress, conflict, without the necessary conditions to exercise the security of citizens. Which can culminate in police suicides, depression and other mental disorders, which lead these professionals to ask for medical leave, or to remain active with aggressive and unlimited behaviors.</pre> <pre> I guess the message we bring with the project is good because Brazilians don´t trust their military police. Nowadays, they´re totally apart from society. Although, they´re humans like everybody, and their work in Brazil make them more susceptible to disease.It brings a critical reflection on the society we want to live.</pre>",https://globoplay.globo.com/v/7925344/,https://globoplay.globo.com/v/7919260/,https://falauniversidades.com.br/policiais-brasileiros-atuam-psicologicamente-desamparados/,https://www.brasildefato.com.br/2019/10/01/para-combater-suicidio-policial-deputados-aprovam-projeto-de-lei-no-rio-de-janeiro/,https://www.anamt.org.br/portal/2019/09/16/no-brasil-pelo-menos-43-pms-sao-afastados-por-dia-por-transtornos-psiquiatricos/,http://www.ensp.fiocruz.br/portal-ensp/informe/site/materia/detalhe/47237,http://sindipoldf.org.br/noticias-sindipoldf/no-brasil-pelo-menos-43-pms-sao-afastados-por-dia-por-transtornos-psiquiatricos/,"Gabriela Rocha, Thiago Guimarães, Ana Carolina Raimundi, Daniel Torres, Jae Ho Ahn","   Main Team:     Gabriela Rocha - producer and reporter   Thiago Guimarães - news editor   Ana Carolina Raimundi - reporter   Daniel Torres - photography director/cameraman   Jae Ho Ahn - video editor   Fabio Covolo, Flavio Fernandes - Graphic Art         Other Contributors:     - Tecnicians: Daniel Durães, Joilson Luiz, Alexandre da Silva, Adriano Moraes, Alex Silva   - Audio: Leandro Araújo, Anderson Souza   - Extra images: Adriano Ferreira, Pedro Acyr   - Apoio: TV Bahia ",,,
Brazil,"Repórter Brazil, Public Eye, Agência Pública",Big,Shortlist,Open data,Cocktail of 27 different pesticides found in drinking water of 1 in every 4 municipalities,15/04/19,"Investigation,Multiple-newsroom collaboration,Open data,Map,Environment,Health,Human rights","Microsoft Excel,R","Our story unveiled, for the first time, nationwide detection of pesticides in the drinking water. Using the Brazilian freedom of information act, we had access to a public database and discovered that the Brazilian water is broadly tested positive for 27 pesticides (11 of which banned in Brazil and 21 in the European Union). It was the first time this information was made public. Our feature-story reports on nationwide contamination and our interactive map allows city level searches pointing out concentrations above the Brazilian safety limit (where standards are considered flexible) and concentrations that breached the European Union standard (world's"," This publication had unprecedented repercussions and the impact is still being measured:   - Water companies were called to explain the data from our map by city councils across the country;   - Public audiences specifically about our findings were held in the state of Minas Gerais;   - Our team received an invitation to be part of a public audience held in the Brazilian Congress in order to present the information investigated and debate measures of transparency;   - Two public defenders opened investigations about the presence of pesticide in the water of their cities based on information unveiled by our publication;   -  Large cities, such as Brasília, the country capital, had never sent the results of their tests to Ministry of Health (database manager). After being held accountable, water companies promised to be more transparent about the result of their tests. We are still following this story.   - The impacts were, in many cases, on a local level, but it was a direct result of the map repercussions: we tracked over 400 local news-stories (Internet, radio and TV) about pesticides in the water quoting our investigation or map as source. For over a month after the publication, our team answered calls, emails and helped local reporters who were following our investigation to write about their region.       The impact was a result of the repercussions in national and international media. Our original news-feature became the headline of the country's second largest news webportal UOL (https://noticias.uol.com.br/reportagens-especiais/coquetel-com-agrotoxicos-esta-presente-na-agua-de-1-a-cada-4-municipios/#tematico-3) and was republished by Exame magazine (https://exame.abril.com.br/brasil/1-em-4-municipios-tem-coquetel-com-agrotoxicos-na-agua-consulte-o-seu/).   At least 3 international media outlets published stories about our findings quoting our investigation as source:    - Telesur (South America): https://www.youtube.com/watch?v=tfE0dJjb5IE&feature=youtu.be   - The Guardian (UK):  https://www.theguardian.com/world/2019/apr/26/brazil-finds-worrying-levels-of-pesticides-in-water-of-1400-towns   - CGTN (China):  https://america.cgtn.com/2019/05/12/water-contaminated-by-pesticides-causes-fear-in-brazil   The findings were also published as part of a broader report by Public Eye on hazardous pesticides: https://www.publiceye.ch/fileadmin/doc/Pestizide/2019_PublicEye_Highly-hazardous-profits_Report.pdf             "," We used R with tidyverse packages (dplyr, tidyr) to clean, explore and statistically process the data. The first prototypes have been designed as  Observable notebooks.   Special care has been required to prepare lightweight spatial data for the online map, while maintaining precision when zooming on even the smallest municipalities of Brazil. The spatial data has been formatted as topojson, and projected with the Brazil Polyconic geographic projection (EPSG:5530), using packages such as topojson, shapefile and d3-geo by Mike Bostocks and Philippe Rivière.   The web application has a simple architecture and does not rely on any specific JavaScript framework. The state and the views, as well as the map, the graphics and the legend are managed with D3.js (d3-dispatch, d3-selection, d3-geo, plus some additional plugins such as d3-annotation by Susie Lu). The map is rendered in a canvas due to the large number of polygons, but all the remaining interactions use SVG elements. The CSS is based on the Bulma framework. All the code is versioned with git and published on GitHub.   The webapp has been integrated into the WordPress website (http://portrasdoalimento.info/agrotoxico-na-agua) as an iframe.  "," We struggled to find a way to present the data to the overall audience in a way that all Brazilians could understand. This was our biggest challenge: to simplify a convoluted database. The information was hidden under layers of  technical codes and jargon that were hard to crack - and that not even all technicians understand. The large repercussions proved that our efforts met this goal, unveiling information of high public interest and prompting a call for transparency across the country.   The large repercussion also led us to another hardship and, later on, another relevant impact:    After being held accountable by local media, some of the companies responsible for testing the water started to question our interactive map. Under severe scrutiny and legal threats, our team had to re-check a few specific cases. We proved there was no mistake in our methodology and, by doing so, we realised that the ones making the mistakes were a few of the water companies (the ones responsible for inputting the data in the database). The questions raised about the map unveiled a technical misunderstanding, which needed to be clarified by the Health Ministry. At least two water companies were making mistakes when inputting their data and these mistakes might have gone undetected for years in case the map had not been published.    In conclusion, our efforts to read the data in a way the overall public could understand helped to correct a communication problem between the Health Ministry (database manager) and some of the water companies (responsible for data input). "," Some very relevant stories are still untouched by journalism. Our team had been covering the issues of pesticides for years, but never thought that our drinking water could offer such risk. Neither that such a rich database existed in Brazil.   International collaboration is hard to operate, but sometimes it is fundamental to find a story. Had we been alone in this front, we might have never looked for this data, as this has never been an issue discussed in our country. Also, the parameters to read the data were largely improved by the comparison to the European Union regulation. Had we not used this parameter, our conclusion might have been that there was no story, since the water contamination was mostly within the Brazilian standards (regulation which is questioned in our report). It was the comparison with the European Union standards that made us “see” the problem.  ",https://reporterbrasil.org.br/2019/04/coquetel-com-27-agrotoxicos-foi-achado-na-agua-de-1-em-cada-4-municipios/,https://reporterbrasil.org.br/2020/02/cocktail-of-27-pesticides-found-in-water-of-1-out-of-4-brazilian-cities/,https://portrasdoalimento.info/agrotoxico-na-agua/#,https://www.theguardian.com/world/2019/apr/26/brazil-finds-worrying-levels-of-pesticides-in-water-of-1400-towns,https://www.publiceye.ch/fileadmin/doc/Pestizide/2019_PublicEye_Highly-hazardous-profits_Report.pdf,,,"Ana Aranha, Laurent Gaberell, Sylvain Lesage, Carla Hoinkes, Luana Rocha, Babak Fakhamzadeh"," Ana Aranha (reporter, editor and coordinator of this project in Brazil / Repórter Brasil) is an investigative journalist covering the pesticides, the Amazon and other public interest stories in Brazil, where her work has been honored with 13 awards;   Laurent Gaberell (researcher, editor and coordinator of this project in Switzerland / Public Eye)  Agriculture, Biodiversity & Intellectual Property researcher at Public Eye;   Sylvain Lesage (data analyst and developer in this project / Public Eye) is a French-Bolivian web and dataviz developer.   Luana Rocha (reporter in this project / Repórter Brasil) is a reporter and documentary producer;   Carla Hoinkes (researcher in this project / Public Eye) is a Agriculture, Biodiversity & Intellectual Property researcher at Public Eye;   Babak Fakhamzadeh (developer / Agência Pública) -  was working in ICT4D before it had a name and never really left it. He brought photomarathons to Africa and has won prizes for his work on three continents. He has an interest in creating mobile solutions for urban discovery that empower the individual. ",,,
Peru,OjoPúblico,Small,Winner,Innovation (small and large newsrooms),Funes: Funes: an algorithm to fight corruption,25/11/19,"Investigation,Database,Corruption,Economy","AI/Machine learning,Scraping,Json,Microsoft Excel,CSV,R,RStudio,PostgreSQL,Python"," Funes is an algorithm that identifies corruption risk situations in publics contractings in Peru. The research project began to take shape in February 2018 and its development began in September of the same year. For 15 months a multidisciplinary team - integrated by programmers, statisticians and journalists - discussed, analyzed, built databases, verified the information and developed modeled an algorithm we call Funes, as the memorable protoganist of the Argentine writer Jorge Luis Borges. The algorithm rates a risk score for each contract process, entity and company. With that information journalists can prioritize their investigations. "," The project was developed in the context of the fiscal investigations of the Lavajato case, which involves the payment of bribes by the Brazilian company Odebrecht in order to take charge of public contracts for the construction of public works. FUNES analyzes the contracts, and during its launch, identified a huge number of contracts with corruption risks. Of these, several were investigated and transformed into published reports.   FUNES is the first tool developed in Peru, and one of the first of its kind in Latin America, which analyzes millions of data, to grant a corruption risk score in public procurement. FUNES identified that between 2015 and 2018 the Peruvian State granted almost 20 billion dollars in risky contracts. These were delivered to a single bidder who had no competition and to companies created a few days before the contest. The amount represents 90 times the civil reparation that Odebrecht must pay for its acts of corruption.   Other published reports identified acts of corruption in companies that sell milk for social programs.  The tool has a friendly interface for readers with several visualizations in which the reader can analyze the situation of public contracts in Peru.   The open source tool has attracted the interest of the control and control entities of Peru, who have requested to share the methodology and possibilities so that they can implement it in their equipment. FUNES warns of risk in thousands of contracts. Therefore, and given the dimension of the findings, OjoPúblico established alliances with regional media to analyze and investigate some of the main cases. Everyone noticed the same thing: irregular public contracts that have now begun to be investigated by the authorities. The investigations continue. ","Funes proviene de una familia de algoritmos denominados modelos lineales para combinar la información de 20 indicadores de riesgo, que fueron calculados a partir de 4 bases de datos. Un modelo lineal tiene la forma de un promedio ponderado: peso_1indicador_1 + peso_2indicador_2 + ... + peso_nindicador_n = riesgo de corrupción Para aprender estos pesos usualmente se utiliza un esquema de regresión, que consiste en intentar predecir la respuesta -que en este caso, sería la corrupción- a partir de variables relacionadas -como llamaremos a los indicadores de riesgo-. De esta manera, los pesos aprendidos para cada indicador son los que mejor ayudan a predecir la respuesta para todos los contratos analizados. Sin embargo, Funes usa una variante de este esquema porque la corrupción en contrataciones públicas -denominada nuestra variable respuesta- es un fenómeno no observable: tenemos seguridad de que los contratos que han sido descubiertos por los fiscalizadores fueron corruptos; pero los que no, no sabemos si están absolutamente limpios o aún no son descubiertos, porque pueden responder a sofisticados y esquemas de corrupción más complejos como sucede, por ejemplo, con el caso Odebrecht y Lava Jato. El método de Funes parte de un esquema de proxies de corrupción, propuesto por Mihaly Fazekas, investigador de la Universidad de Cambridge, y adecuado y p al contexto peruano de l. Un proxy es una variable estrechamente relacionada a la variable no observable. Funes usa dos proxies: 1) que un contrato haya tenido un único postor; 2) la proporción de concentración del presupuesto de una entidad que tiene cada contratista. Entonces, Funes es una combinación de dos modelos lineales, una regresión logística para el único postor y una regresión beta para la proporción de concentración. El resultado de este proceso es un índice de riesgo de corrupción para cada contrato: a más alto, mayor"," The main challenges were related to the construction, access and quality of the data, the need for the team to learn new data analysis tools and the formation of a multidisciplinary team hitherto oblivious to journalistic research. In Peru there is no open data portal for hiring. For 7 months a script was developed and extracted data from a platform, which had blocked mass access through a captcha. The responsible entity blocked our IP to avoid downloading, forcing the team to reformulate the code to make extraction more efficient. To complete this information, 20 requests for access to information were also submitted.   Another challenge was also the learning process on corruption theory, statistics and public procurement laws in Peru. We were not specialists in public bidding and there are 15 regulatory regimes. Meetings with experts were organized to know the process in detail, the processes were documented and each of the legal norms was analyzed.  Another of the challenges was also the definition of the concept of corruption that we were going to monitor and the model that we were going to use to develop the algorithm. Many papers were reviewed and interviews were conducted. In the end, the statistical model promoted by researcher Mihali Fazekas was chosen. The project left a journalistic team with robust knowledge in algorithms, R programming language, public contractings and predictability. "," We learned that the fight against corruption from journalism requires incorporating into its traditional case-by-case methods and massive data analysis, tools with algorithmic models that allow it to anticipate corruption. For them, journalistic teams are required to go beyond spreadsheets and open refining, and learn relational analysis technologies and R., and at the same time learn to convene and work with mathematicians, statisticians, programmers and political scientists. ",https://ojo-publico.com/especiales/funes/,https://ojo-publico.com/especiales/funes/rankings-de-riesgo.html,https://ojo-publico.com/especiales/funes/metodologia.html,https://ojo-publico.com/1499/proyecto-funes-riesgos-de-corrupcion-en-contratos-publicos,https://ojo-publico.com/1331/familia-lechera-los-millonarios-contratos-del-grupo-niisa-con-el-vaso-de-leche,https://knightcenter.utexas.edu/blog/00-21439-peruvian-investigative-site-ojo-publico-develops-algorithm-track-possible-acts-corrupt,https://altec.lat/iniciativa-funes-corrupcion-en-peru-que-esta-haciendo-el-periodismo/,"Gianfranco Rossi, Nelly Luna Amancio, Gianfranco Huamán, Ernesto Cabral, Óscar Castilla"," Editor founder at OjoPúblico, investigative media outlet that is currently widely read in Peru and a reference organization in Latin America media. Investigative journalist specializing in environment, human rigths and corporate power. Her investigations has received international and national awards such as the National Human Rights Award (2015); the Data Journalism Award for Best Investigation of the Year (small newsroom), granted by the Global Editors Network (GEN); the prize for journalistic excellence in the reporting category, granted by the Inter-American Press Association (SIP); and the Scientific Journalism Prize awarded by the Institute of the Americas. In 2019 she and her team obtained the honor mention granted by the Latin American Studies Association (LASA) for her role in covering topics with depth to understand Latin America.  She is member of the Investigative Reporters and Editors (IRE). Collaborator of the International Consortium of Investigative Journalism (ICIJ). She participated in the Panama Papers, the global research that won the Pulitzer Prize 2017. With her leadership, the OjoPúblico team has developed several journalistic applications and cross-border investigations about environmental crimes and corporate power. She was part of the team of journalists who carried out the transmedia project ""Dirty Gold"", a global investigation that identified the main funders of the gold rush that has devastated the forests of Peru, Bolivia, Brazil, Ecuador and Colombia in the last years.  She is a member of the Advisory Committee of the Rainforest Journalism Fund, with the support of the Pulitzer Center on Crisis Reporting. ",,,
Mexico,El Universal,Big,Participant,Best data-driven reporting (small and large newsrooms),The danger of being a young in Mexico,08/12/19,"Investigation,Explainer,Database,Open data,Infographics,Chart,Corruption,Crime,Gun violence","D3.js,Microsoft Excel,CSV,Python"," Two of every ten of killings in Mexico, between 2007 and 2018, were young Mexicans (15-24 years); only in this period have been murdered 59,779. But the problem, it´s more huge: in three years (2007 to 2011) the homicide rate of this population, it tripled and showed a terrific number: 29 murders per 100 thousand young people. This was the proof of extreme violence and it was considered scandalous, but the new trends show violence that is surpassing any existing record.   For the experts, the main reason for this is the lack of justice and corruption in Mexico. "," At the media in Mexico is very difficult to create a multimedia product, but with this microsite, we achieved an entire product to engage users and explain in different forms a difficult phenomenon. Besides, the analysis help to understand what happened in each state of Mexico and this type of product is not common in Mexico media.    We were mentioned by the Global Investigative Journalism Network community  (GJIN) as one of the best data journalism investigations published during that week (August 12, 2019) both for the visualizations and for the analysis that had been done of the homicides of young people in Mexico. Also, the work was republished by more than five electronic media in the country. "," We used Python to clean and analyze the data. Also, for the analysis, it was very important the use of two libraries: Pandas and Numpy.   Matplotlib and Seaborn were used to create graphics to allow explore different patterns that were important for the story.   The data visualization was created with Javascript D3, this tool helps us to join the data with the graphics. The library “graph-scroll” was used to aggregate extra interactions to complete the narrative and comprehension of the visualization through a scrolly telling.   The goal of this visualization was to show the trends of the homicide rate for two groups of age at first view, also create a ranking for each year. We don´t use a map because this would be a limit for the temporal category and it can´t be useful for a comparison of the two groups of age. That the main reason to create a different series of times and then we aggregate a transition to order the states according to its homicide rate for a year.   For the creative process of the visualization, the first step was a sketch in the paper, then we did a static version in Canvas using the library p5.js with real data. However, the curves of the series of times couldn´t see well and the transitions were very difficult and slow; that was the main reason to create a graphic in SVG through the library of Javascript D3, one of the most powerful software to visualize data. ","The objective of this project was to show the danger of being young in Mexico and prove the probability of being killed in different states of Mexico. To try to understand the dynamics of a phenomenon so complex like the homicides in Mexico, you need the data as son a disaggregated possible (temporally and spatially) and have enough years to show real trends of the problematic. The big issue is that data only exists in the different databases in Mexico. Inegi, the agency in charge of providing the main socio-economic indicators of the country, is the most credible source for this topic since their figures come from death certificates, but all data is one year later. It means that the last data of murders in Mexico were to 2018 and at the end of 2020, we have the data of 2019. On the other side, the Secretariado Ejecutivo, an agency of the federal government, has an update data of murders in the country, but these numbers have not the detail with the age of each victim. In theses database, the age is in different ranks and if we wanted to obtain the number of victims and not the files of investigation, we had to limit our analysis from 2015, with what we couldn´t do a temporal analysis which showed real patterns of the time. We determine that the most accurate source for this project was Inegi because in this data we could find patterns for each county and that was a goal for the investigation and it was going to determine the interviews. Another difficult part is that most of the country is very dangerous for interviews and reports of violence. So, we had to select very carefully the counties that were visited. We wanted to show the magnitude of"," One of the points that made this project stronger is to have a multidisciplinary team. Having specialized people for each of the topics in the project was essential.   Understand that the issue of homicides cannot be analyzed lightly. We need a methodology that supports each of the findings and know from the beginning what we want from the data. Since the beginning, we wanted to show the danger of being young in Mexico and that they were (and so far are) the population most affected by the violence in the country. Without the data that supported our hypothesis, we would only have stayed with the anecdotes and the testimonies, but in this case, the analysis of the databases gave us the best starting point.   Be clear, transparency and accurate with the data. And work with people who have different abilities. ",https://interactivo.eluniversal.com.mx/2019/homicidios-jovenes/,,,,,,,"Jetzabel Daniela Guazo, Iñigo Arredondo, Daniel Gómez, Melissa Amezcua, Valente Rosas, Miguel Garnica","  Iñigo Arrendondo . Investigative journalist. Since May 2019 I coordinate the investigation team of El Universal. I used to be the editor-in-chief of Buzzfeed News Mexico and before that, I was an investigative reporter at El Universal. The topics I write about are security, politics, and migration.    Daniela Guazo  is a data journalist based in México City. Guazo has a Bachelor´s degree in Communications from the Universidad Iberoamericana (UIA) in Mexico City and since 2010 she specialized in data journalism. She worked at the magazine Gatopardo, Expansión (specialized in business), and at the Fundacion Mepi, a nonprofit organization focus on data and investigative journalism. In 2015 she starts as a collaborator at the Data Unit at El Universal and since 2016 she coordinates the Data Unit, the first in a newspaper in Mexico. In 2016 she won an Ortega y Gasset award for the investigation of missing persons in Mexico.    Melissa Amezcua.  Investigative journalist since 2017. I report on gender and human rights issues on El Universal. Former Buzzfeed News reporter has published features on Vice, Popula, Broadly and Brut Media covering violence against women, immigration, culture, and politics    Daniel Gomez.  Gomez has a Bachelor´s degree in Physics from the National Autonomous University of Mexico (UNAM). He oriented his studies in programming, data analysis, and complex systems. He worked in interdisciplinary teams with journalists and designers doing data analysis and visualization. Currently, he works at El Universal as an analyst and visualizer.    Miguel Garnica Mejía . Garnica has a Bachelor´s degree in Communications and Journalism from the National Autonomous University of Mexico (UNAM) and a Master´s Degree in Digital Media from the Simon Bolivar University. Currently is the coordinator of the web design team at El Universal. He has 18 years of experience in digital design. He is in charge of the operation for the site eluniversal.com.mx and the interactives for the media.    Valente Rosas.  Photojournalist.  ",,,
Japan,"Japan Initiative, Vdslab, Visualizing.JP & Waseda Chronicle",Big,Participant,Open data,Judgit!,07/11/19,"Database,Open data","Scraping,D3.js,CSV,Python"," JUDGIT!...this database allows users to look up how the Japanese national budget is being spent.    This is available for free, everyone can use it, and it can be useful to investigate journalism.   The data we have parsed is available as open data.         "," Using this database, we can investigate the actual flow of government money, and find the misuse of the fund.    The best part of this DB is that we can look into who and what the money was spent for, not just the amount of the budget.   For example, we examined by focusing on the special account for Fukushima reconstruction.   Search “Special Account for Recovery from the Great East Japan Earthquake” and we found there were “16 reconstruction projects” in “Ministry of Defense”. ","<h3>Data cleansing</h3> <ul>  Data extraction of Excel file by Python.   Data cleansing with Python  </ul> <h3>Backend</h3> <ul>  BigQuery database.  </ul> <h3>Frontend</h3> <ul>  D3.js, crossfilter.js and DC.js.  </ul> <div id=""gtx-trans"" style=""position: absolute; left: 2px; top: 191.609px;""> <div class=""gtx-trans-icon"">    ","<h3>A. Data cleansing. </h3>  Since the original data of JUDGIT! was Excel, and officials have input the data manually.   Identification of names of the organization like ministries, bureau, and the department has required.  <h3>B. Collaborative journalism.</h3>  To make invisible data visible, journalists must have collaboration with other professionals like data engineers and designers or other specialists "," JUDGIT! is in collaboration with four parties.    Japan Initiative  has proposed and realized a system for reviewing Japanese government projects.   VdsLab  has created a BigQuery database from Excel files that have been published only for printing.   Visualizing.JP  has enabled exploratory visualization to find the hypothesis.   Waseda Chronicle  has started using this database to begin investigative journalism.   This could not have been realized even if only one organization was missing.   Although the government disclosed information publicly as open data, it is often the case that the government offers it with low searchability or low accessibility. It was the same in this case.  To make invisible data visible, journalists must have collaboration with other professionals like data engineers and designers or other specialists.  In your country, there must be “Hidden Open Data.” Please find it, and make it open!  <div id=""gtx-trans"" style=""position: absolute; left: -2px; top: -5px;""> <div class=""gtx-trans-icon"">    ",https://judgit.net/,https://news.yahoo.co.jp/byline/yazakiyuichi/20190815-00138538/,http://www.kosonippon.org/project/detail.php?id=806,https://www.wasedachronicle.org/category/articles/judgit/,https://vdslab.jp/projects,,,"Hideaki Kimura, Kyosuke Aragane, Makoto Watanabe, Masahiko Akimoto, Naoki Kikuchi, Shin Ito, Shun Tanaka, Yosuke Onoue, Yuichi Yazaki, Yuko Ryu, Yuta Nagayoshi","  Japan Initiative:     ""Japan Initiative"" is an independent, non-profit organization that creates political policies from the standpoint of ""citizenry. "" For various policy themes that citizenry is facing, Japan Initiative proposes strategies in concrete forms such as bills and ordinances and conduct activities for planning.     <a href=""http://www.kosonippon.org"">http://www.kosonippon.org</a>         Vdslab:     Mr. Yosuke Onoue, who is organizing the Laboratory of Visualization and Data Science (vdslab), Department of Information Science, Nihon University, conducts research and development. We cannot solve problems if we haphazardly visualize and analyze large amounts of complex data. Vdslab aims to create a dialogue between people and data that gives deep insight into real problems through visualization.     <a href=""https://vdslab.jp/"">https://vdslab.jp/</a>        Visualizing.JP:     Visualizing.JP is a organization about data visualization in Japan. They has worked in several Japanese newsrooms including NHK, Nikkei-Shimbun, Yahoo! Japan and so on. They were a media partner in Japan for Data Journalism Awards for 2017 & 2018 and one of the admin members of the Online News Association Japan.     <a href=""https://visualizing.jp/"">https://visualizing.jp/</a>     <a href=""https://notation.co.jp/"">https://notation.co.jp/</a>        Waseda Chronicle     An important role of journalism is to serve as a check on power. Power includes not only state power, but also the power of large companies against which individual citizens can do little. It is the role of journalism to maintain watch against abuses of power; to uncover hidden evil, and to convey the truth of it to the people. Waseda Chronicle is a news organization dedicated to investigative journalism – uncovering hidden facts and revealing inconvenient truth.  Waseda Chronicle is an official member of the GIJN. The Waseda Chronicle joined the network in June 2017, becoming its first Japanese member as a news organization.     <a href=""http://www.wasedachronicle.org/"">http://www.wasedachronicle.org/</a>  ",,,
Brazil,TV Globo,Big,Shortlist,Best data-driven reporting (small and large newsrooms),"In Brazil, at least 43 PMs are dismissed each day due to psychiatric disorders",15/09/19,"Investigation,Solutions journalism,Long-form,Database,Open data,Video,Politics,Health,Gun violence,Human rights","Microsoft Excel,Google Sheets,CSV,PostgreSQL"," In Brazil,at least 43 military police officers are removed from work every day for psychiatric disorders. A worrying fact, especially in a profession whose mission is to protect the citizen.   Permanent alert, stress, risky situations, confrontation, fear.And if all this accumulated over the years turns into a disease? What if this is all your profession? How a police officer is seen and treated by the Corporation when he has some kind of psychological distress?   Fantástico collected data from all the Brazilian military police, who works directly with conflict, to find out about their mental health. And the result is worrying. "," This was Fantástico´s first news report using data journalism, all made by us. It took around five months to get all data information needed for analyzing the current scenario of the matter described previously. Unfortunately, Brazil has differences in terms of compliance with the law and responses to access to public security information requests.   Once this first step was mostly completed, we took a couple months more to finish producing it all before airing, on September 15<sup>th</sup>.   The Institute for Applied Economic Research (Ipea), CICV and FBSP, three of the most important foundations/institutions that uses public security data for their surveys, asked me for sharing the results of my research, so it helps doing future researches about the matter. Indeed, Ipea is starting a study about police victimization.   Finally, police officers (especially the ones from the lowest ranks) and health professionals from the military corporations were encouraged to continue their work and denounce the prejudices and damage that the lack of psychological support brings to police officers and, consequently, to society.   It was also important to reveal the taboo subject within the militaries, as well as to denounce the prejudice suffered by many police officers who seek psychological assistance in the workplace.   We also show the initiatives of some states in Brazil, which maintain activities designed for the military police mental health assistance. "," Initially, I sent several requests through the Law of Access to Information (LAI) to all twenty-seven federative units in Brazil. It took a few months until I could have enough data to analyze the mental health reality inside the military police in the country; Then, I created a database in Excel and tried to insist for most of data I could get from the states, once they didn´t answer all I've asked in the  first replies; I did some data crossing on Excel for analyzing the scenario and finding some headlines (like the one in the first phrase from the first question).   Once this is a scarcely explored subject I had to keep insisting, also by phone, for data and informations that I had already requested through LAI. For the support images, and super-produced takes, I negotiated with Rio de Janeiro, São Paulo, Bahia e Santa Catarina policies.    The tunnel metaphor was also explored to talk about the disorders suffered by these professionals ","     I highlight the difficulty of obtaining data on the subject in the public security of the states, which keeps a highly relevant issue as a taboo in the military, especially due to the conditions in which these police work, and taking into account the violence of the country added to the type of work of these public security professionals. As well as the prejudice on the subject in the military, which make it difficult for these police officers to seek psychological and psychiatric support services. So, too, the numbers of suicides, psychiatric leave, and police killings only increase. ","<pre> About the process,we can learn that the journalist needs to have mastery over the data he´s collecting,and the support of statisticians or data scientists,who can help in the mathematical part, without the journalist losing sight of the analytical character. Doing the process, alone, from start to final moments(when my editor Thiago helped me a lot, including the many times we've checked all data before airing the story) made me realize how fragile are the data provided by the state public security, the lack of a standard of transparency and information on the issues related to the mental health of these professionals who are exposed daily to situations of stress, conflict, without the necessary conditions to exercise the security of citizens. Which can culminate in police suicides, depression and other mental disorders, which lead these professionals to ask for medical leave, or to remain active with aggressive and unlimited behaviors.</pre> <pre> I guess the message we bring with the project is good because Brazilians don´t trust their military police. Nowadays, they´re totally apart from society. Although, they´re humans like everybody, and their work in Brazil make them more susceptible to disease.It brings a critical reflection on the society we want to live.</pre>",https://globoplay.globo.com/v/7925344/,https://www.brasildefato.com.br/2019/10/01/para-combater-suicidio-policial-deputados-aprovam-projeto-de-lei-no-rio-de-janeiro/,https://falauniversidades.com.br/policiais-brasileiros-atuam-psicologicamente-desamparados/,https://www.anamt.org.br/portal/2019/09/16/no-brasil-pelo-menos-43-pms-sao-afastados-por-dia-por-transtornos-psiquiatricos/,http://sindipoldf.org.br/noticias-sindipoldf/no-brasil-pelo-menos-43-pms-sao-afastados-por-dia-por-transtornos-psiquiatricos/,http://www.ensp.fiocruz.br/portal-ensp/informe/site/materia/detalhe/47237,https://twitter.com/showdavida/status/1172314806696591361,"Gabriela Rocha, Thiago Guimarães, Ana Carolina Raimundi, Daniel Torres, Jae Ho Ahn","   Main Team:     Gabriela Rocha - producer and reporter   Thiago Guimarães - news editor   Ana Carolina Raimundi - reporter   Daniel Torres - photography director/cameraman   Jae Ho Ahn - video editor   Fabio Covolo, Flavio Fernandes - Graphic Art         Other Contributors:     - Tecnicians: Daniel Durães, Joilson Luiz, Alexandre da Silva, Adriano Moraes, Alex Silva   - Audio: Leandro Araújo, Anderson Souza   - Extra images: Adriano Ferreira, Pedro Acyr   - Apoio: TV Bahia ",,,
Serbia,KRIK,Small,Participant,Innovation (small and large newsrooms),Corruption links: Minister's Brother Deeply Connected to a Firm Hired on Government Projects,22/07/19,"Investigation,Solutions journalism,Long-form,Fact-checking,Illustration,Politics,Corruption,Business,Crime","Animation,Adobe,Microsoft Excel"," Portal KRIK published an investigation about Predrag Mali, brother of Serbian Finance Minister Sinisa Mali, which discovered and documented that Predrag is using an expensive car owned by “Millennium Team “, private company engaged in numerous state-funded projects. (One of those projects is famous and controversial “Belgrade Waterfront”.) Apart from the car, KRIK found that Predrag was also using the apartment owned by this company, in which his previous girlfriend has lived. Experts agreed that such relationship between individuals (close to people in power) and private companies can be interpreted as a classic example of corruption. "," KRIK couldn't imagine that this story will be turned into one of the biggest smear campaigns against its team. Only few hours after the story about Predrag Mali was out, pro-regime tabloid „Serbian Telegraph“ started publishing fabricated information about KRIK. They claimed that KRIK’s journalist was ""following and harassing Predrag Mali’s wife and their baby"". As an “evidence” the tabloid published a video of an unknown man walking in front of Mali’s building, but that man clearly was not one of KRIK’s reporters.     This was serious and well-organized attack, aimed to discredit KRIK and to distract the public attention from the discovery about minister’s brother. At the same time, Serbian top state officials like Prime Minister Brnabic and President Vucic made very irresponsible statements regarding this case, they were ignoring the discovered corruption and started claiming that KRIK is on “tabloid level”.      In an incredible turn of events, Serbian police have issued award of 5.000$ to anyone who identifies the person shown in the video allegedly following Mali’s wife. This was unprecedented abuse of institutions, because Serbian police was not announcing this kind of awards even in serious cases of political assassinations, but now it did, in supporting the tabloid spin effect. Two weeks later, at KRIK’s insistence, prosecution revealed that the person from that video was actually some random passerby with the initials N.A.  This story has attracted national attention, but since Serbia is captured state and public institutions (such as Anti-Corruption Agency and Prosecution) are under control of the ruling political party, there has not been adequate legal consequences against the minister so far.      "," While working on this story, KRIK’s reporters have used all research journalism techniques available: FOI requests to get official documentation, obtaining information from live sources, researching through national and international business databases, and at the end - extensive field work.      Having in mind that in recent years Serbian state institutions are refusing to deliver requested documents to journalists, it was important for KRIK to develop new innovative ways on how to get and prove information. That is why, in order to prove that Predrag Mali is actually driving “Audi a6”, owned by company “Millennium Team”, KRIK reporters have decided to monitor the entrance to the parking of his building.  It took several days of field work to finally spot Predrag driving that car and to take a photo of him as evidence for the investigative story. Many colleagues from foreign media have found this approach interesting and KRIK reporters have gave the mini trainings on how to do this type of monitoring in practice.      So KRIK journalists have visited the building in which Predrag Mali lives, but none of KRIK team members has spoken to or followed his wife. The building was visited only with the aim of verifying that Mali is still driving “Audi a6”, and this was already thoroughly explained in the original investigative story. KRIK adheres to the highest standards of journalism and respects the rights of the persons it reports about, as well as the rights of their family members.      "," The smear campaign that was immediately launched against KRIK was definitely the hardest part, it was vicious media attack, which lasted for several days. Progovernment tabloid “Serbian Telegraph” was constantly publishing new lies about KRIK’s journalists in order to spin and divert the public from our discovery about minister’s brother.      During that week, editor-in-chief of “Serbian Telegraph” was guest of one TV show and he publicly admitted that his journalists do not really know who the person from that video is and that they are still checking his identity. This did not stop them to lie and mark our reporters as stalkers. During this appearance he has even accused KRIK of using the “Japanese surveillance method” which “allows that target notices that you are there, it is used for intimidation”.     By publishing false accusations, this pro-government tabloid was endangering the safety of our team and a direct consequence was the fact that during those days we were again receiving death threats on our social media profiles.  However, KRIK team decided to fight for its reputation. We immediately launched crowdfunding action titled “We follow corruption, we don't follow babies!”, where our team members made short videos in which we call our readers to support KRIK. We printed special edition of KRIK T-shirts with graphic illustration of this story about Mali, which we were giving out as gifts to all citizens who paid us donation. This was very successful action, because our discovery is now also present in the “offline world” on the streets of Serbia, since our readers become our walking billboards. Also, our journalists have used every opportunity since then to publicly wear those T-shirts in TV shows, award ceremonies and conferences. KRIK team has continued to monitor this case, we will report on any further developments.     "," Sounds immodest to advise other colleagues, but we feel that persistence and attention to details is the most important for investigative journalists. After finding one old document showing that minister’s brother was charged for misdemeanor speeding, our journalists have paid attention to this seemingly irrelevant information, they followed the lead and deeply investigated the case. And at the end they have managed to prove direct links between people closed to the Government officials and private companies which are involved in valuable state funded construction businesses.      And we can say that our project is an example of why it is important to fight for your story and not to succumb to pressure. We as journalists must always be completely transparent to our audience and always first communicate everything with the readers in crisis situations. And the citizens will know to appreciate it and support you.      ",https://www.krik.rs/koruptivne-veze-milenijum-tim-ustupio-audi-i-stan-bratu-sinise-malog/,https://www.krik.rs/en/pro-government-tabloid-launched-another-media-attack-against-krik/,https://www.occrp.org/en/daily/10303-serbia-s-krik-attacked-by-pro-government-tabloid,http://rs.n1info.com/English/NEWS/a531278/KRIK-Anti-Corruption-Agency-fails-to-launch-proceedings-against-Minister-Mali.html,,,,"Dragana Peco, Pavle Petrovic"," Dragana Peco works as an investigative journalist at KRIK and Organized Crime and Corruption Reporting Project (OCCRP) and a staff researcher for the Investigative Dashboard (ID) online platform. For six years she worked for the Centre for Investigative Reporting of Serbia (CINS). She won international 2018 CEI SEEMO Award for Outstanding Merits in Investigative Journalism. In 2019, she received EU Award for the Best Investigative Story in Serbia. As part of the KRIK investigative team, in 2017 Dragana won global Data Journalism Award given by GEN and also national award for ethics and courage „Dusan Bogavac“. Dragana trains other reporters in advanced journalism techniques, how to research business registries worldwide and follow the money.      Pavle Petrovic worked as one of KRIK’s and OCCRP’s researchers, both as data and investigative journalist. As part of the KRIK team, in 2017 Pavle won global Data Journalism Award for the work on Database of Assets of Serbian Politicians. Pavle currently trains journalists in the region on techniques for off-line and on-line security. He is the author of Security guidance for Civil rights defenders.      ",,,
Netherlands,Het Financieele Dagblad,Big,Participant,Best data-driven reporting (small and large newsrooms),Land Speculation: The Fragmentation Of The Netherlands,13/04/19,"Investigation,Database,Infographics,Map,Economy","AI/Machine learning,Drone,Microsoft Excel,Google Sheets","For the first time, data research has been used to chart the speculation in ""hot land"" in the Netherlands. Small pieces of agricultural land were sold, with the assumption to be allocated for building purposes. If that happens the value of the land will rise enormously. Last years 2300 hectares of agricultural land has been cut up in 27.000 small pieces. More than ten thousand people bought these pieces and invested an estimated € 700 million. But our research showed that not a single piece of ""hot land"" has been allocated for building. In fact, municipalities suffer from this fragmentation."," Besides media attention and parliamentary questions, the biggest impact of the publications are court cases. A lot of buyers of ""hot land"" think they are scammed by the traders who sold them the pieces of land. They sometimes paid €250 per m2 while the agricultural price is €8 per m2 or even less. Several sold pastures are not even fit for building purposes, since they are too close to highways or assigned as breeding grass fields for birds.   The need for trading licenses are part of these court cases. Previously, The Dutch Authority for Financial Markets (AFM) supervised the market for land speculation. Several traders were given fines since they did not have the right license. Nowadays, the AFM is not involved in supervising of land trading. Traders found a loophole in the trade laws and laws for the protection of retail investors. This research sheds light on the massive use and scope of this loophole that surprised several national experts on land ownership. "," For researching land speculation we used the online database of the Dutch Land Registry (Kadaster) in combination with maps of Esri (ArcGIS program). Esri used algorithms and machine learning for detection of possible land speculation locations in The Netherlands.   While checking the ownership files of split up pastures, we discovered new traders in the Dutch company register (Kamer van Koophandel). Those traders offered new, unknown locations of land speculation.That's how we build up our data set in Excel and in the end made a map of all locations. On this map you can find all 27000 individual pieces of land sold by traders to retail investors.   Because of the combination of our research and the maps of Esri, we have an almost 100% total coverage of all locations of land speculation in the Netherlands.   After this we surveyed all 355 municipalities in The Netherlands for comment on our results. We asked them if they were aware of the land speculation in their municipality and how this could effect their real estate policy. The response was very high (60+%). "," The hardest part was the Dutch Land Registry. They couldn't help us with our research, because it's not part of their official regulatory tasks. So we had to build a database ourselves, this took some months.   After the first interviews with specialists and scientist, we discoverd no one has ever looked at the scale of speculation in the whole country. The research we did is unique.   The Netherlands are a small country and land for building purposes is extremely scarce. The scale of speculation with agricultural land as we found doesn't solve the problem of the shortage of housing we are facing at this moment in the Netherlands. Because one hectare (10.000 m2) can be divided into 100 pieces (100 m2 each). A municipality has to negociate with 100+ different owners if they have plans for building on that location.   This means 10.000+ retail investors will probably never see any return of their investment. Since this land is hard to sell, even without profit, heirs will probably inherit these small pieces, which makes the number of land owners even larger. "," Work together.   We worked with three journalist on this project. One datajournalist, one journalist specialised in financial markets en one journalist specialised in real estate.   We were lucky a trainee from a high school could work with the maps of Esri, this could help us with our research  and  to visualize the results in the end.    If there's no dataset available, you have to build one yourselves. In this way you get really into your subject.      ",https://fd.nl/weekend/1296801/grondspeculatie-in-nederland-vergeefs-wachten-op-de-hijskranen,https://fd.nl/achtergrond/1296898/niemand-wil-bouwen-op-versnipperde-grond,https://fd.nl/weekend/1296423/dubieuze-handel-in-kavels-veel-omvangrijker-dan-gedacht,https://fd.nl/economie-politiek/1296800/grondspeculant-belemmert-bouwplannen-van-gemeenten,https://esrinederland.maps.arcgis.com/apps/Styler/index.html?appid=1c3936d0afeb4694a103d96cbf1c1814,,,"Josta van Bockxmeer, Gaby de Groot, Erik van Rein"," Josta van Bockxmeer (1987) is ajournalist of Het Financieele Dagblad. She mainly focuses on real estate, housing market (policy) and contruction.   Gaby de Groot (1973) is a datajournalist of Het Financieele Dagblad (Dutch Financial Daily). He mainly focuses on taxavoidance/-evasion, real estate and pensions. Was part of the ICIJ investigation teams of the Panama Papers, Paradise Papers and recently Luanda Leaks. Won several journalistic awards, twice Citi Journalistic Excellence Award (financial-economic reporting) and De Loep (Dutch investigative journalism award).   Erik van Rein (1992) is a journalist of Het Financieele Dagblad. He mainly focuses on asset management, private banking, mutual funds and retail investing. ",,,
Japan,Toyo Keizai Online,Big,Participant,Open data,Disappearance of Surplus on Campaign Funds,07/03/19,"Investigation,Multiple-newsroom collaboration,Open data,Elections,Politics","JQuery,Json,CSV"," This project is a series of articles, data visualization, and open data about unexplained surplus on campaign funds.   In Japan, all members of the Parliament must report income and expenditure in their campaign funds. However, when they have surplus as a result of income more than expenditure, they have no obligation to report the usage of the surplus.   We investigated documents submitted by House of Representatives (lower house in Japan) members, and published all the data and source code for further coverages from other newsrooms. "," The most important impact of this project is that it revealed a large amount of surplus on campaign funds remains unexplained and unreported.   We investigated documents on the 2014 election submitted by House of Representatives members. While some members appropriately reported the usage of their surplus, 268 members, nearly 6 in 10 of the total, have not disclosed how they spent the surplus.   The total amount is approximately 950 million Yen ($8.7 million US) and the maximum is 27 million Yen ($251,000 US). Even some Members of the Cabinet have not disclosed usage of the surplus.   This project contains articles to explain issues on related laws, and an interactive data visualization, where readers can quickly see trends in each political party or region and detailed balances of individual members. As a result, some Members of the Diet admitted that the treatment of surplus was not appropriate and revised their campaign income / expenditure report. "," The data was collected manually from the Election Administration Commissions of each prefecture in Japan. Reporters covered House of Representatives members when their usage of surplus were unexplained.   All the data was converted into JSON file and visualized on a special website. P5.js was used to draw bubble charts, Chart.js was used to draw pie charts, and Japanmap.js was used to draw Japan prefectures map.   Since the purpose of this project is not to denounce someone but to cast doubt on the current system, the special page was designed with subdued colors and tones.     "," The hardest part of this project was how to facilitate further investigations by other newsrooms, using open data.   Conventionally, in Japan, it has not been usual for multiple news organizations to work together on a certain project. However, we consider that it is necessary to have additional coverages from other newsrooms, such as the same issue on regional council or governor, to make a real impact on society.   In order to achieve this goal, we published all the data and source code we gained and created through this project, on GitHub and LinkData, a platform to publish open data in Japan. Perhaps this is the first time in Japan that data obtained as a result of investigation was published as standard open data. We facilitated other journalists to use this data through events explaining how to use this data.   As a result, several magazines and newspapers reported on this topic. For instance, Kumanichi, a local newspaper located in the southern part of the Kyushu region, reported this issue on the headline, using this data and focusing on local members of the parliament.    "," What other newsrooms and journalists can learn from this project is that publishing data gained through an investigation may enhance the impact of the coverage on our society. This project would be less impressive if we had not published the data and source code. Collaboration with designers and engineers, such as creating data visualization, is an effective way to spread these open data projects.    ",https://toyokeizai.net/sp/visual/tko/electoralfund/,,,,,,,Kazuki Ogiwara," This project is a collaborative project of Frontline Press, SlowNews Inc. and Toyo Keizai Online.   Frontline Press is a group of journalists who specialize in investigative reporting. SlowNews Inc. is a company which supports journalists and news organizations, mainly who engage in investigative reporting. Toyo Keizai Online is one of the largest online business news media in Japan. Kazuki Ogiwara is a data journalist and data visualization creator at Toyo Keizai Online.    ",,,
Japan,Toyo Keizai Inc.,Big,Participant,Best data-driven reporting (small and large newsrooms),Excessive Throwing: How Many Pitches of Japanese High School Baseball Players Violate Pitch Count Guidelines,22/08/19,"Infographics,Sports","JQuery,Json"," The national high school baseball championship is one of the most popular sports events in Japan. It is a great honor to play there for players.   However, pitchers are often forced to throw far beyond the appropriate range. Referring to Pitch Smart, an American series of guidelines for youth baseball players to avoid overuse injuries, we published an infographic and an article to show how Japanese high school baseball pitchers are exposed to the risk of injury. ","The most important impact of this project is that it revealed high school baseball pitchers in Japan are throwing far beyond the usual range, and most of them are due to violation of a regulation of rest days. In Japan, there are no guidelines or rules on youth pitchers that limit over-pitching. Since many teams (schools) do not have enough pitchers to rotate them, the pitchers in such teams often had to pitch complete games during the tournament. In this infographic, we displayed the 10 pitchers who threw the most in this tournament after 2000, and applied two rules from Pitch Smart to their pitches. Pitches that violate the daily limit (105 pitches at the age of 17 - 18) are displayed in yellow, and pitches that violate the rest day restrictions (4 days after throwing more than 81 pitches) are displayed in red. While pitches that violate neither are displayed in white, they may still conflict with other rules of Pitch Smart. This infographic shows that high school baseball pitchers are forced to throw excessive pitches, and especially they do not have enough rest days. Since the number of teams gradually decreases later in the tournament, they often have to throw several days in a row. For instance, Yuki Saito (斎藤 佑樹), the pitcher who threw the most in this graphic, threw 948 pitches in 16 days. 88 of them violate the daily limit, and 545 of them violate the rest day limit. 553 pitches were thrown in the last 4 days, including the final and its rematch. After this project was published, this problem drew attention on traditional media such as TV and social media such as Twitter. As a result, in November 2019, Japan High School Baseball Federation decided to make rules against over-pitching for their first"," Since this infographic needed daily pitch counts of each pitcher, we collected the data from the past baseball news from newspapers and magazines.   The data was converted into JSON file. P5.js was used to draw the graphic.    "," The hardest part of this project was how to convey the seriousness of this issue to readers who are not familiar with baseball.   While the total numbers of pitches on each players are sometimes shared on SNS, just the numbers did not explain how much the youth pitchers are at risk of overuse.   Initially we tried to compare to professional baseball, where all teams have enough pitchers to rest them, but since many conditions such as match interval or rules of the tournament, we concluded this comparison is not appropriate.   Then we decided to apply Pitch Smart to Japanese high school baseball. How the Japanese tournament violates the guidelines of Pitch Smart clearly indicated the severity of the problem.   This comparison also revealed that the biggest factor of this over pitching is a violation of a regulation of rest days.    "," What other newsrooms can learn from this project is that storytelling by data changes depending on the comparison axis setting.   While the total number of pitch counts of each pitcher in the past tournaments was already published and shared, the seriousness of     ",https://toyokeizai.net/sp/visual/tko/overpitching/,,,,,,,Kazuki Ogiwara, Toyo Keizai Online is one of the largest online business news media in Japan.     Kazuki Ogiwara is a data journalist and data visualization creator at Toyo Keizai Online.    ,,,
Egypt,InfoTimes,Small,Participant,Best visualization (small and large newsrooms),Naked Facts,22/08/19,"Explainer,Solutions journalism,Database,Open data,Fact-checking,Infographics,Chart,Video,Map,Politics,Environment,Business,Women,Immigration,Health,Economy,Employment,Human rights","Adobe,Creative Suite,Microsoft Excel,Google Sheets"," It is a news service from InfoTimes consisting of one visualization and 150-300 word story tackling issues about environment, the economy, and gender. This project was created to tackle and open a dialogue on issues that were not discussed on mainstream media. The content is mainly targeted for decision makers, academics and researchers. ", We take the data and simplify it for easy consumption and understanding. This familiarizes the average person with data and exposes them to different data formats and sources while training him/her on how to read and consume data. Our platform also allows the audience to have access to the data for personal use by providing them with structured data​. ," Flourish, Data wrapper, Tableau, Tabula. Using these data visualization tools we sort the data in a formate that will translate for the best visualization method. ", Compiling and cleaning the data as well as look for different methods and tools to make it easier for the reader. It is also a challenege to simplofy complex data into an easy-to-digest format for views while making the topics relevent to their scope of interest. , People may not be aware that certain types of data exist such as data measuring the pollution in the air or plastic consumption. Data helps people understand the magnitude of problems and therefore the purpose of naked facts is to shed light on problems that impact our day-to-day lives and those around us. ,https://infotimes.org/tag/naked-facts/,,,,,,,"Nour Eltigani, Aya Nader, Ahmad AlAmrousy,"," Nour Eltigani - Prior to joining Infotimes, Nour has worked as a multimedia reporter and video producer for several media platforms including Egypt Today, Business Forward and Egyptian Streets. She graduated from the American University in Cairo (AUC) in 2017 with a degree in Integrated Marketing Communications and a minor in Business Administration.   Aya Nader - Before joining InfoTimes, Aya reported for Egyptian Streets, and before that, for Daily News Egypt. She has also written for various local and international media platforms, such as Foreign Affairs, The National, and Al-Monitor. Aya holds an MA in Global Communication from Simon Fraser University.   Ahmad  Alamrousy - Data scientist ",,,
Egypt,InfoTimes,Small,Participant,Best data-driven reporting (small and large newsrooms),Analysis of Tweets Showcases Hatred Towards Syrian Refugees Among Lebanon's Elite,15/09/19,"Investigation,Long-form,Database,Crowdsourcing,Illustration,Infographics,Chart,Politics,Corruption,Human rights","D3.js,R"," During the period of our reporting, Lebanon witnessed numerous campaigns and advocacy calls, among which were those triumphing the Syrian cause, or advocating the conditions of refugees in Lebanon. For example, hundreds of Syrian and Lebanese activists have tweeted in the last two years under the hashtag  #عرسال_تستغيث in an attempt to send a distress message about the horrid living conditions refugees endure in camps. s data-driven story revealed that more than half of the tweets included in the analysis sample rejected Syrian refugees.  "," This story created a dialogue surrounding hate speech and the hashtag took over Twitter. The hate speech that emerged through the tweets of some politicians and public figures against Syrian refugees in Lebanon, was not limited to social media, but also turned into forms of aggression in reality. After the story was published, many became aware of the hate speech that was exercised by many politicians and this led to political change in the country. The story was later also republished in Darag (  a Lebanese media outlet) and Serag ( a Syrian media platform).  "," Analyze the political climate through tweets , and analyzed them using the programming language R and used D3 for visualization. Following the software algorithms, we examined and filtered approximately 238,000 tweets to extract tweets related to the subject of the Syrian asylum in Lebanon. A total of 1,454 tweets were written by 68 Lebanese of the total figures monitored in the search process. "," Some  of the twitter accounts of politicians were inactive, which made it difficult to extract current data to analyze.  ", How racism is translated in the from of microagressions and hate speech against Syrian refugees in Lebanon.  ,https://infotimes.org/analysis-of-tweets-showcases-hatred-among-lebanons-elite-towards-syrian-refugees/,,,,,,,"Mohammad Waked, Ammar Al-Khasawneh, Abdul Rahman Al-Khader, Ahmad Rahal, Manar Abu Hassoun, Aya Nader, Mohamed Zidan, Mohamed Bassiki"," Report by: Mohammad Waked, Ammar Al-Khasawneh   Researchers: Abdul Rahman Al-Khader, Ahmad Rahal, Manar Abu Hassoun   Translation: Aya Nader   Edited by: Mohamed Zidan, Mohamed Bassiki ",,,
Spain,"Desprotegides, Fundació puntCAT",Small,Participant,Best data-driven reporting (small and large newsrooms),Desprotegides.cat,07/07/19,"Investigation,Explainer,Documentary,Infographics,Chart,Women,Human rights","Scraping,D3.js,QGIS,Google Sheets,R,RStudio"," Our report revealed that despite having one of the most progressive legislations to eradicate sexist violence, authorities leave women in Catalonia unprotected in many cases. Among other things:  • The legislation was born with less than half of the estimated economic resources it needed.  • The government's data fails to detect the real dimension of violence against women.  • Catalan courts grant the least number of restraining orders compared to the rest of Spain.  • Public policy evaluations highlight a large amount of women who do not go to social services, despite a well-distributed network of centers. "," We held a press conference for the official launch project, after which the documentary appeared in all major news outlets in the region. Because the authors produced it as a public service, with a grant by the puntCAT foundation, and were not seeking any distribution deals, a few outlets adopted it as a permanent link on their homepage for months.   Activist organizations reached out to share the webdoc on their networks and organized events and talks around it —eight events so far, in the six months since launch. We have produced a traditional documentary to be shown at these events, and as an alternative resource for these organizations.   The agency responsible for Transparency is working on addressing some of the issues raised in our report, especially those related to the data collection and budget tracking.   One of the main characters in our documentary, Nuria Balada, who was president of the Catalan Women's Institute —the interdepartmental government agency in charge of suggesting and implementing gender-focused policies— left her post two months later. Despite her long trajectory and influence in shaping the implementation of the ten-year-old Catalan law, she had been widely criticized for some of the issues the administration faces which she admitted to in our documentary.   Finally, we are turning the project into a wider campaign. We are collaborating with some of the main feminist organizations to perform data-driven stunts in the March and November demonstrations (International Women's Day and International Day for the Elimination of Violence against Women). "," We compiled and harmonized data from police, courts, hospitals, and public census data, mapping them to a common model. The estimates we used throughout the report were based on the 2016 Catalan survey —a sample of 10,000 for a universe of about 3.5 million.   Since the government budget doesn’t classify when an investment is linked to the program to combat and prevent violence against women, we requested that breakdown through the Transparency Law to various departments and, after some extensions, we received a breakdown by department and a collection of reports that had previously been released — and that were already in our cache of sources. The criteria used for it were different from those in other partial budget breakdowns.   For the second map on the scrollytelling graphic, we retrieved the areas within a 30-minute drive from every one of the 100 and some Women’s Support and Information Center (SIADs) using Mapbox’s Isochrone API. We then merged them and used the result to clip a raster dataset with population density (with the number of people per cell) to estimate the population within range of one of these centers.   We used R and RStudio to transform and organized the data. QGIS, mapshaper, topo2geo, and other command-line mapping tools to do the cartographic analysis. The charts in the report were produced using d3.   We also filmed interviews with experts and spoke to more than a dozen others to understand the network of professionals involved in cases of violence against women. "," Between 2017 and 2018, Karma Peiró had been involved in project <a href=""http://proyectocuentalo.org/"" rel=""nofollow"">#Cuéntalo</a>, which recorded the stories of thousands of women speaking out about their personal experiences with violence in 140 characters.   These were powerful personal dramas, what was the government doing to address violence against women?   We did not want to just highlight the hardship of a specific moment or event. Our intention was to hold the authorities accountable: the hardest part was showing that despite great efforts and advances in eradicating violence against women, there is much left to do.   It is a complex issue and there is a massive dissonance between what the victims perceive and what the authorities do to respond to it. We interviewed Raquel who was almost killed by her ex-partner. We are especially thankful to her for retelling her story and doing so in front of a camera. She now works helping women going through the same situation.   After it was published, public evaluators, activists, and policy-makers reached out to speak about how the report captured a lot of the nuance often missed in media reports about gender violence. "," We published our methodology and our data on a public repository, in the hopes news outlets use it in the future. We have spoken about the challenges and findings in the documentary to journalism students and we have been invited by feminist organizations to events around gender violence.   We hope media outlets adopt a more nuanced approach to reporting violence against women and expand their understanding of what constitutes that type of violence. Violence against women is not sufficiently visible. For example, women report tens of thousands of incidents of sexual abuse each year, but that does not show the real dimension —70% of the most serious events are not reported. Additionally, media outlets tend to report only on homicides, less than a dozen a year in Catalonia, while estimates show that about 75,000 women have suffered attacks last year alone.   The data, the voices, and the format may also serve as a template for government accountability on the issue. ",http://desprotegides.cat/,https://github.com/xaquingv/desprotegides,,,,,,"Karma Peiró, Xaquín G.V., Rocío Minvielle"," <a href=""https://twitter.com/kpeiro"" rel=""nofollow""> Karma Peiró </a> is a data journalist and instructor from Catalunya. She was the editor in chief at Nació Digital between 2015 and 2018. Before that, she worked and lead teams at Web Magazine, en.red.ando, Catalunya Ràdio, La Vanguardia Digital, and TV3.   <a href=""https://twitter.com/xocasgv"" rel=""nofollow""> Xaquín G.V. </a> is an interactive, data and visual journalist from Galicia. Between 2014 and 2017, he led the Visuals desk at The Guardian in London. Before that, he worked at National Geographic, The New York Times, Newsweek and El Mundo (Spain).    Rocío Minvielle  is a documentary filmmaker, producer, and feminist activist. She's a member of the insurRECtas collective. ",,,
Bangladesh,"The Financial Express, Bangladesh",Small,Participant,Best news application,Drone imagery,09/01/19,"Investigation,Explainer,Solutions journalism,Breaking news,Multiple-newsroom collaboration,News application,Fact-checking,Infographics,Chart,Satellite images,Business","Animation,360,AR,VR,3D modelling,AI/Machine learning,Sensor,Personalisation,Drone,Scraping,Adobe,Microsoft Excel,Google Sheets,PostGIS,OpenStreetMap"," Bangladesh had faced a major threat following outbreak of dengue fever during August-September.  The mosquito-borne dengue fever has claimed 75 lives and infected over 85,000 persons, mostly in Dhaka. We used the drone technology to capture data relating the breeding place of aedes mosquito. The result was huge. Some 36000 potential sources were identified. it was not possible manually as Dhaka city has huge rooftop gardens and its buildings built were not in planned way.  The authorities were convinced by the result and they showed their interest to conduct more such use of drone technology to capture the data. ",       The peoples were aware of the sources of the Aedes mosquito which has been killing hundreds of people of the country for long.   The authorities concerned also came know about the potential sources and they are now considering conducting a large survey trough the technology.   Different newspaper and TV channels also later presented news on the matter    This has human welfare impact.   ," Processing software used in the project. Pix4d, ArcGIS and drone technology used. Colour photgraphy was used to analyse the data.   "," Getting permission from the authorities concerned was the hardest part of the project. Bangladesh has law restricting flying drone and need prior permission to take such type of photography.  The data is big and processing the data is one of the hardest parts of the project. It should be selected as it had many challenges from photography to news publication. Apart from this, this has human welfare aspect. There are many areas where such survey is necessary to identify the disease and other anti-human element. It can be conducted in malaria and other epidemic diseases ","     The others can learn from this project to identify disease and other potentials sources for the wellbeing of the human. There are many hilly areas, deserts and remote areas where men cannot collect data and such type of survey of photography need much time, this is the quest way of getting data from the target place and target people. ",https://today.thefinancialexpress.com.bd/first-page/drone-photography-reveals-grimmer-city-dengue-picture-1567270136,https://www.youtube.com/watch?v=p21u1mJS5es,https://www.youtube.com/watch?v=-tSlquNzWrE,https://www.youtube.com/watch?v=a4Pou_4gQCA,https://www.thedailystar.net/country/dengue-claims-mans-life-in-barishal-1812697,,,"Md zahid Hasan SIddiquee, Sazid Ibn Zaman and Jasim Uddin Haroon","  Proposed Position: Urban Planner     Name: Md zahid Hasan SIddiquee    Mr.Zahid has a 4-year Bachelor degree in Urban Planning with  more tahn10 years of experience. He should have project related working knowledge in preparing GIS cased base map, urban base map, periodic plan, master plan,land use plan, strategic plan and perspective plan.    completed a Post Graduate Diploma in River Basin Management (RBM) with an emphasis on the Integrated Water Resource Management (IWRM) from CDDET Foundation, Madrid, Spain(2014).   From February 2007 to July 2007, Mr Zahid worked as Senior Urban Planner in Ganibangla Ltd in Detail Area Plan (DAP) for Dhaka City. From July 2007 to December 2008, he was working as Urban Planar in CDM Project in BUET.From March 2008 to November 2009, he worked as Urban Planner in Engineering & Planning Consultant (EPC) Ltd with the Barisal Master Plan project.From October 2009 to March 2010, he worked as Lecturer in the Department of Urban & Regional Planning at BUET.From July to December 2002, he worked as Teaching Assistant in the Department of URP at BUET. From December 2005 to July 2006, worked as Scientific Research Assistant in University of Applied Sciences, Stuttgart, Germany.   From November 2009 to till date he has been involved as GIS & RS Specialist in the ICT-GIS Division in the Institute of Water Modelling (IWM) and providing various important inputs as Urban Planner and GIS Expert.   jasim uddin haroon     Curriculum Vitae    Jasim Uddin Haroon      Email:  <a href=""mailto:fecjuh@gmail.com"">fecjuh@gmail.com</a>        Education:        BSS (hons.), MSS (Economics), University of Chittagong     Commonwealth Executive MBA, Open University     Post Graduate Diploma in Film, National Institute of Television and Film.      Post Graduate Diploma in Capital Market, Bangladesh Institute of Capital Market (BICM).           Area of expertise:          Area of interest in journalism:        Conflict sensitive journalism,      Stories with in-depth investigation, and      Feature writing     Data journalism              Awards:          ERF Best Reporting Award-2018     DRU Best Reporting Award-2018     DRU Best Award-2019                 Computer: MS Word, MS Excel, SPSS & Stata 14 and Eviews          Mailing address:      The Financial Express     Tropicana Tower (4<sup>th</sup> floor)     45, Topkhana Road      Dhaka-1000.         P.S: Doublemooring,      P.O: Bandar      Chittagong-4100.    Mr Zahid has overall expertise in preparing GIS based maps, landuse plan, strategic plan using urban planning tools and techniques. ",,,
United Kingdom,"i, The Yorkshire Post, The Scotsman, Sunderland Echo, Portsmouth News, and many more.",Big,Participant,Best data-driven reporting (small and large newsrooms),Hundreds of near-misses between drones and aircraft revealed,26/01/19,"Investigation,Open data,Infographics,Map,Politics,Business","Microsoft Excel,Google Sheets", Anyone who had their holiday plans disrupted during runway shutdowns at Gatwick and Heathrow last winter knows the havoc drones can wreak on air travel.   But did you know pilots had previously reported hundreds of near-collisions with the devices in UK skies?   That’s one of the frankly alarming findings of my data-led investigation into the growing number of near-misses between drones and aircraft being reported to officials.   Local versions of this story were run across dozens of JPIMedia’s regional and local news titles. ," I produced a story pack as part of my role running the JPIMedia Data Unit.    The investigation ran in more than 60 JPIMedia titles.   This included national newspaper the i, which was also one of a number of titles to run an accompanying op-ed article about the issue.   Coverage across the UK included three front-page splashes, nearly 20 double-page spreads and numerous op-eds.   The case studies I provided proved particularly hard-hitting.    There was the paraglider in Derbyshire left terrified after an unknown drone pilot persistently flew a quadcopter just feet away from him in flight, seemingly to obtain film footage but threatening to collide with the thin canopy-to-harness lines keeping him safe.   There was also the drone flown deliberately over the centre of Gatwick’s runway which nearly hit an Airbus A321 coming into land, a full three years before the chaos caused at the airport before Christmas 2018. "," The UK Airprox Board publishes on its website a basic spreadsheet listing each of the country’s near-misses involving drones and aircraft.   I used this as a starting point for my project, delving into the reports of each individual incident.   These are irritatingly published online in a variety of formats, from PDF to web table.   I scraped web tables using Google Sheets’ importHTML function and converted PDFs to machine-readable format using online converter services to build the basic spreadsheet up into an information-rich database.    By doing this, I could begin adding in crucial details from individual incident reports. Was the aircraft involved a passenger plane? Was the drone being flown in restricted airspace? What was the pilot’s account of what happened?   I then took the coordinates of each incident and used these to determine which UK county they had happened in. This allowed me to reveal how many near-misses had happened in each county and region of the UK.   The full reports on each incident are published online, using hyperlinks which are formulaic in nature.   This allowed me to add to my spreadsheet a column giving the hyperlinks to each report. That way, a local reporter wanting to read more about a given incident could simply click through.   Using data given on the severity of each incident, as well as pilot accounts given in the full reports, I identified some of the most dramatic near-misses and turned them into case studies, also writing interviews with experts and background articles.   My colleague Tim Hopkinson used the coordinate data to create a Flourish map showing how the near-misses have been increasing in number over time. He also produced two JPG graphics. "," Gathering open data spread across a variety of formats and using it to build my own database was certainly far more time-consuming than simply downloading a spreadsheet. However, here it gave readers of JPIMedia titles across the UK a real understanding of how a growing national issue was affecting their local area. "," This investigation relied on open data that was hard to access and spread across a variety of sources and formats. By bringing it all together, I could shed new light on an issue that was high in the news agenda at the time, without having to rely on Freedom of Information requests.   This project demonstrates the value of combining data sources to alow for new analyses and also of learning skills such as web scraping to access open data.   The work of the JPIMedia Data Unit, of which this was the first project, shows how data can be used to generate stories of national interest but also numerous stories of local interest using one dataset. ",https://inews.co.uk/news/revealed-hundreds-of-drone-near-misses-reported-by-holiday-jet-pilots-146390,https://public.flourish.studio/visualisation/204113/,https://docs.google.com/spreadsheets/d/1W7HtA-TUlfAP8dUbq1-IIesQc50KePQGvnNZDischFs/edit?usp=sharing,https://www.sunderlandecho.com/news/drones-involved-seven-near-accidents-aircraft-north-east-investigation-shows-147409,https://www.yorkshirepost.co.uk/news/crime/more-than-300-near-misses-between-drones-and-aircraft-across-the-uk-recorded-1-9558792,,,"Claire Wilde, Flourish map by Tim Hopkinson, additional reporting by Dean Kirby, Alison Bellamy"," Claire Wilde is News Editor (Data and Investigations) for JPIMedia, one of the UK's largest regional news companies.    She set up and runs the JPIMedia Data Unit, a team comprising herself and a reporter, supplying data-led stories and investigations to the whole group.   She was previously Crime Correspondent for The Yorkshire Post. ",,,
Russia,Charity Foundation 'Help Needed',Big,Shortlist,Open data,To Be Precise,09/12/19,"Database,Open data,Fact-checking,Infographics,Map,Politics,Environment,Business,Crime,Economy","D3.js,JQuery,Json,Microsoft Excel,Google Sheets,CSV,R,Python,Node.js"," To Be Precise - is a project for NGOs, businesses, journalists, and others who work with social problems in Russia. The mission of the project is to promote an analytical approach in highlighting and solving these problems by representing the reality of each problem (now we got HIV, Prisons, Оrphanhood, Crimes, and Environment for Disabled People). The project contains both open governmental data from relevant departments and data from non-governmental institutions, non-profit organizations and other alternative sources. We are working on an extension of represented social topics. ", NGOs started to use the project for planing their work in regions (we've received 945 mentions on social media and sites of these types of organizations).   We also received 143 works of data-journalists based on our project and mentioned our impact. ," Scraping, parsing, using API to verify NGOs from the catalog at the governmental bases. ", To get enough governmental data. That is why we were trying to duplicate them by NGOs and Business data sometimes.  , Everybody can better understand what social problems are the most actual for Russia and it's regions. ,https://tochno.st/,https://tochno.st/problems/hiv,https://tochno.st/problems/disability,https://tochno.st/problems/prisons,https://tochno.st/problems/crime,https://tochno.st/problems/orphanhood,,"Elizaveta Yaznevich, Anna Arzamasova, Ksenia Babikhina, Karina Pipiya, Artem Tinchurin, Oleg Pimonov, Valeriya Mescheryakova, Andrey Melentev, Alexander Kozlov, Natalia Freik, Sofiya Savina, Kristina Zvezdova, Petr Butaev, Katerina Abramova"," Elizaveta Yaznevich - the Head of Researching Department, a sociologist who worked in the NGO sector earlier with the topic 'orphanhood'.   Anna Arzamasova - the Product Manager at To Be Precise. Bachelor in Applied Math and Informatics who worked in the NGO sector for 4 years with the topic 'social entrepreneurship at tech'.   Ksenia Babikhina - the Analyst at Researching Department, a sociologist who worked in the NGO sector earlier with the topic 'HIV'. ",,,
Germany,"Tagesspiegel, Correctiv",Big,Participant,Best data-driven reporting (small and large newsrooms),Who owns Berlin?,22/02/19,"Investigation,Explainer,Multiple-newsroom collaboration,Crowdsourcing,Infographics,Economy","Scraping,D3.js,Google Sheets,CSV,Python"," 80 percent of Berliners live in rented accommodation. Many of them actually do not know who owns the apartment they live in, because Germany does not have a central property register. Government officials and the civil society do not know, which property companies own apartments in Berlin. Powerful actors remain invisible and difficult to reach for political debate.   The editors of Tagesspiegel and Correctiv wanted to bring more transparency to the Berlin real estate market by using crowd sourcing. The aim was to uncover problems, highlight larger structures, make grievances visible and provide a factual basis for a public debate. "," The research was conducted as a crowd investigation. From October 2018 onwards, tenants were able to use an online platform, to provide information on the owner of their apartment and on the experience, they have had with their landlord. In order for the entries to be verified, they were asked to submit supporting documents. Thousands of submissions were received.   Starting in January 2019, the editorial offices of the Tagesspiegel Innovation Lab and Correctiv spent several months verifying each and every one of the readers' submissions, as well as evaluating background interviews, files, anonymous tips and other databases. At discussion events, they invited readers, politicians, representatives of the real estate industry and researchers to discuss the capital's housing crisis and seek solutions. Hundreds of Berliners attended.   Thanks to numerous tips, the research team succeeded, for example, in uncovering the growing business model behind furnished apartments after several hints. With this model, landlords circumvent the rent brake imposed in Berlin.   In the process, the journalists also came across previously unknown large property owners. The team was able to prove for the first time that a silent real estate mogul “Pears”, an English family, owning more than 3000 apartments in the city. This is an important limit for politicians in the debate about affordable housing, who until now knew nothing about this owner.   Furthermore, it was uncovered in detail how the real estate and the financial markets are connected. Many tenants in Berlin have invested in residential property shares themselves via funds, without knowing it. That’s because some tenants living in an apartment of the equity real estate companies have pension plans investing their clients' money in their own landlord. The team received the German “Reporterpreis” for this investigation. "," First, the Crowdnewsroom was important for “Who owns Berlin?”, a platform that Corrrectiv developed especially for citizen research. It was accordingly possible for tenants to transmit their data, including documents, securely and anonymously.   During the project it became clear that additional data sources were needed to make structures of the Berlin housing market visible. So numerous journalists from both editorial offices searched annual reports for shareholders, compared company data of the companies across international databases and procured missing information in commercial registers in order to reveal piece by piece company constructs of real estate owners.    This was combined with web scraping, where data was available on commercial platforms that were not openly accessible. "," Thanks to the great interest of Berlin citizens in the research, the team was faced with a particularly time-consuming task: verifying the entries in the crowd newsroom and managing a rapidly growing community.   Since the participants were not prescribed which document they could submit (tenancy agreement, utility bill, bank statements) and, moreover, all these documents are designed differently, depending on the landlord, the team manually checked and verified thousands of entries.   The research also showed that many central public registers are not citizen-friendly as well as incomplete.   Many registers charge fees. In the case of the German transparency register, journalists must prove ""legitimate interest"" in order to gain access. In addition, real estate companies are spread all over Europe, where each country has its own regulations for its registers.   Particularly alarming was the resistance of the real estate associations, some of which even called for a boycott of the project. "," The project ""Who owns Berlin"" has shown that Citizen research brings great added value - especially in local journalism.   Thanks to the thousands of submissions, we were able to set new topics, generate unusual leads and work investigatively.   On the other hand, the project shows that cooperation between two editorial offices can lead to a great exchange of knowledge. Together, such large and extensive projects can be mastered better.   The reactions of Berliners as well as the players in the real estate market show how important the topic of affordable housing is for society. Rising rents are not a phenomenon limited to Berlin or Germany. Journalists around the world should try to bring more transparency to the housing market. In many cities, other media outlets already started similar projects, many even re-using the same technology of Correctiv used for “Who owns Berlin?”. ",https://interaktiv.tagesspiegel.de/wem-gehoert-berlin/,https://interaktiv.tagesspiegel.de/wem-gehoert-berlin/artikel/moebel-statt-mietpreisbremse/,https://interaktiv.tagesspiegel.de/lab/das-verdeckte-imperium/,https://interaktiv.tagesspiegel.de/lab/eigentuemersuche/,https://interaktiv.tagesspiegel.de/lab/mieten-und-renditen/,https://interaktiv.tagesspiegel.de/lab/wem-gehoert-der-boxi/,,"Tagesspiegel Innovation Lab (Andreas Baum, Michael Gegg, Hendrik Lehmann, Michael Gegg, Helena Wittlich); Correctiv (Anne-Lise Bouyer, Justus von Daniels, Michel Penke, Simon Wörpel)"," The  Tagesspiegel Innovation Lab  develops new narrative formats that make greater use of the potential of digital journalism. In addition to the development of new ways of presentation such as interactive graphics and mixed media stories, the team's initial focus is on citizen research, sensory journalism and evaluations using machine learning. The Innovation Lab is interdisciplinary with software developers, editors and experts for artificial intelligence.    Correctiv  is the first non-profit research center in the German-speaking world. A strong society needs investigative journalism. In exchange with our readers, we are committed to a future worth living. For our children - for our lives - for our society - for measure and middle. Correctiv is financed primarily through donations and endowment contributions. This guarantees independence from advertising revenues, sales figures and quotas. We are committed exclusively to truthfulness and the common good. ",,,
Myanmar,Frontier Myanmar,Small,Participant,Best data-driven reporting (small and large newsrooms),Power play How Chinese money damned Myanmar's economic transition,30/09/19,"Investigation,Explainer,Long-form,Cross-border,Open data,Chart,Map,Politics,Environment,Corruption,Money-laundering,Economy,Human rights","Scraping,Adobe,Microsoft Excel,CSV,R,RStudio,Python"," This project explores the secretive multi-billion dollar hydroelectric power agreements between China and Myanmar that will determine the future of millions of Myanmar’s impoverished citizens. The story unravels Myanmar’s broader financial entanglements with China and weaves together the limited data available to map out the future lives and livelihoods of Myanmar citizens, including indigenous communities who may be swept away in the dam floodwaters. "," The story was much discussed on Twitter and at events among the Southeast Asia aid community, Chinese Belt and Road Initiative scholars and regional English-language media. But the larger goal was to reach Myanmar citizens. This is the first story in a series that provides comprehensive data-driven explanatory reporting to Myanmar citizens in the Myanmar language about the long term impact of foreign investment on the country’s future, as they try to steer their way to democracy. "," Exploratory data analysis was done in R and R Studio. Final data analysis was done in Python. <a href=""https://github.com/Frontier-Myanmar/chinadams/blob/master/story_01_overview/draft_presentation/Story%2001%20-%20Overview%20%5BDraft%20Viz%202%5D.ipynb"">Notebook</a> and <a href=""https://github.com/Frontier-Myanmar/chinadams/tree/master/data"">cleaned data</a> can be accessed in our <a href=""https://github.com/Frontier-Myanmar/chinadams"">Github repository</a>. Visualizations were done with Flourish and Tableau. "," Data reliability is a major challenge when working with data supplied by both Myanmar and China. Frontier analyzed economic data provided by Myanmar's <a href=""https://www.dica.gov.mm/"">Directorate of Investment and Company Administration</a>, <a href=""https://comtrade.un.org/"">UN Comtrade</a> as reported by both Myanmar and China, which revealed significant trade flow discrepancies as highlighted in the story, and Myanmar's <a href=""https://pyidaungsu.hluttaw.mm/uploads/pdf/post/vbB1pC_(10-2018)Annual%20Debt%20Report%20(final).pdf"">Parliamentary Debt Report</a>. Data on electrification was sourced from the <a href=""https://www.moee.gov.mm/en/ignite/page/45"">Ministry of Electricity and Energy</a> and geo-located data on active armed groups was provided by <a href=""https://data.opendevelopmentmekong.net/dataset/list-of-ethnic-armed-organizations-prod2?type=dataset"">The Asia Foundation</a>. The most elusive dataset, <a href=""https://www.ifc.org/wps/wcm/connect/d2a8eeb7-f765-4f08-838c-2c281970a4ab/Copy+of+18+02+07+MYA+SEA+HP+Database+WB-IFC.xlsx?MOD=AJPERES&CVID=m6eCITR"">disaggregated data on planned hydropower dams</a>, was collected by the Ministry of Natural Resources and Environmental Conservation and the Ministry of Electricity and Energy and vetted by the IFC. Access to the regions of the country impacted by the dams is also challenging, with our team of in-country reporters making several trips to understand the on-the-ground dynamics in the region and capture the experiences of those who have already been displaced. "," We hope we demonstrated that collaborative data journalism projects with deep roots in the communities impacted are essential to good data reporting on international issues. It’s possible and important to produce data journalism stories that resonate with international and local audiences. In a lot of countries, it does not take data to convince people of corruption; it is a fact of life. We figured out that the best way to tackle injustice is to focus on the people at the bottom, the ones who are being stolen from, not the ones at the top who are doing the stealing. We think that starting with a goal of enabling citizens to understand the forces at play and make better decisions about their country’s economic future allows a much more nuanced and enlightening story to emerge from the data.  ",https://frontiermyanmar.net/en/power-play-how-chinese-investment-damned-myanmars-economic-transition/index.html,https://frontiermyanmar.net/mm/power-play-how-chinese-investment-damned-myanmars-economic-transition/index.html,https://github.com/Frontier-Myanmar/chinadams,,,,,"Victoria Milko, Clare Hammond, Ye Mon, Eva Constantaras"," Victoria Milko is a multimedia journalist and writer.       Clare Hammond is a journalist and the digital editor of Frontier Myanmar, an independent magazine in Yangon.       Ye Mon started as a reporter at the Pyithu Khit news journal in 2011. Prior to joining Frontier, he worked as a reporter at the Myanmar Times and on the DVB English team.       Eva Constantaras is an investigative data journalist specialized in establishing data units in mainstream media around the world. ",,,
United States,FiveThirtyEight,Small,Participant,Best visualization (small and large newsrooms),One Way To Spot A Partisan Gerrymander,15/03/19,"Explainer,Chart,Elections,Politics","Animation,D3.js,R,Python"," This project is a scroll-based explainer of ""partisan bias"" a specific measure of how skewed an electoral map might be. Partisan bias was mentioned in the Supreme Court case on partisan gerrymandering in North Carolina and this piece first explains how the metric works using congressional elections data from the past 30 years, and then uses real historical political maps from three states to talk about some recent instances of gerrymandering and corrective maps that have been put in place as solutions.  ", The project was well recieverd and cited by gerrymandering experts (such as creator of the efficiency gap Nicholas Stephanopoulos) as a clear explainer of a widely used metric in gerrymandering. Gerrymandered maps are notoriously hard to define (just ask the Supreme Court) but this project gives readers a clear explanation of one way to go about it.  ," The project was built primarily with D3.js and Stickybits.js for the scroll-based animation. Because this was a complicated thing to explain, we felt that going step-by-step using scroll was the way to go. The analysis of elections data was done in Python and R. We also built a statistical model to account for unopposed elections. "," There were three hardest parts to this project: the first was collecting all the congressional vote data going back to 1992. We had some of this data already in our databases, but much of it we had to find and collect ourselves. The second was building the statistical model to account for unopposed races. We considered excluding unopposed races from the analysis, but realized it was integral to the integrity of the project, so we did the extra work to account for them. The last most-difficult part was structuring the step-by-step words and animation for clarity and sense. It took much editing and user testing to come to the right sequence. "," Firstly, we hope they can learn about partisan bias and some of the history of gerrymandering in America over the last three decades. I also think this is an instructive piece in how scroll and animation can help clearly explain a complicated concept.  ",https://projects.fivethirtyeight.com/partisan-gerrymandering-north-carolina/,,,,,,,"Ella Koeze, William T. Adler"," Ella Koeze is a visual journalist for FiveThirtyEight.    Will Adler a 2019–2020 AAAS Congressional Science and Engineering Fellow, sponsored by mathematical/statistical societies ASA/ACM/AMS/IMS/MAA/SIAM. Previously, he was the Computational Research Specialist at the Princeton Gerrymandering Project.  ",,,
Qatar,Al Jazeera,Big,Participant,Best visualization (small and large newsrooms),How does your country vote at the UN,23/09/19,"Explainer,Quiz/game,Database,Open data,News application,Infographics,Politics","Personalisation,D3.js,Canvas,Json,R,RStudio"," Every September, leaders from around the world gather at the United Nations in New York to debate and pass resolutions on the biggest issues facing the planet. To understand what the key issues have been over the years, Al Jazeera visualized all 6,112 roll-call votes of all 193 UN member states from 1946 to 2018.   The visualization provides a decade by decade explanation of major moments in history as well as a robust exploratory feature which allows readers to discover exactly how their country voted on key issues over the entire history of the UN. "," This project is the largest visual analysis of the UN General Assembly. This visualization revealed several fascinating insights including how nearly one-fifth of all resolutions have been about the Israeli-Palestinian conflict. This, as well as other interesting findings generated a lot of engagement with Al Jazeera’s global audience in English as well as Arabic.       Since launching in September ahead of the 74<sup>th</sup> annual General Assembly the story has been viewed nearly 200,000 times and has had an average time on page of 4:11. The story has continued to generated a long tail of web readership, months after publishing and is being used in universities and schools around the world as an international relations and politics resource. Members of the UN's data and creative technologies team reached out to Al Jazeera and invited us to present the project as a new way of visualizing otherwise complex data which has a direct impact on people from all over the world. "," First the data was collected from Harvard’s Dataverse. Next the data was cleaned and analysed using R, a statistical programming language. All of our source code was made available in a blog post to ensure transparency and reusability.   The visualization as well as the interactive quizzes were built using D3.js and React.js. The user interface was customized for desktop as well as mobile in order to reach our global audience.   Finally, social friendly charts were built using flourish.studio. ", Abstracting over 1.2 million votes was the biggest challenge. Rather than simplifying parts of the story we used a collection of explanatory and exploratory data visualization techinques to represent the entire data set.   This meant that users could get a big picture understanding of the topic as well as drill deeper into the story to see how their country voted over the course of 7 decades.   One of the other challenges was to produce a visualization that works in both English and Arabic on desktops and on mobile. This meant reworking complete sections to work from right to left including the data visualization. , Data visualization has the power to discover new stories and insights from an otherwise complex dataset. This project demonstrated that in a way that was both interesting to our readers and our own team. ,https://interactive.aljazeera.com/aje/2019/how-has-my-country-voted-at-unga/index.html,https://medium.com/@ajlabs/how-ajlabs-uses-data-to-tell-stories-fd946188c389,,,,,,"Mohammed Haddad, Usaid Siddiqui, Owais Zaheer", Mohammed Haddad is a data journalist with Al Jazeera's AJLab's data storytelling unit. Usaid Siddiqui and Owais Zaheer are producers with Al Jazeera's online web department.  ,,,
Singapore,Singapore Press Holdings - The Straits Times,Big,Participant,Best data-driven reporting (small and large newsrooms),Singapore's National Day songs: 26 patriotic songs but which one hit the mark?,30/06/19,"Infographics,Chart,Audio,Lifestyle,Culture","D3.js,Google Sheets,R"," A fun side project to approach the annual National Day Parade in Singapore. Each year a new song is released by local artists, commissioned by the National Day Parade organising committee. We looked at all 26 songs musically to show their differences and decade trends by using the Spotify API. "," Reactions were very light-hearted and fun. There was a competition attached to the interactive to giveaway much coveted NDP tickets. Comments on social media were nostalgic, reminiscing about certain songs or totally forgetting about others. It was amongst our top 10 graphics for the year in pageviews. "," For data analysis, using R and the Spotify API. For the final project, we use our usual combination of Javascript frameworks like D3.js and Vue.js to construct the page. Adobe Illustrator for the images and illustrations. ", Keeping it funny and not getting sucked into the musical jargon that many of our readers may not be familiar with. The Spotify API is very detailed and interesting but we had to edit a lot and use scrollytelling to explain how to interpret the data in a light way so it didn't feel like an 'assignment' for the reader. , We really had to remember our audience and not make it overly complicated for them to understand otherwise we would definitely lose them. ,https://graphics.straitstimes.com/STI/STIMEDIA/Interactives/2019/06/hit-or-miss-rank-your-ndp-songs/index.html,,,,,,,"Alyssa Karla Mungcal, Charles Tampus, Denise Chong, Lee Sujin, Lim Ling Li, Ng Huiwen, Rebecca Pazos, Tin May Linn and Wang Yanhua."," The Straits Times' Digital Graphics team is a multi-disciplined team of developers, designers and journalists who work in the newsroom on any editorial projects that require customised visuals or using technologies such as AR/VR or other creative forms of storytelling. We are passionate about data storytelling as a form of journalism that helps augment the daily reporting of the newsroom. We want to tell fun, beautiful and courageous stories with clean, precise and efficient code. ",,,
Singapore,Singapore Press Holdings - The Straits Times,Big,Participant,Best data-driven reporting (small and large newsrooms),"On a little street in Singapore… A road map of history, culture and society",29/06/19,"Explainer,Long-form,Infographics,Culture","Personalisation,D3.js,Adobe,Google Sheets,CSV"," This project, published during the year Singapore celebrated its bicentennial in 2019, takes a look at how the country’s past is quietly preserved in the names of its streets. For a country which doubled the number of its streets in just half a century, the names reveal throwbacks to British colonial times, a stunning array of Malay words and other surprising references.    The data for this was manually compiled from various resources, including two books publishing on the topic. Over 3,500 streets were classified, geolocated and individually checked to create this data-driven lifestyle story. "," This project was shared by many on social media, particularly amongst the local heritage community.    The authors of the two books used in this project were overwhelmed by the visualisation of their work. They have asked us if they can include links to this project in upcoming reprints of their books.    This project appeals to anyone who lives in Singapore or who has an interest in Singapore’s history, and it will serve as a reference piece for years to come. "," For the data collection, we just used a Google spreadsheet and pivot tables for the analysis. For the final project, we use our usual combination of Javascript frameworks like D3.js and Vue.js to construct the page. Adobe Illustrator for the images and illustrations. "," There were two books we had to use to manually compile all the meanings and classify them. It took around 2 to 3 weeks to go through as many streets as we could, compare the explanations in each book as they would differ on occasion, and then verify with another resource. This was the most difficult part but after doing this, we know have a great spreadsheet that we are sharing with our community, other friends in the industry and colleagues because the data can be interpreted and used in many different ways. "," Don't be scared to embark on getting your own data. We had a group of interns help us with the collection of the data and they enjoyed being able to get to the end of this and see the fruits of their labour by suggesting visual ideas for the final project because they had the best understanding of the data. Also, doing the collection process meant that we were able to tell a more complete picture and this project continues to be used as a resource for the Singapore heritage community. ",https://graphics.straitstimes.com/STI/STIMEDIA/Interactives/2019/06/singapore-street-names/index.html,,,,,,,"Aloysius Tan, Alyssa Karla Mungcal, Bessa Nicoletta, Goh Yan Han, Hannah Yan, Lee Sujin, Raihanah Ruslan, Rebecca Pazos, Trinity Tan and Yu Sheng Sin."," The Straits Times' Digital Graphics team is a multi-disciplined team of developers, designers and journalists who work in the newsroom on any editorial projects that require customised visuals or using technologies such as AR/VR or other creative forms of storytelling. We are passionate about data storytelling as a form of journalism that helps augment the daily reporting of the newsroom. We want to tell fun, beautiful and courageous stories with clean, precise and efficient code. ",,,
Qatar,Al Jazeera,Big,Participant,Best data-driven reporting (small and large newsrooms),Forced Out: Measuring the scale of conflict in South Sudan,09/12/19,"Investigation,Explainer,Long-form,Infographics,Chart,Video,Map,Satellite images,Politics,Women,Human rights","Drone,Adobe,Creative Suite,CSV"," ""FORCED OUT"" tells the story of forced displacement in South Sudan following years of civil war. Al Jazeera used an innovative mobile phone survey to measuring the scale of the conflict in South Sudan. We dialled more than 35,000 random phone numbers to paint an accurate picture of land rights and forced displacement across the entire country.   The data collection reached people nationwide and provided us access to people who are in rural areas, in opposition territory, currently displaced, and who are illiterate. No other organisation had ever collected land rights data on this scale. "," To our knowledge this is the most comprehensive data-driven look at the situation in South Sudan. While the data provides necessary context, the human stories are what made this story resonate with our global audience.   The story generated a lot of discussion about how land rights are important in the context of the current peace agreement, including from analysts, land rights experts, diplomats and academics. As well, the page views were quite high for such a project:  110,000 page views, and 4:42 average time on the page, 70% from mobile devices. "," Almost all the analysis was done using Excel and Google Sheets. TerraServer and Google Earth were used for all the mapping and satellite imagery analysis.   To reach the largest possible audience the web page itself was built exclusively on AMP (Accelerated mobile pages). This meant a lightning fast user experience with completely put the storytelling experience ahead of any gadgetry. Visual elements including video testimonies, before/after sliders and drone footage were carefully selected to provide a detailed account of the realities on the ground. "," Understanding and quantifying the realities on the ground for millions of South Sudan was not an easy process. To make matters more difficult, international journalists are all but barred from reporting from South Sudan. Rather than conceding to the reality that ""there's no data"", we decided to make our own.   It took more than eight months to run the survey, running first testing and countering the challenges of false returned data, then trying to improve the drop-off rates. Meanwhile it took many months to conduct the satellite research and on-the-ground reporting with a local journalist.   The result is a story that would have otherwise gone untold if it wasn't for a data-driven approach.     "," Sometimes there's just no other way to tell a story than to use data. In this case, data represented the thousands of people who were forced from their homes and displaced. These are the types of data stories that Al Jazeera prides itself in and would love to share with others from around the world. ",https://interactive.aljazeera.com/aje/2019/south-sudan-forced-out/index.html,,,,,,,"Carolyn Thompson, Kristen van Schie, Lagu Joseph Jackson, Thomas Holder, Anealla Safdar, and Mohammed Haddad", Carolyn Thompson and Kristen van Schie are freelance data journalists who cover stories from the global south.  ,,,
Singapore,Singapore Press Holdings - The Straits Times,Big,Shortlist,Best news application,Tiger Mum's guide to getting your 6-year-old into the ‘right' school,28/06/19,"News application,Chart,Culture","Personalisation,Scraping,D3.js,Json,R"," Think it’s tough fighting for a spot in prestigious colleges? In Singapore, the competition starts at primary, or elementary, schools. The process is competitive, with many having to ballot for entry at various stages of the registration process to the most popular schools.    Based on data from the past 13 years, we build a machine learning model to predict the risk of over-subscription. And, to make it engaging for our readers, we allow them to simulate this process many times with our simulation tool.    "," This interactive is a popular graphic, going beyond providing information to readers that would normally be conveyed in a typical article format to become a valuable, reusable widget for parents to get an idea of the chances of their child gaining entry to a school of their choice at each phase of the registration process. Engagement is high for this interactive, with average time on page reaching over 5 minutes, easily doubled the site's average.    "," We collect data from the past 13 years of Primary 1 registration exercises, and build a machine learning model that predicts the number of applicants by school and by phase, given the number of vacancies in that phase is known. Outcomes of this prediction (number of applicants vs vacancies) are used to determine if balloting will be required in that phase. We repeat the prediction using bootstrapping to simulate many different outcomes. For example, if in 100 simulations, 33 ended up with balloting (applicants > vacancies), the likelihood of the school going into balloting in that phase will be indicated as 33%.    "," We have been improving the model for this every year and it has become a mainstay for our reporting for the past two years. However, with this longevity comes the problem of keeping it relevant and interesting, especially when people have seen it in the past. This year, we went back even further and got more data to add to our machine learning for a more complete picture that aligns with the Chinese zodiac and its influence on birth patterns and cultural tendencies. "," We think picking the right topic to do a story on is half the battle. If the story is good from the start, the execution of the idea is much easier. In this one, anything we do around Primary 1 registrations will be of interest to a large portion of our audience so this helps us in terms of reach and engagement. ",https://graphics.straitstimes.com/STI/STIMEDIA/Interactives/2019/06/primary-1-registration-guide/,,,,,,,"Jocelyn Tan, Lee Pei Jie, Rebecca Pazos and Thong Yong Jun."," The Straits Times' Digital Graphics team is a multi-disciplined team of developers, designers and journalists who work in the newsroom on any editorial projects that require customised visuals or using technologies such as AR/VR or other creative forms of storytelling. We are passionate about data storytelling as a form of journalism that helps augment the daily reporting of the newsroom. We want to tell fun, beautiful and courageous stories with clean, precise and efficient code. ",,,
Brazil,Ecumenic Creative Operations,Small,Participant,Open data,Ecumenic Ecosystem,20/06/19,"Investigation,Solutions journalism,Long-form,Breaking news,Cross-border,Documentary,Database,Open data,Crowdsourcing,Politics,Environment,Corruption,Money-laundering,Arts,Lifestyle,Business,Culture,Women,Agriculture,Immigration,Health,Crime,Economy,Employment,Human rights,Terrorism",Scraping,"The Ecumenic Creative Operations (https://ecumenic.github.io) is developing an extense open-source research of the laic and faith-inspired creative industry ecosystem (https://bit.do/ecumenic-ecosystem). With its community of agents and scenario indicators data they look after improving its production reliability engineering technologies, regenerativity ecology, sustainable economy, and ecumenical laic-interfaith diplomacy development. From the analysis of this information, as presented in their project (https://bit.do/ecumenic-project) they global charters compliance (https://bit.do/ecumenic-compliance), dignified inclusivity accessibility code of conduct (https://bit.do/ecumenic-conduct) collaboration guidelines (https://bit.do/ecumenic-collaboration), contract template (https://bit.do/ecumenic-contract). These and other materials (https://github.com/ecumenic/project) are available for use from any faith-inspired or laic cultural development organizations under the Creative Commons CC-BY-NC-SA-4.1 license"," The enterprise open-community is developing a set of data-ethics policies from this research of extreme importance for world community, such as a global standards charters compliance commitment (<a href=""https://bit.do/ecumenic-compliance"">https://bit.do/ecumenic-compliance</a>), a dignified inclusivity accessibility entrepeneurial code of conduct (<a href=""https://bit.do/ecumenic-conduct"">https://bit.do/ecumenic-conduct</a>), an open-source devops collaboration guidelines (<a href=""https://bit.do/ecumenic-collaboration"">https://bit.do/ecumenic-collaboration</a>), a fair-trade contract template (<a href=""https://bit.do/ecumenic-contract"">https://bit.do/ecumenic-contract</a>), and an open-educational-resource (OER) on total-artwork operatic ecomuseology cultural development artscraft production modular instruction program (<a href=""https://bit.do/ecumenic-ecomuseology"">https://bit.do/ecumenic-ecomuseology</a>)  which are also available for any peacebuilding compliant faith-inspired or laic cultural development organizations under the Creative Commons CC-BY-NC-SA-4.1 license (<a href=""https://bit.do/ecumenic-license"">https://bit.do/ecumenic-license</a>).   With this we have been able to collaborate with many communities such as craftspeople, seekers gatherings, animals, cinema sets staffs, artistic collectives, theatre and dance companies, archiengineers, indigenous tribes and nations, street marketers, infrastructure developers, rule-of-law settlements, jail population (jailers included), pregnants, plants, midwives, business developers, neurodiverses, psychonauts, scholars, policemen, transqueers, homeless and landless people, civil movements, grafitti crews, legislators, landscapers, semiotic multimedia researchers, cleaners, fablabs, seniors schools, physicians, carers, urban renewal ngo's, open-software communities, cosmoethicians, broiderers, metarecyclers, independent medias, farmers and herdsmen, street marketers, scientists, sustainable intentional communities, human-rights advocates, research residencists, social bankers, among other euthenic oriented people our whole gratitude.     "," The open data we are sharing with you (<a href=""https://bit.do/ecumenic-ecosystem"">https://bit.do/ecumenic-ecosystem</a>) contains a thorough bibliography on the methodology used in the development of the ecosystem report itself.   It was collaboratively edited through three years of presential and online workshops using Riseup Etherpad, Telegram, Whatsapp, Google Docs, and Github.   It was organized, reviewed three times, and diagrammed using GNU/Linux Peppermint OS 6.6. Distribution, Gedit 3.10, Libre Office 4.2, Gimp 2.8, Chromium 65.0, and Freemind 0.84. "," The unsurmountable size of the data gathered and the compliance detailment makes it very difficult to be handled without the proper consentment ontological intorperability, so we began to research the WC3 semantic-web and further developments on this path.   The dangers and urgency of the matters involved in its have a very profound negative psychological effect on the gathering of collaborators without the proper mandate, so the vast majority of participators did not commit to the project, which led us to write articles (<a href=""https://bit.do/ecumenic-articles"">https://bit.do/ecumenic-articles</a>) to explain its interdisciplinar necessity better, including a research on blockchain, so that we may promote it economically.   I must confess the last times have been very difficult for our works in Brazil due to the increasing religious persecution we've been having. I've been personally trying to connect to international laic and interfaith community, specially the artistic and academic one, without results. I've considered many times giving up, after assaults and threats, without being able to find refuge to foster this work.    Even if you do not think our book, perhaps for its pedagogical nature, is worth of your award, please help us promoting and sharing its useful technologies. God bless you, yours, and everyone in your paths. "," That data-science must includes humanities and cultural studies so that data-analysis using human collective-intelligence under a holistic compliance reliability engineering ci-cd devops, may develop peacebuilding humanitarian data-ethics policies. ",https://bit.do/ecumenic-ecosystem,https://ecumenic.github.io,https://github.com/ecumenic/project,,,,,The Ecumenic Creative Operations," Ecumenic Creative Operations is a interfaith-based laic-compliant cultural development compassionate ethic sustainable peacebuilding inclusive collaborative open-source art studio & opera company.   Its projects are creaed in compliance to United Nations Sustainable Development Goals (UNSDG), Peta Vegetarian Institution Pledge, RFBF Religious Freedom In Business Pledge, Scientific Methodology Researchers Pledge, Info-Ethics Tech Pledge, ICOM Ecomuseology, DROPS Ecomuseum Platform, General Data Privacy Regulation (GDPR), The Motion Picture Association of America (MPAA) Parental Guidance, UNESCO Heritage Safeguard and Open Education Resources (OER), and Creative Commons CC BY-nC-SA 4.1. ",,,
United States,Univision Digital News,Big,Participant,Best data-driven reporting (small and large newsrooms),The record for number of Latinos in Congress was achieved with the help of districts with few Hispanics,01/02/19,"Investigation,News application,Infographics,Chart,Elections,Politics","Animation,D3.js,Json,CSV"," Most Hispanics win in areas where the community is more than half of the population, but since 2011 districts with less than 10% of Latinos are electing Hispanic legislators to the House of Representatives, mainly from the Republican Party.  We visualize who these representatives are and how the numbers have changed since 1990. "," We were able to show our hispanic audience how represented they are in their communities and whether this representation in congress is proportional to hispanic populations all around the country.  Since it is clear that the proportion of hispanic legislators is not equal, but much less, to their representative population, it gave our audience a window into how their voices and concerns are heard in government.  ", For this project we used a scrolly-telling technique to describe different representatives in the 116th congress that had just been elected.  We used Adobe Illustrator for the first graphic and d3.js for the chart and transitions to the scrolly-telling aspect of the piece.   We used javascript for the tooltips showing a more detailed description of each congressperson. ," Doing the research for this piece, we had to come up with guidelines of what is considered a hispanic congresperson.  It was a challenge going back through the different congresses and determining whether not only did we consider them hispanic, but if the congressperson themselves considered if they were hispanic.  We needed to call many different campaigns to get clarity on this subject.  After that in the technological aspect of the article, it was difficult to align all charts and scrolling motion together to make sure there was smooth and understandable transitions for the audience to not get ""lost"". "," As we and our audience learned, it's important to know how we are represented in congress.  We all want to know if our voice is heard in government and this visualization gives hispanics a glimpse of how this is not very proportional, although improving, in the present configuration of the house of representatives. ",https://www.univision.com/especiales/noticias/infografias/2019/record-latinos-congreso-se-logro-con-ayuda-de-distritos-con-pocos-hispanos-english/,,,,,,,"Javier Figueroa, Ana Elena Azpurua, Mariano Zafra"," Javier Figueroa is a graphics developer at Univision.  He has been at Univision for the past 4 years.  Before that, he ran his own web development company for 19 years in Puerto Rico and Miami after graduating from the University of Michigan with a degree in Political Science and Latin American and Caribbean studies.  Javier has taught Data Viz at the University of Miami and has won several awards in his field.   Ana Elena Azpurua is a digital Graphics reporter with Univision Noticias. Before that, she covered local news and Latino issues for  Al Día , a Spanish-language outlet published by  The Dallas Morning News . She began her journalism career in Caracas, Venezuela, as a staff writer with  El Nacional  newspaper. Ana holds a master’s degree in journalism and International Affairs from Columbia University, where she was a fellow at the Toni Stabile Center for Investigative Journalism.   Mariano Zafra is the Storytelling and Graphics Editor at El PAIS (Spain). Previously he worked as a Graphics Reporter in The Wall Street Journal in New York and he was Graphics Editor for the infographic and visualization department of Univision News Digital in Miami. Before moving to the United States, he spent seven years working at the two most prominent national daily newspapers in Spain: El Mundo and El País. His work as a graphic editor has been recognised with thirty different awards from the SND (Society of Newspaper Design), Malofiej, as well as by the Inter American Press Association.     ",,,
Kenya,"BBC News (Africa), BBC News (World), BBC Amharic, BBC Afaan Oromoo, BBC Swahili, BBC Tigrinya, BBC Gahuza",Big,Participant,Best visualization (small and large newsrooms),Rwanda leads fight against cervical cancer in Africa (story) What you need to know about HPV (interactive),28/11/19,"Explainer,Long-form,Cross-border,Multiple-newsroom collaboration,Open data,Fact-checking,Illustration,Infographics,Chart,Video,Map,Audio,Women,Health","Animation,3D modelling,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV"," This project focused on the Human Papilloma Virus and it's link to cervical cancer in Africa (the 2nd most common cancer among women in Africa). It features a captivating interactive delving into the frequently asked questions around HPV using various visual mediums (video, maps, charts, infographics, animations) which then sits in a written story on how Rwanda is in the lead when it comes to fighting cervical cancer in Africa. "," The project was quite impactful as it was not only published in English it traveled even further on the continent to go live in Swahili, Amharic, Afaan Oromoo, Gahuza, Somali and Tigrinya. The format we chose to implement for this project is something that is quite innovative for the audiences of the language services mentioned as most of their content is usually limited to text and/or video content only.   Due to the unique challenges on the continent such as high data costs, a large part of our audience resorts to using browsers that turn off Javascript allowing them to consume non-interactive content. We were able to adapt to this by creating core content that gave them a similar experience as well as pushing the content on various social media platforms for their consumption - which meant that no one was left out. "," The project brought together a diverse range of skill-sets from our team, from online journalists to an animator, software developer and a user interface & user experience designer. The tools utilized throughout the project include CPS, Adobe Illustrator, Adobe After Effects, Adobe Premier Pro, Adobe Audition, Abode Photoshop, Blender 3-D, Final Cut Pro, Sketch, Marvel, Vanilla Javascript, Handlebars, CSS (Sass) and Google Sheets (database/JSON). These were used to not only tell the story of how Rwanda is at the forefront when it comes to fighting cervical cancer in Africa, but also in developing the interactive that sat within the article and broke down the key things to know about HPV (Human Papilloma Virus) - which has been linked to cervical cancer cases. "," The hardest part was coordinating the multi-media project that involved covering a vast geographical area between Kenya, Rwanda, Malawi and Sierra Leone (the latter of which was not successful). We were able to overcome this in the end by working with fixers in countries that were not accessible to us at the time. As one of our main aims is to have our content travel to underserved audiences (primarily women and youth), we were cognisant that we had to produce something that would be able to travel in various languages in the region from Swahili, Amharic, Afaan Oromoo, Gahuza, Somali and Tigrinya. Ensuring that the content appeals to each service and coordinating the translations of infographics, voice-overs and scripts was a tedious process. Through the use of project management tools such as JIRA, we were able to successfully publish on the website and social media platforms of seven different African languages.  "," Our ambition with this project was to bring an issue that is rarely discussed and seen as taboo to the forefront. Conversations around cervical cancer, HPV and the HPV vaccine on the African continent aren't openly had and that is something that also reflects in the data openly available. Being able to attract a younger audience from social media platforms, we wanted to break down the stigma and start a conversation around this social issue that would be impactful, especially to our youth and women audiences. ",https://www.bbc.com/news/world-africa-50262017,https://www.bbc.com/amharic/news-50583016,https://www.bbc.com/afaanoromoo/oduu-50584253,https://www.bbc.com/swahili/50575128,https://www.bbc.com/tigrinya/news-50588705,https://www.bbc.com/gahuza/50585375,https://www.instagram.com/p/B5awd5RguZm/,"Ashley Lime, Esther Ogola, Purity Birir, Millie Wachira, George Wafula, Sheila Kimani, Anthony Makokha, Hugo Williams, Muthoni Muchiri"," The team behind this project is the ""BBC Africa Nairobi Innovation Hub"". A new hub launched 2 years ago with the aim of developing digital journalism in Africa while also creating content that reaches underserved audiences on the continent. This 'Visual Journalism' team has an aim to unlock the potential of multimedia storytelling across platforms - desktop, tablet, mobile and TV. Using graphical tools and combining those with an understanding of data and journalistic rigour, the team aims to produce powerful visual explanations that improve understanding of the news, and which audiences highly appreciate. The team brings together online journalists, graphic designers and developers with TV designers and craft editors. ",,,
Netherlands,NOS,Big,Participant,Best visualization (small and large newsrooms),Will Zandvoort make Formula 1 exciting again?,07/12/19,"Investigation,Explainer,Long-form,Illustration,Infographics,Video,Sports","Animation,Adobe,Creative Suite,Microsoft Excel,Google Sheets"," According to the race results, Formula 1 isn't the most exciting sport to watch. Four out of five races in the last five years were won from the front row. We asked ourselves why overtaking in Formula 1 is so hard and if this might be influenced by the layout of the circuit. Because such circuit data isn't available, we made our own dataset measuring every Formula-1 circuit by hand. We found out that Zandvoort, the new Dutch grand prix, will be one of the shortest and most tortuous races on the calendar with little opportunities to overtake. "," We have published the story on multiple platforms, with a visual longread and a Youtube video as main products. Besides that, we published several news articles, posted on Instagram and we made a podcast about it.   Our explainer video on Youtube reached nearly 800.000 views. The video appeared in Youtube's 'trending' videos in The Netherlands (as #1), which means that the Youtube algorithm noticed the engagement, caused by our video. In the comment section, our viewers posted more than 1.300 comments and started a discussion about the topic. The majority of our viewers were under 35 years old, so we succeeded in reaching a young audience with our investigation.   The visual longread didn't reach that much readers (almost 70.000), but the readers stayed on our page for more than 8 minutes. Comparing to the attention span of our young target audience, we think that's a lot. The feedback we got on our investigation was mainly positive. We expected that autosport enthousiasts would like our story, but we were surprised with the positive feedback from people who say they aren't interested in Formula 1 at all. Our visual approach made them better understand Formula 1, they wrote us. "," Because the data we needed didn't exist, we created our own dataset. It required a lot of hand work: we needed to measure every circuit by hand. We started by measuring the circuit's length and width using Google Earth. But to measure the turns, we had to go a bit more analog than we were used to: we measured every turn of every circuit using a set square.   We aggregated and analyzed the data with Excel and Google Spreadsheets, by which we created two datasets: one of every circuit (total length, number of turns, length of DRS-zones etc.) and one of every corner: length, width and angle.   We combined text, illustrations, infographics and data visualizations to make sure that every part of story would be told in the best possible way. We created the visualizations and illustrations with Adobe Illustrator and Adobe Photoshop. The visual longread was build with Vue.js, the GSAP Animation Suite and Scrollmagic.js. The video graphics were made with Adobe After Effects. "," The hardest part of this project was the absence of data about the circuits. To prove our hypothesis (the circuit layout influences the possibility to overtake), we had to measure every circuit by hand. That took us a few days but resulted in a unique dataset. But when we combined all the data, we didn't find the correlation we hoped for. A difficult moment in our proces, because we thought that all our measuring might be for nothing. On the other hand, the (ex-)racers we interviewed, told us that overtaking is harder on a tortuous circuit. It made us realize that measuring the circuit can be done in many ways. Besides that, a Formula 1 race can be influenced by a lot of circumstances. The weather, for instance.   We chose to add these other circumstances, like the weather and the safety car, to our research. Combined with the interviews with racers, we concluded that it was still possible to tell our story, but that datavisualization itself wasn't enough. For some parts of the story, like the perfect race line and the DRS-effect, we chose to tell the story with explanatory infographics. "," We think that the absence of data shouldn't be a reason to kill an idea. In some instances, aggregating your own data is worth the investment. And sometimes the story requires methods that can't be done with Python, QGIS, R or Excel, but forces you to go analog. ",https://app.nos.nl/op3/formule1/,https://www.youtube.com/watch?v=1JdSj04KV6w,,,,,,"Arnoud van der Struijk, Lars Boogaard, Ruben Sibon, Wiel Wijnen, Jurjen IJsseldijk, Emil van Oers, Ben Prins","  Arnoud van der Struijk:  research, data-analysis    Lars Boogaard:  data-analysis, datavisualization and graphics    Ruben Sibon:  development    Wiel Wijnen (intern):  research, data-analysis    Jurjen IJsseldijk : editor, projectmanagement    Emil van Oers:  presentation and script-writing    Ben Prins : motion graphics and design     ",,,
Belarus,TUT.BY,Big,Participant,Best data-driven reporting (small and large newsrooms),Belarusians rest less than other nations. Isn't it time to change it?,05/11/19,"Investigation,Explainer,Solutions journalism,Long-form,Chart,Lifestyle,Culture,Health,Economy","Scraping,Adobe,Microsoft Excel,CSV,Python"," In Belarus, there are an average of seven and a half public holidays a year for every working person. This is noticeably less than the global average. But do we need more? Note that the text found at link No. 1 is dated February 3, 2020. The article is just a translation into English, users have not seen it. The original text was published in Russian on May 11, 2019. Link No. 2 leads to the original text. "," At the end of the text, we added a feedback form, where we invited users to send their options for holidays. We received 2150 letters and wrote a separate article with the results. It confirmed our belief that the topic does matter and is of great importance for the people. At the beginning of 2020, local authorities added one day off to New Year holidays and, who knows, maybe it was our project that made them take such a decision. "," MS Excel, Python, Tabula, Adobe Illustrator. "," It was challenging for us to single out the days worthy to become a new holiday. After collecting user options, we had to sort them out and highlight all the proposals, since people wrote in an arbitrary manner, and their notes were often accompanied by comments and extensive considerations. "," This project shows how to find a topic that excites a large number of users by studying the popularity of last year's news. Besides, it demonstrates how to build a sufficiently large user dataset if you ask people to comment on the topic in question. ",https://news.tut.by/society/671155.html,https://finance.tut.by/news632630.html,,,,,,"Anton Devyatov, Svetlana Baksicheva, Elena Pashinina"," TUT.BY is the largest Belarusian independent media outlet. We have 70 journalists and editors working in the newsroom.   Anton Devyatov. Education: an energy engineer. I have been engaged in data visualization (search, collection, analysis and product development in various techniques from static illustrations to interactive JavaScript works) for more than 8 years. Over the past few years, I have also been producing editorial special projects.   Svetlana Baksicheva. In 2017, I  graduated from the philological faculty of Belarusian State University with a degree in Russian philology. In my student years, I worked as a freelance journalist for various media organizations.   I have been working as a TUT.BY journalist since November 2017.   Elena Pashinina. I have worked in media for seven years, and although I'm not a professional journalist (I'm a linguist and English interpreter by profession). I've been always interested in writing news, keeping people updated and sharing useful content. In the last academic year, I got a job in TUT.BY, one of the most popular Belarusian news outlets, where I joined Naij.com (later Legit.ng) team. Five years later I had my own team of editors and several African media platforms to write for (Tuko in Kenya and Yen in Ghana).  Now I work for BelarusFeed, the media project for and about Belarusians, foreigners living in and outside Belarus, ex-pats and anyone interested in one of the less discovered countries in Europe ",,,
Russia,Agenda.media https://www.agenda.media/,Small,Participant,Best visualization (small and large newsrooms),Video infographics for Delo.media social networks,17/10/19,"Database,Open data,Fact-checking,Illustration,Infographics,Chart,Video,Map,Audio,Politics,Lifestyle,Business,Health,Economy","Animation,Adobe,Microsoft Excel,Google Sheets","  Delo.media  is a new media. For two years we have been making  entertainment videos about business , promoting economic topics in social media. We decided to experiment with an existing data animation format. We combined text explanations with data to check how interesting it would be for our audience.    We created videos on hot topics such as d ynamics of global brands, oil production in Russia, history of Russian companies . The characteristic feature of the project is that we supplemented  data animation with storytelling  — the text explanations that make the world of data more accessible to the average person. "," The format of  video visualizations  proved to be in demand among our audience no less than classic videos with subtitles on popular topics. For example, on one of our main platform Instagram, our usual video on average gets 40-50k views. While visualizations on average gained 30-40k views. The most popular visualization was about how  Russian billionaires have grown rich and became poor over the past 15 years .    That’s how we realized that hot and ambiguous topics can be covered much more objectively with data visualization. We were very surprised that Instagram users were interested in watching such complex and meaningful content. After creating 5 videos, we decided to launch a separate  Data Show channel in English  in 2020. So we plan to expand the audience, a range of topics and continue to develop new forms, as well as to advance the visualization format for B2B direction and advertising. "," Our team analyzed  statistical data  and converted collected information into a script on the  Google Sheets platform . Data visualization is presented in the form of an  infographic  animated in  Adobe After Effects  (compositing design techniques, shape animation). The  voice-over  was made in  Adobe Premiere Pro .   We used only  reliable media  as sources of the information and were guided by journalistic  fact-checking  standards. Each visual element in projects plays its own informative part. For example, colour is a guide to history. Due to  colour solutions,  we change the focus of attention in the video. Using a mix of elements, we give the audience an opportunity to maximize immersion in the topic in a short time, due to the video time limits in social networks.     After learning how to animate graphs with big data, we moved further and made a  map animation . In the infographics released in 2020 on the  obesity in the United States for 30 years  topic, we showed a map of obesity in dynamics, integrated animation of other important elements, such as the  population  and the  number of people with obesity . We also came up with an idea of using a spectacular element — the figure of  David , which with the help of  morphing technology  gains weight. In future, we plan to use more  entertaining elements  to attract attention and enhance the effect of data perception.   "," Our company works online, so it was difficult to build effective communication between people from different parts of the world. It was even more difficult to assemble a team of specialists with the rare skills required for this project: deep research, a broad journalistic outlook, and rigorous design. None of us had any completely relevant experience in visualizing video data. We all have been constantly learning and we still are. Our purpose is to combine our knowledge to create an innovative product. "," We have directed our extended experience in video production on the creation of this innovative product.  Now we want to make data more understandable through videos . We create videos on hot topics such as  dynamics of global brands, social media, oil production in Russia, top Russian companies, billionaires, obesity in the USA . For example, with the help of the interactive map, we highlight that nowadays there is no state or territory in the USA that has a prevalence of obesity less than 20%. It’s storytelling with a purpose because obesity is preventable. Data visualization helps to tell stories by converting data into a form easier to understand, highlighting the trends. Effective data visualization means  balance between form and function . We would like to share our experience and learn from others. ",https://www.youtube.com/playlist?list=PL53_LHpFQRUSh7JjDquXYcX72UBAOwphb,https://www.youtube.com/playlist?list=PL53_LHpFQRUTgFid6i-rzHufjx_R6RDao,https://www.facebook.com/delo.channel/playlist/1590222307779624/,https://www.instagram.com/p/B5AgjqLnmTn/,https://www.instagram.com/p/B6QU2IZHhjX/,,,"Anastasia Volosatova, Lydia Lotkina, Alena Rakhmankulova, Vladimir Kotov, Natalia Brusnitsyna, Anna Kulish, Anton Lobutinsky","  Anastasia Volosatova ,  producer, executive editor . Head of the Delo channel, a certified journalist with more than 5-year experience in video journalism. Live in Moscow and Rome. She is responsible for editorial work, trends and new formats, one of which the data visualization has become.    Lydia Lotkina,   analyst-supervisor . Bachelor of Economics from Novosibirsk, Russia. She combined her experience of participating in student case-studies, outlook and analytical thinking to become the brain of our project.    Alena Rakhmankulova , a  junior   designer  from Ulyanovsk, Russia. Instead of being a math teacher, she mastered the designer profession. Mastering her skills in our pilot project, she is the creator of all visualizations and identity.    Vladimir Kotov , from Moscow, is an  art director  of the project and the entire company Agenda.media. He is designing 24/7, therefore, out of all the numerous versions of the project, only the perfect ones were selected.     Natalia Brusnitsyna ,  film editor, sound director . She is a professional editing director from Moscow, Russia. She is one of those unique people who are responsible not only for the picture but also for the rhythm. Therefore, in our project, sound and picture are a united creation.    Anna Kulish ,  editor . She is a guru in SMM, a journalist, a political scientist from Vinnytsia, Ukraine. Senior author of the Delo channel, a key contributor to the launch phase. She combined covering the manager, journalist, researcher and editor responsibilities.    Anton Lobutinsky ,  junior editor . Young journalist with an economics degree, from Kyiv, Ukraine. Being a regular author of the Delo channel, for the project he understood the ups and downs of Russian business. ",,,
Singapore,Singapore Press Holdings - The Straits Times,Big,Participant,Best visualization (small and large newsrooms),South-east Asia's meth menace - one man's struggle,12/01/19,"Investigation,Long-form,Cross-border,Illustration,Infographics,Video,Map,Crime","D3.js,Canvas,Json,Adobe,Creative Suite,Google Sheets,R,OpenStreetMap"," The drugs feature, which zooms in on the meth problem plaguing the region and Asia’s cannabis conundrum, is a good example of solid investigative reporting made even more compelling with strong visuals, videos and interactive elements.    There are two parts to it: The first part zooms in on meth, which has trumped heroin as the drug of concern in Singapore since 2015. The second part examines Asia’s cannabis conundrum over legalising medical cannabis. The second part is behind paywall. We can provide a login for viewing and will email it too.     "," It immediately was one of our most viewed pages for the year. It presents a different side of Singapore's fight against drugs by telling the story of Tim, who has been through a prison rehabilitation centre twice. He was able to share his story of recovery and how truly difficult it is for someone who is an addict which helped provide a realness to the piece as opposed to just showing how meth was becoming a problem based on numbers alone. "," For data analysis, using R and Google sheets. For the final project, we use our usual combination of Javascript frameworks like D3.js and Vue.js to construct the page. Adobe Illustrator for the images and illustrations. "," Interviewing Tim. He was unsure about how his story would be used and through a careful relationship built with the main reporter, Zaihan, and planning for the interview (we needed different shots & styling for the video and for the interactive), we had to shoot this in one go and quickly so as not to spook him. "," A data-story should first be a human story. Whenever you have huge numbers that are talking about people, our goal for 2020 is to continue to search for the humanity, the emotions, in order to build a real connection with our audience. ",https://www.straitstimes.com/multimedia/graphics/2019/11/south-east-asia-meth-menace/index.html,https://www.straitstimes.com/multimedia/graphics/2019/11/smuggling-meth-across-the-myanmar-thai-border/index.html,,,,,,"Alyssa Mungcal, Bessa Nicoletta, Muhammad Azim Azman, Rodolfo Pazos, Tampus Charles Singson, Tan Tam Mei, Trinity Tan and Zaihan Mohamed Yusof."," The Straits Times' Digital Graphics team is a multi-disciplined team of developers, designers and journalists who work in the newsroom on any editorial projects that require customised visuals or using technologies such as AR/VR or other creative forms of storytelling. We are passionate about data storytelling as a form of journalism that helps augment the daily reporting of the newsroom. We want to tell fun, beautiful and courageous stories with clean, precise and efficient code. ",,,
Germany,"Bayerischer Rundfunk (BR), Norddeutscher Rundfunk (NDR)",Big,Shortlist,Innovation (small and large newsrooms),Winnti - Attacking the heart of the German industry,24/07/19,"Investigation,Long-form,Multiple-newsroom collaboration,OSINT,Podcast/radio,Politics,Business,Crime,Economy",Python,"   The project demonstrates how hackers have been spying for years on enterprises all over the world. In a campaign very likely executed on behalf of the Chinese Government, the hackers focused on chemical and technology companies in Germany and elsewhere (Siemens, BASF, Roche, Bayer) apart from airlines, hotels and telecommunication. Their apparent goal was industrial espionage and –presumably– spying on politically interesting persons. Scanning networks and analyzing malware, the reporters followed the hackers’ traces and identified their targets. The sheer number of targets and the shift to political targets had been unknown to the public thus far.   ","   Several German DAX companies as well as businesses from around the world and even the Government of Hong Kong admitted that their networks had been infected by the Winnti malware. The fact that the breaches date back some time is proof that they would have never talked about it without this investigation and the public would have never known about the magnitude and depth of this espionage operation. In the aftermath of the reporting, the German domestic security agency BfV for the first time ever published a detailed technical warning about Winnti.   The investigation was published in various channels of German public broadcaster ARD (of which BR and NDR are part of), among them a long form piece of radio reporting, a piece on the German national TV news program Tagesschau, a news article on national outlet tagesschau.de and an interactive web project in German and English, explaining in detail how the hackers work.   Many media outlets in Germany and the world picked up on the reporting. In Germany, among others, Süddeutsche Zeitung, Handelsblatt and Der Spiegel referred to the results of the investigation. Also, international news agency Reuters published a report which led to many articles in news media as well as specialized IT security publications all over the world. The US magazine Vice Motherboard mentioned the investigation as one of the “Cybersecurity stories we were jealous of in 2019”.   The threat is ongoing. As recent reporting by the same set of journalists shows, companies became aware of Winnti and started scanning their own networks. As a result, they now find out that a breach has happened. One of the companies that detected Winnti in their network is Lanxess, also a highly specialized company within the chemical sector.   ","   The investigation’s technical part played a key role in identifying Winnti’s targets. Mainly, two methods were used: identifying targets by scanning servers for Winnti infections and analyzing samples of the Winnti malware itself.    The first method is based on an nmap scan script published on GitHub by the security department of German industry giant ThyssenKrupp, which had been attacked by Winnti in the past. The script takes advantage of the fact that the Winnti malware initially behaves passively once it has infected a computer, waiting for remote control commands. The reporters have used the script to send false Winnti control messages to a list of different company networks. The software per se is harmless, but capable of simulating control commands designed to lure Winnti out of hiding. In all cases where Winnti was installed, the malware will respond to the request. This makes clear: That company has been hacked.   The second method comprised in-depth analysis of the Winnti malware itself: The reporters gained access to samples of the malware uploaded anonymously to the popular online service Virustotal, which security departments of companies use to check file samples for malware. The reporters were provided with a clue to find those samples: in some instances, Winnti operators had written the names of their targets directly into the malware, obfuscated with a relatively simple mechanism called a rolling XOR cipher. In a first step, the reporters tried to verify the information, using a simple Python script. They then used Yara rules to hunt for Winnti samples. Moritz Contag, a security researcher at Ruhr Uni Bochum, provided crucial support for this part of the investigation. Completed by traditional reporting methods (the reporters talked to more than 30 sources for this story) the investigation paints a comprehensive picture of the Winnti threat.   ","   The publication is proof of a new and innovative way for journalists to find, gather and verify information on cybersecurity. Thus, the submission as innovative data journalism project for the Sigma awards. The team of journalists developed new ways of technical reporting: They wrote computer programs that searched for patterns in malicious software, helped by a researcher of Ruhr Uni Bochum. They also found further Winnti targets by taking advantage of a toolset published by IT security experts for checking if their own networks are infected by the malware. For the first time ever, the reporters demonstrated to the German public in such close detail how IT forensic analysis works and how to investigate hackers.     After the publication of the project, the reporters released the used tools and scripts on BR Data's GitHub page: <a href=""https://github.com/br-data/2019-winnti-analyse"">https://github.com/br-data/2019-winnti-analyse</a>     Combining these technical methods with traditional reporting, the reporters managed to get the story done. The hardest part of the project was probably finding the Winnti samples and developing sources who would help with that.       ","   The project shows that there are ways to report about highly technical topics like cybersecurity without having to rely solely on the judgments of experts. By combining traditional research techniques with coding skills and technical investigative methods, journalists nowadays are able to corroborate the information themselves. It’s proof that data journalism today can be more than merely crunching statistics and building fancy graphics. In investigative newsrooms, data journalism has evolved into a reporting method for finding information and providing scoops that would not have been possible without code-savvy journalists.   ",http://web.br.de/interaktiv/winnti/english/,https://www.tagesschau.de/investigativ/ndr/winnti-101.html,https://www.br.de/radio/b5-aktuell/sendungen/der-funkstreifzug/cyber-spionage-deutsche-industrie-100.html,https://github.com/br-data/2019-winnti-analyse,https://www.reuters.com/article/us-germany-cyber/basf-siemens-henkel-roche-target-of-cyber-attacks-idUSKCN1UJ147,https://www.tagesschau.de/investigativ/ndr/hackerangriff-chemieunternehmen-101.html,,"Hakan Tanriverdi, Maximilian Zierer, Rebecca Ciesielski (BR), Jan Strozyk, Svea Eckert (NDR)","     Hakan Tanriverdi   is the leading reporter of this project. Hakan works as a cybersecurity reporter for BR Data/BR Recherche, the data and investigative unit of German public broadcaster Bayerischer Rundfunk. In the past, he has worked for Süddeutsche Zeitung as a cybersecurity reporter in New York City.      Maximilian Zierer   is a data and investigative reporter at BR Data/BR Recherche covering a wide range of topics. Apart from the reporting, he produced the long form radio pieces for this project.     Rebecca Ciesielski   works as a science editor for German public broadcaster ZDF. As part of her training period, she spent several months at Bayerischer Rundfunk, where she contributed to the Winnti investigation with her reporting and coding skills.     Jan Strozyk   covers white-collar crime for German public broadcaster Norddeutscher Rundfunk (NDR). In the past, he was part of the team that worked on both Panama and Paradise Papers. Jan produced most of the news writing for this investigation. He also wrote scripts to analyze the Winnti malware.     Svea Eckert   covers hackers and data leaks for NDR. Svea presented her research repeatedly on top tier conferences like DEFCON and Chaos Communication Congress. In this project, Svea was responsible for the TV production and reporting.   ",,,
Bosnia and Herzegovina,"Organized Crime and Corruption Reporting Project, Kloop",Big,Participant,Best data-driven reporting (small and large newsrooms),"Public Land, Private Hands",23/10/19,"Investigation,Long-form,Multiple-newsroom collaboration,Quiz/game,Database,Open data,Illustration,Infographics,Map,Satellite images,Environment,Corruption","Drone,Scraping,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,OpenStreetMap"," Over several years, nearly a third of a beloved park in Bishkek, the capital of Kyrgyzstan, was turned into an elite private neighborhood.   The land was given away with no oversight or public explanation, and the names of the recipients were never released.    Reporters tracked down the history of each plot, showing that nearly half were given to government officials, public figures, and businessmen or their relatives — for free.   Reporters produced an interactive map of the entire territory with detailed profiles of each recipient.   Four accompanying stories explain how this was allowed to happen — and who was responsible. "," The publication provoked an immediate, angry reaction on social media, receiving thousands of shares and comments in which Kyrgyz citizens voiced their disgust with official corruption and with the destruction of the formerly public land.   Kyrgyz reporters took creative opportunities to further amplify the reach of the project by conducting an in-person tour of the elite neighborhood in the park and streaming it online.   As is typical in Central Asia, however, the reaction from the government was muted. Almost uniformly, politicians and officials declined to comment or offered unconvincing evasions. President Sooronbai Jeenbekov, who received one of the plots, was pressed to explain himself by multiple outlets. Though he promised to share legal documents showing that he did not profit very much from his plot, he has not done so.   In the days after publication, Kloop's website was targeted by a DDoS attack and rendered inaccessible for around 24 hours, suggesting that the reporting had hit a sore spot.    Though there is little prospect for immediate change in the Ataturk Park case, the project has been <a href=""https://eurasianet.org/kyrgyzstans-journalists-dare-where-authorities-do-not"">recognized</a> as having ""broken new ground"" for the region journalistically. In a first for a project of this scope, the ""<a href=""https://eurasianet.org/kyrgyzstans-journalists-dare-where-authorities-do-not"">meticulous and in-depth reporting</a>"" was carried out entirely by local journalists — who conceived it in the first place — with the guidance of international editors and support staff.   The project laid the groundwork for a new generation of independent investigative journalism in a region that badly needs it.     "," Reporters spent nearly a year using a variety of tools, techniques, and sources to trace the history of the many dozens of privatized plots in Ataturk Park.   The process began by pulling the identification numbers of the plots from cadastre maps and using these to purchase individual land records for each plot — a costly and lengthy process.   This information was placed into a master database that was used to establish each plot’s history, identify patterns and priority areas, and begin building the map of the park.   This information also contained references that enabled reporters to dig deeper. Now knowing where to look, they obtained and reviewed thousands of pages or decrees, court records, and other government documents and filed many dozens of information requests to learn which mechanisms — and which officials — were responsible for giving away each plot.   Meanwhile, each person identified as a recipient became an entry in a separate database that tracked their family connections and businesses.   To write full profiles for each recipient, reporters interviewed dozens of people and even went undercover, posing as buyers, to get a glimpse of the park’s new crop of mansions and villas.   Finally, to create the interactive map, a drone was used to film the park and compare what the records showed with reality. Sophisticated stitching and tiling techniques were required to make the map available online. A free open-source library by Knight Lab called Storymap.js was used to create an interactive layer over the map to display icons, hot spots, and profile information. "," The inaccessibility of government documents presented this project’s most serious challenge.   Obtaining the initial decrees on the basis of which the land plots were awarded was the most difficult part. Though these documents should have been public, they were not. When reporters tried to obtain them through the land registry, officials refused to provide them. Information requests to the mayor’s office also resulted in refusals. Reporters then had to knock on the doors of dozens of officials’ offices in the local administration before being directed to archives where some of the documents were available.   Since some of the plots had been merged or split, arriving at a final, definitive list of privatized plots was an additional challenge.   Some of the land records purchased online also turned out to be missing critical information or to have had underlying data removed entirely. Additional information requests became necessary to fill in the gaps.   All of this work — resulting in the most comprehensive set of land records for a journalistic project in Central Asia — was done by very young reporters who had never before done this kind of work. They were essentially learning on the spot.   Finally, publishing the project simultaneously in English, Russian, and Kyrgyz — while fixing last-minute technical issues with the map, fact-checking each profile, and addressing usability concerns — was a major technical and project management challenge. The collaboration of a large team of reporters, editors, technical staff, and others was essential to getting it done. "," One critical lesson is about database design. Unlike many other data-driven journalistic projects, the reporters in this case were not analyzing an already-existing database — they had to create it from scratch and populate it manually.    To fulfill the project’s journalistic goals, the database had to be structured in a specific form, and this had to be carefully thought through ahead of time to avoid repopulating or reconstructing the entire thing. To solve this challenge, the reporters and editors held a series of brainstorming sessions ahead of time to figure out which information they needed.   Another important lesson is to understand how the system works — before embarking on an investigation. It is important to know which agency has which information, how to obtain it, and who else might have it. The reporters had to undergo a crash course about Kyrgyz urban planning — who can approve what, who holds which records, which ones are legally required to be public, and how to obtain them.   Persistence is important too. When some officials withheld important documents, reporters had to make repeated requests, often showing officials excerpts from the law and arguing that the information should be public. In some cases, this aggressive approach was successful.   But creativity is important too. When some documents could not be obtained from official channels, reporters approached the homeowners directly and, acting as potential buyers, were able to obtain proofs of ownership and other materials.   A final lesson is that storytelling is no less important than data analysis. And reporting for storytelling should start at the beginning of the project. Data projects can sometimes be dry, so looking for characters and color is important. ",https://www.occrp.org/en/public-land-private-hands/,https://www.occrp.org/en/public-land-private-hands/ataturk-map/,https://www.occrp.org/en/public-land-private-hands/the-park-that-was-lost,https://www.occrp.org/en/public-land-private-hands/the-officials-who-gave-It-away,https://www.occrp.org/en/public-land-private-hands/big-man-no-campus,https://www.occrp.org/en/public-land-private-hands/from-public-park-to-millionaires-row,https://kloop.kg/blog/2019/10/30/soberi-uchastok-chinovnika-v-parke-atatyurk/,"Metin Dzhumagulov, Alexandra Li, Danil Lyapichev, Olga Gein, Anna Kapushenko, Eldiyar Arykbaev, Miranda Patrucić, Julia Wallace, Ilya Lozovsky, Kira Zalan, Jodie DeJonge, Vlad Lavrov, and Edin Pašović"," This project was produced by reporters from Kloop, OCCRP’s Kyrgyz member center, along with editors, fact checkers, and other staff from OCCRP itself.   OCCRP is a non-profit media organization dedicated to reporting on organized crime and corruption around the world. It provides an investigative reporting platform for the OCCRP Network: non-profit investigative centers in more than 30 countries, scores of journalists and several major regional news organizations across Europe, Africa, Asia, the Middle East and Latin America. OCCRP works to turn the tables on corruption and build greater accountability through exposing the abuse of power at the expense of the people.   Kloop is one of the few independent news outlets in Central Asia, known for its strong reporting on politics and human rights. The organization also prides itself on hosting a media school that aims to train the next generation of Central Asian journalists. ",,,
Austria,Der Standard,Big,Participant,Best visualization (small and large newsrooms),"„Die Welt bangt um die drei Astronauten"" // „The World Fears for the Three Astronauts""",19/07/19,"Explainer,Long-form,Infographics","D3.js,Json,Google Sheets,Node.js"," The article collects over 400 excerpts of contemporary documents from the days surrounding the first manned moon landing on July 20th, 1969 and presents them chronologically as a journey through this period of time. The documents encompass official recordings of transmissions between the crew of Apollo 11 and mission control as well as the technical protocols, miscellaneous photographs and headlines of newspaper articles in selected US-American and Austrian newspapers.   The material is presented in a mix of German and English, using German translations of the audio transcripts and the technical protocols while keeping the original language of the newspaper headlines. "," Since Austrian newspaper articles from the time are mostly only available in physical storage in the Austrian National Library, the article offers a unique review of how the moon landing was received in Austria’s media landscape and how this compared to the media response in the USA and the sequence of events on board of the Apollo 11 mission. "," The elements of information are placed along a timeline using the d3-scale library which also allowed us to easily employ a second, stretched time scale for the night from the 20th to the 21st of July to account for the the greater amount of material associated with the hours around the actual moon landing.   The list of displayed elements was compiled in a Google Spreadsheet and cleaned up and formatted into a JSON-file with consistent timestamps and all necessary information via a Node.js-script. "," Austrian newspaper articles from the sixties are in large parts not yet digitized, so the headlines had to be researched in the Austrian National Library, where newspaper pages from the time are kept on microfilm and microfiche.     The placement of those newspaper clippings on the timeline also provided a challenge and required careful consideration since the media response to events on the mission was usually delayed by at least a few hours. "," Because the sheer volume of elements even after selection resulted in a prohibitively long piece, we decided to offer two distinct reading modes to our users: The default „highlight“-mode used about 50 elements out of the 400 to provide a more streamlined experience and contains the most relevant information. The additional material stays visible as ticks on the overarching timeline and the reading mode can be switched seamlessly at all times with the click on a button. This way, readers always have the opportunity to gain a comprehensive view of some of the more absurd pieces of coverage, e.g. a report that there won’t be a change in working hours in Austria because of the moon landing. ",https://derstandard.at/interaktiv/stories/2019-07-zeitleiste-mondlandung/,,,,,,,"Daniela Yeoh, Sebastian Kienzl","  Daniela Yeoh  studied political and communication sciences and has been working at  Der Standard  for more than 20 years. At the newspaper, she has worked in positions as a science journalist, editorial manager of the feminist section  Die Standard  and as a technical project manager. Lately she has brought her editorial experience to the data journalism team where she researches data-driven stories, often with a community-focussed approach.    Sebastian Kienzl  studied graphic design and mathematics and works as a data journalist and developer for interactive graphics in the data journalism team of  Der Standard . The main focus in his work is the design and implementation of data visualizations and online storytelling formats. ",,,
Poland,"Gazeta Wyborcza, Le Figaro, Le Soir, El Pais",Big,Participant,Best data-driven reporting (small and large newsrooms),"From people with cow heads to bleach treatment. When epidemics come back again, the days of the anti-vaxxers will be numbered.",21/12/19,"Investigation,Explainer,Long-form,Cross-border,Multiple-newsroom collaboration,Documentary,Database,Open data,Fact-checking,Infographics,Chart,Map,Health","Scraping,Adobe,Microsoft Excel,Google Sheets","  Vaccination in Europe, increasing problem with anti-vaccination movement.  Cooperation four newsrooms in Europe: Gazeta Wyborcza, Le Figaro, Le Soir and El Pais.     History of immunization across Europe with focus on the anti-vaccinations movement, number of cases per 100.000 citizens during the years, number of deaths, date of vaccine introduced, mandatory vaccinations schedule, vaccination coverage levels. We also created map with vaccination coverage levels in every EU countries, separate for different diseases and timeline with situation in countries. "," This investigation was a collaborative work four data journalism departments from Poland (Gazeta Wyborcza), Spain (El Pais), Belgium (Le Soir) and from France (Le Figaro). Measles is an illness for which humanity has had an effective vaccine for five decades. But in 2018, nearly 9 million people developed measles world-wide, and 142,000 died. On average, almost 400 people die every day; most of them children.  We collect data from health public institutions and arguments from ant vaccination movements, sceptics parents and rules in public institutions – for example schools. We collect also trick how parents try to avoid vaccinations their kids. We present myths and science facts.   The first publication was in Poland in Gazeta Wyborcza, on paper and also on our data journalism department page (biqdata.wyborcza.pl) with interactive charts, in Polish and  English. Then our partners published their materials with a local focus.   The public discussion about vaccination, actions and campaigns founds by antivaccination movement are in media, open areas in every European country. It is very dangerous because if general vaccination coverage will decrease above 95% all of us are not safe, ale people vaccinated years ago, the kid with another serious willes who could not be protected.   Our readers are extremely interested in health topics. Antivaccination movements are a global problem, also indicate by WHO in their recommendation. People based on myths and scam, exposing theirself and others for dangerous.       "," We based on open datasets - statistic provided by national health institutions and official European websites. We also checked all the anti vaccinations page, network, connections, publications and actions.   Charts were prepared in open tools - tableau public and datawrapper.de.   Publication in the paper was prepared in adobe. During our work, we met with antivaccination activists, academics, and other authorities.         "," The hardest part was to contact with antivaccination movement and collecting their arguments, myths. We also tried to find the connections and network across countries. We find out that they still based on the same scam - In 1998, Andrew Wakefield published the results of his research on children in the medical journal The Lancet. In 2010, Wakefield was struck off the medical register and stripped of the right to practice for the rest of his life. But the rumours about the association of autism disorders with vaccinations spread around the world, causing the development of antivaccination movements. And young parents still belives for that.  "," This problem was that antivaccination movement is not the same news in our four countries. In Poland it is a hot topic, but for example in Belgium not. So, if you decide to analyze the same problem in different countries and create impressive materials with high quality for a difference audience, you must try to find local aspects, not just translate one version with a unique focus. Every partner adapted our collective work for their readers.  ","https://biqdata.wyborcza.pl/biqdata/7,159116,25651896,from-people-with-cow-heads-to-bleach-treatment-when-epidemics.html","https://biqdata.wyborcza.pl/biqdata/7,159116,25523745,od-ludzi-z-krowimi-glowami-po-leczenie-wybielaczem-gdy-epidemia.html",https://elpais.com/internacional/2020/01/03/sonar_europe/1578065782_443367.html,https://plus.lesoir.be/269743/article/2019-12-29/la-confiance-envers-les-vaccins-un-cycle-sans-fin,https://www.lefigaro.fr/actualite-france/la-france-premier-pays-anti-vaccins-en-europe-20200107,,,"Danuta Pawlowska (leader, main author) cooperation with: Cedric Petit, Daniele Grasso, Marie Coussin"," Danuta Pawłowska - head of reports department and editor in data journalism department in Gazeta Wyborcza - BIQdata.wyborcza.pl.   After graduate technical university, she used to be an auditor in Deloitte and verify financial reports. She is also experienced in preparing economic rankings. As a journalist, she has been working on EU-related subjects and spent a few months in the European Parliament as a Schuman Scholarship fellow.      ",,,
United States,The Trace and BuzzFeed News,Big,Participant,Open data,Free to Shoot Again,24/01/19,"Investigation,Long-form,Multiple-newsroom collaboration,Database,Open data,Infographics,Chart,Crime,Gun violence","Microsoft Excel,CSV,R,RStudio,Python"," Across America, the odds that a shooter will face justice are abysmally low and dropping. Police make an arrest in fewer than half the murders committed with firearms. If the victim survives being shot, the chance of arrest drops to 1 out of 3. Thousands of nonfatal shooting cases every year are never investigated. The shooters are left free to strike again, fueling cycles of violence and eroding the public’s trust in law enforcement.   The Trace and BuzzFeed News’ series “Free to Shoot Again” interrogates this failure in policing with a data-driven investigation unprecedented in its methodology, scope, and findings. "," Our sweeping national findings are illustrated through nine shootings in Baltimore linked by a common victim or suspect. Police had closed just two of the nine cases. One case was reopened after we found the shooting had been pinned on a dead man without any evidence. In the other closed case, the man who had been serving a life sentence for the murder was <a href=""http://www.thetrace.org/rounds/subject-of-trace-buzzfeed-investigation-found-innocent-in-murder-retrial/"">freed from prison</a>, in part by citing information we uncovered.   Baltimore leaders <a href=""http://www.thetrace.org/rounds/baltimore-shootings-impact/"">called for more resources</a> toward solving shootings and for an entity to review open cases. The police department has since added more detectives to investigate homicides.   Due to the complexity of the analysis, we felt a strong need to share our materials and methods with our readers. Between the documents, data, code, and detailed methodologies we made public, we believe this is among the most extensively documented data-driven investigations to date.   Our analysis on linked shootings was the first to take suspects into account. Andrew Papachristos of Northwestern University, whose network analyses on shootings informs violence intervention efforts across the country, said he plans to incorporate our methodology into future work. Daniel Webster of Johns Hopkins said he used the materials we posted for the analysis in a report to the mayor and police chief on strategies to reduce gun violence.   Detailed data on violent crimes can be difficult to obtain. For that reason, we made nearly all of the raw data we obtained public: 4.3 million murders, rapes, robberies, and assaults from 56 agencies. The data was downloaded more than 500 times within the first month of posting, and numerous researchers, journalists, and advocates have written to us about the data. Webster said he is using it for a multi-year research proposal. Crime analyst Jeff Asher called the release “a remarkable service.” "," To examine long-term trends in clearance rates, we analyzed data from three major FBI crime-reporting programs: the Supplementary Homicide Report, the National Incident-Based Reporting System, and Return A. Each dataset is structured completely differently, so we standardized the FBI’s raw files into order to compare results between the three datasets. We also used rigorous statistical controls to ensure the integrity of the analysis. We posted our <a href=""https://github.com/the-trace-and-buzzfeed-news/federal-crime-data-analysis"">standardized data, the code to produce the standardized data</a>, and an <a href=""https://www.documentcloud.org/documents/5692683-Methodology-for-National-Analysis-of-Clearance.html"">extensive methodology</a> on GitHub.   To examine present-day trends in clearance rates in the nation’s largest cities, we standardized data from 22 police departments. This remains the largest-scale analysis of clearances for nonfatal shootings that we’ve come across. We also compared the results with our analysis of FBI data and found them to be substantiated. We posted the <a href=""https://github.com/the-trace-and-buzzfeed-news/local-police-data-analysis"">standardized data, code</a>, and <a href=""https://www.documentcloud.org/documents/5692688-Methodology-for-Local-Police-Data.html"">methodology</a> on GitHub.   To examine the linkages between fatal and nonfatal shootings, we created a network analysis of Baltimore Police data that included the names and birthdates of shooting victims and suspects. That analysis was the first of its kind to be published; others, by various academics, did not take suspects into account. We posted the <a href=""https://github.com/the-trace-and-buzzfeed-news/baltimore-shootings-analysis"">data, methodology, and code</a> on GitHub.   To examine the over-policing/under-policing paradigm that exists in black and Latinx neighborhoods across America, we used the following datasets: crime incidents, homicide and shooting case characteristics, and sworn officer assignments from the Chicago Police Department; police district and census tract boundaries from the City of Chicago; Census statistics from the Census Bureau; and sunrise and sunset times from sunrise_sunset.org. We posted the <a href=""https://github.com/TeamTrace/Chicago-Over-Under-Policing"">data, methodology, and code</a> on GitHub.   We did our analysis in Python and R. "," We filed more than 200 public records requests for data, case files, staffing audits, and database documentation. Nearly every agency gave us significant resistance and frequently quoted exorbitant fees. For instance, the Baltimore Police Department initially demanded $1,800 for one murder case file. Negotiations and appeals stretched on for longer than a year.   Once we got data, we often found problems that were difficult to resolve, such as major discrepancies with the agency’s reported statistics. Some agencies produced corrected versions of the data. But we often had to “appeal” missing incidents as an incomplete fulfillment of our request. We also threw out numerous datasets because of integrity issues.   We wanted to illustrate how law enforcement’s failure to arrest shootings fuels vicious cycles of unchecked gun violence. But we had to examine dozens of datasets to find one with all the information necessary to conduct a network analysis. This took months.   Once we decided on Baltimore, we encountered significant challenges in the field reporting. The areas where the shootings occurred have among the highest rates of poverty and gun violence in the nation. Residents often have insecure housing situations, so it took weeks of door-knocking and poring through court records to find key individuals. Many were deeply traumatized, and scared of retaliation, so we had to approach our interviews with sensitivity and deference to their concerns.   Initially, the Maryland Division of Corrections said we could not interview Devon Little, an inmate at the time. Gaining approval, partly by working through his mother, took months.   The Baltimore Police initially refused to work with us because we were national media outlets. It took months of persistence just to get a short interview. More on-the-record responses came only after we repeatedly demanded answers to our findings. "," We’ve shared our methods and materials for this project online and in presentations, which has helped other people in their work in many ways:  <ul>    The FBI analysis: Our <a href=""http://methodology"">extensive methodology</a> details important nuances in three of the FBI’s most important crime datasets. The raw formats of the datasets require advanced coding skills to process and analyze. We’ve shared <a href=""https://github.com/the-trace-and-buzzfeed-news/federal-crime-data-analysis"">standardized versions of the data and relevant code</a> so our work can be replicated.       The analysis of internal police data: Our <a href=""https://www.documentcloud.org/documents/5692688-Methodology-for-Local-Police-Data.html"">methodology</a> identifies differences across agencies that must be considered when combining them. This data has already been used by numerous researchers and journalists for their own work.       Network analysis: Our <a href=""https://github.com/the-trace-and-buzzfeed-news/baltimore-shootings-analysis"">methodology and code</a> can be used to analyze networks of crime victims and suspects, which is particularly important for shootings, which have a high rate of retaliatory violence.        Over-policing/under-policing analysis: Our <a href=""https://github.com/TeamTrace/Chicago-Over-Under-Policing"">methodology, data, and code</a> shows how data from multiple sources can be integrated into a single analysis. Some of the data was obtained through public records requests, and is available for use.       The <a href=""https://www.thetrace.org/violent-crime-data/"">raw violent crime data</a> has aided countless research and reporting efforts. Numerous journalists have told us it’s also helped them get more data out of the agencies that they cover.       In January 2020, we <a href=""https://www.thetrace.org/2020/01/police-data-documentation-public-records-requests/"">published an article</a> in partnership with MuckRock that links to <a href=""https://www.documentcloud.org/search/Project:%20%22Police%20Database%20Documentation%22"">thousands of pages in documentation</a> detailing the architecture of the most common police records- and case-management databases. This documentation can be key to winning public records battles for police data.       That article, along with <a href=""https://docs.google.com/presentation/d/1jea-8ivPxqONKaP_eSdsUnzUnE-hP2PkQsCE_sO3eHg/edit?usp=sharing"">numerous presentations</a> that we’ve given at conferences and classrooms, gives detailed strategies on making public records requests for data. People regularly contact us seeking further advice on their own requests.       We spoke at NICAR 2019 on <a href=""https://docs.google.com/presentation/d/1drquKLVXOOsJ5F9wXkFkM2sVr0Yq5CPg6xsYMnsO_aM/edit?usp=sharing"">finding human stories in data</a>, based on our reporting in Baltimore.    </ul>",https://www.thetrace.org/features/murder-solve-rate-gun-violence-baltimore-shootings/,https://www.thetrace.org/2019/01/gun-murder-solve-rate-understaffed-police-data-analysis/,https://github.com/the-trace-and-buzzfeed-news/introduction,https://www.thetrace.org/2019/11/most-shooters-go-free-in-chicagos-most-violent-neighborhoods-while-police-make-non-stop-drug-arrests/,https://github.com/TeamTrace/Chicago-Over-Under-Policing,https://www.thetrace.org/violent-crime-data/,https://www.thetrace.org/2020/01/police-data-documentation-public-records-requests/,"Sarah Ryley, Jeremy Singer-Vine, and Sean Campbell","  Sarah Ryley  was an investigative journalist at The Trace. Prior to joining The Trace, she was the data projects editor at the New York Daily News, where her work on the NYPD’s use of “broken windows” policing triggered numerous reforms, including the passage of 23 new laws. Her work has been the recipient of the Pulitzer Prize for Public Service, two Sidney Awards, and the James Aronson Award for Social Justice Journalism. She has taught investigative reporting at The New School and data journalism at CUNY J+, and is a frequent speaker at conferences and universities.    Jeremy Singer-Vine  is the data editor for the BuzzFeed News investigative unit. Before joining BuzzFeed News in 2014, he worked on investigations at the Wall Street Journal. Stories he has co-reported have received a National Magazine Award, two Sidney Awards, and two Scripps Howard Awards ; a series he co-reported at the Wall Street Journal was named a finalist for the Pulitzer Prize for National Reporting. He’s also the author of Data Is Plural, a newsletter that highlights useful and interesting datasets.    Sean Campbell  was a senior investigative fellow at The Trace. He is currently a senior reporting fellow at ProPublica and an adjunct assistant professor in the Columbia Graduate School of Journalism. He received his Master of Science in data journalism from Columbia Journalism School. ",,,
Austria,Der Standard,Big,Participant,Best visualization (small and large newsrooms),Rundreise um den Mond // Trip around the moon,17/07/19,"Explainer,Long-form,Infographics,Map,Satellite images","Animation,D3.js,Three.js,Json,Node.js"," On occasion of the 50th anniversary of the first moon landing, the article takes readers on a trip around the moon and aims to convey part of the fascination that has drawn researchers to the earth’s satellite. Datasets from scientific and government sources were compiled and worked into an interactive storytelling format. The project was published in German and is split into the following parts:   (for the purpose of an overview in English, this is continued in the next section ""Impact"") "," (cont. from ""Short Description"")  <ul>  Part one revolves around the various surface features of the moon and their evocative naming conventions. The section uses elevation data recorded by the LRO satellite mission.   The second part addresses the moon’s geology and formation history using imagery of the moon’s gravitational field that hints at the origin of some of the largest observable features on the moon such as the  maria  on its earth-facing side.   The final part is about the human presence on the moon, starting with the first impacts of Soviet probes and manned US-missions and leading up to the most recent exploration efforts involving Indian, Chinese and Israeli landers.   </ul> <hr />   What was the impact of the project?    The article was published as part of our coverage of the anniversary of the first moon landing and was received very positively by our community of online readers. As one of the team’s most technically ambitious projects to this date, it expanded our confidence in approaching such ideas and the editorial staff’s flexibility in conceptualizing our online storytelling.  "," The visualization was realized using different web technologies, with the JavaScript-library three.js and its 3D rendering capabilities being especially important. The moon globe was set up as a 3D sphere which, in combination with a color and a bump map is rendered as an adequately detailed model of the moon which can be interacted with by the user at every moment.   The data used was either directly acquired or collaboratively compiled in Google Spreadsheets and subsequently cleaned and formatted for use using short scripts in the Node.js-environment using d3. "," One of the greatest challenges was synthesizing the available data and displaying it on the moon’s surface accurately. This meant considering the coordinate system used on the moon, cross-checking with different, visual sources and even adjusting the focal length of the virtual camera and its distance from the globe to approximate a natural-looking perspective on the moon’s features.   An additional challenge was delivering high-fidelity 3D-visuals in the browser – especially on mobile devices – without relying on big network payloads and large amounts of processing power. This was especially difficult considering the multiple layers of imagery used on the moon globe. There a color map and a bump map are employed to give the impression of a highly detailed model that can be dynamically lit in a 3D-scene. A lot of time went into image processing and deliberations concerning the tradeoff between fidelity and usability to arrive at a reasonable balance. "," The article, as an explainer, aims to convey a large amount of information and employs a few techniques to make it more palatable and enjoyable to follow:  <ul>  the storytelling is anchored by a dominant visual that mirrors the information given in text (color-coded in both versions), so while scrolling multiple channels can be used to attract the viewer’s attention   in order to still offer comprehensive information, lots of content is hidden behind interaction with the globe for individual discovery   changes in the nature of information conveyed is always accompanied by a distinct visual change   </ul>",https://derstandard.at/interaktiv/stories/2019-07-rundreise-zum-mond/,,,,,,,"Daniela Yeoh, Sebastian Kienzl","  Daniela Yeoh  studied political and communication sciences and has been working at  Der Standard  for more than 20 years. At the newspaper, she has worked in positions as a science journalist, editorial manager of the feminist section  Die Standard  and as a technical project manager. Lately she has brought her editorial experience to the data journalism team where she researches data-driven stories, often with a community-focussed approach.    Sebastian Kienzl  studied graphic design and mathematics and works as a data journalist and developer for interactive graphics in the data journalism team of  Der Standard . The main focus in his work is the design and implementation of data visualizations and online storytelling formats. ",,,
Canada,http://www.concordia.ca/artsci/journalism/research/investigative-journalism/projects/water-credits.html?utm_source=vanity&utm_campaign=watercredits,Big,Participant,Best data-driven reporting (small and large newsrooms),Tainted Water,11/04/19,"Investigation,Explainer,Solutions journalism,Long-form,Breaking news,Cross-border,Multiple-newsroom collaboration,Documentary,Database,Open data,Fact-checking,Infographics,Chart,Video,Map,Politics,Environment,Business,Culture,Agriculture,Employment,Human rights","Animation,Scraping,QGIS,Adobe,Microsoft Excel,Google Sheets"," Canadians have every reason to believe that the water that runs from their taps is beyond reproach: abundant, clean and safe. But the “Tainted Water” investigation, an unprecedented national collaboration of universities and news organizations, exposed the risks faced by millions of Canadians whose drinking water contains elevated levels of lead, a powerful, insidious neurotoxin, and other contaminants. "," An analysis revealed that in the cities of Montreal, Regina, Moose Jaw, Saskatoon and Prince Rupert, along with parts of Gatineau, lead levels in water were comparable to or higher than they were in Flint, Michigan during its crisis in 2015. Further reporting uncovered a staggering lack of transparency by municipal and provincial governments, with widespread failures to warn residents about lead levels and the associated health risks. In some cases, municipal workers were found to be relying on discredited testing methods that provided misleadingly low results. The investigation found that the federal government was choosing not to intervene with mandatory rules and standards. The investigation also focused on schools, where researchers have repeatedly warned governments for decades that older drinking fountains have been contaminating children’s drinking water with lead -- and children are particularly susceptible to the health effects of the neurotoxin, which is associated with learning disabilities. Data from the freedom-of-information requests revealed that schools and daycares from Nova Scotia to British Columbia had measured levels of lead in their drinking water up to tens of thousands of times the national standard, but outside Ontario, school authorities rarely shared results with parents or warned them about what had been found. Internal documents from provincial offices in Alberta and Quebec included discussions about research showing that levels were high enough that the IQs of children and toddlers could be permanently lowered, but no public alert had been issued. In Alberta, schools had not been directed to test for the neurotoxin despite these findings. "," MS Excel and Access, Tableau, Google Sheets, Google My Maps, QGIS for preparing geospatial data and converting between formats, online data scraping tools   Initial data compilation and analysis was done principally by the consortium over a period of eight months. Our analyses were then sent for validation and comment to leading academics in Canada and the U.S. whose work on lead in drinking water is widely respected -- in most cases, they had advised the Canadian cities in question about how to reduce lead levels. Further validation and checking was completed by the participating media partners. Finally, the analyses were sent to municipal and provincial authorities for comment.    "," Municipal test data was obtained from individual municipalities and in a few cases, from provincial departments regulating drinking water quality. In total, the consortium obtained the results of 79,000 tests, and our freedom-of-information requests cost CAN$9,000.    The biggest obstacles we faced were a lack of data and poor-quality data -- many municipalities do not test at all, and the Province of Quebec had for decades employed a testing method that was designed to measure the minimum level of residential exposure, rather than the maximum. This method has been widely discredited and in the United States, authorities have faced charges for taking similar actions that would lower results.    To gather reliable data for our database, the consortium engaged in a groundbreaking project to empower hundreds of concerned citizens to test their own water. Student journalists used municipal property records to identify older homes that were likely to have lead pipes. Coached by the students and member journalists, residents in 32 cities across the country collected their own samples, which were then taken by the team to accredited commercial laboratories for analysis. The consortium spent more than CAN$50,000 on water testing at commercial laboratories.   In all, we built two databases -- of the 79,000 tests gathered by municipal workers and our own -- plus repositories of survey results, interviews and documents. More than 220 hours of interview audio was transcribed, at a cost of nearly CAN$20,000. ","The consortium of universities and media companies that came together to research, produce and publish ""Tainted Water"" was conceived as a uniquely Canadian solution to the challenges posed by the nation's vast distances and scattered subscriber bases, compounded by the financial pressures facing the news industry. The size of the consortium, regional differences in approaches to reporting and wide variety in experience of members were all obstacles that we overcame together, in weekly national meetings, two editorial gatherings, and tens of thousands of messages and calls. But one of the biggest obstacles that student journalists and professionals faced was disbelief -- that there could be contaminants in Canada's famously pure water and that trusted municipal officials would withhold such information. The nationwide effort fostered deep public engagement, as the massive citizen science project provided hundreds of residents with the information they needed to make good decisions for their families and as voters. At the heart of the issue is Canada's weak freedom-of-information laws -- our nation has lagged behind most other developed countries, ranking 55th on the Centre for Law and Democracy's scale, published in 2018 with Access Info Europe. Marc Edwards, a researcher from Virginia Tech who is widely credited with uncovering the crisis in Flint, told the Regina Leader-Post that the city's elevated lead levels ""would be publicly disclosed in the US."" He continued, ""It's worse than Flint, and you saw how people were criticized for not disclosing that problem in Flint."" Only a collaboration among many of the country's leading media companies, working in concert with smaller regional news organizations, could have uncovered the scale of this hidden problem. This was an unprecedented learning experience for more than 100 journalism students, demonstrating to professional reporters and students what can be accomplished when journalists work together in the",https://apnews.com/24628f49af1e45219ee4b06c0a9a1229,https://www.thestar.com/news/investigations/2019/11/04/is-there-lead-in-your-water-canada-wide-investigation-exposes-chronic-extreme-exceedances-of-toxic-metal.html,https://www.thestar.com/news/investigations/2019/11/04/how-lead-levels-in-5-canadian-cities-compared-to-those-in-flint-michigan.html,https://globalnews.ca/news/6126731/children-school-daycares-unsafe-water-canada/,https://globalnews.ca/news/6037087/investigation-lead-drinking-water-quebec/,https://leaderpost.com/news/saskatchewan/parched-experts-urge-transparency-on-lead-tainted-water-in-some-regina-saskatoon-and-moose-jaw-homes,https://www.youtube.com/watch?v=l0h55NoUBmg&t=8s,http://www.concordia.ca/artsci/journalism/research/investigative-journalism/projects/water-credits.html?utm_source=vanity&utm_campaign=watercredits," Coordinated by the Institute for Investigative Journalism (IIJ), “Tainted Water” is the largest project of its kind in Canadian history, and possibly the largest student-led project worldwide. The consortium brought together more than 120 journalists, student journalists and faculty members from nine post-secondary institutions and six news organizations and their bureaus over a period of 18 months to report the series. Journalism students and reporters combined their findings and produced local, regional and national investigative features, released as a series of print, digital and TV stories, making international headlines. The IIJ unites media organizations and journalism schools across Canada in order to provide students with practical training and carry out large-scale investigations in the national public interest. ",,,
United States,"International Consortium of Investigative Journalists, The Intercept, NBC News, Univision, WNYC, Grupo SIN, Plaza Pública, Mexicanos contra la corrupción",Big,Participant,Best data-driven reporting (small and large newsrooms),Solitary Voices,21/05/19,"Investigation,Cross-border,Multiple-newsroom collaboration,Database,Immigration,Human rights","Microsoft Excel,Google Sheets,Python"," Solitary Voices exposes how U.S. immigration authorities use solitary confinement as a go-to tool to manage and punish the most vulnerable detainees, with devastating consequences.   An ICIJ analysis of ICE records showed that the agency has locked thousands of mentally ill, LGBTQ and disabled detainees in isolation for weeks and months. More than half of the isolation reports described stays that lasted longer than 15 days. The United Nations has said solitary confinement stays longer than 15 days amount to torture and should be banned, and ICE itself says it should be used only as a last resort. "," The project resonated in Washington, D.C., where leading lawmakers seized upon our findings to press ICE to explain the abusive practices outlined in the reporting, and to introduce legislation in the U.S. Senate to stop them.    Citing ICIJ reporting in a letter to the head of ICE, Senator Elizabeth Warren (D-MA) condemned the use of solitary confinement on immigrant detainees as “cruel and unnecessary,” and asserted that ICE is likely violating its own detention directives. Chuck Grassley (R-IA) and Richard Blumenthal (D-CT) followed up with another letter to the agency, demanding answers on “recent allegations of the misuse of solitary confinement.”   In November, Kamala Harris (D-Calif.), Cory Booker (D-N.J.) and Dick Durbin (D-Ill.) cited Solitary Voices in legislation that takes aim at a range of practices identified in our investigation. In releasing the bill, the senators called ICE's use of isolation “rampant and unnecessary.” The bill is highly detailed and would outlaw locking detainees in solitary confinement in most instances as a punishment.    We expect additional government action in the months to come.       Solitary Voices was also consequential in legal proceedings seeking to reform ICE's use of isolation cells. Our reporting is cited extensively in a major lawsuit filed by the Southern Poverty Law Center and American Civil Liberties Union on behalf of detainees. The lawsuit claims the agency's misuse and overuse of solitary confinement is unconstitutional under the Americans with Disabilities Act.   We’ve also seen this project impact the lives of the people who trusted us with their stories. “It has been such a long time behind those walls,” Dulce Rivera told ICIJ partner NBC News after she was freed. She is a 36-year-old transgender woman from Honduras whose experience featured prominently across many of our stories. "," In the fall of 2018, an envelope arrived for Spencer Woodman, an ICIJ reporter. Inside was a disc that contained 8,488 incident reports from 2012 to early 2017, including narrative details, describing placements of immigrants from at least 160 countries in solitary confinement by U.S. Immigration and Customs Enforcement, or ICE. The Freedom of Information Act request was made nearly two years before.   The data sometimes contained information that was important for the analysis in different fields, including some with narratives. As part of the analysis process, ICIJ did text analysis of the fields “detailed reason” and “additional comments export” to identify, for example, cases in which minorities were affected. ICIJ also calculated the number of days each detainee was held in solitary confinement, and added the fields “days in solitary,” “LGBT,” “suicidal” and “record id,” as part of the analysis.   Our analysis, supported by more than 100 interviews and reviews of thousands of pages of documents obtained through other sources, was the first to bring to light the punishingly long confinements in isolation, the misuse of medical isolation, the abuse of ""suicide watch"" — and how detainees were routinely locked in isolation cells for stunningly minor incidents. We also spent months locating and interviewing dozens of current and former detainees around the world who had been placed in solitary by ICE.   The work was a powerful combination of shoe-leather reporting with data analysis done through a journalistic collaboration. The team coordinated its efforts and shared its findings through ICIJ’s communication platform, the Global I-Hub.   ICIJ made the data public in an interactive we built that allows users to explore the data, along with some highlights from our analysis. Excel, Google Sheets, Python, Javascript, CSS3 were used for the analysis and the creation of the interactive. "," It took all of our efforts to track down and interview people who had spent time in solitary confinement in an ICE facility and were willing to share their stories on the record.  As the data team analyzed the records, reporters hit the ground, spending months interviewing immigrant detainees. We made a spreadsheet of more than a hundred immigrant volunteers and advocates who may have had knowledge of ICE’s solitary confinement practices. These sources proved pivotal in connecting our data with human stories. Team members travelled as far as India to talk to people who had served long stints in an ICE isolation cell before deportation.   The data obtained by ICIJ came with no documentation. ICIJ sent several follow-up questions to ICE after obtaining the data to get more details about how the data was gathered, the methodology, specifics and nuances of the different fields in the data.    The team talked to experts and checked internally to confirm that there was a good understanding of the data.   The data was anonymized. Each row represented one incident, but some detainees were put in solitary multiple times. ICIJ focused on the incidents and we were careful to avoid double counting.    The data was anonymized, sometimes contained non-standardized fields and information that was important for the analysis in different fields, including some with narratives.    ICIJ did text analysis and standardized fields to address this.   The data does not reflect the total universe of detainees placed in solitary confinement from 2012 to 2017. ICE said it does not keep records of every solitary confinement placement. Instead it tracks only cases where detainees were held in isolation for more than 14 days, and where immigrants with a “special vulnerability” were placed in isolation. Some ICE detention centers may also under-report solitary confinement placements. "," The project started with a Freedom of Information request that took almost two years to be responded. Planning information requests with time can be of help for investigative projects.   The trove of data we obtained paved the way for a unique understanding of how ICE uses, and misuses, solitary confinement. It also shows the importance to check for data documentation and talk to experts to get a proper understanding of the information, while working on the data analysis.    The project also shows how powerful a combination of shoe-leather reporting with data analysis is. It was important to get testimonies from people under confinement that confirmed the findings in the data. The testimonies also provided more insights about the incidents that put the detainee under confinement and the treatment received in the different facilities.    We focused on the experiences of people who spent months locked in tiny cells, alone with their thoughts and their fears. Theirs is a dehumanizing experience, and a damaging one — and we knew the project would register most with readers if we could convey powerfully what it feels like to languish alone for months.    The accompanying videos amplify this effort, allowing readers to hear detainee voices for themselves.   Having established the harm, we then zero in on the systemic failures that allow the abusive treatment to persist. Comparing ICE policy against our findings spotlights the misuse and overuse of a form of punishment intended only as a last resort. Comments from the United Nations, and the DHS whistleblower, describing the treatment as tantamount to torture underscore our major finding.    Our visual explainer seeks to drive home the fact that each “incident” in our data represents an actual person, a civil detainee, subjected to treatment deemed overly harsh in many U.S. prisons. ",https://www.icij.org/investigations/solitary-voices/thousands-of-immigrants-suffer-in-us-solitary-confinement/,https://www.icij.org/investigations/solitary-voices/how-us-immigration-authorities-use-solitary-confinement/,https://theintercept.com/2019/08/29/ice-solitary-mental-health-corecivic/,https://www.nbcnews.com/nightly-news/video/whistleblower-sounds-alarm-over-immigrants-in-solitary-confinement-60067909735,https://www.univision.com/univision-news/immigration/the-chronology-of-luis-lovos-mental-problems-after-he-was-put-into-an-ice-punishment-cell,https://www.plazapublica.com.gt/content/silvio-y-los-inhumanos,https://elinformeconaliciaortega.com/cientos-de-dominicanos-sometidos-a-confinamiento-solitario-en-centros-de-detencion-en-eeuu/,Reporting & Data Teams," Solitary Voices is an investigation by the International Consortium of Investigative Journalists; Grupo SIN in the Dominican Republic, Plaza Pública in Guatemala;  Mexicanos contra la Corrupción in Mexico; and NBC News, The Intercept, Univision and WNYC in the U.S. It was a collaboration of 30 journalists. The reporting, conducted over five months, found widespread misuse — and overuse — of solitary confinement in detention centers overseen by the U.S. Homeland Security Department’s Immigration and Customs Enforcement (ICE) agency.        The ICIJ Team    Director: Gerard Ryle   Project Manager: Fergus Shiel   Project Editor: Ben Hallman   Lead Reporter: Spencer Woodman   Data Editor and Partnership Coordinator: Emilia Díaz-Struck   Online Editor: Hamish Boland-Rudder   Community Engagement Editor: Amy Wilson-Chapman   Data Reporter: Karrie Kehoe   Associate Editor and Fact Checker: Richard H.P. Sia   Copy Editor: Joe Hillhouse   Additional Editing: Tom Stites   Video Reporter: Scilla Alecci   Web Developer: Antonio Cucho Gamboa   Illustrator: Rocco Fazzari        Reporters        Alicia Ortega (Dominican Republic)   Julia Ramírez (Dominican Republic)   Enrique Naveda (Guatemala)   Suchit Chávez (Guatemala)   Alejandro García (Guatemala)   Valeria Durán (Mexico)   Daniel Lizárraga (Mexico)   Andrew Lehren (United States)   José Olivares (United States)   Hannah Rappleye (United States)   Vanessa Swales (United States)   Maryam Saleh (United States)   Lynn Dombek (United States)   Talya Cooper (United States)   Moiz Syed (United States)   Tamoa Calzadilla (United States) ",,,
Argentina,LA NACION,Big,Participant,Best data-driven reporting (small and large newsrooms),Milking a prices monitor to report on Argentina's inflation,14/03/19,"Investigation,Explainer,Database,Open data,News application,Mobile App,Infographics,Chart,Economy","D3.js,Json,Google Sheets,PostgreSQL,Python,Node.js"," If we had to pick a word that identifies Argentinians, it wouldn’t be tango. It would be inflation.   Argentina is one of the five countries with the highest inflation (IMF). Supermarket prices rise week after week.   We created  a price monitor that allows a weekly and monthly monitoring of goods. It presents a group of 365 from leading brands that are marketed nationally in 2,561 places and are representative of household consumption in Argentina.  At this point,  we have 54 million prices registered and it's a great tool for data driven reporting, built in house from scratch. "," <a href=""https://www.lanacion.com.ar/economia/arma-tu-changuito-chequea-como-cambian-precios-nid2197909#/""> “Shopping Cart LA NACION” </a> is used primarily by our newsroom to monitor and report on inflation.   We published a mix of content throughout 2019 using the database. Reporting varied in focus and type of data visualization.   Sometimes the story involves using the database and sometimes in combination with others.   Data was part of a substantial piece of reporting and stands on a foundation of data acquisition and analysis to tell the following stories.   <a href=""https://www.lanacion.com.ar/economia/la-caja-navidena-aumento-64-ano-nid2317673"">“Th e Christmas products basket is 64% more expensive than last year”  </a>(Interactive Datavis and reporting on a selected special combination of products)  – Dic 2019)   <a href=""https://www.lanacion.com.ar/economia/desde-1999-hoy-que-se-puede-hacer-nid2312667""> “From 1999 to today: What can be done with a $ 100 bill?” </a> Interactive Infography (December 2019)   <a href=""https://www.lanacion.com.ar/economia/la-canasta-de-precios-de-la-nacion-tuvo-un-alza-de-187-en-las-ultimas-cuatro-s-nid2232905""> “The Price Basket of LA NACION had an increase of 1,87% in the last four weeks” </a> (Automated content with dedicated weekly evolution graph and designed ranking of 43 products – March 2019)    <a href=""https://www.lanacion.com.ar/economia/jugo-yerba-leche-productos-mas-aumentaron-ultimos-nid2238915"">“Juice, yerba mate and milk, the products that increased the most in the last six months”</a>  (Open Data Table)    <a href=""https://www.lanacion.com.ar/economia/en-abril-familia-necesito-29494-no-caer-nid2250568""> </a> <a href=""https://www.lanacion.com.ar/economia/en-abril-familia-necesito-29494-no-caer-nid2250568"">“In April, a family needed $ 29,494 to avoid falling into poverty”</a>  (Tableau Public interactive graph)   <a href=""https://www.lanacion.com.ar/economia/los-10-productos-mas-subieron-ultimas-cuatro-nid2228570"">“Th</a> <a href=""https://www.lanacion.com.ar/economia/los-10-productos-mas-subieron-ultimas-cuatro-nid2228570"">e 10 products that rose the most in the last four weeks”</a>  (Automated content for top 10 products with dedicated interactive charts and graphs – March 2019) "," We decided to scrape the products ourselves from the <a href=""https://www.preciosclaros.gob.ar/#!/buscar-productos"">official Precios Claros website.</a> To do so, we activated a software that runs twice a week and records the information it collects in a database.   The updating process is fully automatic, since data is automatically collected twice a week, saved in a database, and then the front end updates, and the user may thus obtain the price variation of each product.   On the other hand, if it is necessary to add or remove products for users to see, we have an admin and back-end where the same journalists may modify the arrangement of a basket or create new ones to follow some other project, such as a school basket when school time begins, a Christmas basket or a set of products related to holidays.   First we developed one for the ""LN Canasta"" in SQL so we could prove the concept, then for ""LN Changuito"" we needed to automate it and connect it with both outputs, so we developed another version with Python and hosted in a Lambda with CloudWatch and PostgreSql, using in RDS for the database.   An ADMIN developed with Django/Python is used as a back-end to be able to make  sure that all processes work correctly.    From the back-end, after updating the data weekly, a json file is created and hosted in S3 with the prices of the last four weeks for each product in each city.    For the front-end an application developed with VUE.js, D3.js, CSS3 HTML5 was programmed. This one uses the json files published by the back-end in S3. Each user can choose different options to find out the price variation of the products he/she consumes through sets of goods that we call baskets "," The hardest part of the project was getting the data. After three unsuccessful FOIA requests we decided to scrape the information we needed. For this, we had to define an initial list requested by the editor and then automate the process and make a flexible admin interface.   From the moment we began to save the data in the database, we checked the official website to verify and control the information. As we found inconsistencies in the prices published on the site, we decided to discard the outliers.   Jury must understand that reporting on inflation is not journalism as usual. Very few countries in the world have this macro ongoing economy struggle.   This project was an effort to counteract the government's possible denial of the real inflation index and a way to help the newsroom and our audience to get stories that are meaningful depending on the time of the year (school classes, Christmas, etc) helping to bring down an abstract concept as inflation.   As a major national news outlet there is an editorial responsibility on how to report on this to avoid a spiral of escalation to hiperinflation. A delicate balance. "," First of all they can visualize a problem that happens in very few economies of the world and get an idea on how to report on this kind of specific beat.   Journalists used to visit supemarkets every day with a notebook and pen to report on this issue, reporting on different neighbourhoods.   Sometimes we create tools or interactives that we consider useful but then they fail to engage newsroom or audience.   In this case we built a tool that is actually critically NEEDED by the newsroom and so it's being used and loved to find news stories with different angles.   In this project we tackled a national, vital and ongoing beat while solving our journalists processes, builiding a database from scratch, updating it 2 a week and producing orginal reporting.     ",blogs.lanacion.com.ar/projects/data/milking-a-prices-monitor-to-report-on-argentinas-inflation/,https://www.lanacion.com.ar/economia/la-caja-navidena-aumento-64-ano-nid2317673,https://www.lanacion.com.ar/economia/desde-1999-hoy-que-se-puede-hacer-nid2312667,https://www.lanacion.com.ar/economia/en-abril-familia-necesito-29494-no-caer-nid2250568,https://www.lanacion.com.ar/economia/jugo-yerba-leche-productos-mas-aumentaron-ultimos-nid2238915,https://www.lanacion.com.ar/economia/la-canasta-de-precios-de-la-nacion-tuvo-un-alza-de-187-en-las-ultimas-cuatro-s-nid2232905,https://www.lanacion.com.ar/economia/los-10-productos-mas-subieron-ultimas-cuatro-nid2228570,"Pablo Fernández Blanco, Sofía Terrile, Gabriela Bouret, Cristian Bertelegni, Nicolás Bases, Gastón de la LLana, Giselle Ferro, Pablo Loscri, Luciana Coraggio, Bianca Pallaro, Florencia Fernández Blanco, Aldana López, Delfina Bertolotti"," La Nación Data is the data journalism unit from LA NACION, whose goal is to facilitate open data journalism for change in Argentina.    Created in October 2010 we have used data to tell stories since 2011. Much of our work and effort is invested in building datasets from scratch, with the goal of opening data to citizens´s collaboration and discovering new stories .   We add value using data in daily reporting, and also produce long-term investigations that cover topics of interest with the intention of producing a positive social impact in our community.   We always work on projects and in teams with journalists, interactive designers and thinking in every output including social media.    We all have a shared purpose which is to do something meaningful through remarkable journalism that improves people’s lives and generates knowledge. All of our members have a “data aware” mindset that is transferred daily to all the journalists in the newsroom.   We consider ourselves a strategy and a team that joined forces to support and develop innovation in journalism. All of our work is based on technology, data, openness and teamwork.    And we never get tired of learning something new. This is why we participate and host multiple open data events and hackathons to engage with civil society organizations and embrace collaborative journalism. We even offer free training and lectures for NGOs, students and journalists who are interested in building a transparent future ",,,
United Kingdom,The Economist,Big,Participant,Best news application,Who is ahead in the Democratic primary race?,21/08/19,"Explainer,News application,Infographics,Chart,Elections,Politics","D3.js,CSV,R,RStudio,Node.js"," The Economist's Democratic Primaries project aggregates national and state-level polling of America's Democratic Primaries from a number of firms. Additionally, it uses microdata from the weekly Economist/YouGov poll to explore the demographics of each candidate’s support in detail. It breaks down candidates’ support by race, age and education, and also looks at which candidates have scope to expand their support beyond their core supporters. "," The project conveniently aggregates a lot of information about the state of the race, and it has helped to shape our coverage of the race for the Democratic nomination. Because we’re able to work with the microdata of a weekly tracking poll, we’ve been able to take a relatively nuanced look at the nature of Democratic candidates’ support, and how and where it has ebbed and flowed over time.   It has proved popular with readers and pundits alike. It has been viewed half a million times since its launch, and has been cited by reporters from other media outlets, such as FiveThirtyEight, the Huffington Post, and the Washington Post. "," We aggregate polls at both the state and national level to build a polling average. We use an in-house method that combines Bayesian statistics, hierarchical Dirichlet regression and splines (akin to a kernel smoother or LOESS) to draw trend lines through data over time. Our custom method allows us to control for firm-level biases in ways that other polling aggregators cannot, as well employing a hierarchical structure in the underlying regression that interpolates state-level aggregates based on movement in the national data. We’ve also used betting markets to give a sense of the likelihood of any given candidate winning the nomination.   We’ve used R to aggregate the data, Stan to model it and ggplot to analyze it and produce sketches of our charts. We then created a bespoke website for the project using Next.js and D3, backed by a Google Doc and ArchieML to allow easy editing of the website’s text content.   We’ve used a number of techniques to convey information, to try to make different types of data clearly delineated, and to show uncertainty where appropriate. Sometimes, as with the principal trend chart, we show uncertainty only on interaction. In other charts, such as the “beeswarm” charts on the candidate page, we ignored uncertainty so we would be able to show the whole great mass of candidates and position each one within that swarm. "," Setting up the site itself was probably the hardest part. In the past, we would have structured a project like this as an article and manually updated it regularly. However, since we had so much data to present and explore in this case, we instead built a separate “microsite”, running on AWS and Google Docs, and automated as much of the process of running and updating it as we could. We put this together into a template for creating complex, data-driven microsites, one which we used again a few months later for our coverage of the UK election in December. "," We were one of the first news sites to launch a Democratic Primaries polling page so we were able to inform readers and other journalists about the state of the race from August 2019. We also have access to microdata from YouGov that are not publically available and have presented them in an easily digestible way, allowing readers to quickly find an angle to the story. ",https://projects.economist.com/democratic-primaries-2020/,,,,,,,"Martín González, Evan Hensleigh, Matt McLean, G Elliott Morris"," Martín González, Evan Hensleigh and Matt McLean are visual journalists in the data journalism team at The Economist. They create static and interactive charts for the newspaper and website. G Elliott Morris is a data journalist in the same team. His design of the underlying model for our polling aggregates combined Bayesian statistics, multi-level modeling and smoothing terms to average polls in each state, controlling for pollster-level biases against each presidential candidate and interpolating state-level polls using national data. ",,,
Argentina,LA NACION,Big,Participant,Best visualization (small and large newsrooms),2019 Argentine Election: the challenge of reinventing,08/01/19,"Breaking news,Database,News application,Crowdsourcing,Infographics,Chart,Map,Elections,Politics","Scraping,JQuery,Json,Adobe,Microsoft Excel,Google Sheets,CSV,PostgreSQL,Python,Node.js", How to innovate for an event that takes place every four years? How can we distinguish ourselves from the competition and from our past selves? The key was understanding the needs of the audience and generating DATA visualizations through innovative techniques surrounding data acquisition and analysis. From interactive maps and electoral calculators to specific applications and pieces targeting young audiences that conveyed key journalistic findings in a clear and concise way.      ," It is important to always keep the journalism thermometer intact and detect the new needs that arise in the moment. Former president Mauricio Macri was unexpectedly trounced in primary elections. So everybody was asking: would he have the chance to reach a second round and remain in power, or would Alberto Fernández win in the first round? To answer the question, we created a custom calculator that became so popular that according to trusted soruces, even ministers and advisors from the Government House used the application to make arrangements about the future of the campaign. As regards to metrics, the application was highly valued by the audience with almost 500,000 page views and the electoral maps with the results and all its features hit the record of 685,173 page views. Maps and visualizations about the new composition of the Congress and the general presidential results were used by several TV argentine channels with a national range. And the visualisations that targetted young audiences: Google search interest, Questions selected by our social media account followers and answered by all 9 candidates, the debate game among others, became the most shared articles through social media.          "," The applications created for the election coverage were divided into two groups: the ones updated in real time and the static. For the election maps and the calculation of seats in the Congress (both applications received data from the election counting in real time) a CMS was developed in Python with Diango library. Data of candidates and parties that were going to participate in the elections were previously loaded though CMS. Data was stored in a Postgres database and, during the election, in real time, data was crossed with the information of the official results of the elections and used by the different applications created. Data parsing was made in the back-end with Python. The front-end of all the election web apps was made with Javascript frameworks. Vue.js was mainly used for real time apps, for DOM and for data binding. The Javascript library called D3js was also used to create maps and graphics. The rest of the static applications were made in HTML and Javascript, Vue.js was also used for the DOM. In some other applications it was used jquery, and leaflet.js was used to create tile maps. "," The most difficult challange of the project arose when we found oyt that the company in charge of the election counting in Argentina was changed. In order to automate the results and display them in real time, the team needs to have data structure with a proper advance to be able to adapt data to our application software. The night before the elections we still didn't had that information and, in many cases, we had to inform them about errors or failures in the information sent. Expecting, perhaps, some complications with data automation during the live election night, especially in the first hours when all the media organizations compete for the scoop, we decided to implement an information system to feed our result boards by hand to avoid depending on the official submission that showed deficiencies. To achieve an immediate publication, a group of 10 persons were in charge of the data uploading and we were the first to reflect the results since the automation did not work and the other media did not have this alternative plan. When the sending was stabilized, we “plugged in” with the company and the rest of the counting was automatic, which allowed us to feed not only the result boards but all the pieces that involved large volumes of data such as the maps of the entire country, the new composition of the Congress in real time and the comparative “weighing scales” of the two main political forces with respect to the previous elections. "," There are four lessons or, at least, suggestions that we could offer based on our experience with this project. On the one hand, to sharpen perception of reality to make data driven reporting according to the needs of the audience. Secondly, understand that we can be facilitators/means and bring solutions or provide answers through technologies that serve the needs of those who consume us or of those who may start to do so if we show them something new, that has quality and is useful. Thirdly, and based on the story about the disadvantages of automating the results data, it is important to always have a Plan B, especially when the success or failure of an application does not depend on us but on a third party (in this case the company in charge of the election counting). Finally, it is worth mentioning the crossover between platforms such as social media. It is really important to focus on the future readers needs, especially young audiences.  ",https://blogs.lanacion.com.ar/projects/sin-categoria/2019-argentine-election/,https://www.lanacion.com.ar/politica/mapa-resultados-elecciones-generales-2019-nid2300184#/presidente,https://www.lanacion.com.ar/politica/paso-2019-proyeccion-bancas-que-diputados-senadores-nid2274718,https://www.lanacion.com.ar/politica/paso-2019-elecciones-2015-vs-2019-cuanto-nid2275716,https://www.lanacion.com.ar/politica/elecciones-2019-proba-calculadora-verifica-si-hay-nid2293415,https://www.lanacion.com.ar/politica/aborto-medio-ambiente-otros-temas-te-importan-nid2274816#/,https://www.lanacion.com.ar/politica/elecciones-2019-ranking-busquedas-google-candidatos-presidenciales-nid2300314#/,"Pablo Loscri,Florencia Fernández Blanco,Nicolás Bases,Cristian Bertelegni,Gastón de la Llana,Mariana Trigo Viera,Nicolás Rivera,Gise Ferro,Alejandra Bliffeld,Carlos Araujo,Florencia Abd,Juana Copello,Florencia F. Altube,Bianca Pallaro,Delfina Arambillet"," La Nación Data is the data journalism unit from LA NACION, whose goal is to facilitate open data journalism for change in Argentina.    Created in October 2010 we have used data to tell stories since 2011. Much of our work and effort is invested in building datasets from scratch, with the goal of opening data to citizens´s collaboration and discovering new stories .   We add value using data in daily reporting, and also produce long-term investigations that cover topics of interest with the intention of producing a positive social impact in our community.   We always work on projects and in teams with journalists, interactive designers and thinking in every output including social media.    We all have a shared purpose which is to do something meaningful through remarkable journalism that improves people’s lives and generates knowledge. All of our members have a “data aware” mindset that is transferred daily to all the journalists in the newsroom.   We consider ourselves a strategy and a team that joined forces to support and develop innovation in journalism. All of our work is based on technology, data, openness and teamwork.    And we never get tired of learning something new. This is why we participate and host multiple open data events and hackathons to engage with civil society organizations and embrace collaborative journalism. We even offer free training and lectures for NGOs, students and journalists who are interested in building a transparent future ",,,
Kenya,Nation Media Group,Big,Participant,Best data-driven reporting (small and large newsrooms),Murder at Home,12/01/19,"Investigation,Database,Infographics,Chart,Map,Women,Human rights","JQuery,Json,Microsoft Excel,CSV,Python","  Murder at Home  provides a broad perspective of the problem of gender violence by analysing and presenting data in a way that establishes context and depict trends. The project presents the data and stories through a dynamic dashboard, interactive visualisations, photos, multimedia piece as well as moving and compelling stories from individuals impacted by the violence. The project is the most comprehensive publicly accessible database of deaths resulting from gender violence in Kenya. The project collects information from the media, police and court reports documenting the details of persons killed in gender-related violence, including name, age, and relationship with killer. "," Published during the 16 days of activism against gender-based violence the  Murder at Home  project spurred debate on gender violence and women's rights  in the media and public events that were held to mark the period. The discussions explored why reports of gender-related deaths were on the rise. Speakers – male and female – condemned domestic abusers and questioned why there were few cases in court of people suspected of gender-related killings and hardly any convictions. Challenged on what they were doing to end gender violence, national and county government officials in the  relevant institutions  took the opportunity offered by the debates to highlight actions and policies that are in place or will be in the near future to address the problem. Murder at Home is now a resource publicly available to anyone interested in gender violence for reference. "," Data sourcing:  Tabular  for scrapping some the datasets  used in stories and A dvanced search tools  to research on gender violence and find relevant media reports on gender-related killings.   Data analysis:  CSV, Microsoft excel  and  Tableau.    Data visualisation: I ndesign  for infographics in stories;  Data wrapper  and J avaScript  for charts and maps on dashboard and in stories;   Gimp  and  JavaScript  for pictures.   Dashboard development:  Python, JavaScript  and   JQuery.  ","  The recent frequent media coverage of gender-based violence has raised awareness on a trend that needs to be addressed. However, such incidents, standing individually, lack the elements and the nuances needed to adequately understand the phenomenon for appropriate intervention. As one of a kind, the project serves as a point of reference for public discourse and policy formulation at a time when increasingly data plays a central role in decision making. Our core dataset came from reviewing mainstream and social media stories on gender-related killings yet many of the articles lacked a lot of the information we needed to capture.  This necessitated a follow-up process that involved calling  reporters who did the gender-related killings articles and individuals who posted  cases  on social media. It was a lot of work since  only one person was assigned to do the task even as they worked on other N ewsplex  projects. But it was worth it as in the end we created the most comprehensive publicly accessible database of deaths resulting from gender violence in Kenya.   ","  Just because the information you are looking is not easily available does not mean you should give up an assignment.  Expect unforseen challenges and adjust your project to address them the way  Newsplex  did by  coming up with a plan to follow up with story sources . The small Newsplex team withstood  the pressures of daily newsroom deadlines and spent months painstakingly creating a project  that exposed the magnitude of gender killings in Kenya, showed that anyone is a potential victims and connected with the readers by focusing on victims including children who  were killed, mostly  in intimate partner related violence.   Murder at Home  also revealed that even  though the problem appears to be getting out of hand, no strong support is being provided to address the menace of gender-related killings by the various stakeholders.   ",https://www.nation.co.ke/newsplex/murder-at-home-database/-/2718262/5444980/-/ctj2uj/-/index.html,https://www.nation.co.ke/newsplex/murderathome/2718262-5368456-v9iqiw/index.html,https://www.nation.co.ke/newsplex/domesticabuse/2718262-5368682-wsp0ny/index.html,https://www.nation.co.ke/newsplex/onlinegendercampaigns/2718262-5368756-fpvkmz/index.html,,,,"Dorothy Otieno, Daniel Ofula, Victor Oluoch, Joshua Mutisya and Churchill Otieno","  Churchill Otieno  is the Managing Editor (Online & New Content) at Nation Media Group. He was instrumental in the setting up of #NationNewsplex, NMG's data journalism project in 2016. A pioneer in online journalism in Kenya, he has been at the centre at newsroom change in East Africa to drive new media adoption.    Dorothy Otieno  is a data journalism editor at Nation Media Group, which she joined in 2015 as the first editor for  Newsplex,  the media house’s data journalism team. In its short lifespan,  Newsplex  has won numerous data journalism awards including the best Data Visualisation award at the 2017 Digital Media Awards and 2017 Africa Fact-Checking Award for its ‘ Before you vote’  series, which fact-checked the claims made by election candidates prior to Kenya’s last  general election. Prior to joining NMG the award winning journalist was instrumental in introducing  data journalism in Kenya as the Data Journalism Team Lead at Internew, an international media capacity building organisaton.      Victor Oluoch  is an online sub-editor at the Nation Media Group with 13 years of experience in broadcast, print and online media. He has also served in development organisations as a communications specialist. Heis committed to using different tools and strategies to tell powerful stories that impact lives.    Joshua Mutisya  is a data journalist who enjoys telling data driven stories, researching and analysing data to provide fresh perspectives on issues confronting society today.    Daniel Ofula  is a machine learning / data science enthusiast and practitioner who enjoys solving problems by leveraging technology and data. ",,,
United Kingdom,Financial Times,Big,Shortlist,Best data-driven reporting (small and large newsrooms),How top health websites are sharing sensitive data with advertisers,13/11/19,"Investigation,Long-form,Chart,Video,Business","Scraping,Google Sheets,CSV,Python"," We looked at the UK’s most popular health websites to see how they treat sensitive data from their users. Our investigation revealed that some of the health sites are sharing this information, including symptoms and drug names, with hundreds of third parties including Google and Facebook. In some cases this included an identifier potentially allowing the data to be tied to an individual. This was done without the explicit consent that is legally required in the UK. "," The story was the front page splash on the UK print edition of the Financial Times on 14th November 2019. It was picked up by the tech and health press, including the <a href=""https://www.technologyreview.com/f/614708/health-websites-are-sharing-sensitive-medical-data-with-google-facebook-and-amazon/"">MIT Technology Review</a> and <a href=""https://boingboing.net/2019/11/13/popular-uk-health-websites-sha.html"">BoingBoing</a>. The UK's Information Commissioner's Office (ICO), which is responsible for data protection, <a href=""https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2019/11/why-special-category-personal-data-needs-to-be-handled-even-more-carefully/"">issued guidance later that day</a> on what the law says about dealing with the highly-personal ‘special category' data, such as health.   The following day Google, which our story showed was the biggest recipient of this data of the sites we looked at, <a href=""https://www.ft.com/content/7717bce6-06e5-11ea-9afa-d9e2401fa7ca"">announced plans</a> to limit advertisers' access to personal data when they bid for adverts.     "," I started with a list of the top 100 health sites produced by SimilarWeb, based on average UK monthly traffic. I ran this list through WebXray, an open-source tool. This opens each site and records all the HTTP requests made to third parties. It produces data which shows which sites contact third parties, and the domain names of those third parties.   Manual research primarily using Whois was used to link those domain names to companies, as many companies use multiple domains, and often do not include the company name.   However, this didn’t show exactly what information was being sent. To do this I used a tool called HTTP Toolkit, which intercepts all the requests being sent out by a site, and lets you search and explore the information they contain. We picked a few sites to look closer at that both ask for specific health information and were contacting many third parties. I loaded up each site and filled in some information.   At this point I could check, before giving any consent, whether the site dropped a cookie. I also looked for anything that looked like the information I had given it, and anything that looked like a user identifier.   One of the more complex elements of the story to understand is that the third parties that have their ‘tags’ (code snippets) on the page often connect to other third parties, which in turn can bring in others, and so on. Furthermore, the extent to which this happens varies substantially in different parties of the world, likely because of different regulatory environments. To visualise this we gained access to Trackermap, a tool that displays the structure of these networks from different servers around the world. I scraped the data it produced, and processed it in Gephi using its layout algorithms.    "," The hardest part of this project was making a story based around arcane technical concepts and legal requirements understandable and meaningful to readers.   We knew this could be an issue from the start. In fact, the original intention was more generally to look at ‘special category’ data, which has extra legal protection -- including information about race, political opinions, and genetic data, as well as health. The decision to focus on health was made as it is a topic that affects absolutely everyone, giving us the broadest audience of people who would be able to personally relate to the story.   We also wanted to ensure that readers understood how this type of information was being tracked and shared without their knowledge or even consent in some instances. In particular, we wanted to explain how exactly this happened, and touch on the technical details that underpinned our reporting, but in a way that was understandable to the typical reader.   To do this, we picked one particular site, the WebMD symptom checker. This was a good example as not only was it a very popular well-known site, but, looking at the page, it doesn’t have a lot of adverts -- just two straightforward images. It appears benign. However almost all the incredibly personal data that people are likely to enter is sent to Facebook’s advertising platform. We created a video walking through someone entering their symptoms, and then me showing how this information was ending up in HTTP requests to Facebook’s domain. We aimed to avoid technical jargon, but without dumbing-down.    "," (1) That it is often the simplest ideas that hold most relevance to readers and create the biggest results. I have been involved in many projects where a lot more time has been invested for a lot less impact.   (2) A story such as this has many technical details. Details that I’d spent a lot of time obsessing over, and were essential to the data we’d collected that formed the foundation of the story. But you need to be brave enough to leave many of them out to produce a story that’s comprehensible to the readers we wanted to reach -- whilst also giving enough so that readers trust our process. It is a difficult balance, and the temptation is to include all the numbers, interleaved with caveats and technicalities, so any potential criticism can be swatted. But just making a conscious effort to get this right can make a big difference.   (3) One criticism that I was expecting, but didn’t really end up receiving was -- ‘don’t we already know this?’ There have been many stories on privacy and the advertising industry, cookies, etc. I believe the reason we didn’t is because we brought it into situations close to readers’ lives. That’s manifest in picking health as a topic, the video walking through a real situation, and the juxtaposition in the piece between the hippocratic oath of times past, and the graphic laying out the technical and commercial reality of who receives information about your health today.    ",https://www.ft.com/content/0fbf4d8e-022b-11ea-be59-e49b2a136b8d,https://www.youtube.com/watch?v=AcE3nXEkGbU,,,,,,"Max Harlow, Madhumita Murgia"," Max Harlow works on the FT’s visual and data journalism team. Before joining the FT he worked on software and stories at the Guardian, on investigative projects at the Bureau for Investigative Journalism, and with startups at Ordnance Survey.   Madhumita Murgia writes about technology for the FT. She was previously a reporter and editor at WIRED and The Daily Telegraph.     ",,,
United Kingdom,The Economist,Big,Participant,Innovation (small and large newsrooms),Brexit ternary plots,22/02/19,"Explainer,News application,Infographics,Chart,Politics,Brexit","D3.js,CSV,R,RStudio","  In late February we produced a piece for the “Graphic detail” print section of the newspaper on Britons’ attitudes towards Brexit: deal, no deal, or stay in the EU. Along with each anonymous person’s response, YouGov provided us with detailed demographic variables for 90,000 people: the respondents’ age; sex; education status; household income; region, previous voting history; and so on.  ","  While ternary plots aren’t a new invention — they have been an oft-seen feature in scientific papers for years — they are rarely used by media outlets. But since we published our story in February 2019  we have seen ternary plots become more commonplace. We have also, for the first time, published a static ternary in the print edition of the Economist.   ",  The data was crunched in Rstudio making use of data.table and tidyverse libraries. Our voter model was created using mutli-nomial regression analysis in nnet package.   ,"  Plotting every single survey response — 90,000 points in total — was overwhelming. We needed another solution. In order to whittle-down the data, we first created a set of 675,000 hypothetical voters for each combination of sex, education, income, and so on. We took a weighted random sample of 2,500 of our observations. Because our random sample picked the most prevalent profiles, we were able to represent some 25% of the British electorate with just 2,500 observations of individuals. Plotting this number of points gave us the right balance between exploration and clarity.  ","  With line charts, a bar or a column chart, or an x-y scatter, each one takes two variables— GDP and time, for example — and plots them in a two-dimensional space. But a three-dimensional “ternary” plot can be used effectively to demonstrate the relationship between three inter-linked variables.   ",https://www.economist.com/graphic-detail/2019/02/22/profiles-of-a-divided-country,https://www.economist.com/graphic-detail/2019/02/23/british-voters-are-unimpressed-by-theresa-mays-brexit-deal,https://medium.economist.com/plotting-the-brexit-conundrum-fb8e804474fa,https://github.com/TheEconomist/graphic-detail-data/tree/master/data/2019-02-23_opinion-on-brexit,,,,"James Fransham, Martín González, Matt McLean",  James Fransham is a data journalist at the Economist         Martín González is a visual journalist at the Economist.         Matt McLean  is a visual journalist at the Economist.  ,,,
Brazil,Folha de São Paulo,Big,Participant,Best data-driven reporting (small and large newsrooms),Airbnb's hosts are often companies with up to 157 listings,24/05/19,"Investigation,Explainer,Database,Infographics,Chart,Map,Business,Economy","Scraping,Json,Adobe,CSV,R,RStudio,PostGIS,OpenStreetMap,Node.js"," The article investigates Airbnb's hosts profiles in Rio de Janeiro and São Paulo using a combination of our own scraping and data from Inside Airbnb. The main findings are that few accounts concentrate hundreds of listings and that some of them are run by real estate companies. Since temporary renting laws are not well-defined in Brazil, the article unveils a conflict between the hotel industry, subject to taxes, and Airbnb. A spin-off article was produced using data from Rio to visualize the concentration of expensive listings in specific regions of the city.  "," The article raises questions on the usage of Airbnb for business purposes in Brazil. The capilarity of Airbnb was explored in Brazil's largest cities, Rio and São Paulo. It was the first article to understand the distribution of Airbnb listings in these capitals. The investigation provides an at-a-glance overview of the relationship between Airbnb and state regulations, often misunderstood by the Brazilian audience. Moreover, it furthered a debate on legal issues of temporary renting, examining the controversies surrounding tourism and Airbnb's platform. "," Scripts were built in R 3.5.0 to analyze georreferenced listings and calculate hosts metrics in a single pipeline. The R's packages dplyr and tidyr were employed to analyze the data on listings for every user, as well as their reviews.  Next, the earnings for a host were estimated simulating a scenario in which all properties were rented. The frequency of listings in specific regions of the cities configured tendecies that were then plotted using QGIS 3.4.15, generating the main visualizations for the story. The spin-off article later produced was made with deck.gl, a large-scale WebGL-framework for JavaScript which helped us build 3D visualizations over Rio's map. "," The journalistic investigation faced its own particular challenges. The anonimity permitted by Airbnb's platform limited the verification of the informations provided by characters featured in the story. Often, interviews had to be conducted through social media with the omission of the interviewed's last name. However, all the information provided could ultimately be confirmed by a thourough interview of the data. From a technical standpoint, the scraping of São Paulo's data alone took about two weeks to be completed, which delayed the analysis and configured the biggest challenge. Yet, it was overcome by a strict time management schedule, ensuring the story to fit the fast pace of Folha's newsroom. "," The story focuses on the concentration of listings by a limited number of users that provide services that compete with those previously offered exclusively by hotels. Additionally, although the legality of Airbnb's is a topic of discussion all over the world, the platform's operations adapt to every country its present, presenting specific challenges for both the state and the application. The article therefore approaches those particularities in Brazil and the conclusions address the limitations of Brazilian's regulations, which are yet to adapt to contemporary types of businesses. ",https://www1.folha.uol.com.br/mercado/2019/05/maiores-anunciantes-no-airbnb-sao-empresas-com-ate-157-imoveis.shtml,https://arte.folha.uol.com.br/cotidiano/2019/anuncios-airbnb/rio-de-janeiro/,,,,,,"Leonardo Diegues, Marina Gama Cubas, Fábio Takahashi, Rubens Fernando Alencar","  Leonardo Diegues : Data journalist at Folha de S. Paulo, Bachelor in Social Sciences by University of São Paulo;   Marina Gama Cubas : Data journalist at Folha de S.Paulo, graduated in Language & Literature and Journalism as well and with Master's Degree in Investigative Journalism, Data and Visualization from Universidad Rey Juan Carlos;   Fábio Takahashi : Data journalism desk editor at Folha de S. Paulo, founder of Brazilian Education Journalists Association, Spencer fellow at Columbia University;   Rubens Fernando Alencar : Lead Front-end developer at Folha de S.Paulo, Big Data Analyst by Fundação Instituto de Administração. ",,,
United Kingdom,The Times,Big,Participant,Best visualization (small and large newsrooms),The Times general election coverage,12/09/19,"Explainer,Breaking news,Illustration,Infographics,Chart,Map,Elections,Politics","Scraping,D3.js,Adobe,Google Sheets,R,RStudio","The interactive team was at the heart of the Times' and the Sunday Times' political coverage before, during and after the election. What follows is a snapshot of some of the visual work we are most proud of. Our mammoth and unprecedented turnout project looked at voter interest in every constituency going back to 1918. Our stunning 'ring map' of the seats which always predict the general election told a thousand narratives of how political power has . Finally, our post-election charts – some of which went viral on the Friday morning – formed part of a highly engaging analysis"," The work of the interactive team consistently ranked among the most-read and most-engaged piece of the election. All three of the pieces below had very high engagement among our readers, who praised the complexity and clarity of our work in the comments. Our concentric circle '<a href=""https://www.thetimes.co.uk/edition/news/elections-results-mapped-60-seats-that-always-back-the-winner-l3zjqjlb9"">ring map</a>' (for want of a better term) which featured in the 'seats that always pick a winner' piece was praised for the number of fascinating narratives held within it. For example, readers were able to explore the country's swing seats which constantly changed hands at each election, or see which seats Labour had won in 2005 but never since, thus showing how far away the party is from a sizeable majority. For the charts built on the morning after the election, being able to script them beforehand meant we could finish and share the charts before the results had even been declared. This gave us an early-morning <a href=""https://twitter.com/MattChorley/status/1205387348856496128?s=20"">viral hit</a> when the Times Red Box editor shared the ""arrow-swing"" map. Finally, our work across the election also raised the profile of our team internally, which has resulted in a huge increase in commissions and requests to work with us since. "," Our election coverage helped us to develop some truly innovative tools and techniques. Much of our data analysis for the different pieces was done using R, the standard programming language of the Times interactive team.   However, we also built a new responsive graphics tool for the election which allowed us to upload a set of images of different sizes. This would produce embed codes for our CMS which would serve readers different sized images depending on the size of the reader's screen. This proved to be a far less buggy approach than using Ai2html, as labels don't jump around unexpectedly on static images.   Preparing for the morning after the election, we wrote R scripts that would automatically download the election data and produce a set of complex visuals with ggplot in the Times style that we wouldn't have been able to do in Datawrapper: things like arrow swing maps, histograms and ""brick"" charts, as well as several that we didn't end up using. We then wrote a set of functions which woulid export these charts as different sized images for various screen sizes, meaning we could upload directly from ggplot to our responsive graphics tool. The function had a built-in faceting option, which meant that two charts side by side on the desktop image would be stacked vertically for mobile, complete with appropriate text sizes and labels.   All of these developments meant we could speed up the production of quite complex visuals. The rest of the visuals were a combination of Datawrapper charts and bespoke D3 interactives, which we used to add personalisation to the stories. "," The turnout project was particularly challenging. We had to factor in hundreds of boundary changes into our analysis, while digitised maps featuring constituency boundaries back to 1918 are not readily available. This meant we had to do a lengthy process of manually checking which historic constituencies best corresponded with the current ones, and weight the average turnout proportionally across the constituencies it straddled. As we later found out, some constituencies changed boundaries without being renamed, making it even harder to spot changes. If we were to do this again, we might want to consider working more closely with the House of Commons library to see if there's any way they can digitise historic election maps for us. Nevertheless, the result was a unique piece of research unlike any of its kind which we hope others will be able to build on in the future. "," We hope that others take some inspiration from the visuals we've produced throughouot this election, just as we've taken inspiration from others before us: we couldn't have done the red wall ""brick"" chart in the results piece without the work of twitter map legend Alasdair Rae, for example. On the technology side, we think we've found a reliable and quick way of getting graphics built with R and other tools onto our website as responsive visuals that others might want to emulate. Finally, we think the team has set a clear example of how to successfully cooperate with a large newsroom of busy journalists. On the morning after the election, the interactive team became the focal point as busy reporters and editors looked to put the Tories' win into context. ",https://www.thetimes.co.uk/edition/news/uk-general-election-results-2019-maps-charts-k6xx25xx9,https://www.thetimes.co.uk/article/election-turnout-britains-most-politically-engaged-seats-revealed-tqbq75f6w,https://www.thetimes.co.uk/edition/news/elections-results-mapped-60-seats-that-always-back-the-winner-l3zjqjlb9,,,,,"Tom Calver, Sam Joiner, Rosa Ellis, Anna Lombardi, Dan Clark, Ryan Watts, George Greenwood, Anthony Cappaert"," The Times and Sunday Times interactive team is an eclectic mix of visual and data journalists all united in the common goal of telling stories with data. Between us we have a red belt in Taekwondo, a PhD in experimental physics and more than 20 years' experience playing the mandolin. We work across both papers to find exclusive stories in data and present them in a visually appealing way. We had a very strong 2019, resulting in a dozen front page stories and culminating in a triumphant general election where we produced visuals that became central to the Times' political coverage. Our belief in consistent and on-brand visuals has even been known to extend beyond work hours, resulting in the whole team wearing yellow jumpers on nights out. ",,,
Brazil,The Intercept Brasil,Small,Shortlist,Innovation (small and large newsrooms),SHELL SHOCKED,16/12/19,"Investigation,Crime,Gun violence","Animation,Adobe,Creative Suite,Microsoft Excel,Google Sheets"," For 100 days we collected bullet shells in twenty-seven neighborhoods of the metropolitan region of Rio de Janeiro, immediately after the gun fights, between police officers, drug dealers or militiamen. We scoured the streets of Rio de Janeiro and showed the story the bullet shells tell. The gun violence that plagues Rio is made possible by ammunition made largely in Brazil, but also from all over the world. We collected the evidence.     ","     The investigation had a huge repercussion between the big journals of the country, who suited the new. Parliamentarians and public managers quoted the content in their speeches and social media, asking for transparency and boasting the uncontrolled army violence that affects Rio de Janeiro. With the denial of the company and authorities who must in thesis offer the information about the bullet shells, a parliamentarian prepared a requirement of information to call formally informations about these bullet shells to know where they came from and where they should be. "," For 100 days we collected 137 bullet shells in twenty-seven neighborhoods of the metropolitan region of Rio de Janeiro, immediately after the gun fights, between police officers, drug dealers or militiamen. After the collection, we split the material according to place, date, and agent involved in the conflict. With all this information ready, the material was repassed to the “Instituto Sou da Paz” and to the Small Arms Survey. They identified the manufacturer, caliber, batch and the year of production of the ammunition. We have searched on the press and in the database of the platform “Fogo Cruzado” - that maps shootings and shootouts - all the information referring to the current events of the day to understand who was Involved in these conflicts and the similarity between the material used by them. Thereby, we could put in a map the history of these bullet shells - that travelled thousands of kilometers - until they were fired on the favelas of Rio de Janeiro. We analyzed the actual legislation to understand where are all the gaps - especially to the Brazilian production - and we identified a huge fragility in the inspection done by the army and an absolute lack of transparency from Taurus. "," The most difficult part of this survey was to develop a network of collaborations to access the capsules immediately after the shootings - in security and confidentiality. Despite the difficulty in building and articulating this network, this work was done. On the other hand, there was extreme difficulty in the access of government data on ammunition manufacturing lots. Of the 94 national capsules we collected, it was only possible to identify the batch marking in 53 of them. However, we were only able to discover their buyer in just four, crossing information with a survey by the Federal Public Ministry of Paraíba.We requested information on these lots to the press office and the Access to Information Law. All requests were denied, alleging secrecy or that the data was not within their competence. This was the difficulty that we were unable to overcome.     "," Quality journalism depends on time and resources. This guideline remained more than one year to be done. It was needed to talk with a lot of people - inside and out of Brazil - and walk at all Rio de Janeiro for months. Analyses, video, texts and products for different platforms. It was needed to have a sharp team in their positions so that a huge investigation like that reaches the maximum repercussion.This project involves a difficult guideline of covering because of the lack of information and  the risks involved. The principle lesson that remains is that the way to pass thought this kind of difficulty is to innovate in inquiry, to search for new ways of approaching complex themes and innovative ways to show the public.      ",https://theintercept.com/2019/12/16/brazil-bullets-guns-ammunition-analysis/,,,,,,,"Cecília Olliveira, Leandro Demori","<ul>  Cecilia Olliveira is a journalist with a postgraduate degree in Crime and Public Security and Public Administration with an emphasis on Social Management. She has studied drug policies, HIV and human rights at the Intercambios Asociación Civil and reporting on drug trafficking at the University of Texas at Austin and participated in the Open Society Latin American Advocacy Fellowship Program on Drug Policy Reform in London.Cecilia was formerly a consultant for Amnesty International, where she worked as a researcher and led the development of the Fogo Cruzado armed violence data platform, now managed by the Update Institute. She was also a communications consultant for LEAP Brasil and a communication advisor for PRVL (Program for the Reduction of Lethal Violence against Adolescents and Youth), an initiative of the Favelas Observatory carried out in conjunction with UNICEF and the Brazilian President's Human Rights Secretariat. She also coordinated the communications team at the Redes da Maré NGO, where we published the  Maré de Notícias newspaper.  </ul>     <ul>  Leandro Demori is the Executive Editor of The Intercept Brasil and is based in Rio de Janeiro. He is the author of “Cossa Nostra in Brazil: The History of the Mafioso Who Took Down the Empire” (Companhia das Letras, 2016) and is a board member of the Brazilian Association of Investigative Journalism (Abraji). He was previously digital editor of revista piauí.  </ul>    ",,,
India,Mint,Small,Participant,Innovation (small and large newsrooms),Plain Facts - India's only daily data journalism column,01/01/19,"Open data,News application,Fact-checking,Elections,Politics,Environment,Agriculture,Health,Economy","QGIS,Microsoft Excel,Google Sheets,CSV,R,RStudio,OpenStreetMap,Python","  <a href=""https://www.livemint.com/topic/plain-facts"">Plain Facts</a>  is Mint’s and India’s only daily data journalism column.  Over the last year, from Monday to Friday, with just a team of 7, we have examined a major issue in India using data. Each story is centred around at least three charts and has 650  to 750 words narrative in support. In India’s noisy media environment, data journalism remains a nascent concept and visual data journalism in print is rarer still.  Plain Facts  has showcased the possibilities of using data in journalism - and especially how critical data is in revealing the truth.  "," Through our daily data journalism page, we’ve generated fresh, data-driven insights into both enduring and contemporary issues in India.  Many of these issues, such as the functioning of Indian courts, had never been examined rigorously using data by the media or even academia.  Consequently, our stories have not just informed readers but also influenced practitioners. For instance, our <a href=""https://www.livemint.com/Politics/tzhLriWMkIxwe3XtG5U5yH/Supreme-Court-pending-cases-from-Aadhaar-to-Ayodhya.html"">series on the problems in India’s legal system</a> influenced the design of a ‘Data for Justice’ challenge, organized by a legal non-profit,  to discover other problems in the system.     Plain Facts  has also generated data in places with traditionally scarce data. Our series on cities, for instance, brought together different datasets to objectively assess India's biggest cities on a range of metrics.  Moreover, our data-based stories have been able to powerfully hold the government to account. For instance, ahead of the 2019 Indian elections, our <a href=""https://www.livemint.com/politics/policy/how-nda-2-and-upa-2-compare-on-mgnregs-performance-1551661777277.html"">report card series</a> examined the performance of the ruling Bharatiya Janata Party (BJP) in different sectors (e.g. power, water and roads). Within this instance, <a href=""https://www.livemint.com/news/india/under-nda-more-toilets-less-open-defecation-1552842931107.html"">one story on sanitation</a> revealed the extent to which the BJP was exaggerating its achievements about improving India's sanitation situation and triggered a response from the government. Like this, the series debunked many claims made by leaders across the political spectrum and added rigour to an otherwise frenzied approach to elections.  "," In generating our daily column, we use all the standard tools of data journalism: excel, R, python, illustrator and the Google suite (Google Docs for editing and Google Groups for internal communication). In addition to technical tools, we rely on expertise.  With almost every story, we speak to an expert, someone who has worked with the story’s data extensively, to ask them how we should analyze and interpret the data.      "," Data in India suffers from major issues -- both in terms of quality and quantity. Datasets (such as regularly updated employment numbers) that are considered commonplace elsewhere are nonexistent in India. And what data that does exist often lacks credibility and quality.  Operating a daily data page in this data-scarce environment, with just a team of 7, has been our biggest challenge. This has meant some articles are less data-intense than others. For instance, a few stories simply describe a newly-released dataset. Yet we believe that this is still worth doing.  The Indian media is yet to acquire an appetite for data-based stories so even our simpler stories add value to the news cycle.  "," In operating a daily page in a data-scarce environment, we have shown what can be done with limited data. We’ve been forced to be innovative in reusing and combining existing data from different sources. For instance, ahead of the elections, we compiled a member of parliament (MP) performance index that used data from three different sources to assess MP performance.  In another story, we used rally location data and elections data to estimate the effect of a political rally by a certain leader on vote swing. We’ve also used proxy data to track major issues. For example, we’ve used Google Trends to examine important cultural trends in India (including the <a href=""https://www.livemint.com/Industry/aNtBKthyukX2mo9BBnHRvO/Indias-growing-obsession-with-mobile-porn-videos.html"">rise in porn consumption</a>). Taken together, we believe we’ve shown other newsrooms what are the possibilities for data journalism in India. ​ ",https://www.livemint.com/topic/plain-facts,https://www.livemint.com/industry/media/what-are-india-s-greatest-and-worst-movies-1550595211954.html,https://www.livemint.com/industry/media/what-are-india-s-greatest-and-worst-movies-1550595211954.html,https://www.livemint.com/news/india/the-roots-of-india-s-deepening-rural-water-crisis-1564323444810.html,https://www.livemint.com/politics/news/a-cleaner-ganga-nda-s-unfulfilled-promise-1556462580465.html,,,"Pramit Bhattacharya, Vishnu Padmanabhan, Sriharsha Devulapalli, Nikita Kwatra, Sneha Alexander, Pooja Dantewadia, Surbhi Bhatia"," Plain Facts is led by our editor, Pramit Bhattacharya, who has been a prominent figure in India’s data journalism movement for five years now.  Supporting him in running Plain Facts is the Assistant Editor - Vishnu Padmanabhan. Vishnu joined Mint from J-PAL and uses his economics and public policy background to both edit and write pieces.  Sriharsha Devulapalli is the team’s lead developer responsible for almost all the coding, interactive stories, scraping and graphics that the team needs. Nikita Kwatra is the team’s economics journalist writing about Indian economic developments using data. Sneha Alexander helps run the team’s fact-check work where she uses data to verify government claims.  Pooja Dantewadia is the team’s second developer and supports Sriharsha in any coding or interactive-related work. Surbhi Bhatia is the team's latest addition and has so far focused on the coverage of Indian states and the budget. ",,,
United States,The Washington Post,Big,Shortlist,Best data-driven reporting (small and large newsrooms),The Opioid Files,16/07/19,"Investigation,Explainer,Long-form,Breaking news,Database,Open data,News application,Infographics,Video,Politics,Business,Health","QGIS,JQuery,Json,CSV,R,RStudio,Python,Node.js"," The Opioid Files for the first time identified not only the counties flooded with the highest amount of prescription opioid pills at the height of the prescription drug crisis, but the specific manufacturers, distributors and pharmacies that were responsible for bringing those pills into communities. The Post found that over a seven-year period from 2006-2012, over 76 billion pills of hydrocodone and oxycodone were shipped to pharmacies across the country, more than enough for one pill per person per day in some communities. "," The database was the largest that The Post has ever published, containing 380 million records. We made it searchable for the public and other journalists, generating at least 150 stories in 35 states by other media outlets (133 local and 17 national) and more than 50,000 downloads of the data by individuals interested in doing their own digging.   The outlets included the Philadelphia Inquirer, the Detroit Free Press, Minneapolis Star Tribune, the Boston Globe, the Chicago Sun Times, the Arizona Republic, the Columbus Dispatch, the Tampa Bay Times, the Fort Lauderdale Sun-Sentinel and the Portland Oregonian.   Many smaller outlets wrote as well, from the Daily Mountain Eagle in Alabama to the Paintsville Herald in Kentucky to Wenatchee World in Washington state.   The documents The Post obtained shed light on the industry strategy to expand the market and fight DEA’s attempts to hold companies accountable.   The documents provide answers to the enduring mystery of how the drug companies were able to weaken the DEA’s most powerful enforcement weapon at the height of the crisis, by enlisting member of Congress and developing “tactics” and a “Crisis Playbook” to aimed at undermining the DEA. "," Upon receiving the ARCOS data, our first challenge was finding a way to parse the large dataset. We used wrote an ETL pipeline with unix, Python and R scripts that broke the massive CSV file into smaller chunks, converted the files into Apache Parquet files (a columnar-based data format used for large-scale data analysis), and loaded the data into memory as needed.   We fine-tuned our scripts to run everything in parallel (using the pandas and dask Python libraries) which allowed us to reduce the time of our analysis from hours to minutes. This process allowed us to quickly iterate on new ideas as the story unfolded without waiting for all the data to load. And by doing the analysis in both Python and R, we were able to audit each other's analyses and make sure our methodologies were sound. We then published the county-level data on Amazon S3 to allow other reporters and researchers to download the data.   Next, we geocoded (generated coordinates based on an address) every single pharmacy address in the database. (We had to do this manually for at least 3,000 pharmacies.) Then, with node.js, we pulled U.S. census data from IPUMS and analyzed the number of pills distributed within a 5- to 10-mile radius using buffers generated with turf.js.   In sum, the project realized several technology stacks and analysis approaches to dig into the data. "," We originally filed a Freedom of Information request to the DEA, but the agency did not provide the data. The Post then intervened in a civil lawsuit against two dozen drug companies and pharmacies in Cleveland to gain access to the data.   It took more than three years and the intervention in the opioid lawsuit to obtain the data. The Post could not find an attorney at any of the large D.C. law firms because they were already representing drug companies or pharmacies in the case.   We were able to hire a sole practioner from Akron, Ohio, who successfully argued to the 6<sup>th</sup> Circuit of Appeals that the DEA data, along with internal company documents in the case, should be unsealed and released to the public.   On July 15, when we eventually received the data, it contained 380 million transactions. The Post made an emergency purchase of a custom-adapted Dell Precision 5820 Workstation. Working around the clock, we produced our first story two days after the data was released.   But the stories could not have been written without old-fashioned shoe-leather reporting and the careful cultivation of sensitive sources. We needed expert guides who could explain the numbers and point us to the most compelling documents, out of the tens of thousands that were released from the lawsuit, including depositions and internal emails.   More than that, we needed the resources of the entire Post newsroom, eight departments working together, to create a public-facing interactive database and an online repository for the most important documents. We did this under enormous deadline and competitive pressure. "," This project would not have happened without sources and expert legal assistant. Our advice would be to cultivate sources deep inside the agency or company you are investigating to understand how the place works and what kinds of documents are available. Our sources were able to to tell us about the existence of the database at the DEA, the kind of information it contained and what that information might reveal about the opioid epidemic. We also relied heavily on legal counsel both inside and outside of The Post. Our outside lawyer filed to intervene in the federal litigation and convince a U.S. appeals court in Ohio to release the database and unseal tens of thousands of corporate emails, memos and other documents. ",https://www.washingtonpost.com/investigations/76-billion-opioid-pills-newly-released-federal-data-unmasks-the-epidemic/2019/07/16/5f29fd62-a73e-11e9-86dd-d7f0e60391e9_story.html,https://www.washingtonpost.com/graphics/2019/investigations/dea-pain-pill-database/,https://www.washingtonpost.com/investigations/opioid-death-rates-soared-in-communities-where-pain-pills-flowed/2019/07/17/f3595da4-a8a4-11e9-a3a6-ab670962db05_story.html,https://www.washingtonpost.com/investigations/little-known-generic-drug-companies-played-central-role-in-opioid-crisis-documents-reveal/2019/07/26/95e08b46-ac5c-11e9-a0c9-6d2d7818f3da_story.html,https://www.washingtonpost.com/graphics/2019/investigations/pharmacies-pain-pill-map/,https://www.washingtonpost.com/graphics/2019/investigations/opioid-pills-overdose-analysis/,https://www.washingtonpost.com/national/2019/07/20/opioid-files/?arc404=true,Staff," The team included dozens of journalists from around The Washington Post. The project, while centered around the investigative unit, found contributions from our National, Local, Business, Healthcare, Graphics, Design, Photo and Video departments. Our engineering department also chipped in on deadline to help with the work. ",,,
United States,The Washington Post,Big,Participant,Open data,The Opioid Files,16/07/19,"Investigation,Explainer,Long-form,Breaking news,Database,Open data,News application,Infographics,Video,Politics,Business,Health","QGIS,JQuery,Json,CSV,R,RStudio,Python,Node.js"," The Opioid Files for the first time identified not only the counties flooded with the highest amount of prescription opioid pills at the height of the prescription drug crisis, but the specific manufacturers, distributors and pharmacies that were responsible for bringing those pills into communities. The Post found that over a seven-year period from 2006-2012, over 76 billion pills of hydrocodone and oxycodone were shipped to pharmacies across the country, more than enough for one pill per person per day in some communities. "," The database was the largest that The Post has ever published, containing 380 million records. We made it searchable for the public and other journalists, generating at least 150 stories in 35 states by other media outlets (133 local and 17 national) and more than 50,000 downloads of the data by individuals interested in doing their own digging.   The outlets included the Philadelphia Inquirer, the Detroit Free Press, Minneapolis Star Tribune, the Boston Globe, the Chicago Sun Times, the Arizona Republic, the Columbus Dispatch, the Tampa Bay Times, the Fort Lauderdale Sun-Sentinel and the Portland Oregonian.   Many smaller outlets wrote as well, from the Daily Mountain Eagle in Alabama to the Paintsville Herald in Kentucky to Wenatchee World in Washington state.   The documents The Post obtained shed light on the industry strategy to expand the market and fight DEA’s attempts to hold companies accountable.   The documents provide answers to the enduring mystery of how the drug companies were able to weaken the DEA’s most powerful enforcement weapon at the height of the crisis, by enlisting member of Congress and developing “tactics” and a “Crisis Playbook” to aimed at undermining the DEA. "," Upon receiving the ARCOS data, our first challenge was finding a way to parse the large dataset. We used wrote an ETL pipeline with unix, Python and R scripts that broke the massive CSV file into smaller chunks, converted the files into Apache Parquet files (a columnar-based data format used for large-scale data analysis), and loaded the data into memory as needed.   We fine-tuned our scripts to run everything in parallel (using the pandas and dask Python libraries) which allowed us to reduce the time of our analysis from hours to minutes. This process allowed us to quickly iterate on new ideas as the story unfolded without waiting for all the data to load. And by doing the analysis in both Python and R, we were able to audit each other's analyses and make sure our methodologies were sound. We then published the county-level data on Amazon S3 to allow other reporters and researchers to download the data.   Next, we geocoded (generated coordinates based on an address) every single pharmacy address in the database. (We had to do this manually for at least 3,000 pharmacies.) Then, with node.js, we pulled U.S. census data from IPUMS and analyzed the number of pills distributed within a 5- to 10-mile radius using buffers generated with turf.js.   In sum, the project realized several technology stacks and analysis approaches to dig into the data. "," We originally filed a Freedom of Information request to the DEA, but the agency did not provide the data. The Post then intervened in a civil lawsuit against two dozen drug companies and pharmacies in Cleveland to gain access to the data.   It took more than three years and the intervention in the opioid lawsuit to obtain the data. The Post could not find an attorney at any of the large D.C. law firms because they were already representing drug companies or pharmacies in the case.   We were able to hire a sole practioner from Akron, Ohio, who successfully argued to the 6<sup>th</sup> Circuit of Appeals that the DEA data, along with internal company documents in the case, should be unsealed and released to the public.   On July 15, when we eventually received the data, it contained 380 million transactions. The Post made an emergency purchase of a custom-adapted Dell Precision 5820 Workstation. Working around the clock, we produced our first story two days after the data was released.   But the stories could not have been written without old-fashioned shoe-leather reporting and the careful cultivation of sensitive sources. We needed expert guides who could explain the numbers and point us to the most compelling documents, out of the tens of thousands that were released from the lawsuit, including depositions and internal emails.   More than that, we needed the resources of the entire Post newsroom, eight departments working together, to create a public-facing interactive database and an online repository for the most important documents. We did this under enormous deadline and competitive pressure. "," This project would not have happened without sources and expert legal assistant. Our advice would be to cultivate sources deep inside the agency or company you are investigating to understand how the place works and what kinds of documents are available. Our sources were able to to tell us about the existence of the database at the DEA, the kind of information it contained and what that information might reveal about the opioid epidemic. We also relied heavily on legal counsel both inside and outside of The Post. Our outside lawyer filed to intervene in the federal litigation and convince a U.S. appeals court in Ohio to release the database and unseal tens of thousands of corporate emails, memos and other documents. ",https://www.washingtonpost.com/investigations/76-billion-opioid-pills-newly-released-federal-data-unmasks-the-epidemic/2019/07/16/5f29fd62-a73e-11e9-86dd-d7f0e60391e9_story.html,https://www.washingtonpost.com/graphics/2019/investigations/dea-pain-pill-database/,https://www.washingtonpost.com/investigations/opioid-death-rates-soared-in-communities-where-pain-pills-flowed/2019/07/17/f3595da4-a8a4-11e9-a3a6-ab670962db05_story.html,https://www.washingtonpost.com/investigations/little-known-generic-drug-companies-played-central-role-in-opioid-crisis-documents-reveal/2019/07/26/95e08b46-ac5c-11e9-a0c9-6d2d7818f3da_story.html,https://www.washingtonpost.com/graphics/2019/investigations/pharmacies-pain-pill-map/,https://www.washingtonpost.com/graphics/2019/investigations/opioid-pills-overdose-analysis/,https://www.washingtonpost.com/national/2019/07/20/opioid-files/?arc404=true,Staff," The team included dozens of journalists from around The Washington Post. The project, while centered around the investigative unit, found contributions from our National, Local, Business, Healthcare, Graphics, Design, Photo and Video departments. Our engineering department also chipped in on deadline to help with the work. ",,,
United States,The Washington Post,Big,Participant,Best news application,The Opioid Files,16/07/19,"Investigation,Explainer,Long-form,Breaking news,Database,Open data,News application,Infographics,Video,Politics,Business,Health","QGIS,JQuery,Json,CSV,R,RStudio,Python,Node.js"," The Opioid Files for the first time identified not only the counties flooded with the highest amount of prescription opioid pills at the height of the prescription drug crisis, but the specific manufacturers, distributors and pharmacies that were responsible for bringing those pills into communities. The Post found that over a seven-year period from 2006-2012, over 76 billion pills of hydrocodone and oxycodone were shipped to pharmacies across the country, more than enough for one pill per person per day in some communities. "," The database was the largest that The Post has ever published, containing 380 million records. We made it searchable for the public and other journalists, generating at least 150 stories in 35 states by other media outlets (133 local and 17 national) and more than 50,000 downloads of the data by individuals interested in doing their own digging.   The outlets included the Philadelphia Inquirer, the Detroit Free Press, Minneapolis Star Tribune, the Boston Globe, the Chicago Sun Times, the Arizona Republic, the Columbus Dispatch, the Tampa Bay Times, the Fort Lauderdale Sun-Sentinel and the Portland Oregonian.   Many smaller outlets wrote as well, from the Daily Mountain Eagle in Alabama to the Paintsville Herald in Kentucky to Wenatchee World in Washington state.   The documents The Post obtained shed light on the industry strategy to expand the market and fight DEA’s attempts to hold companies accountable.   The documents provide answers to the enduring mystery of how the drug companies were able to weaken the DEA’s most powerful enforcement weapon at the height of the crisis, by enlisting member of Congress and developing “tactics” and a “Crisis Playbook” to aimed at undermining the DEA. "," Upon receiving the ARCOS data, our first challenge was finding a way to parse the large dataset. We used wrote an ETL pipeline with unix, Python and R scripts that broke the massive CSV file into smaller chunks, converted the files into Apache Parquet files (a columnar-based data format used for large-scale data analysis), and loaded the data into memory as needed.   We fine-tuned our scripts to run everything in parallel (using the pandas and dask Python libraries) which allowed us to reduce the time of our analysis from hours to minutes. This process allowed us to quickly iterate on new ideas as the story unfolded without waiting for all the data to load. And by doing the analysis in both Python and R, we were able to audit each other's analyses and make sure our methodologies were sound. We then published the county-level data on Amazon S3 to allow other reporters and researchers to download the data.   Next, we geocoded (generated coordinates based on an address) every single pharmacy address in the database. (We had to do this manually for at least 3,000 pharmacies.) Then, with node.js, we pulled U.S. census data from IPUMS and analyzed the number of pills distributed within a 5- to 10-mile radius using buffers generated with turf.js.   In sum, the project realized several technology stacks and analysis approaches to dig into the data. "," We originally filed a Freedom of Information request to the DEA, but the agency did not provide the data. The Post then intervened in a civil lawsuit against two dozen drug companies and pharmacies in Cleveland to gain access to the data.   It took more than three years and the intervention in the opioid lawsuit to obtain the data. The Post could not find an attorney at any of the large D.C. law firms because they were already representing drug companies or pharmacies in the case.   We were able to hire a sole practioner from Akron, Ohio, who successfully argued to the 6<sup>th</sup> Circuit of Appeals that the DEA data, along with internal company documents in the case, should be unsealed and released to the public.   On July 15, when we eventually received the data, it contained 380 million transactions. The Post made an emergency purchase of a custom-adapted Dell Precision 5820 Workstation. Working around the clock, we produced our first story two days after the data was released.   But the stories could not have been written without old-fashioned shoe-leather reporting and the careful cultivation of sensitive sources. We needed expert guides who could explain the numbers and point us to the most compelling documents, out of the tens of thousands that were released from the lawsuit, including depositions and internal emails.   More than that, we needed the resources of the entire Post newsroom, eight departments working together, to create a public-facing interactive database and an online repository for the most important documents. We did this under enormous deadline and competitive pressure. "," This project would not have happened without sources and expert legal assistant. Our advice would be to cultivate sources deep inside the agency or company you are investigating to understand how the place works and what kinds of documents are available. Our sources were able to to tell us about the existence of the database at the DEA, the kind of information it contained and what that information might reveal about the opioid epidemic. We also relied heavily on legal counsel both inside and outside of The Post. Our outside lawyer filed to intervene in the federal litigation and convince a U.S. appeals court in Ohio to release the database and unseal tens of thousands of corporate emails, memos and other documents. ",https://www.washingtonpost.com/investigations/76-billion-opioid-pills-newly-released-federal-data-unmasks-the-epidemic/2019/07/16/5f29fd62-a73e-11e9-86dd-d7f0e60391e9_story.html,https://www.washingtonpost.com/graphics/2019/investigations/dea-pain-pill-database/,https://www.washingtonpost.com/investigations/opioid-death-rates-soared-in-communities-where-pain-pills-flowed/2019/07/17/f3595da4-a8a4-11e9-a3a6-ab670962db05_story.html,https://www.washingtonpost.com/investigations/little-known-generic-drug-companies-played-central-role-in-opioid-crisis-documents-reveal/2019/07/26/95e08b46-ac5c-11e9-a0c9-6d2d7818f3da_story.html,https://www.washingtonpost.com/graphics/2019/investigations/pharmacies-pain-pill-map/,https://www.washingtonpost.com/graphics/2019/investigations/opioid-pills-overdose-analysis/,https://www.washingtonpost.com/national/2019/07/20/opioid-files/?arc404=true,Staff," The team included dozens of journalists from around The Washington Post. The project, while centered around the investigative unit, found contributions from our National, Local, Business, Healthcare, Graphics, Design, Photo and Video departments. Our engineering department also chipped in on deadline to help with the work. ",,,
Spain,eldiario.es,Big,Participant,Best visualization (small and large newsrooms),"One year, three elections in Spain",28/04/19,"Explainer,Breaking news,Chart,Map,Elections,Politics,Women,Immigration,Economy,Employment","D3.js,QGIS,Canvas,Json,Adobe,Microsoft Excel,CSV,R,RStudio,Python,Node.js"," In 2019, Spain public agenda was defined by political issues. The country lived 2 general, and 1 regional, local and european elections during this period. eldiario.es data team - 4 members -  did more than 60 publications based on data to explain:  the rise of the far right political party, the victory of social democrats, the fall of traditional conservative parties, the weaknesses of the electoral system or vote inequality by income level in maps, among others. This project integrates different visualization solutions to explain a complicated electoral (national, regional and local) and multi-party political system for all publics.     "," According to the Sociological Research Center (CIS), eldiario.es is the digital native media most read in Spain to get informed about political issues in Spain. Readers trust on eldiario.es to follow up the coverage related to the electoral process in terms of quality and reliability. According to our metrics, the outcome of the main visualizations generated around the electoral year accumulate in total more than 1M page views. Also, more than half million users has read the stories delivered behind the data.    Moreover, many mainstream political talk shows on TV as Al Rojo Vivo in La Sexta TV, one of the biggest TV channels in Spain,   mentioned eldiario.es visualizations during the electoral period, that stands out our journalistic work and the relevance of the data. The information spread has also inspired other journalist and opinion leaders to write their own personal analysis of the political scene, for example: Ignacio Escolar (1 million followers) wrote an opinion about who poor people and rich people vote for and why using our data to understand than phenomena.     To make better reporting about the electoral process, the data journalist team at eldiario.es has focus on analyzing and storytelling the voting results in terms of gender, ideology, society behaviour, voting evolution or life standards as the main topics.  This figure shows that the society are interested not only on knowing the percentages but the narrative concerning voting behaviour and results.       Social Media also played a significant role to impact due to many academics, professors, politicians and journalists has shared the content helping eldiario.es to loudspeaker the impact and get significant recognition. For example, José Fernandéz-Albertos, one of the most respect political analyst in Spain, shared our publication asking people to subscribe to the membership program of eldiario.es, because of the quality of our material.     "," For this project, we used Excel, R (tidyverse, dyplr and others), Open Refine, SPSS, Node and Python for data analysis and data binding. D3, React, Canvas Javascript, Illustrator, Mapbox GL, Tippecannoe, Datawrapper, Flourish, Scrollama y QGIS for data visualization, scrollytelling and maps.    For example, we used Open Refine to clean a hundred thousand names of all municipality mayors in Spain’s democratic history to figure out that half of all councils have never had a mayoress. We analysed with R electoral results and income distribution from 34.000 voting precincts to explain how conservative parties are stronger in rich areas and how far-right party is growing faster in poor areas.   Our main maps and graphics were made with D3 combining Canvas with SVG and enhanced with React to improve loading and transition. To create voting precincts maps, we used QGIS and Tippecanoe to reduce the size of geographic files to optimize mobile navigation and Mapbox GL javascript library to map the results.    The election application is served on Google Cloud Platform with Firebase and is composed of several technologies. For the backend we have a series of microservices in nodejs, both to extract the data and to serve them, additionally a microservice in python to combine two large csv and in the frontend we have React.     ","   Spanish electoral system is a multi-party proportional unlike other majoritarian electoral systems. This means that the results are not divided in two antagonical blocks. That created a huge first challenge: Our system has a multi-party proportional representation with many local and regional realities that must be pictured. For example: all the graphics and maps have to tell who is the winner, but also the second and third place parties that are important in the negotiations to choose the president.  We had to deal with more political parties than ever in Spain history, and most of them only participated in regional and local election. Some political parties only participated in one of 8,000 municipalities or one of the 17 regions.   This together with the range of our users ages that goes from 20 to +65 years a bigger challenge related with usability, interaction and comprehension issues. To solve that our little data team (two data journalists, three developers and a graphic designer) developed a multi-platform through desktop and mobile that showed the information hided behind the data. As a result, many visualizations has pop-ups showing additional information, or maps ar subdivided in categories to better understand the meaning of the data.    Our project helped the audience to understand the behaviour of the spanish citizens in term of voting does not regard to a single issue. Despite the elections are the central thread of all visualizations, the data was not the same for each visualization delivered. That’s why our other challenge was to collect data and create more that 20 databases related with income, demography, age, gender, born place or historical voting. This was a work of more than 12 months that allow us to be ready to compare and tell stories with new focus to explain the electoral process.         "," How to represent electoral results in multiparty political systems.      Coding and technical tools to have data visualizations and analysis in real time.      How to compare and explain a political transformation and patterns in a country or region with other issues such as gender, economics and other geographical and demographic data. We think our method could be extended and create an analysis that apply to whole Europe.     Methods to create collaborative work between sections of a newspaper. In this project we work with each one of the thematic sections of the newsroom to analyze how electoral results can affect all the edges of a country.     ",https://elecciones.eldiario.es/resultados10n/,https://elecciones.eldiario.es/scrolltelling/,https://www.eldiario.es/politica/Vox-zonas-pobres-PP-Ciudadanos-10N_0_963104386.html,https://www.eldiario.es/politica/barrios-pobres-van-votar-elecciones_0_884011872.html,https://www.eldiario.es/politica/cambiado-votantes-partido-ultimos-anos_0_887861651.html,https://www.eldiario.es/politica/municipios-gobernados-mujeres-democracia_0_916908610.html,https://www.eldiario.es/politica/votaron-resultados-elecciones-generales-calle_0_962404599.html,"Raúl Sánchez, Pablo J. Álvarez, Héctor Figueroa, Gilberto López, Jorge Rodríguez, Ana Ordaz, Henar Álvarez"," Raúl Sánchez:  Spanish  data and investigative journalist covering stories of inequality, gender and corruption at eldiario.es. He coordinates eldiario.es data team.   Pablo J. Álvarez: Graduated in Media and Communication Studies and specialized in production and graphic design. After 4 years in Iceland working in the audiovisual industry, now he works as a graphic designer at eldiario.es   Gilberto López: He is a computer engineerand had worked in applications development for 10 years. He worked with different technologies as a Full Stack, DevOps, project manager and CTO in many companies. Now he is XXX at eldiario.es   Hector Figueroa : He is a  Computer engineer. Currently he is working as a Front-end developer at eldiario.es building apps, tools and sites.   Jorge Rodríguez: Jorge is a a developer with 5 years of experience. Currently he is working at eldiario.es as a full-stack developer.   Henar Álvarez: SHe is a Physics PhD passionate of javascript and nodejs, developer and communicator. Currently she works as a Frontend developer at eldiario.es.    Ana Ordaz: she is currently data journalist at eldiario.es creating stories from data and making graphics and visualization for the newsroom.    ",,,
United Kingdom,The Economist,Big,Participant,Best data-driven reporting (small and large newsrooms),Analysing political bias in Google's news algorithm,06/06/19,"Investigation,Explainer,Infographics,Chart,Politics","Scraping,D3.js,Adobe,CSV,R,RStudio,Python"," This article aimed to measure whether the algorithm that Google uses to serve news articles contained an ideological bias. To quantify this, I scraped search results from Google for a selection of 31 keywords on each day in 2018. Then I collected data for variables that Google claims to use in its algorithm (such as how trustworthy and reputable a news source is). After building a statistical model to replicate the algorithm, which predicted how often each news source should show up, we found no evidence that Google was disproportionately favouring publications on the left or right. "," This was the most popular article that The Economist’s data team published in 2019 on its Graphic Detail page, with over 350,000 page views on our website. I also tweeted a <a href=""https://twitter.com/J_CD_T/status/1137063752606605314"">lengthy thread</a> about how I put the analysis and article together, which generated more than 1.5 million impressions. (A link to the Twitter thread is included in this submission.)   Other people have also published analyses of potential bias in online search engines. But I believe that this is the first study that has attempted to explicitly replicate Google’s algorithm by building a statistical model. "," I built the scraper to collect news results from Google in Python, using the Selenium package to manipulate my browser. I also used this approach to collect data from Meltwater, a media tracking tool, about how often each publication had mentioned each keyword in its coverage.   For data about how much Americans trust each news source, The Economist asked YouGov, with whom we regularly conduct polls, to pose this question to 1,500 respondents.   The statistical model was built in R, using a logistic regression to predict how often we would expect each publisher to show up in Google’s news tab for each keyword.   The static charts for the article were created using ggplot2 in R, and then styled up using Adobe Illustrator.   The interactives were created using the React and D3 libraries. "," There were two tricky aspects of this project.   The first difficulty was scraping the data from Google. Google is very good at identifying bots, so I could only scrape 20 pages’ worth of results at a time, before closing my incognito browser and starting again. It is also extremely hard to convince Google that you are somewhere other than your actual location. For this experiment to work, we had to make sure that the search engine was not tailoring its results to our office in London. We ended up using a VPN server in a swing district in Kansas, to represent a politically neutral part of America. But even after switching on the VPN, Google could still locate my computer as being in London (which we checked by opening Google Maps within the incognito browser). It was only after turning off location services on all my devices that we could apparently convince Google that our browser was actually a new user, with no search history, based in Kansas.   The second difficulty was building a statistical model that could replicate the variables used in Google’s news algorithm. We read through the documentation that the company gives to its human “search quality evaluators”, which suggests various measures of “expertise”, “authoritativeness” and “trustworthiness”. We then tried to quantify some of these concepts, using variables such as Pulitzer prizes, ratings by online fact-checkers, polls of the general public, age of publication, and so on. Gathering these figures was time-consuming, but an interesting challenge.    "," I think the main learning from this project is not necessarily the article’s conclusion: it is entirely possible that different researchers, using a different set of criteria and variables, would have reached an alternative finding.    Instead, I hope that this is a useful example of how journalists might be able to apply scrutiny to aspects of technology (such as proprietary algorithms) that shape our lives significantly, but which have little transparency. Scraping is not the only way to do this, but it does allow data journalists to gather a reasonable sample of data in a systematic way. ",https://www.economist.com/graphic-detail/2019/06/08/google-rewards-reputable-reporting-not-left-wing-politics,https://twitter.com/J_CD_T/status/1137063752606605314,,,,,,"James Tozer, Dan Rosenheck, Matt McLean, Evan Hensleigh"," Dan Rosenheck is the data editor at The Economist. James Tozer is a data journalist, Evan Hensleigh and Matt McLean are visual journalists. ",,,
India,Mint,Small,Participant,Best data-driven reporting (small and large newsrooms),Life in the Metro series (Mint - Plain Facts),09/09/19,"Explainer,Infographics,Chart,Map,Satellite images,Environment","Scraping,QGIS,Json,Adobe,Microsoft Excel,Google Sheets,OpenStreetMap,Python","  Life in Metro was a 10-part visual data journalism series by Mint that examined different aspects of city life in the six largest urban agglomerations of India.  The goal was to shed light on cities themselves (like which areas are better-off than others) and to compare cities with each other. From mobility to migration, green spaces to housing, each part of the series used data to map and chart out components of city life - making it a first-of-its-kind project in India  ","  The project generated significant buzz both internally in our newsroom and externally. Academics and journalists reached out asking for some of the data while others were intrigued by how we developed the maps.  Our final rankings of cities received significant traction on social media (Twitter, LinkedIn and Reddit).   ","  Like several data journalism projects in this newsroom and elsewhere - this project largely relied on the standard repository: Python, MS Excel, QGIS, Adobe Illustrator and Google Earth Engine All scraping was done in Python. Most analysis then happened in MS Excel and Python. All maps were generated in QGIS and styled in Adobe Illustrator for print.          What makes the series different is not the tools but how these tools were used to achieve our stories.  The mandate was to use data that was publicly available. This came with two challenges : Data in India is often not publicly available and if it was, it was probably restricted to one city.  The lack of relevant official data at a city level meant we had to improvise. Road speeds for 150 major roads were extracted from Google maps, without an API. Population data at a neighbourhood level were generated from the World Population Dataset. Similarly, green cover was estimated from Google Earth Engine.  Data was also not geocoded. We used publicly available shapefiles and (even generated new shapefiles) to take advantage of census datasets for our stories on migration and public facilities - to generate never-before-seen neighbourhood level maps of major Indian cities.       ","  One of the challenges of a daily page meant that our small team of 7 often has to deal with multiple stories at the same time, and also other responsibilities such as the production of the page for print. This 10 part series was largely anchored by two members in our team (Sriharsha Devulapalli and Vishnu Padmanabhan)  with some data help from a consultant (HowIndiaLives). For each story in the series, we had a lead time of five days - from data sourcing to print. This meant getting the data, making sure it’s error-free, brainstorming a story, writing it and generating highly detailed graphics - and then repeating this process every week for two and half months. We can confidently say that this process of consistently producing high-quality visual data stories is unheard of in an Indian newsroom.   ","    Despite India’s burgeoning newspaper industry, there remain very few large venues for visual print journalism in the country. Plain Facts, by virtue of being a daily data journalism page has largely also pioneered visual data journalism in the last one year. This series, among all the other stories produced at Plain Facts, remains a good model of doing high quality, rigorous and accurate visual data journalism within short turnaround times.   ",https://www.livemint.com/news/india/the-slowest-roads-in-urban-india-1567955358892.html,https://www.livemint.com/news/india/india-s-public-transport-challenge-1568355574941.html,https://www.livemint.com/news/india/the-geography-of-caste-in-urban-india-1569564507580.html,https://www.livemint.com/news/india/the-most-popular-dine-out-hubs-in-india-11570968468566.html,https://www.livemint.com/news/india/the-state-of-social-infrastructure-in-metros-11571582503597.html,https://www.livemint.com/news/india/which-is-india-s-greenest-metro-11572280645457.html,https://www.livemint.com/news/india/which-is-india-s-best-financed-metro-city-11572797317414.html,"Sriharsha Devulapalli, Vishnu Padmanabhan, How India Lives","  This series was anchored by Sriharsha Devulapalli and Vishnu Padmanabhan. Sriharsha Devulapalli is the team’s lead developer responsible for almost all the coding, scraping and graphics for the series. Vishnu Padmanabhan - the Assistant Editor helped in the editing, writing, and analyzing data for the series.  HowIndiaLives - our consultant has provided data for several stories in the series. The series was supported by the editorship of Pramit Bhattacharya, the data editor at Mint, who provided significant intellectual inputs.   ",,,
Spain,Civio,Small,Participant,Open data,Civio Data website,13/11/19,"Investigation,Solutions journalism,Long-form,Database,Open data,Politics,Environment,Corruption,Women,Health","Scraping,Microsoft Excel,CSV,R,PostgreSQL","<h3><a href=""https://datos.civio.es/"">  Datos Civio   or </a> <a href=""https://datos.civio.es/"">Civio Data</a>  website is a repository of more than  50 datasets used in Civio's investigations for the last five years . It includes the first and only archive of all pardons granted in Spain, a list of every fire recorded in Spain since 2001 or several datasets of public contracts that do not comply with Spanish law, among others. These datasets are creative commons and open for everyone, which allows: 1)  readers to check the facts themselves ; 2) media professionals to  re-use data that was not publically available before .</h3>","<h3>Civio Data has been recently released, however, Civio's commitment to open data has a long track record. In our efforts for transparency, in public administration and in our own work,  we have always made available the datasets used in our investigations,  encouraging colleagues to re-use the data we liberated. The website is the result of years of work and  use of these datasets by readers, subscribers and colleagues . Sometimes, the data we published ended up being the base for  scientific papers . For instance, the paper ‘<a href=""https://ieeexplore.ieee.org/document/8123860"">Using Firefighter Mobility Traces to Understand Ad-Hoc Networks in Wildfires</a>' uses Civio's project Spain in Flames' database for the analysis of wildfire patterns in the area. Regarding the repository of pardons, the data we made available has been also used in the paper: ‘<a href=""https://dialnet.unirioja.es/servlet/articulo?codigo=7042350"">Are pardons in Spain proportional to sanctions?</a>' by the University of Palma de Gran Canaria.</h3> <h3>Moreover, all our data is creative commons, which allows other  media outlets to use and find different approaches . Sometimes, the use of databases with a local scope has allowed different journalistic pieces from the same information. For example, the repository of forest fires has been used by local journalists to explain in-depth endemic <a href=""https://www.eldiariomontanes.es/cantabria/pueblos-siempre-arden-20180930173114-nt.html?ref=https:%2F%2Fwww.google.com%2F"">environmental problems in Cantabria</a>, as well. More recently, our database of public contracts was also used by local journalists call on local and regional authorities to take responsibility, <a href=""https://www.elcorreogallego.es/galicia/ecg/bng-pide-explicaciones-diputacion-ourense-un-informe-acusa-atribuir-55-contratos-publicos-dedo/idEdicion-2020-01-27/idNoticia-1226366"">as it happened in Galicia,</a> a northwestern region in Spain. Data about anticonceptives' use worldwide was <a href=""https://ojo-publico.com/642/el-24-de-mujeres-en-el-peru-no-accede-metodos-anticonceptivos"">used in countries like Guatemala</a>.</h3> <h3>Since the launching of the Data Civio website in November 2019, there has been more than  350 downloads of our datasets. </h3>","<h3> Each database available at Civio Data website has a story of its own . As a data-driven organization, we are constantly using new technologies in cleaning and analysing data. The pardon repository, for example, was  scraped from the Spanish Official Gazette  (BOE). Scrapping the BOE has also been key to get the data about public workers who have lost their jobs since 1996 on account of conviction.</h3> <h3>However, that is not the only way to get the data that is afterwards published on the website. Sometimes,  we have to collect the data manually from different sources.  This was the case with the information on the <a href=""https://datos.civio.es/dataset/composicion-de-los-consejos-de-administracion-de-las-empresas-publicas/"">composition of the board of directors</a> from public or semi public Spanish companies gender-wise. In order to obtain the data on doctors who were paid more than €50K from one big pharma company we had to convert the original PDFs from the Pharma companies into structured data. Most of the time,  we stand by the Freedom of Information Act to request and access data from public administrations to later, make it available to citizens.  Following this procedure, we have been able to access, process and release data on <a href=""https://datos.civio.es/dataset/resultados-de-las-inspecciones-de-seguridad-alimentaria-en-establecimientos-de-madrid/"">hygiene and sanitary inspections in food premises shops, schools or kindergartens in Madrid</a>.  This data was never published before and had a remarkable local impact.  It created general awareness about the importance of access to information.</h3> <h3>Sometimes, having these databases ready to be analyzed takes days or weeks of work from several people at Civio, who clean and assess the data for days. These tasks range from finding doubled names or numbers; creating small databases from larger ones for colleagues with no programing skills, converting the data to usable formats. Our multidisciplinary team puts a lot of effort in making the databases useful to the public.  </h3>","<h3> Our motto is to liberate the data from public administrations to be available for citizens.  This means that most of the time we have to struggle with bureaucracy -working with the Freedom Information Act- to access the information. The Spanish administrative model, which divides the territory in 17 so-called autonomous communities or regions, does not make it easier. Sometimes we have to do 17 requests for information, one for each region, since each region in Spain has its own Freedom of Information Act. The same happens when we request information from several ministries about, for example, the names of public workers that didn't follow the regular process to become a civil servant.</h3> <h3>Frequently, the data we request can be  considered sensitive by the government,  so our job is to try all the possible paths to get the information. This translates into months of allegations, complaints, arbitration and even court cases, like with the <a href=""https://civio.es/acceso-informacion-en-peligro/"">information about the names of the president's companions in each of the trips in the official airplane</a>. </h3> <h3>It has been very  challenging to make the information available  with the human resources we count on. We are a small non-profit organization of 10 employees. The team has to know the law to release the data, but also needs to clean and structure it, understand it and make it easy for people to access it.</h3>","<h3>Civio Data shows the  importance of open data for public governance . All the data we liberated was not publically available before.  Now other journalists working in smaller media with no skills on data analysis, and the general public can access it  at Civio Data.  Access to information is key to a healthy democratic system , that is why we will keep feeding the project with new datasets we work on. </h3> <h3>What can others learn from this project? Basically:  sharing is caring . By making all the data we use in our investigations open for consultation or analysis, we are not only strengthening democracy, but also  reinforcing journalism . Sharing our datasets  adds value to our journalistic work because our readers can know where exactly the conclusions come from . Moreover, that data can be used by other colleagues.</h3>",https://datos.civio.es/,,,,,,,"Eva Belmonte, David Cabo, Miguel Ángel Gavilanes, Ángela Bernardo, Carmen Torrecillas, Raúl Díaz, María Álvarez del Vayo","<h3>The  Civio Foundation  (Fundación Ciudadana Civio) is an independent,  non-profit newsroom of journalists, developers and data scientists  which investigates in depth topics using an unique mix of  journalism, technology and impac t. We are passionate about data that tell stories, and some of those we have told have received the top awards in international journalism, such as the Gabriel García Márquez prize for innovation, or the Best Investigation of the Year and Best Individual Portfolio at the Data Journalism Awards. We are proof that journalism can be done differently, brilliantly, and from the sidelines of the mainstream media. And more importantly:  journalism that generates change. </h3> <h3>Our job doesn't finish when we publish our stories. We lobby to solve the problems we encounter during our investigations, a model we pioneered (as acknowledged <a href=""https://www.niemanlab.org/2018/11/this-spanish-data-driven-news-site-thinks-its-work-goes-past-publishing-stories-to-lobbying-the-government-and-writing-laws/"">by Nieman Lab</a>). Sometimes we push for having more information so journalists can do their job better as we did recently litigating -and winning- to know the name of the advisors working for public bodies o when we managed to include new articles in the public procurement law to make mandatory publishing way more information on each contract. </h3> <h2> Team </h2> <h3>We are a multi-disciplinary team of 10 (four journalists, three developers and experts in communications and marketing, institutional relations and business development) with international patronage and advisory board. Although we do (increasingly) work on international projects, the focus of our activity is on Spain, and on our headquarters of Madrid.</h3> <h2> The main areas we investigate are: </h2> <ul>   <h3>Transparency, access to information generated by public institutions and how these are held accountable for the decisions they make.</h3>     <h3>Public procurement, its workings, irregularities and the measures necessary to halt corruption.</h3>     <h3>Networks of power and influence, conflicts of interest, revolving doors and good governance.</h3>     <h3>Access to healthcare and the relationship between pharmaceutical companies, doctors, associations and public entities.</h3>     <h3>The correct functioning of the justice system, its independence - or not - and inequality before the law, the granting of pardons and the strengthening of the State of Law.</h3>     <h3>Management of public spheres, the right to know what every single euro is spent on, and how the problem of forest fires is being dealt with.</h3>   </ul>",,,
United Kingdom,The Times,Big,Participant,Innovation (small and large newsrooms),London's first air quality school league table,05/09/19,"Investigation,News application,Infographics,Chart,Map,Environment","Personalisation,D3.js,QGIS,Json,Adobe,R,RStudio,Node.js"," In May, The Times launched it’s “Clean Air for All” campaign, aiming to shine a light on Britain's air pollution crisis and demand the Government update the Clean Air Act.   We built the first ever school league table for air pollution by combining 20m by 20m point data from the Greater London Authority with shape files for every school in the capital. We used this to calculate a pollution score for every school.   The results revealed that every school in London breached the WHO daily guideline of 10ug/m³. Our tool puts pollution data in context for schoolchildren and their parents. "," Our analysis revealed that 6,500 schools educating 2.6 million children are in areas where fine particles in the air exceed the World Health Organisation recommended limit. Our findings led the campaign and featured on the front page of the paper on it’s first day.   In response to our story, the prime minister, Boris Johnson, pledged a “successor to the 1956 Clean Air Act” inspired by The Times campaign. While it is still unclear what the new environment bill will look like, this newspaper continues to campaign for measures to improve air quality in 2020. Our findings also prompted reaction from environment campaigner Chris Packham, UNICEF and London mayor Sadiq Khan who joined The Times in calling for an updated environment bill.   More than half of the 3,200 schools in our database were searched at least once in the interactive published with the story and the tool was received positively by readers. "," The majority of our data analysis was performed in R while the tool itself was built using React. We downloaded open data on air pollution from the Greater London Authority (GLA) for 2013, 2020, 2025 and 2030. The GLA published an updated release while we were working on the story and we were able to add data for 2016 too.   We asked the Ordnance Survey for mapping data for schools: these were shapefiles of the land owned by every school in the country which we used to represent a school's grounds. We buffered these by 10m to include the roads around schools.   To work out the average, maximum and minimum air pollution level found at each school we used points in polygon analysis (“over” from R library SP) to associate each point to a school and then summarised these grouped points.   Our school shapefiles did not include any identifying information about the school other than the UKPRN (a unique property number) and the name. We matched UKPRNs to a scrape of edubase, a database of school details. Where we were unable to find matches, we matched on school names and then finally wrote a script to match the last few schools manually. This allowed us to calculate the number of pupils at a school over the WHO limit.   Our tool also identifies the ten nearest schools to the user’s selected school. To work out where these were we used a nearest neighbour algorithm, again in R. "," We faced challenges in finding mapping data of a high enough resolution to make the tool useful as changes in air quality can occur over small distances. After a lot of research we decided to limit our analysis to London as we could guarantee 20 by 20m gridded data would provide us with a reasonable level of detail.   Communicating a technical topic effectively was also a challenge. The problem of air pollution is both important and complex: The units of measurement need explaining, the weight given to different pollutants requires thought and determining whether the results are good or bad when the full health implications are unknown is complicated.   We feel this project is innovative as it’s the first time a school-by-school breakdown of air quality in London has ever been published. The techniques used to perform the spatial analysis are a first for The Times and it’s a method of finding stories we plan to use more often in the future. "," Take as much time considering the look, feel and message of your output. As involved and complicated the data analysis used to get to your findings is, it won’t be useful unless you can communicate those findings effectively.   When dealing with a technical subject, it helps to take the time to workshop a tool with information designers and then test it on people outside of the project. We put the league table on a test link and sent it to the wider newsroom asking for feedback. This proved to be such a useful exercise that it became an important part of the process in the projects that followed. ",https://www.thetimes.co.uk/article/air-pollution-on-streets-is-poisoning-2-6m-schoolchildren-xgcrzmlzk,https://medium.com/digital-times/how-we-used-interactives-to-bring-the-times-clean-air-campaign-to-life-f2f9780eb595,,,,,,"Ryan Watts, Ben Webster, Sam Joiner, Daniel Clark, Anthony Cappaert"," Ryan Watts is an interactive journalist at The Times and The Sunday Times. He produces charts, maps and graphics to tell data-led stories. He joined The Times in 2018 from a local newspaper in Buckinghamshire.   Ben Webster is environment editor at  The Times , covering the most important environmental stories in the UK and around the world.   Sam Joiner is the interactives and new formats editor for  The Times  and  The Sunday Times . From charts and maps for daily news stories to large-scale projects and interactive games, his team focuses on data-led visual storytelling, bringing ideas to life for readers online.   Daniel Clark is an interactive journalist for  The Times  and  The Sunday Times . He specialises in creating bespoke charts, maps and graphics to tell data-led stories.   Anthony Cappaert is a digital designer for The Times and The Sunday, currently working with the interactive team to visualise data-driven stories. He has over 18 years experience in the media industry. ",,,
Argentina,LA NACION,Big,Participant,Best news application,How were the election results in your voting booth?,28/10/19,"Explainer,Database,Open data,News application,Infographics,Map,Elections,Politics","Personalisation,Scraping,Json,PostgreSQL,Python"," On Election Night we analyzed and presented the results of each one of the approximately 100,000 voting booths throughout the country. We first associated the data from the polling places with the one from the tables and geolocalized each one. Then, we scraped the data from the forms and uploaded them for the users to check and view the results of their table, and checking the original handwritten document with signatures from table authorities and political parties monitors.   This is a relevant news application hat empowers users to find their own narrative within the larger dataset.     "," In Argentina, the designated authority of each one of the 100,000 voting tables prepares a form containing the results. The form is then scanned and sent to the counting centers that load the forms by hand for the vote counting.   The opportunity to use technology to transfer control to the voters and politcal parties monitors and let them see, analyze and compare in detail. In this case our interactive data visualization served to the transparency of the whole election result as we could reach and show the detail of every polling station including the original form manually completed and signed by the desk authorities.   This news application was part of the whole elections package and got + 245.000 views, providing our users with the tools they needed to sift through the datasets in order to discover what was most relevant to them.      ","  Backend     Python 3.6  as main programming language,  Postgre SQL  database and  Cloud services of AWS  were used at the backend.   The first step was to relate the database of schools and that of the associated voting tables compiled from different places. Then we geolocated them using previous databases and compared name and address of each school of both databases. Moreover, we used the  Google Cloud  service to geolocate the remaining ones or verify them. We were able thus to locate about 15,000 schools throughout the country.   The next step was to collect all data on the election day. This process was made as abstract/customizable as possible because we didn't know how the government was going to show the results of the vote at the schools on their website or how they were going to upload the pdf version files of the telegrams.   Two days after the elections, the results were published, the scraper was modified a bit, then we began to save the results in a database and created a link to download the telegram pdf files.   The last step was to create a  json  with all the information of the votes received from voting tables of schools plus their telegrams in json format so that the frontend may be fed and process data.        Frontend    The project uses a  json  which contains votes cast from school along with a link to download the pdf files of telegrams. The technologies used were  Vue.js, LeafletJS  and  Mapbox f or the map. ", The hardest part was discovering that Government had changed which polling stations ID numbers were related to the schools.    We had to restart the process and revised all the documentation again.   Also the geolocalization was corrupted   We downlaoded again the schools and we had to join the schools and polling stations again using SQL.    The tables were a relational database. We had to link every classroom (polling station) to each school and then geolocated them in the map.     , Never take for granted that your work is half done with your past elections databases.    The population is not static.   Every election (every 2 years in Argentina) you have to refine your data.   We could  finally polish our databasese so we  could relate reporting from Primaries 2019 to Primaries 2017.     ,https://blogs.lanacion.com.ar/projects/data/elections-data/mapping-electoral-results-for-each-polling-station/,https://www.lanacion.com.ar/politica/elecciones-2019-mapa-resultados-todo-pais-escuela-nid2300190#/,https://blogs.lanacion.com.ar/projects/sin-categoria/power-bi-to-comb-elections-2019-results/,,,,,"Pablo Loscri, Florencia Fernandez Blanco, Nicolás Bases, Cristian Bertelegni, Nicolás Rivera, Gastón de la LLana, Gabriela Bouret, Gabriela Miño, Bianca Pallaro, Florencia Rodriguez Altube, Giselle Ferro, Ana Gueller"," La Nación Data is the data journalism unit from LA NACION, whose goal is to facilitate open data journalism for change in Argentina.    Created in October 2010 we have used data to tell stories since 2011. Much of our work and effort is invested in building datasets from scratch, with the goal of opening data to citizens´s collaboration and discovering new stories .   We add value using data in daily reporting, and also produce long-term investigations that cover topics of interest with the intention of producing a positive social impact in our community.   We always work on projects and in teams with journalists, interactive designers and thinking in every output including social media.    We all have a shared purpose which is to do something meaningful through remarkable journalism that improves people’s lives and generates knowledge. All of our members have a “data aware” mindset that is transferred daily to all the journalists in the newsroom.   We consider ourselves a strategy and a team that joined forces to support and develop innovation in journalism. All of our work is based on technology, data, openness and teamwork.    And we never get tired of learning something new. This is why we participate and host multiple open data events and hackathons to engage with civil society organizations and embrace collaborative journalism. We even offer free training and lectures for NGOs, students and journalists who are interested in building a transparent future ",,,
Italy,Wired Italy,Small,Participant,Open data,Vaccini d'Italia,31/05/19,"Investigation,Open data,Map,Health","Google Sheets,CSV"," For a couple of years we submitted tens of Foia request to NHS local offices all across the country asking for vaccine coverage data in infants younger than 24 months old. We gathered data to build the most detailed ever vaccinal coverage map of the country.   We built this site to gather together all the data we collected and to show how coverage changed during time.   In particular, we wanted to show the effect of a new law approved in may 2017, raising the number of mandatory vaccines from 4 to 10. "," The story was shared on social media for thousands times. International media such as Cnn and The Times covered it, several local italian media shared their region vaccinal coverage maps.   Also researchers from italian university asked for the data.   Since we used Foia to collect data, we decided to released them in open format. "," I used Tabula to open data (I got several .pdf files), I processed them with Libre Office, I used Tableau to design the maps. "," The hardest part was getting the NHS local offices to release the data. Despite a new Foia law we met resistance.   Some office never answered, others denied the data, other released them just partially.   We have more than one hundred NHS local offices in Italy. "," This investigation was one of the first conducted in Italy using Foia, a law approved in late 2016. It was a pioneer experience in Italian journalism landscape. ",vaccini.wired.it,https://next.wired.it/nextfest-milano-2019/vaccini-ditalia-quanto-e-garantita-la-salute-del-nostro-paese/,https://drive.google.com/file/d/1ZB-itJPlLF89N33C-6-YKq_EItymJcP8/view?usp=sharing,,,,,Riccardo Saporiti," Riccardo Saporiti, freelance datajournalist, contributor at Wired, Il Sole24Ore, Varesenews, co-curator Festival Glocal (digital journalism festival) ",,,
El Salvador,El Diario de Hoy,Big,Participant,Open data,El Salvador Congress Observatory,03/01/19,"Database,Open data","Scraping,Json,CSV,Python"," El Diario de Hoy created an observatory to follow what the members of El Salvadors congress do. This include open data contained in images and PDF files about how congressmen vote in every issue, what laws they propose and how many times they don't go their job. Using some algoritms, the observatory transforms congress data in open data for the public. ","<pre> The impact of the observatory is that it will detect citizenship and the Diario de Hoy will obtain data that demonstrate the deputies approve all the laws requested by the president of the country, as well as detect the lack of parliamentary control over the work of the central government. In the same way, data will be generated that will allow monitoring throughout the years of the labor parliamentarian, know what each legislator has focused on and see if they have actually attended work. On the other hand, the data produced by the observatory allows to connect the work in the commissions that analyze the proposals of law with which it is carried out in the voting plenary session to determine whether or not it becomes the law of the country, something that is not could do until before the production of the new open data.</pre>","<pre> Mainly techniques were used to process and extract text from images contained in PDF documents (scanned), as well as the Python programming language to mass process thousands of documents and download them from the Legislative Assembly website. There was also a need to create a specific algorithm to extract the names of legislators who vote for each law. The difficulty of this was that the Legislature does not use the full names of legislators and there are many with similar or similar names.</pre>","  <pre> The most difficult part was relating the content of the opinions contained in the bills with their respective vote since the Assembly does not associate each vote with its respective file. To connect both data, there was a need to process the images to extract data such as the number of the opinion, the commission that prepared it and the date of it to be able to associate it unequivocally with each vote. There were also repeated votes and duplicate opinions so there was a need to debug.</pre>     <pre>  </pre>  ",<pre> One of the most valuable lessons has to do with the need for parliaments to properly identify the documents that are put to the vote so that it is not very difficult to connect data on how each legislator votes in each subject or bill.</pre>,http://observatoriolegislativo.eldiariodehoy.com/,,,,,,,"Lilian Martínez, Eduardo Sosa",        <pre> Lilian Martínez is editor of the Data Unit of El Diario de Hoy and has extensive experience in data journalism. Eduardo Sosa is a data analyst at El Diario de Hoy and has experience in the use of technology to process and analyze data.</pre>        ,,,
Brazil,openDemocracy,Small,Participant,Best data-driven reporting (small and large newsrooms),Hundreds of Europeans ‘criminalised' for helping migrants – as far right aims to win big in European elections,18/05/19,"Investigation,Cross-border,Politics,Immigration,Human rights","Microsoft Excel,Google Sheets"," In this cross-border data story, we revealed how hundreds of European citizens have been ‘criminalised’ for helping migrants, particularly in places where the far-right holds national or local power. We worked with journalists across Europe to compile the longest known list of more than 250 people across 14 countries who have been arrested, charged or investigated under a range of laws over the last five years for supporting migrants. These cases suggest a sharp increase in the number of people targeted. At least 100 people were arrested, charged or investigated in 2018 alone (twice as many as in 2017).  "," The resulting story, published the weekend before the European Parliament elections, ahead of which far-right politicians in several countries were running on anti-migrant and ‘law and order’ agendas, had wide pickup globally in several languages, including in <a href=""https://www.alalamtv.net/news/4226126/%D8%AF%D8%B1%D8%A7%D8%B3%D8%A9-%D9%85%D8%A6%D8%A7%D8%AA-%D8%A7%D9%84%D8%A3%D9%88%D8%B1%D9%88%D8%A8%D9%8A%D9%8A%D9%86-%D8%A7%D8%B9%D8%AA%D9%82%D9%84%D9%88%D8%A7-%D8%A8%D8%B3%D8%A8%D8%A8-%D8%AA%D8%B6%D8%A7%D9%85%D9%86%D9%87%D9%85-%D9%85%D8%B9-%D8%A7%D9%84%D9%85%D9%87%D8%A7%D8%AC%D8%B1%D9%8A%D9%86-%D9%88%D8%A7%D9%84%D9%88%D8%B6%D8%B9-%D9%8A%D8%AA%D9%81%D8%A7%D9%82%D9%85"">Arabic</a>, <a href=""https://www.seznamzpravy.cz/clanek/zatceny-knez-nebo-hasic-stovky-evropanu-celi-trestum-za-pomoc-uprchlikum-72280"">Czech</a>, <a href=""https://www.quotidianosociale.it/solidarieta-con-i-migranti-centinaia-di-donne-anziane-vigili-del-fuoco-e-sacerdoti-arrestati-rivela-nuovo-studio-di-opendemocracy-diffuso-oggi/"">Italian</a>, <a href=""https://ecodiario.eleconomista.es/sociedad/noticias/9888076/05/19/Alrededor-de-250-europeos-sancionados-por-ayudar-a-migrantes-desde-2015.html"">Spanish</a>, and <a href=""http://www.haberdar.com/dunya/multecilere-yardim-eden-avrupalilar-tutuklaniyor-h135509.html"">Turkish</a>. Journalists, researchers and NGOs from several countries asked us to share the underlying dataset to facilitate further follow-up work. These include a reporter at PRI who looked at similar trends in the US and <a href=""https://www.pri.org/stories/2019-06-06/crimes-compassion-us-follows-europes-lead-prosecuting-those-who-help-migrants"">heavily cited our research</a>; the Migration Policy Group which was working on a related report for European policymakers; and the Church of Sweden. We also shared our findings in advance with <a href=""https://www.independent.co.uk/news/world/europe/migrants-refugees-solidarity-europeans-arrested-europe-opendemocracy-a8919686.html"">the Independent</a> newspaper in the UK with whom we co-timed publication.   Responding to our findings, Danish MEP Margrete Auken said: “The EU has a humanitarian responsibility, and a moral duty, to rescue and assist people in danger. But is currently far from living up to its own values”. Dunja Mijatović, the Council of Europe’s commissioner for human rights, said “instead of clamping down on those who help migrants live a more dignified life,” European leaders must “recommit with human rights, the rule of law and European values. This is both a legal and a moral duty”. "," We constructed and analysed the largest known, cross-Europe dataset of individuals reportedly arrested, charged or investigated under a range of laws over the last five years for supporting migrants. To do this, we drew on and compiled previous research by think tanks and NGOs (where it existed, in some countries) and conducted structured searches of online news reports in half a dozen languages, compiling this information in a spreadsheet, with one record (row) per individual, and coding each record based on whether the individual was publicly identified as an activist, religious leader, etc.   First, we compiled cases listed in available research reports from the Institute for Race Relations in the UK (in 2017 & 2019) and records of cases in France (and some other countries) documented by the <a href=""https://www.gisti.org/spip.php?page=sommaire"">Gisti</a> advocacy group. We then worked with journalists across Europe to conduct a first round of ‘wide net’ research online to develop lists of keywords / search terms commonly used in reports about these cases (ex. the expression ""délit de solidarité"" in France).    Journalists searching for cases in online news archives in different countries followed common instructions to fill out a shared Google spreadsheet consistently. We also individually contacted dozens of civil society groups across Europe including migrant rights and lawyers associations knowledgeable about these cases and incorporated any examples and information they shared with us into our spreadsheet, in the same format.   Prior to publication, each record in the spreadsheet was separately double-checked with its listed source(s). In some instances we excluded records from our list upon further review. We also produced pivot tables summarising results by country, for example, and a detailed methodology note explaining our steps, the spreadsheet’s contents and how to interpret it (including limitations of the data stemming from its reliance on news reports). "," There is a lack of good cross-country data on issues like these that are extremely topical and of huge public interest. But there are reasons for this -- it is difficult and time consuming to compile such information, across languages, and sometimes without common terms for these laws or related punishments.    Understanding the difficulties in gathering this information was key to helping us realise, for example, that the first thing we needed to do was develop a definition of what we mean by “solidarity crimes” and a common methodology (way of searching, and recording information), to use across countries including lists of country-specific search terms. As noted above, in France for example the expression ""délit de solidarité"" was often used in reports.   While there have been previous, narrower studies that focused on such cases under anti-trafficking laws specifically, we choose a wider definition: recording any case of an individual arrested, charged or investigated under a range of laws over the last five years for supporting migrants. Most of the cases we compiled involved people providing migrants with food, shelter or other direct assistance. A minority involved people arrested while supporting migrants indirectly, for example while demonstrating for migrants’ human rights.    As we did this research, we found hundreds of local media reports and blogs on cases that had never made it into the national media spotlight -- and had thus gone “uncounted” in national debates. Coding the records in our spreadsheet based on descriptions of individuals and their professions (noting if they were described as activists, for example, or religious leaders, or farmers), was also essential to see the real story in the data: not just the numbers but the huge diversity of people affected by these laws. "," Many people unfortunately think that data journalism is only ‘about numbers’. This project was about people. It applied data journalism techniques in particular in developing a methodology for consistent data collection, analysis and coding across countries. But the ‘tools’ we used are easily accessible (we used Microsoft Excel and Google spreadsheets for our dataset, for example, and online search engines and Factiva to research archive news reports. We produced pivot tables and used filters to focus on records that we had coded in different ways. But many of the people involved in the project had never worked with spreadsheets on a journalistic investigation before. Main learnings that others can take from this project are:   Substantive:   <ul>    Most of the cases we found occurred in just seven countries: Italy, Greece, France, the UK, Germany, Denmark and Spain       People have been arrested, charged or investigated under various laws for giving migrants without legal papers with food and other assistance       Individuals affected are not only activists, and have also included about half a dozen priests, first-responders including a Spanish firefighter, several rural farmers and villagers, elderly women, journalists and writers, including a children’s author, and Catholic volunteers    </ul>  Methodological:   <ul>    If there is no data about something, that is topical and of obvious public interest, consider why? Asking this question is what drove us to develop definitions and lists of search terms in different languages as a first step       Coding information you collect is a powerful tool. Without this step, we might not have seen the story -- the huge diversity of people impacted.       Share the datasets you construct, and a clearly written methodology note, with experts that have done similar research before, to have them reviewed independently before publishing your findings.    </ul>",https://www.opendemocracy.net/en/5050/hundreds-of-europeans-criminalised-for-helping-migrants-new-data-shows-as-far-right-aims-to-win-big-in-european-elections/,https://www.independent.co.uk/news/world/europe/migrants-refugees-solidarity-europeans-arrested-europe-opendemocracy-a8919686.html,https://www.pri.org/stories/2019-06-06/crimes-compassion-us-follows-europes-lead-prosecuting-those-who-help-migrants,https://www.seznamzpravy.cz/clanek/zatceny-knez-nebo-hasic-stovky-evropanu-celi-trestum-za-pomoc-uprchlikum-72280,https://www.quotidianosociale.it/solidarieta-con-i-migranti-centinaia-di-donne-anziane-vigili-del-fuoco-e-sacerdoti-arrestati-rivela-nuovo-studio-di-opendemocracy-diffuso-oggi/,https://ecodiario.eleconomista.es/sociedad/noticias/9888076/05/19/Alrededor-de-250-europeos-sancionados-por-ayudar-a-migrantes-desde-2015.html,https://www.alalamtv.net/news/4226126/دراسة-مئات-الأوروبيين-اعتقلوا-بسبب-تضامنهم-مع-المهاجرين-والوضع-يتفاقم,"Nandini Archer, Claudia Torrisi, Claire Provost, Alexander Nabert, Belen Lobos","  Nandini Archer  is 50.50's Assistant Editor. She covers stories relating to gender, sexuality, feminism and social justice. She is also an active member of the feminist direct action group <a href=""https://twitter.com/SistersUncut"">Sisters Uncut</a> and previously worked with the <a href=""http://www.safeabortionwomensright.org/"">International Campaign for Women's Right to Safe Abortion</a>.     Claudia Torrisi  is a freelance reporter for <a href=""https://opendemocracy.net/5050"">openDemocracy's gender and sexuality section</a> and the ongoing <a href=""https://www.opendemocracy.net/en/5050/tracking-the-backlash/"">Tracking the Backlash</a> investigative project. She is based in Rome.    Claire Provost  is editor of openDemocracy 50.50 covering gender, sexuality and social justice. Previously she worked at  The Guardian  and was a fellow at the Centre for Investigative Journalism at the University of London, Goldsmiths.    Alexander Nabert  is a journalist based in Berlin. He has collaborated with <a href=""https://taz.de/"">Taz.de</a> and <a href=""https://www.welt.de/"">Welt</a>.    Belen Lobos  is an independent journalist, documentary photographer and filmmaker. She has a postgraduate in International Relations and the Global South. She has worked as a freelance foreign correspondent in Ukraine, the UK, Italy and Spain, having collaborated with <a href=""https://www.bbc.co.uk/programmes/b06g7rxl"">BBC radio</a>, <a href=""https://www.pikaramagazine.com/2016/03/una-mirada-feminista-sobre-la-crisis-de-refugio-en-europa/"">Pikara Magazine</a>, <a href=""https://www.elperiodico.com/es/internacional/20131226/las-protestas-se-perpetuan-en-ucrania-2960332"">El Periódico</a>, <a href=""https://issuu.com/wellesleyglobalist/docs/frontiers"">The Wellesley Globalist</a> and <a href=""http://www.publico.es/internacional/496430/la-ciudadania-ucraniana-cada-vez-mas-amordazada"">Público</a>, among others.  ",,,
United Kingdom,The Times,Big,Participant,Best data-driven reporting (small and large newsrooms),The state of Britain's rivers,08/02/19,"Investigation,Open data,Illustration,Infographics,Chart,Map,Environment","Adobe,Creative Suite,CSV,R,RStudio"," Our investigation into the health of the nation’s waterways revealed that pollutants found in rivers were at their highest levels since testing began: one in ten tests revealed a serious breach, with at least one recorded at half of all testing sites.   We visited some of the most idyllic rivers flagged by the data in the height of summer to see if residents using them for bathing were aware of the dangers.   Our in-depth reporting humanised the data while also exploring the context behind the story: that sampling levels were down significantly and budget cuts had prevented effective monitoring. "," The project prompted a strong response from our readers and generated a lot of discussion. Some shared their experiences of wild swimming, others shared their shock at our findings and at the revelations of how many fewer tests were now taken.   A group campaigning for better regulation of the Ilkley river in the North of England praised our investigation and have since gone on to apply for the river to recieve bathing certification from the Environment Agency. This would mean the EA would be forced to test the water quality with greater scrutiny. "," We used open data published by the Environment Agency to filter millions of samples taken across the country to find all tests for “hazardous pollutants”, as identified by the Environment Agency. We had to collate 20 years worth of data across multiple, large files and used Apache Spark to query a database too large for R to handle on its own.   To determine whether a test was a breach or not, we compared the recorded result against a CSV file of “hazardous pollutants” published in the government’s guidance to all businesses with Environment Agency permits.   We grouped the breaches by the name of the river it was recorded at to determine how many breaches are recorded annually at each site and we grouped the data by year to calculate the proportion of all tests taken that year that revealed a breach.   We used additional datasets published by the EA to determine that 86 per cent of the countries waterways fell short of the EU’s ecological standard at last assessment, again performing our analysis in R. We mapped this using a shapefile we generated by combining scraped files of individual rivers on the EA website, combining them into a single shapefile, and then joining with our summarised data. "," Our headline figures did not reveal the full picture. Once we had completed our analysis, and discovered that the proportion of tests that revealed breaches was at its highest since testing began, we wanted the answer to why this was happening. The Environment Agency told us that more breaches meant that sampling had become increasingly targeted and risk-based.   Through our reporting we discovered that the testing regime itself was as interesting as the results. By digging into the number of tests taken to discover that sampling was down dramatically over 20 years, we found a new angle for the story.   Ensuring this report was appropriately nuanced and considered the scale of sampling, the responsibilities of water companies and the Environment Agency, and the true dangers of the substances being tested for helped make this in-depth piece a strong example of data-informed reporting. "," We put our findings to people swimming in the River Wharfe on a warm summer day. They, alongside fishermen, volunteers, conservationists and other stakeholders in the health of the river, provided an important perspective as to why water quality testing was so important.        We used the data to identify rivers of interest – those with significant breaches or where the number of tests was falling disproportionately – and then focussed our reporting there. Using the figures to inform our journalism from the very beginning made the process easier and helped us find good case studies. ",https://www.thetimes.co.uk/article/pollution-no-river-in-england-is-safe-for-swimming-q8thdx678,https://www.thetimes.co.uk/article/river-pollution-toothless-watchdog-lets-water-giants-set-own-fines-x6z8vml3z,https://www.thetimes.co.uk/article/swimming-in-sewage-children-taken-ill-as-treatment-plants-fail-jk2kh899j,https://www.thetimes.co.uk/article/the-times-view-on-pollution-in-britain-s-rivers-filthy-business-tjfgfx9vx,https://www.thetimes.co.uk/article/what-are-the-effects-of-river-pollution-rbl3rf8cs,,,"Ryan Watts, Rhys Blakely, George Greenwood, Dylan Lewis"," Ryan Watts is an interactive journalist at The Times and The Sunday Times. He produces charts, maps and graphics to tell data-led stories. He joined The Times in 2018 from a local newspaper in Buckinghamshire.       Rhys Blakely has reported for  The Times  from South Asia, the US West Coast and Washington DC. The paper has sent him to hunt giant rats in Mumbai and to Oscars parties in Hollywood. In Nashville, Dolly Parton once sang to him. He is now  The Times  science correspondent and lives in Cambridge   George Greenwood is a data journalist at  The Times  and  The Sunday Times  who specialises in using freedom of information requests and analysing open data for investigative reporting. He started his career at BBC News, then joined  The Times  in 2018.   Dylan Lewis interned for The Times during the summer of 2019. He is a student at the University of Manchester. ",,,
Canada,"The Globe and Mail, Quartz",Big,Shortlist,Open data,Facebook Political Ad Collector – joint submission (The Globe and Mail and Quartz),06/03/19,"Investigation,Cross-border,Multiple-newsroom collaboration,Database,Open data,News application,Fact-checking,Crowdsourcing,Elections,Politics","AI/Machine learning,Scraping,Json,CSV,PostgreSQL,Python,Node.js"," The Facebook Political Ad Collector is an open-source crowdsourcing project that monitors political advertising on Facebook. It asks readers to install a browser extension that collects ads from their feeds, submitting them to a database for analysis by journalists in newsrooms around the world. Machine learning is used to classify whether or not each ad is political. Originally built by ProPublica in 2017, the tool was taken over by The Globe and Mail and rewritten in 2019 ahead of Canada's federal election. It has been used extensively by The Globe and Quartz for daily stories and major investigations. "," This international journalistic collaboration is no flash in the pan. After the spotlight left Facebook following the U.S. presidential election, American and Canadian journalists persevered through Facebook's technological hurdles to expose wrongdoing by Facebook and its advertisers.   In Canada, reporters used the ad collector to stay on top of candidates and parties' messaging during the hectic federal election campaign, and as a tool to identify third party advertisers. The project also led to a story revealing that political parties were uploading voters' email addresses to Facebook for targeting purposes, a possible violation of federal privacy laws. It was also used to determine that ads bought by Elections Canada, the country's federal elections authority, were being automatically directed at particular groups of people – often young men – by Facebook's algorithm, meaning the agency was effectively advertising to the electorate in a biased manner.       In the U.S., the project has led to investigations into the shady advertising sold by Facebook. First, the banking industry used a targeting technique, called ""Lookalike Audiences,"" that experts suggest may be illegal and discriminatory. Second, <a href=""http://metals.com/"" rel=""noopener noreferrer"" target=""_blank"">Metals.com</a> is a precious metals retailer that's now under investigation for securities violations. It sells highly-marked up silver coins to conservative retirees, sometimes wiping out half of their savings the moment that they make a purchase. We revealed that it found its customers through millions of dollars in ads from numerous fake grassroots groups with names like ""Retired Republicans"" and ""Sean Hannity Viewers"". Several seniors told us they found the investigation just in time, preventing them from investing; local cops, too, said it helped in (ongoing) investigations.       The extension has been downloaded tens of thousands of times since it was first released in 2017, and nearly 8,000 people currently participate in the project. "," The project consists of a constellation of apps and services written in at least five languages (JavaScript, Python, Ruby, Rust, and bash), all running in unison to provide a back-end that journalists can use to report on political advertising on Facebook.   There's the front-end ad collector, a browser extension for the Firefox and Chrome browsers built in JavaScript and Node.js, which monitors a participant's Facebook feed for ads. There's also a back-end, running a combination of Rust and Ruby on Rails applications backed by a PostgreSQL database to receive ads submitted from installed extensions and serve up the interface journalists use to filter and search for political ads. Finally, a machine learning algorithm known as a Naïve Bayes classifier, written in Python, is used to assign a political likelihood to each ad based on its content. Shell scripts written in bash tie the whole system together and provide automatic database archiving in CSV format for analysis by journalists.       The entire project is open-source and available on GitHub. "," Facebook does not like that we are crowdsourcing the political ads and targeting parameters our readers see on its website.       As a result, they've been aggressive in using technical tricks to change how ads are displayed in an effort to fool our participants' browser extensions, reducing the transparency of political advertising on their platform. In the past, Facebook has made parts of its ad HTML invisible, hidden bits of the ad text or images in CSS, broken the word ""Sponsored"" into nine different HTML entities (one per letter), or even added extra letters to that text, turning it into ""SpSonSsoSredSSS"" in an effort to defeat our collector.       They've even added code that appears targeted only at us, using a JavaScipt hack to make clicking the ""Why am I seeing this?"" button impossible for our extension. It seems Facebook would rather journalists not have access to political advertising targeting data.       But every time Facebook's developers make a change, we find a solution and tweak the ad collector code in turn. It's a technical and arcane game of tennis between the ad collector team and engineers in Menlo Park, played methodically over many months.       To hold the company accountable, we have also written a story each time there's a breaking change. The most recent came during the Canadian federal election in October, when we went from collecting targeting information on nearly 90% of ads to collecting the information just 16% of the time. "," The Facebook Political Ad Collector has set a new standard for global, multi-newsroom collaboration on a deeply technical project – one with huge consequences for global democracy. Since being built by ProPublica in 2017, the project has added dozens of participant media organizations in Canada, the United States, Italy, Switzerland, Australia, Germany, the Netherlands, Denmark, Belgium, Mexico, Latvia and beyond. This can be difficult logistically, such as when we had to figure out why ads weren't being collected in Italy, or when we tweaked the browser extension's code to recognize ads for readers using Facebook in Burmese. Keeping everyone up to date took serious work.   But the effort was worth it. The project enabled organizations across the world to hold Facebook accountable for the political ads bought on its platform. The international nature of the project brought us strength, too, as Facebook's efforts to thwart us earned quiet counterpressure from transparency advocates in several world capitals. ",https://github.com/globeandmail/facebook-political-ads/,https://www.theglobeandmail.com/politics/article-globe-and-mail-takes-over-global-facebook-ad-monitoring-project/,https://www.theglobeandmail.com/politics/article-federal-parties-uploading-voters-e-mail-addresses-to-facebook-to-show/,https://www.theglobeandmail.com/politics/article-politics-briefing-who-is-seeing-political-ads-on-facebook/,https://qz.com/1751030/facebook-ads-lured-seniors-into-giving-savings-to-metals-com/,https://qz.com/1733345/the-fight-against-discriminatory-financial-ads-on-facebook/,https://qz.com/1537686/facebook-blocks-propublica-and-mozillas-ad-transparency-tools/,"Tom Cardoso, Jeremy B. Merrill, Steve Mickeler", Tom Cardoso is a reporter and data journalist at The Globe and Mail.   Jeremy B. Merrill is a reporter and data journalist at Quartz.   Steve Mickeler is a senior cloud solutions architect at The Globe and Mail. ,,,
Canada,The Globe and Mail,Big,Participant,Best data-driven reporting (small and large newsrooms),Why does any Canadian need a handgun?,20/09/19,"Investigation,Long-form,Infographics,Chart,Elections,Politics,Crime,Gun violence","CSV,R,RStudio"," In 2018, a mass shooting in Toronto killed two young women and injured 13 others, shocking the city and prompting policy-makers to draft urgent solutions to Canada's growing gun violence problem. But in the wake of the massacre, all levels of government revealed they didn’t track where crime guns originate from. The culmination of a year-long reporting effort to source Canada's crime guns, our story revealed police forces' poor data practices, bringing questions of gun policy to the fore in last year's federal election campaign and challenging leaders to devise a way forward informed by numbers rather than cheap politicking. "," We deduced that firearms sourcing data existed, but was being held by dozens of municipal forces across the country, so we went about trying to do the work government had shied away from, asking for these databases through freedom-of-information requests. Ultimately, we filed more than 40 requests to governments and the largest police forces in the country. The plan was to collate the gun databases into a cohesive, national picture. But what we got back from these requests was a mess: no two police forces in the country seem to collect their data the same way.    That plan failed. But this failure revealed, in part, why the gun policy debate in Canada has remained at an impasse: there's not enough data to support arguments for or against the tightening of gun regulations. Because the data is so poor, both sides tend to reach a stalemate.    Our work did yield some unexpected findings. A ballistics database acquired from the Royal Canadian Mounted Police showed that assault-style rifles are rarely involved in violent crime in the country, making up just 0.66 per cent of the guns that have been analyzed on murder and attempted murder cases. The day after our story ran, Prime Minister Justin Trudeau took to the campaign trail to advocate a full ban on assault-style rifles as a way of combatting gun crime. Anyone who'd read our story knew it was a political ruse. We also obtained a cache of Toronto Police documents that showed legally-imported handguns make up a sizable portion of firearms used in the city's crime.   In the lead-up to this story, our reporting also prompted Ontario's regional police force to change its inspection policies for gun shops, and revealed a government-run survey on gun control had been gamed by an anti-gun control activist. "," The story – the culmination of a year of source-building and publishing stories on the state of gun crime and gun regulation in Canada – relied heavily on freedom of information requests. In total, we filed more than 40, and extracted gun databases from many major metropolitan police forces and the federal government.   To analyze the data, we relied on tools like R and the tidyverse set of packages, along with a data journalism R template developed called startr, which we developed in-house and which was open-sourced last year.   The story, and the methodology piece breaking down how we failed to arrive at the national gun sourcing figures we hoped to compile, underscore the messiness of the data journalism process: sometimes, no matter how much you wrangle sources, the language of freedom of information requests and the data itself, the answer you hope to find doesn't exist. But that non-answer in itself is a finding, and one we couldn't have arrived at without tools like R, startr, and the dozens of sources who spoke to us. "," In Canada, devising gun policy boils down to one giant problem: data, or rather, a lack of it. In the wake of the Toronto's Danforth Avenue massacre, all levels of government revealed that they didn’t track where guns used in shootings originate from. That has huge policy implications. If the shooters are using guns legally imported for sale in Canada, then gun control can be a helpful measure. But if the weapons are arriving via U.S. smuggling routes, then placing new restrictions, or even bans, on Canadian gun sales would do little to stem the flow of firearms into criminal hands. Without those sourcing figures, Canada can never move forward on evidence-based gun policy.    We also spoke with countless law enforcement sources and balanced their views against dozens of gun enthusiasts, many of whom attended a Vegas-style gun show in the greater Toronto area featured in the article.   But while gathering the data might seem like the hardest part of putting this story together, that wasn't it. It was speaking with those affected by gun violence.   Throughout the story, clarity comes in the figure of Patrick McLeod, an ex-cop whose daughter was caught up in the Danforth shooting. A former gun enthusiast, he says the shooting convinced him that all handguns need to be banned, data be damned. He bases his argument on 18-year-old Reese Fallon, his daughter’s childhood best friend, who died in the shooting. The gun that killed her was Canadian. He says he doesn't need any other data to form his opinion. Why should Canada's dearth of data be an excuse for even one more death?  "," The story was only possible due to the many pieces and reporting we did in the year leading up to the federal election. Without context from law enforcement, government and gun industry sources, and the stories that built on each other and staked The Globe and Mail as a publication regularly reporting on issues of gun policy, we couldn't have crafted the definitive picture of the gun regulation debate during the federal election. Investing the time to meet with sources, and listening to them on what freedom of information requests we should file and what stories we should look into were crucial.   We also became adept at filing freedom of information requests that extracted databases from police forces, no easy feat in Canada, where freedom of information laws are much stricter than in the United States. As our open requests piled up, we figured out the language we needed to use and the things we needed to ask for that would get us the data we wanted. Anyone looking to harvest data across multiple government departments via freedom of information requests should be patient, file as many requests as they can up front, and learn from their successes (and failures) while other files remain open. ",https://www.theglobeandmail.com/canada/article-why-does-any-canadian-need-a-handgun-what-the-gun-control-debate-is/,https://www.theglobeandmail.com/canada/article-how-the-globe-tried-and-failed-to-find-the-source-of-canadas/,https://www.theglobeandmail.com/canada/article-ontario-government-flouts-own-law-on-gun-data-collection/,https://www.theglobeandmail.com/canada/article-they-haunt-us-for-the-rest-of-our-careers-one-doctors-experience/,https://www.theglobeandmail.com/canada/article-critics-question-ottawas-online-survey-that-found-strong-opposition/,https://www.theglobeandmail.com/canada/article-new-report-on-potential-handgun-ban-finds-canadians-divided-on-gun/,https://www.theglobeandmail.com/politics/article-liberals-want-to-prohibit-more-assault-weapons-rather-than-ban/,Tom Cardoso and Patrick White, Tom Cardoso and Patrick White are crime and justice reporters at The Globe and Mail. ,,,
United States,"Chicago Tribune, ProPublica Illinois",Big,Shortlist,Best data-driven reporting (small and large newsrooms),Chicago Tribune,19/11/19,"Investigation,Explainer,Solutions journalism,Long-form,Breaking news,Multiple-newsroom collaboration,Database,News application,Infographics,Map","Animation,3D modelling,JQuery,Json,Adobe,Creative Suite,Google Sheets,PostgreSQL,Node.js"," This project is a partnership between the Chicago Tribune and ProPublica Illinois. Together, they exposed how Illinois schools put special education students in seclusion rooms or physically restrained them for unlawful reasons, sparking swift government action.   The two lead reporters on the project, Jennifer Smith Richards of the Tribune and Jodi S. Cohen of ProPublica Illinois, worked side by side through every step of the story. The collaboration also included members of the data, visual, editing and production teams at both news organizations.  "," “The Quiet Rooms” resulted in immediate, meaningful change that will affect the lives of thousands of children.     Emergency actions:  A day after publication, Gov. J.B. Pritzker called the state’s seclusion practices “appalling” and directed the Illinois Board of Education to<a href=""https://www.propublica.org/article/illinois-school-students-seclusion-rooms-state-emergency-action-pritzker-carroll""> take immediate action</a>. State officials issued emergency rules prohibiting schools from putting children into isolated timeout behind locked doors. For the first time, the state is monitoring timeout and restraint. Schools must notify state officials within 48 hours of an intervention. The state board opened complaints against eight districts named in “The Quiet Rooms.” Already, schools have reconfigured rooms, removing doors and turning these spaces into more welcoming places.    Permanent rules:  The state board plans to make its emergency rules permanent this spring. One proposal would ban prone restraint, in which children are held face down on the ground. Other proposed rules would require additional training that includes de-escalation techniques and behavior management.    State legislation:  Members of the General Assembly have proposed bills banning seclusion and severely limiting physical restraint. They<a href=""https://www.propublica.org/article/illinois-hearing-school-seclusions-restraints""> held a hearing in early January</a> and plan to vote on legislation in May.     Federal action:  U.S. senators and 10 members of Congress, all but one from Illinois,<a href=""https://www.propublica.org/article/illinois-lawmakers-letter-nationwide-ban-isolated-timeouts-students""> urged federal education officials to issue guidance to schools to ban seclusion</a>, limit restraints and encourage “evidence-based alternatives.” A federal bill to ban seclusion is being revived. "," To provide the most thorough and systematic analysis of seclusion and restraint across the state, we designed a database and hand-entered details from 35,000 incidents at more than 100 school districts. That allowed us to determine the total number of interventions, the average length of time children were secluded, the most commonly-used restraints, and much more.    The database compiling details of students’ experiences with seclusion and restraint for the first time. Though most of the database was entered by hand into a Google sheet, reporters used optical character recognition software and programming to clean and load some data when possible.   A robust data diary and data dictionary also were maintained in a Google sheet.   The presentation from beginning to end had innovative elements intended to seamlessly weave narrative with data through a combination of text, photos, video portraits, documents and interactives. These components helped readers to gain a child’s-eye view of the issue as well as see the scale of use  across the state.   - The animated panorama was created using javascript and an iPhone panoramo setting.   - The interactive map was created using mapbox, javascript and jQuery.   - Evidentiary documents were made zoomable with a javascript library called drift-zoom.   - We used FOIA to obtain floorplans and recreated different rooms on the same scale using Lightwave 3D modelling.   - To demonstrate techniques of restraint we used FOIA and attended a real training session used models to show the approaches. We drew over the photos with Adobe Illustrator.         "," Although Illinois law required schools to document each incident of seclusion and restraint of a student in detail, there was no requirement that the state — or any monitor — review them. That meant that no one had ever collected the incident reports.   A number of school districts refused to turn over records; a lawyer for the media organizations interceded, leading to the release of additional documents. And officials at almost every school contacted by reporters refused to allow them to view the seclusion rooms, or even to step inside the buildings. Reporters then submitted public records requests for floor plans, images and other records that would show the rooms. Some districts refused to even provide those, citing public safety concerns. We asked children to draw pictures of the rooms. We ended up publishing some of the school-supplied photos and student drawings.   Reporters traveled the state to speak with families, school employees and advocates. They interviewed more than 120 people for the story, many of them in person in their homes or in local coffee shops, restaurants and libraries. "," This project was a partnership between the Chicago Tribune and ProPublica Illinois, and the two lead reporters on the project worked side by side through every step of the story. But the collaboration also included members of the visual, editing and production teams at both news organizations. In short, innovation is enhanced by collaboration.   The unique visual presentation of “The Quiet Rooms” seamlessly integrates text, photography, silent video portraits, annotated documents and interactives. The first image readers encounter is a panning video of a padded seclusion room, accompanied by heart-wrenching quotes from children that school workers wrote down while documenting isolated timeouts. ProPublica creative story technologist Agnes Chang created the opener using panoramic photos taken by the Chicago Tribune’s Zbigniew Bzdak.   To make it easier for readers to examine annotated documents without clicking away from the story, the project team built a zoom feature. Two interactive data tools — one on seclusion, the other on restraint — allow readers to look up their school district’s use of these practices.   To work in collaboration, the two newsrooms needed to be both innovative and flexible. The teams split up tasks; for example, the Tribune built the lookup tools and other visual elements, while the ProPublica team focused on the stunning opening experience. A coordinated design process ensured the presentations on each website were identical.   Finally, publishing the stories under a Creative Commons license (typical for ProPublica but a first for the Tribune) meant other news outlets could — and did — republish the work. The republication and widespread citations of the stories permitted many more people to learn about this issue and begin to take action. ​ ",https://graphics.chicagotribune.com/illinois-seclusion/index.html,https://graphics.chicagotribune.com/seclusion-gages-lake/index.html,https://graphics.chicagotribune.com/seclusion-restraint/index.html,https://www.chicagotribune.com/investigations/ct-watchdog-isolation-seclusion-schools-20191120-2t2wxtccafhu5nmia6itfbn4pq-story.html,https://www.chicagotribune.com/investigations/ct-isolation-seclusion-reaction-isbe-meeting-20191123-qedt3mblofb5ffbcp3wypjrl3y-story.html,https://www.chicagotribune.com/investigations/ct-isolation-rooms-methodology-data-analysis-20191119-n2yh6bya4rdofehtda2nx2cs6y-story.html,,"Jennifer Smith-Richards, Jodi Cohen, Lakeidra Chavis","  Jennifer Smith Richards  has been a reporter at the Chicago Tribune since 2015. Jennifer has a specialty in data analysis and previously covered schools and education for more than a decade at newspapers in Huntington, West Virginia; Utica, New York; Savannah, Georgia; and Columbus, Ohio. Her work has touched on everything from sexual abuse in schools to police accountability to school choice.    Jodi S. Cohen  is a reporter for ProPublica Illinois, where she has revealed misconduct in a psychiatric research study at the University of Illinois at Chicago, exposed a college financial aid scam and uncovered flaws in the Chicago Police Department’s disciplinary system. Previously, Jodi worked at the Chicago Tribune for 14 years, where she covered higher education and helped expose a secret admissions system at the University of Illinois, among other investigations.     Lakeidra Chavis  is a reporter in Chicago, most recently a reporting fellow for ProPublica Illinois. Previously, Lakeidra was a producer for WBEZ’s News Desk (Chicago Public Media), where she reported an in-depth piece on how Chicago’s black communities have been impacted by the opioid crisis. ",,,
Brazil,Folha de São Paulo,Big,Participant,Innovation (small and large newsrooms),Ideological GPS - Analysis of the political debate on Twitter,05/06/19,"Explainer,Infographics,Chart,Politics","D3.js,R,RStudio"," One thousand accounts of influencers (politicians, media, commentators) on twitter and 1.7 million users were positioned on the ideological spectrum according to who they followed or were followed. This classification was used to determine the formation of virtual bubbles, formed by an echo chamber, where users of a certain ideology tend to only share content from people ideologically close. In a second step, this project also made it possible to analyze how different political sides react to events that are trending on social media.  "," In 2019 Ideological GPS had one of the highest audiences among all content produced by Folha de S. Paulo (one of the main media outlets in Brazil), both in terms of page views and retention time.   The article boosted interest in the debate on ideology as measured  by Google Trend. The word ""ideology"" reached its annual peak at the platform after the publication. To date the project appears in 4 of the 5 related queries when searching for the term on the platform in Brazil.   Study groups from two of the most prestigious universities in the country (Universidade de São Paulo and Fundação Getúlio Vargas) invited the authors to debates with academic experts on the subject.  "," To store the data we use a mysql database. To extract the data we used the  rtweet  package in the R programming language. The identification, for later removal, of accounts that probably belonged to bots was done using the  tweetbotornot  package in R. The calculation of the ideological position of each account was done through a correspondence analysis using the the  ca  package. All other analysis were made in R and the visualizations were made in d3. "," In this project we:       Identified Twitter accounts that belonged to deputies, senators, governors, mayors, candidates for presidents, ministers of state, media and influencers;       Created a structure capable of extracting and storing the data of millions of Twitter users who followed those accounts;       Excluded accounts tagged as bots;       Created a method to identify and delete inactive accounts;       Calculated an ideological position of the users from the information of the followers;       Validated this position using data from others sources (votes in plenary sessions and percentage of votes for president by city);       Extracted posts from these accounts related to recent political events.       Analyse the data to show how it demonstrates the existence of virtual ideological bubbles.       After this first material we used the data generated to continuously analyse how people with different ideologies react to trending political news.      An analysis was also done using the data created in this project to investigate emojis that are more or less likely to be used in the profile descriptions of politically left or right user. "," This project addresses the chalenge that the press routinely faces of categorizing political views. It started from the reading of the academic article  Tweeting From Left to Right: Is Online Political Communication More Than an Echo Chamber  by Pablo Barberá and collaborators. The adaptation and implementation of the methodology proposed in the paper gave the basis to objectively analyze the use of social media by politically left and right users.   Thus, from a simple data (who follows who on Twitter) we can extract the political affinity from the user. This can be used as a source for other insights, like the formation of virtual bubbles and analysis of how different ideological groups react to events.   At a time when different political sides are fighting for control of narratives on social media, being able to organize and identify where certain speeches come from, where they gain strength and how they are interpreted by different political sides is of paramount importance. ",http://temas.folha.uol.com.br/gps-ideologico/,https://temas.folha.uol.com.br/gps-ideologico/as-bolhas-na-rede-social/saiba-quais-emojis-mais-representam-a-direita-e-a-esquerda-no-twitter.shtml,https://temas.folha.uol.com.br/gps-ideologico/as-bolhas-na-rede-social/com-lula-livre-esquerda-tripudia-moro-no-twitter-direita-fala-em-nojo.shtml,https://temas.folha.uol.com.br/gps-ideologico/as-bolhas-na-rede-social/ao-debater-educacao-no-twitter-esquerda-e-mais-ativa-e-puxa-centro.shtml,https://temas.folha.uol.com.br/gps-ideologico/as-bolhas-na-rede-social/direita-usa-criticas-a-alvim-para-atacar-comunismo-no-twitter.shtml,,,"Daniel Mariani, Fábio Takahashi, Thiago Almeida, Simon Ducroquet","  Daniel Mariani : Graduated in biological science in 2009,  worked with genetics and psychiatric disorders studies until 2015 when started working with data journalism at  Nexo Jornal. In 2017 went to Folha de São Paulo to be part of the newly created group focused on data journalism, Deltafolha.    Fábio Takahashi:  Data journalism desk editor at Folha de S. Paulo, founder of Brazilian Education Journalists Association, Spencer fellow at Columbia University.    Thiago Almeida:  Designer and developer, completing his bachelor in computer science and working in journalism for 7 years.    Simon Ducroquet:  Visual reporter for Folha de S.Paulo. His work is focused on visual narratives in multimedia, incorporating motion graphics, data visualization, cartography and interactive. Before that, he was an infographics animator at Fusion, a joint-vision ABC-Disney-Univision, and art editor at Nexo Jornal. He has received several awards in the area of design and journalism, including Malofiej, Society for News Design, SIP and the Brazilian Graphic Design Biennial. ",,,
Brazil,Folha de São Paulo,Big,Participant,Best visualization (small and large newsrooms),Daily hits shows that Brazil is the most musically isolated country in the world,14/10/19,"Explainer,Infographics,Chart,Audio,Arts,Culture","D3.js,Adobe,CSV,R,RStudio"," How similar is the musical taste of different countries? To answer this question we compared the percentage of daily hits similarities in 51 countries on Spotify. The analysis revealed 4 clear clusters, these are formed by:   1. Spanish speaking countries;   2. Most European countries, USA, Canada, Australia and New Zealand;   3. Most Asian countries.   Six countries (Brazil, Japan, Turkey, India, Australia and Italy) presented hits that did not clearly place them in these clusters. The most clearly distinctive of these was Brazil. ", This project demonstrated for many Brazilians that the idea that the country is a major exporter of music is not currently valid. This was something unexpected for many and made the story have great reaction on social media. , The data was downloaded using Spotify's API via R. The analysis was made in R. The graphics were produced in Adobe Illustrator. ," The choice of visualization was the most challenging part. The decision for the radial display format allowed all distances to be shown simultaneously from a single central country while referring to a traditional format for viewing geographical distances, such as on an aviation radar. The simultaneous visualization of all distances between all countries (without a central option) was not prioritized because it  produced too many distortions and excess of visual noise. "," This project was able to test a clear hypothesis, that Brazil was musically isolated. In addition, we were able to identify clusters of countries defined by their musical tastes on Spotify's hit charts. ",https://www1.folha.uol.com.br/ilustrada/2019/10/brasil-e-o-pais-mais-isolado-musicalmente-no-mundo.shtml,,,,,,,"Daniel Mariani, Simon Ducroquet, Fábio Takahashi","  Daniel Mariani : Graduated in biological science in 2009,  worked with genetics and psychiatric disorders studies until 2015 when started working with data journalism at the beginning of Nexo Jornal. In 2017 went to Folha de São Paulo to be part of the newly created group focused on data journalism in the newspaper.    Fábio Takahashi:  Data journalism desk editor at Folha de S. Paulo, founder of Brazilian Education Journalists Association, Spencer fellow at Columbia University.    Simon Ducroquet:  Visual reporter for Folha de S.Paulo. His work is focused on visual narratives in multimedia, incorporating motion graphics, data visualization, cartography and interactive. Before that, he was an infographics animator at Fusion, a joint-vision ABC-Disney-Univision, and art editor at Nexo Jornal. He has received several awards in the area of design and journalism, including Malofiej, Society for News Design, SIP and the Brazilian Graphic Design Biennial. ",,,
Spain,Datadista,Small,Participant,Best data-driven reporting (small and large newsrooms),Mar Menor: deep story of a disaster,10/11/19,"Investigation,Explainer,Long-form,Open data,Infographics,Chart,Video,Map,Satellite images,Environment,Corruption,Agriculture","Animation,Scraping,QGIS,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,CSV"," This investigation shows how the Mar Menor, a coastal saltwater lagoon Spain -Western Mediterranean’s largest saltwater lagoon- was polluted after three decades of laws being broken, half-truths about where irrigation water came from and an underground chaos of canals, desalination plants and discharge points that was first fostered and then left to grow without control. We show in exclusive the situation of the thousands of legal water wells that extract water from the Campo de Cartagena aquifer and official statements recognizing part of the problem.  "," All the investigation, interviews, multimedia and web development was done by the two founding journalists of Datadista. Eldiario.es and La Sexta TV as collaborated only for spread the story and gain more impact. Twelve hours after the publication of the investigation thousands of dead fish washed up on Mar Menor. This circumstance helped to amplify our work beacuse we explained the real reasons for this environmental disaster in front of the official regional government version.   The investigation had more than 162,000 unique visitors and more than 250,000 pageviews during the first five days en Datadista.com and in special version for eldiario.es. Two news pieces based on this investigation had broadcast four times in La Sexta News (Octubre 12 and 13) with an audiencie between 826,000 and 1,217,000 spectators each. @Datadista Twitter acount had 1.7 million impressions 42,363 interactions during the first seven days. More than 8,587 Twitter accounts have published links to the report on eldiario.es and datadista.com, some from famous users like the singer Alejandro Sanz, the Spanish actor Santiago Segura or the author of international bestsellers Arturo Pérez Reverte. Sum of RT for this tweets are more than 22,216. On Facebook we had 38,253 shares, comments and reactions. This work has been recognized by the GIJN (Global Investigative Journalism Network) as one of the eight best research in Spanish in 2019, the only one of its annual selection made in Spain. "," This journalistic investigative is based on the analysis of data from public bodies, access to documentation obtained from our own sources and requests for public information. We have also had access to complaints, reports and transcripts of declarations at judicial headquarters. During this research we have interviewed more than thirty sources that have given their testimony in interviews recorded in audiovisual format and off the record.    We used web-scraping to catch thousands of official documents and public contracts to do an analysis of the work done in the last decades by administrations such as the Segura Hydrographic Conference. We have analyzed the public accounts of dozens of large agro-industries, farmer groups and their international subsidiaries that are being investigated by justice for contaminating the lagoon. We have also analyzed data documents leaked with thousands of water wells and agricultural plots in the Campo de Cartagena area with geographic information tools (QGIS).  We have used After Effects, Premiere, Photoshop and illustrator to develop the videos and animations. In web development we have used HTML5, CSS3 and Javascript (jquery and other dozens of libraries). For the visualization of some interactive graphics we have used Flourish. "," We have had to go back many decades to be able to investigate and understand all the causes that have contributed in an interrelated factors to the pollution of the lagoon. We had to read and analyze thousands of official documents, interview more than thirty sources and make half a dozen requests for information. We have obtained part of the oficial data information through leaks, because the administrations, although they are obliged, have not provided the required information. With all the key documents and data we have been able to demonstrate the lack of regulation and inspection by the administrations of the farmers of the Campo de Cartagena, many of them large exporting agricultural producers to Europe and the United Kingdom. The lack of regulation and inspection has contributed to the pollution of the lagoon. Administrations have allowed farmers to exploit he water resources, digging illegal wells and canals that ruined the quality of subterranean clean waters. "," This investigative reporting was difficult and complex to explain to audience because their have to understand very well technical aspects of the functioning of ecosystems, legislative normative at European, national and regional level to know demonstrate how the administrations really did not do their control work. To solve this we have used a visual narrative that allows users to understand at first look the main problems, how this situation has been reached and then ther can to be able to deepen into each story. For this we have used resources based on videos with short interviews, video animations, clear visualizations, satellite images and web animations. We have also made a thread on Twitter to help understand the main headlines of the story. (Project link 5 and 6 ). Because we are very small organisation media, to achieve a greater impact and reach a greater audience, we have reached an agreement with two Spanish national media (eldiario.es and La Sexta TV) to republish and cover the main conclusions of the investigation.         ",https://datadista.com/medioambiente/desastre-mar-menor/eng/,https://datadista.com/medioambiente/desastre-mar-menor/,https://especiales.eldiario.es/desastre_mar_menor/,https://gijn.org/2019/12/04/editors-pick-of-2019-eight-investigative-stories-in-spanish/,https://twitter.com/datadista/status/1182925084283097088 ,https://twitter.com/datadista/status/1224270629702193152,,"Ana Tudela, Antonio Delgado"," Ana Tudela is co-founder of Datadista. Investigative economic journalist specialized in new narrative with 20 years of experience in national media in Spain. She has worked as an economics and business reporter at El Economista, Público and El Español and was Chief of Content in the Spanish edition of Forbes. Collaborate in media like eldiario.es, El País, Jot Down and Ctxt. She is the author of Crisis S.A. (Akal, 2014) and co-author of Playa Burbuja (Datadista, 2018), research on the consequences of the real estate bubble on the Mediterranean peninsular coast. Collaborating professor in different masters of investigative journalism, data and innovation in Spain. Among his investigations, the one that led in 2011 to the dismantling by the Justice of the dome of the General Society of Authors and Publishers (SGAE) stands out. His data and visualization works done in Datadista have won three bronze medals in the ÑH2019 awards for the best journalistic designs in Spain, Portugal and Latin America, as well as a fourth bronze in collaboration with the RTVE Lab for the special #losDATOS speak for the General Elections.   Antonio Delgado is co-founder of Datadista. Datajournalist specialized in interactive visualization and editorial innovation. He has been responsible for Editorial Innovation in Weblogs, head of the Data Unit of El Español and head of Projects in Vocento. He is co-author of Playa Burbuja (Datadista, 2018), research on the consequences of the real estate bubble on the Mediterranean peninsular coast. Collaborating professor in different masters of research journalism, data and innovation in Spain. His data and visualization works done in Datadista have won three bronze medals in the ÑH2019 awards for the best journalistic designs in Spain, Portugal and Latin America, as well as a fourth bronze in collaboration with the RTVE Lab for the special #losDATOS speak for the General Elections.     ",,,
Spain,Datadista,Small,Participant,Open data,Hiperia,31/01/19,"Investigation,Multiple-newsroom collaboration,Database,Open data,News application,Illustration,Video","Animation,JQuery,Json,Adobe,Creative Suite,CSV"," Hiperia is a spanish platform to manage and follow up journalistic projects based on Freedom of Information requests and Open Data. Users can manage FOI requests, documents, data, links and contacts for each investigative project and share this projects with others registered users. Users can manage hundreds of information requests at three administrative levels: national, regional and local. Hiperia notifies you with automated alerts about time limits of response and claim for FOI request depending on the selected administration. Users can share their projects with others and manage all documents, links, datasets and contacts associated with each journalistic project. "," Hiperia was lunched as private beta in January 2019. Journalists from different media in Spain have been testing the tool during 2019. In Spain, there are dozen of transparency laws at national and regional administrative levels. Also different local transparency regulations. For each informative piece about (i.e.) health or education, due to the administrative distribution of competencies, it is necessary to make a minimum of 18 information requests and manage their deadlines and claims. With Hiperia users can follow up journalistic investigations based on requests for public information and data. This investigations do not end when the news is published. It is necessary to follow up the information and back to repetive the FOI requests at least every year. "," Hiperia has been built using open technologies like PHP, MYSQL, HTML, CSS and Javascript. "," We have been working for months on an exhaustive comparative legislative study of 16 transparency regulations in Spain (at national and regional level) and we have converted this legislation to code to allow users to inform and alert you of the status of their requests, show jurisprudence or interpretative criteria of the Spanish Transparency Council to help users make their requests for information and management of a journalistic project based on requests for information and open data. "," Hiperia  wants to be a hub that hosts all the jurisprudence on transparency in Spain as a weapon to obtain public information from journalists, activists and researchers. With Hiperia we want journalist to systematically use transparency laws as a fundamental weapon to do their job. Our mission is to help simplify the complexity of the different legislations, their response times and the management of a research project based on open data. Projects can be shared with other platform users  ",https://hiperia.datadista.com,,,,,,,"Ana Tudela, Antonio Delgado"," Ana Tudela is co-founder of Datadista. Investigative economic journalist specialized in new narrative with 20 years of experience in national media in Spain. She has worked as an economics and business reporter at El Economista, Público and El Español and was Chief of Content in the Spanish edition of Forbes. Collaborate in media like eldiario.es, El País, Jot Down and Ctxt. She is the author of Crisis S.A. (Akal, 2014) and co-author of Playa Burbuja (Datadista, 2018), research on the consequences of the real estate bubble on the Mediterranean peninsular coast. Collaborating professor in different masters of investigative journalism, data and innovation in Spain. Among his investigations, the one that led in 2011 to the dismantling by the Justice of the dome of the General Society of Authors and Publishers (SGAE) stands out. His data and visualization works done in Datadista have won three bronze medals in the ÑH2019 awards for the best journalistic designs in Spain, Portugal and Latin America, as well as a fourth bronze in collaboration with the RTVE Lab for the special #losDATOS speak for the General Elections.   Antonio Delgado is co-founder of Datadista. Datajournalist specialized in interactive visualization and editorial innovation. He has been responsible for Editorial Innovation in Weblogs, head of the Data Unit of El Español and head of Projects in Vocento. He is co-author of Playa Burbuja (Datadista, 2018), research on the consequences of the real estate bubble on the Mediterranean peninsular coast. Collaborating professor in different masters of research journalism, data and innovation in Spain. His data and visualization works done in Datadista have won three bronze medals in the ÑH2019 awards for the best journalistic designs in Spain, Portugal and Latin America, as well as a fourth bronze in collaboration with the RTVE Lab for the special #losDATOS speak for the General Elections. ",,,
United Kingdom,The Times,Big,Participant,Best news application,Food hygiene ratings: search and compare Britain's restaurants,10/04/19,"Investigation,Open data,News application,Illustration,Infographics,Chart,Health","R,RStudio,Node.js"," Through an analysis of Food Standard Agency (FSA) data, we revealed that 15,000 restaurants in the United Kingdom (one in 20) were failing according to the departments rating system.    We downloaded the ratings for half a million british businesses and built a searchable tool to allow consumers to check the score of any restaurant.   We took the story further with a country-wide audit of the hygiene standards of restaurants featured on the nation’s favourite takeaway apps. Over the course of a week we ran a script which scraped the details of every establishment on Just Eat, Deliveroo and UberEats. "," The results were shocking: 4,000 failing establishments were hosted by Deliveroo; one in eight takeaways on Just Eat were in need of improvement and 30 out of a sample of 500 on Uber Eats scored lower than acceptable.    Deliveroo and UberEats both told the Times that they are looking into adding the FSA ratings to restaurants they feature following our investigation. The story caused a strong reaction from our readers who debated the experiences they had had in restaurants around the country. Some readers even told us that the investigation had changed their dinner plans.   Having easy access to the FSA data with our tool has also led to a few follow-up stories including a story on the hygiene ratings of school caterers. "," We started by downloading the FSA’s entire database. We set up a script to backup this file once a week so we could compare the number of failing restaurants at the start of the process to those in the data at the end. We had to do this because historical data had not been provided by the FSA.   We then used Selenium and R to direct a web browser to each restaurant’s profile page on Just Eat, Deliveroo and Uber Eats. We were unable to find these pages for every restaurant on Uber Eats, so instead scraped as many as we could.   At the time, Just Eat had already started to publish FSA ratings on their site so once we scraped the pages we were able to easily determine the businesses latest rating. This was not the case with Deliveroo and Just Eat so we used reconcile-csv (a command line tool) and fuzzy matching to combine the two datasets.   Our hygiene rating lookup tool is intended to improve upon the user experience of the FSA's own website. We worked with designers to find the best way to present the information available and it was then built using React.   In addition to redesigning the existing offering, we decided not to use the FSA's API but to instead download the data and serve it to the tool ourselves. This allowed us to georeference the restaurant addresses before it was added to the database so readers could search for restaurants within a certain radius of their inputted postcode. Readers wouldn't have to rely on a simple text search like on the FSA site. "," The greatest challenge was scraping each of the three takeaway websites. We used Selenium and a web server to load pages how they would appear on a normal browser so that we could access the right information. We had to script lots of delays so as to ensure we didn't send too many requests to the site at once which would break the scraper. This meant the final scrape took about a week to run.   This project goes above and beyond by taking an existing service, considering how it's likely to be used by consumers, and improving it. Hygiene standards are an important consumer issue and something the public has to take responsibility for by doing the research themselves. Our tool makes that process as easy as possible and opens up the data to everyone. "," Our tool did not only benefit readers but helped us find decent case studies for the wider investigation. We sent a working preview of the tool to the newsroom to ask for help testing it. They sent back useful observations including examples of failing restaurants in their local areas that they were shocked by. We learn that the more interesting case studies came from the experiences of people using the tool and so getting the app, intended to be used by readers, into the hands of journalists early on proved fruitful. ",https://www.thetimes.co.uk/article/food-hygiene-ratings-15-000-restaurants-are-putting-customers-at-risk-5t8wnvmt7,https://www.thetimes.co.uk/article/food-hygiene-ratings-councils-struggle-to-keep-up-with-restaurant-inspections-spg9w8735,https://www.thetimes.co.uk/article/tripadvisor-restaurant-scores-contrast-with-food-hygiene-ratings-qcp26gl27,https://www.thetimes.co.uk/article/just-eat-deliveroo-and-uber-eats-host-thousands-of-unhygienic-restaurants-p2s22gh5r,https://www.thetimes.co.uk/article/food-hygiene-ratings-check-your-local-restaurants-grdvfl83s,,,"Ryan Watts, Kaya Burgess, Sam Joiner, Daniel Clark"," Ryan Watts is an interactive journalist at The Times and The Sunday Times. He produces charts, maps and graphics to tell data-led stories. He joined The Times in 2018 from a local newspaper in Buckinghamshire.   Kaya Burgess covers breaking news and religion stories. He has also written the paper's satirical TMS column, runs the award-winning Cities Fit for Cycling campaign, and worked on the Time to Mind child mental health campaign. He joined  The Times  in 2008.   Sam Joiner is the interactives and new formats editor for  The Times  and  The Sunday Times . From charts and maps for daily news stories to large-scale projects and interactive games, his team focuses on data-led visual storytelling, bringing ideas to life for readers online.   Daniel Clark is an interactive journalist for  The Times  and  The Sunday Times . He specialises in creating bespoke charts, maps and graphics to tell data-led stories. ",,,
Peru,Mina57,Small,Participant,Best data-driven reporting (small and large newsrooms),Mapa de ubicación política de activistas en Twitter sobre la Disolución del Congreso,28/12/19,"Explainer,Open data,Elections,Politics,Culture","Scraping,Json,Microsoft Excel,CSV,Python"," Mina57 Data-Selfies collected data from Twitter-Peru users in order to obtain a map of political position concerning the dissolution of the Peruvian Congress on September 30, 2019. Supported by a Knowledge-Database populated during the last two years, the further analysis of this map, allowed us to measure the effect of social media "" Echo Chamber. "" We follow the ""<a href=""http://www.code-of-ethics.org/code-of-conduct"" target=""_blank"">Code of conduct for professional Data Scientists</a>"" .    ","<ul>  First of all, statistics and technical reports in Peru have a small audience. Furthermore, maintaining a neutral position brings upon you all the radicalized accounts attack.    We posted on Twitter a summary of the project report. The Twitter post thread obtained 60K impressions, 3K interactions, was recommended by the former INEI (National Institute of Statistics and Informatics) chief, and doubled the quantity of our Twitter account followers.    Our conclusions have shown that the "" Echo Chamber "" effect was increasing polarization in the political discussion. The maps generated displaying interactions between Twitter accounts shown two main groups: "" Fujimorismo "" and "" Anti-Fujimorismo "". Because our approach was without political bias, the report introduced as a project product was received for both communities as an input for further analysis. We received 4K visits on the website where our report was published.      </ul>","<ul>   Twitter APIs, scrapping, data cleaning, Python : Used to create a framework that enables the collection of posts published on Twitter, accounts profiling, and networks discovery.     Data visualization, Gephi : Used to analyze the networks created in Python. We applied OpenOrd, YifanHu, and ForceAtlas2 algorithms.   </ul>","<ul>  In order to understand Twitter-Peru communities dynamics, we have built a Knowledge-Database with rules that evaluate a set of variables that change in a significant-political-event basis. We do not identify individual accounts, but those handled by public figures or recognized Twitter activists. We are not applying machine learning because models became obsolete very quickly. At this time, we can apply this tool to update a map populated with Twitter-Peru communities. From a technical point of view, designing and building this Knowledge-Database using only Open Source software made the hardest part of the project.  </ul>     <ul>  Peruvian citizens trust neither established media nor government reports. Polarized discussion in social networks favors fake-news propagation and the ""Echo-chambers"" effect. This project tries to be politically unbiased and provides quantitative information about the quality of Twitter accounts (age, spam rate, activity, popularity) discussing Peruvian politics and the relations established between activists, news providers, and regular accounts. This way, we aim to earn public trust in our data (we provide data tables on our website) and introduce in the discussion the need to listen to everyone, even when the other voice opposes your political position. Therefore, from an effective communication point of view, find a way to better deliver our message was the hardest part of the project.  </ul>    ","<ul>  The effect of similar-minded groups isolation in political discussion. We need to listen to each other and open our minds to different points of view.    Data visualization techniques. We have been approached to teach some lessons related to Gephi use, but we are not able to provide these services. Instead, we have answered the requirement with a road-map, including MOOC courses and Open Source technologies.  </ul>",http://www.mina57.com/mapa-de-ubicacion-politica-de-activistas-en-twitter-sobre-la-disolucion-del-congreso/,http://www.mina57.com/en/map-of-political-position-of-twitter-activists-about-the-dissolution-of-congress/,https://twitter.com/minacincosiete/status/1211077222800535557,https://twitter.com/matuk/status/1211110733938855936,,,,Maria Bravo,"  Maria Bravo    Born in 1963 in a small Peruvian town. Systems Engineer by the Peruvian University of Applied Sciences and MSc by Research in Computer Science by the University of Western Australia. Coursera, Udacity, and edX learner. At this time, I am trying to apply academic research to provide awareness about political discussion characteristics on Twitter-Peru.      ",,,
United Kingdom,Reach,Big,Participant,Best news application,Index of Multiple Deprivation,26/09/19,"Explainer,Breaking news,News application,Crime,Economy,Employment","Microsoft Excel,CSV,OpenStreetMap,Python"," The Index of Multiple Deprivation is a rich and complex piece of research that provides a detailed look at relative deprivation across a country. The research is published every four to five years, and last year saw the publication of indexes for England and Wales.   The indexes involve a number of separate indicators across several domains covering different types of deprivation. All neighbourhoods are then ranked according to their level of deprivation relative to other areas.   The interactive personalises this information in a format that is clear and understandable, while the surrounding analysis provides context. "," Indexes of Multiple Deprivation are something that will shape our readers lives as they are used as the basis for considerations of deprivation in planning and policy.   With this is mind it was important for us, for both England and Wales, to be able to clearly explain what the index was, how it was put together and how the ranking worked, to enable greater public understanding of the topic.   As well as providing background information on the index, it was also important to make the information relevant to people living in the area, by telling them which areas were the most deprived locally. By making the information as relevant as possible to a local audience, it ensured more readers clicked through to the story, meaning more people gained a greater understanding of the index of multiple deprivation.   A key part of both making these statistics more relevant to a local audience and making them more understandable is the interactive embedded in the story, which empowers users to find their own narrative within these much larger datasets. The interactive allows people to look up their postcode and find out how the neighbourhood they live in ranks in the index, both overall and on each of the individual indicators. This is an effective way to impart a large amount of information in a clear visual manner.   The combined effect of this story package, with a clear news angle, simple and clear explanation, and an element of personalisation to bring the story to life, is to make these statistic relevant and understandable to readers.   The versions linked below were one of dozens across England and Wales published on Reach's regional sites, amplifying the impact and ensuring that thousands of readers across the country gained a deeper understanding of this statistics release. "," The success of this project relied on a number of elements.   Firstly, the indexes were breaking news, with national and regional outlets relying on quick analysis to begin live blogs and other coverage that would be built on through that day and beyond. As the indexes analyse deprivation in great detail and at hyperlocal level, there was next a need for more indepth analysis that highlighted variations within local authorities and gave newsrooms across the country a basis to begin producing their own versions of the story. This part of the project involved quick spreadsheet analysis skills, with an element of robot journalism to quickly generate localised versions of the story to be shared with local reporters, including generating links to maps of the areas described making identification quick and easily verifiable.   Secondly, ensuring that the interactive had great design and was user friendly was key. This was achieved with clear design, with high levels of contrast so information is easy to read and process. Using the interactive is simple - people just need to enter a postcode to get all the available information about their neighbourhood. The map gives a clear idea of what area is being talked about - the lower super output areas the indexes use for the analysis are statistical rather than relating to areas as people experience them in everyday life so a map is a simple way to explain this to individuals.     "," With the indexes breaking news, being able to provide as much information as possible as quickly as possible to individual newsrooms was vital, which was challenge. It was important to ensure accuracy and have everything ready to go. Parts of the interactive could be prepped beforehand, building on elements in previous projects that we knew worked well, but getting the data into the interactive and checking that everything worked and was right had to be done to tight deadlines. "," The importance of clear, precise, easy to understand interactives both gives peoplethe tools to explore complex datasets in a way that is useful to them, and empowers them with a greater understanding of the research and analysis that shapes public policy and their lives. Coupling this with in-depth analysis and helpful background information created a package that could be picked up in newsrooms across the country to great effect, with high engagement from readers. ",https://www.kentlive.news/news/kent-news/new-data-named-20-most-3367784,https://www.hulldailymail.co.uk/news/hull-east-yorkshire-news/most-deprived-areas-hull-revealed-3364515,https://www.liverpoolecho.co.uk/news/liverpool-news/merseysides-most-deprived-areas-ranked-16988411,https://www.gloucestershirelive.co.uk/news/gloucester-news/what-its-like-live-most-3366227,https://www.walesonline.co.uk/news/politics/welsh-index-multiple-deprivation-poverty-17323462,https://www.birminghammail.co.uk/black-country/ten-most-deprived-neighbourhoods-black-16987333,https://www.plymouthherald.co.uk/news/plymouth-news/most-deprived-areas-plymouth-revealed-3398057,"Claire Miller, Marianna Longo, Carlos Novoa"," Claire Miller is a journalist for the Reach Data Unit. Her role is to analyse data, find news lines, and write stories that use data to bring a greater understanding of important issues to readers of national and regional news sites.   Marianna Longo is a designer for the Reach Data Unit. Primarily responsible for designing the daily print pages that feature in daily newspapers across England and Wales each day, she also contributes design work for interactives.   Carlos Novoa is a developer for the Reach Data Unit. He creates and codes the interactives, working closely with reporters and designers in the team to ensure such products offer the best experiencing of interacting with the data. ",,,
United States,Univision,Big,Participant,Best data-driven reporting (small and large newsrooms),La ruleta rusa de las drogas: una sola pastilla de este opioide puede ser mortal,04/03/19,"Investigation,Database,Infographics,Chart,Map,Health,Crime","D3.js,QGIS,CSV,Python", Deaths by overdoses tied to Fentanyl have skyrocketed in the past few years.  Just a small quantity can be lethal and my don't even know they are taking it. We explain in this special how it has expanded and its connection to the Mexican cartels and China. , Graphics and data visualization is a fantastic way to inform our audience on any topic.  We are very proud to have been able to convey this important message about fentanyl and its deadly consecuences in a way that easily explained and conveyed our message to all who were able to read our piece.  Many in the hispanic community suffer with this epidemic and it opened many eyes to those who saw it. ," We researched the CDC database to download several databases of drug induced deaths. We parsed the data using python and php and created comma seperated value files that we then translated to several infographics using, d3.js and adobe illustrator. "," The hardest part of the project was navigating the CDC database and finding the correct nomenclature that they use for very specific opioid deaths.  The CDC contains large amounts of information on all kinds of deaths and determining how to find the correct information was a collaberative effort that, although took some doing, was very rewarding in the end. "," Our audience can learn how potent fentanyl is, especially compared with other opioids and narcotics.  They can also see how and why the epidemic of deaths has spiked in recent years.  Also, they can see through maps and corresponding tooltips, where the fentanyl related deaths are more concentrated in the United States.  Finally, they can see how it's entering the United States through the southern border and from China. ",https://www.univision.com/noticias/salud/la-ruleta-rusa-de-los-drogas-una-sola-pastilla-de-este-opioide-puede-ser-mortal,,,,,,,"Javier Figueroa, Amaya Verde, Juanje Gomez, Ana Elena Azpurua"," Javier Figueroa is a graphics developer at Univision. He has been at Univision for the past 4 years. Before that, Javier ran his own web development company for 19 years in Puerto Rico and Miami after graduating from the University of Michigan with a degree in Political Science and Latin American and Caribbean studies. Javier has taught Data Viz at the University of Miami and has won several awards in his field.   Amaya Verde is a Graphics Journalist at Univision. She began working as a Visual Journalist and Information Designer at the Spanish newspaper El Mundo. She was Graphics Editor at Negocio and Graphics Reporter at El Economista, both focused in business and economic news. She created her own company specialized in explanatory graphics, and taught infographics and data visualization at International University of La Rioja in Spain. Generating visual narratives has always been a fervor of hers.   Ana Elena Azpurua is a digital Graphics reporter with Univision Noticias. Before that, she covered local news and Latino issues for  Al Día , a Spanish-language outlet published by  The Dallas Morning News . She began her journalism career in Caracas, Venezuela, as a staff writer with  El Nacional  newspaper. Ana holds a master’s degree in journalism and International Affairs from Columbia University, where she was a fellow at the Toni Stabile Center for Investigative Journalism.   Juanje Gomez has been supporting and working closely with journalists for more than 17 years. In that period he has designed and developed websites, mobile apps, games and infographics. Some of them have been awarded by the Society of News Design and Malofiej. Also, he has created some tools to improve journalists’ endeavors, i.e. a multi-purpose editor which has a series of catalog formats intended to facilitate the construction of news for a finer comprehension and allure to the reader's eye. ",,,
El Salvador,El Diario de Hoy,Big,Participant,Best visualization (small and large newsrooms),El Salvadors presidential results,02/11/19,"Investigation,Documentary","D3.js,Json"," The elaborated visualization allows you to see the results, municipality by municipality, on which political party won in the presidential elections of the year 2019. You also have a search engine that facilitates the comparison of the results of the year 2019 with the results of the last presidential election of the year 2014. ","   The impact was to allow citizens to make a quick comparison of the results of the last two presidential elections, as well as the votes for each municipality. The Salvadoran electoral authority placed the data in such a way that it was slow and tedious to be able to see the votes in each municipality and it was necessary to make many clicks, which was resolved with the interactive map.           "," Las herramientas usadas fueron la librería D3.js y Jquery. Para el buscador, se hizo uso de javascript junto a Jquery y tecnología AJAX. "," The most difficult part was to obtain the data of the electoral registry of the electoral authority in El Salvador, for which there was a need to use a tool to scraping the data contained in the disclosure portals of the respective authority. "," Uno de los aprendizajes es la posibilidad de usar mapas para dar un vistazo rápido sobre los resultados electorales, acompañados de un buscador para profundizar más en los datos. ",https://www.elsalvador.com/noticias/nacional/estos-son-los-municipios-donde-gana-derroto-a-arena-y-al-fmln/566393/2019/,,,,,,,Carlos Palomo," Carlos Palomo es analista de datos, con conocimientos en el lenguaje Python y visualización de datos. ",,,
Canada,The Globe and Mail,Big,Participant,Innovation (small and large newsrooms),startr – a template for data journalism in R,04/12/19,"Database,News application,Fact-checking,Infographics,Chart,Map","Scraping,CSV,R,RStudio"," All data journalists know the pain that comes with starting a new project: hours spent setting up folders, creating files and writing boilerplate code before starting on the analysis. Worse yet, how should you set up your project so others can quickly and easily collaborate and verify your work? Startr, an open-source project built by data journalists at The Globe and Mail, is a tool for the statistical programming language R that streamlines the data journalism process, reducing the amount of time and effort journalists spend setting up and maintaining a project so that they can focus on the analysis. "," Startr has completely changed The Globe and Mail's data journalism practice, which at this point is mostly focused on R-driven analyses. Previously, journalists all had different ways of setting up their projects and structuring their files, had different coding styles in R, and would often include code that could only be run on their own computer (because they were pointing to a folder that only existed on their computer, for instance). This made sharing work and analysis an incredibly painful process, as a new collaborator would spend upwards of an hour setting up folders, installing packages and rewriting parts of the code so that they could reference files on their own system. With startr and a <a href=""https://www.github.com/globeandmail/startr-cli"">sister command-line tool</a>, that time has been cut down to just 10 seconds.   Crucially, the project has also made the work of data verification – essentially, fact-checking the analysis and making sure there are no bugs in the code – much more straightforward. Before startr, it was something Globe data journalists dreaded doing. Now, it's a fast and enjoyable task, since the analysis is structured into discrete steps.   Startr has also streamlined the process of creating web and print graphics. Using R's own visualization tools, we can now generate Globe-styled complex graphics that are ready to be dropped into Adobe Illustrator for final fine-tuning, either for use with a tool like the New York Times' ai2html online or to be laid out in print.   Finally, startr has drastically reduced the amount of time we spend on an individual analysis project, since all the upfront setup work and thinking about structure has been abstracted away. An analysis or story that used to take us days to do can now be done in hours, in some cases. "," As an R project, startr makes heavy use of the ""tidyverse"" set of packages (ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr and forcats) as well as scraping tools (such as rvest) and other R-specific technologies like RMarkdown, which we use to generate HTML ""reports"" that can be shared with non-coding reporters.   At its core, startr is essentially a series of folders and R files that orchestrate the data analysis on behalf of the data journalist. The structure is designed around three data ""steps"":       Processing: This is where a user imports their source data files, tidies them, fixes errors, applies manipulations and saves out a CSV ready for analysis.       Analysis: Using the files generated during processing, this is where all of the true ""analysis"" occurs, including grouping, summarizing, filtering, etc.       Visualization: This uses the analysed data gathered from the second step to generate charts.       An optional fourth step, generating reports using RMarkdown, has made it easier to create HTML files full of charts and other insights that can be shared with non-coding reporters.   Analysis reproducibility is a key consideration in startr – in essence, we want two different people running the same analysis on two different computers to come to the same conclusions. To that end, we enforce certain rules throughout the project: no ""output"" data files from the processing step get saved into version management software like Git, no variables should ever be reassigned, and so on. Our extensive Readme file documents all the ""rules"" a startr user should try to follow.   Startr also assumes that data will sometimes come from a scraping task, so it comes by default with rvest, a scraping tool for R, a special folder to hold scraping code, and a default Node.js version recommendation if the user chooses to scrape in JavaScript.     "," Boiling down the analysis process into a simple set of explainable steps took a lot of work. Everyone likes to work differently, and the two creators of this project are no different, so we had to dig deep to find where we had common ground in our approaches to analysis and build up from there. It took a lot of conscious un-learning of bad habits and effort in developing better ones.   We debated the structure of projects, and tested ideas out on some ongoing projects to refine our thoughts before scaffolding what eventually became the startr project. Even then, we learned as we went along, figuring out which packages to import by default, how to handle data being saved into version management tools, etc.   What we ended up with is a structure – both literally, in the sense of folders and files, and philosophically, in terms of our assumptions about the data journalism process – that feels ""bulletproof,"" and both obvious yet insightful into how data analyses should be done. Our hope is that if we were to explain the project and its philosophy to a seasoned data journalist, they would say ""Oh, yeah, that makes a lot of sense.""   The project is still evolving, as we use it on a daily basis. We're still learning about the template's limitations, and evolve it on a weekly and monthly basis with new helper functions, instructions in the readme guide, and so forth. "," Standardizing your analysis workflow requires an up front investment, but pays off significantly down the line. With startr, we've been able to cut down on almost all of the time we used to spend setting up folders, explaining (and apologizing) for why our code looks one way or the other, and so on.   When it comes to data-driven projects, being organized is key. Any disorganization reveals itself in the final product: either findings that can't be replicated, or a project that can't be run from scratch by a new user without serious tweaking. ",https://github.com/globeandmail/startr,https://github.com/globeandmail/startr-cli,https://www.theglobeandmail.com/canada/article-why-does-any-canadian-need-a-handgun-what-the-gun-control-debate-is/,https://www.theglobeandmail.com/canada/toronto/article-toronto-ontario-housing-rental-eviction-data-landlords-tenants/,https://www.theglobeandmail.com/canada/article-data-gap-transit-sexual-assault-investigation/,https://www.theglobeandmail.com/politics/article-pmo-vets-potential-judges-with-liberal-database/,https://www.theglobeandmail.com/canada/article-provincial-and-territorial-legislatures-spend-fewer-days-in-session/,Tom Cardoso and Michael Pereira," Tom Cardoso is a crime and justice reporter and data journalist at The Globe and Mail, based out of Toronto.   Michael Pereira is a data journalist, formerly with The Globe and Mail. He lives and works in Winnipeg. ",,,
United States,Univision,Big,Participant,Best visualization (small and large newsrooms),"Vaping, ‘juuling': the new addiction that your kids say is 'cool'",17/10/19,"Explainer,Infographics,Lifestyle,Health","Animation,Adobe", This interactive project explains the teen vaping epidemic that is putting at risk millions of youth in the United States and why more regulation is urgently needed to protect young people. ," Our audience was able to see in a visual way how vaping has become an epidemic and describes in a scrollytelling way how the juuls are constructed and how they work.  Data visualization is a great way to convey important messages to our audience, and this is no exception, by showing not only the dangerous appeal of juul pods but why they are used and their lack of regulation in the market today. "," Using a scrollytelling technique to animate the juul pod we were able to show the inside of these apparatuses.  We also used embedded video to show the smoke as a visual cue. Furthermore, we used javascript to create a dropdown menu to see the different ages of the decline in cigarette use just as vaping is on the rise.  Finally, with Adobe illustrator we graphed some of the reasons that teenagers do vaping. "," As with all scrolltelling, the synchronization of  techniques needs to be very precise so that all aspects of what we are trying to tell our audience is conveyed in the correct manner.  This took some doing on all our parts but we were able to pull it all together to show a beautiful visualization. ", The vaping epidemic is on the rise and it is very dangerous.  The use of infographics and visual story telling is an excellent way to convey our message.  Many of the dangers of vaping can be seen in this article in an easy to understand way. ,https://www.univision.com/especiales/noticias/infografias/2019/vaping-english/index.html,,,,,,,"Amaya Verde, Ana Elena Azpurua, Javier Figueroa"," Amaya Verde is a Graphics Journalist at Univision. She began working as a Visual Journalist and Information Designer at the Spanish newspaper El Mundo. She was Graphics Editor at Negocio and Graphics Reporter at El Economista, both focused in business and economic news. She created her own company specialized in explanatory graphics, and taught infographics and data visualization at International University of La Rioja in Spain. Generating visual narratives has always been a fervor of hers.    Ana Elena Azpurua is a digital Graphics reporter with Univision Noticias. Before that, she covered local news and Latino issues for  Al Día , a Spanish-language outlet published by  The Dallas Morning News . She began her journalism career in Caracas, Venezuela, as a staff writer with  El Nacional  newspaper. Ana holds a master’s degree in journalism and International Affairs from Columbia University, where she was a fellow at the Toni Stabile Center for Investigative Journalism.   Javier Figueroa is a graphics developer at Univision. He has been at Univision for the past 4 years. Before that, Javier ran his own web development company for 19 years in Puerto Rico and Miami after graduating from the University of Michigan with a degree in Political Science and Latin American and Caribbean studies. Javier has taught Data Viz at the University of Miami and has won several awards in his field. ",,,
United States,"Broke in Philly, all newsroom partners",Big,Participant,Best news application,Broke Elections Database,05/07/19,"Explainer,Solutions journalism,Multiple-newsroom collaboration,Database,News application,Crowdsourcing,Politics,Economy,Employment","JQuery,Json,Microsoft Excel,Google Sheets","The Broke Election Database is a tool for the public and journalists to see specific responses from candidates for Philadelphia City Council regarding the city's economic mobility. The database was put together for Broke in Philly, a reporting collaborative of more than 25 newsrooms in Philadelphia that produces solutions journalism focusing on economic mobility. We leveraged community engagement and crowdsourcing tools to generate eight questions from the public that we then posed to candidates. After fact-checking and ensuring the responses were substantial, we published answers from TK candidates, a TKmajorityTK of the city's largest city council race in thirty years.","<ul>  All City Council members are now aware of the collaborative reporting efforts and focus of Broke in Philly, helping the rest of the collaborative's work to become a primary news source for elected officials.   The database now functions as a quick and easy accountability reference for reporters and citizens to see what candidates have promised and compare that to what they are delivering while in office.   Spotlight on Poverty, a national non-profit forum that discusses solutions to poverty, reached out to learn about how we used community engagement to crowdsource the questions for our survey with the idea of replicating the process for their organization in the 2020 election cycle.   MinnPost in Minnesota was interested in learning the process behind creating a collaborative election database in multiple languages.  </ul>","When preparing to send out the survey to City Council and Mayoral candidates, Broke in Philly partners leveraged digital community engagement tools to make sure that our work was based on community feedback and not produced in an editorial bubble. Google forms - Our first round of outreach was conducted with Google Forms, which helped us field more than 90 individual responses and generate more than 300 questions from Philadelphians. Hearken - Once we distilled the questions for candidates, there were still too many to include in a survey, so we used Hearken to conduct a voting round and select the top three questions out of nine. The Broke Elections Database had two main iterations: the first in time for the primary elections on May 21 and the second in time for the general elections on Nov. 5. The first round only included Republican and Democratic candidates for City Council and Mayor. Because Pennsylvania primary elections do not include third-party candidates, those folks were not included until the November version of the database was being constructed. Airtable - Once responses were edited, the content was transferred into an Airtable base that allowed us to use the API and publishable views to share the content in English and Spanish. We were also to update the content in real-time without rebuilding the site each time. HTML, CSS, JS - The database's first version was built using Javascript, HTML, and CSS from scratch. The final public-facing version was built using HTML, CSS and Javascript that displayed views of the data gathered into Airtable. Translation - A critical part of this database was ensuring that as much of the content shown was not just in English, but also in Spanish. We accomplished this by leveraging contacts with our multi-lingual and Spanish-speaking partners to connect"," The hardest part of this project was that it could not be owned by one specific newsroom partner in the collaborative, so the resources that newsrooms could commit to producing the database were very limited. This resulted in only having one person to coordinate, collect and then produce all of the responses. Many of our newsroom partners are small hyperlocal outlets, and therefore do not have development teams to specifically help with building the app. The newsrooms that do have development teams could not spare them to work on a project that required working on restricted servers and access (due to the site hosting being done through a specific partnership.) There was also little to no budget to complete the project, so technology capabilities were restricted to what the one person dedicated to the project knew how to do or had the bandwidth to learn.    At one point in the project, we came very close to creating an entirely custom app using React. However, this was a brand new framework for the project lead and there was no time to fully learn how to use React effectively or contract someone to complete the development. ",<ul>  How to structure a long-term project related to elections   How to develop shared journalistic resources among newsrooms  </ul>,https://brokeinphilly.org/2019/11/01/election/,https://election.brokeinphilly.org/,https://codesandbox.io/s/brokeelectiondatabase-yxq81,,,,,Broke in Philly,"  Broke in Philly is  a collaborative initiative among 24 local news organizations to provide in-depth, nuanced and solutions-oriented reporting on the issues of poverty and the push for economic justice in Philadelphia. ",,,
Argentina,Datos Concepcion,Small,Participant,Best news application,Datos y periodismo a website to spread data journalism in Latin America,02/01/19,"Investigation,Solutions journalism,Cross-border,Multiple-newsroom collaboration,Documentary,Database,Open data,News application","Scraping,D3.js,QGIS,Microsoft Excel,Google Sheets,CSV,OpenStreetMap","<pre> ""Datos y periodismo"" (data and Journalism) was born as a site specially dedicated to disseminate and develop journalistic projects based on data. This modality that is promoting journalism in Argentina and Latin America needed to have its own space. </pre>","<pre> The initiative was launched in February 2019 and has since gained recognition among journalists specialized in data. By 2020 it is planned to create the network of correspondents based on the Latin American network of data journalists that we founded in 2018 in Costa Rica. The site brings visibility to data-based initiatives, and that has allowed us to obtain a first state of data journalism in Latin America. The library of publications on data journalism that we attach to the site is also highly valued. And we are adding online workshops that allow more journalists to be trained in the use of data.</pre> <pre>  </pre>","<pre> In the site we embed interactive visualizations developed with different tools: tableau, infogram, flourish, etc.</pre> <pre> In addition, we dedicate a special report by month to explain step by step what techniques were used in a prominent story, with links to the tools and databases used.</pre>","<pre> As it is a website dedicated to disseminating and enhancing data journalism, the most important barriers are associated with achieving visibility of the initiative. In planning for 2020 we have planned a positioning campaign that aims to install the site as a reference space for data journalism in Latin America. The jury must assess that it is an initiative launched by a small work team (we are 4 members) that understands that Latin American journalism needs a quality leap, and that data journalism can provide that fee that is needed to base the stories in data and not in opinions.</pre> <pre>  </pre>","<pre> This initiative allows more journalists to know what data journalism is and what are the differential advantages over traditional forms of journalism. In turn, the spirit of the project is to consolidate a network of alliances throughout the region in order to reach instances of collaborative and cross-border work, much needed in many of the journalistic investigations.</pre> <pre>  </pre>",https://datosyperiodismo.com.ar/,https://datosyperiodismo.com.ar/portfolio/visualizaciones/,,,,,,Datos Concepcion," Adrian PINO - Data Scientist Jr. Data analysis. Data Journalist. Coordinator and Founder of ""Datos Concepción"" (Arg).  Dedicated to journalism research and innovation in LATAM. Trainer in data journalism - Open Data and Transparency #OpenGov.       Soledad Arreguez - Journalist. Founder and Education programs coordinator in Datos Concepcion. Data journalist and data journalism trainer. Proffesor at university. Academic Research.  ",,,
United Kingdom,Reach,Big,Participant,Best data-driven reporting (small and large newsrooms),Long-term B&B,21/08/19,"Investigation,Cross-border,Open data,Economy","Microsoft Excel,Google Sheets,CSV"," The ongoing housing crisis, and rising levels of homelessness, is an important issue to readers of regional news sites across the Reach network. Given this it is important to be able to use statistics, both published and unpublished to give a greater insight into the scale of these problems.   The figures, obtained through Freedom of Information requests to all councils across Britain, build on publically available statistics on homelessness and temporary accommodation. It gives readers a greater insight into the reality of families spending more than six weeks in B&Bs, showing that for many more than six weeks means months. "," The statistics obtained through FOI came at a time when they could be used to build on research by Children's Commissioner for England on the use of B&B and other temporary accommodation for homeless children and families, showing many were spending time living in hugely unsuitable housing. The figures give both a local and in-depth perspective on the implications of that report, and is a story can only be revealed through the collection and analysis of this data.   The reaction of the public bodies to the publication of these figures shows the stark reality of the policy choices under pressure councils are being forced to make - with Wrexham council admitting that high demand for services has led to such long stays - suggesting that these stories, and the concerns of the Children's Commissioner, may be difficult to address without much wider societal change. "," The data for the story was uncovered using the Freedom of Information Act and required the painstaking collection of information from nearly 400 councils across Britain. This infromation needed to be collected into a single spreadsheet, organised and analysed to discover the stories within the data. ", The challenge of sending FOI requests is dealing with public bodies' failure to provide the requested information. This meant the need to be organised with complaints and internal reviews. Being able to build on previous successful challenges to the use of exemptions made it easier to get the data (as well as strengthening the case for other journalists to be able to access similar data). ," The importance of looking deeper than the published official data.Government statistics can be a jumping off point for investigations as they rovide some but not all of the story. FOI is an excellent tool for doing this, allowing journalists to build their own national datasets that tell the stories in more depth. ",https://www.dailypost.co.uk/news/north-wales-news/homeless-children-north-wales-spent-16807542,https://www.gloucestershirelive.co.uk/news/gloucester-news/homeless-children-gloucestershire-living-bbs-3254236,https://www.grimsbytelegraph.co.uk/news/grimsby-news/homeless-child-grimsby-spent-more-3235909,,,,,Claire Miller," Claire Miller is exceptional data journalist. Her role within the Reach Data Unit is to analyse data, find news lines, and write stories that use data to bring a greater understanding of important issues to readers of national and regional news sites. She is akeen user of the Freedom of Information Act, and passionate about improving access to information both for herself and for other journalists. Her determination to access previously unavailable information that public bodies are not keen to share means a lot of complaints to the Information Commissioner - she was responsible for one in 20 of all FOI complaints dealt with last year. However, she has an excellent track record at getting decisions in her favour and the information she needs to produce great stories.    The Reach Data Unit, which is responsible for creating these stories and designing and building the interactives, is a fierce champion in promoting greater understanding of statistics in society, producing hundreds of stories a month that use statistics to illuminate and explain issues that are key in the lives of the local readership across the Reach network. ",,,
United States,Univision,Big,Participant,Best visualization (small and large newsrooms),"Interactive: Days over 100°F to become more common across US, Hispanics more exposed to 'killer' heat",17/07/19,"Investigation,Infographics,Map,Environment","D3.js,QGIS,Adobe,CSV", This interactive project shows how heat waves are becoming more frequent nationwide. The analysis of data from the Union of Concerned Scientists revealed that Hispanic communities will be especially affected. , This project was extremely imporant in that using the data from the Union of Concerned Scientists we were able to show how hispanic communities will be affected by global warming.  Hispanics majorities and pluralities live in counties that will have temperatures rise to dangerous levels in the coming years to the warming of the earth. This is visually conveyed in our work. , We used d3 maps and scroll telling techniques to show the effect of changing temperatures in the coming decades.  We also used javascript and csv to create an interactive graphic that shows what the average temperatures will be per state if there is no action or slow action to reverese the effects of global warming in these communities. ," The hardest part for this project was coming up with the way to show the changing temperatures per decade in an impactful way.  We had the data from the center, but we couldn't come up with a way to make the information impactful until we used d3 to paint the maps and see how incredible they looked.  This gave way to use the scrolly telling technique to see the change in the country across the coming decades. "," Although the piece is geared toward hispanics and how they will be affected, the impact of the maps can let others see how we are all affected in our country by global warming.  The data was for the entire country and we visualized that in a very astonishing way. ",https://www.univision.com/especiales/noticias/infografias/2019/killer-heat-interactive/,,,,,,,"Amaya Verde, Javier Figueroa, Ana Elena Azpurua, David Adams"," Amaya Verde is a Graphics Journalist at Univision. She began working as a Visual Journalist and Information Designer at the Spanish newspaper El Mundo. She was Graphics Editor at Negocio and Graphics Reporter at El Economista, both focused in business and economic news. She created her own company specialized in explanatory graphics, and taught infographics and data visualization at International University of La Rioja in Spain. Generating visual narratives has always been a fervor of hers.    Javier Figueroa is a graphics developer at Univision. He has been at Univision for the past 4 years. Before that, Javier ran his own web development company for 19 years in Puerto Rico and Miami after graduating from the University of Michigan with a degree in Political Science and Latin American and Caribbean studies. Javier has taught Data Viz at the University of Miami and has won several awards in his field.   Ana Elena Azpurua is a digital Graphics reporter with Univision Noticias. Before that, she covered local news and Latino issues for  Al Día , a Spanish-language outlet published by  The Dallas Morning News . She began her journalism career in Caracas, Venezuela, as a staff writer with  El Nacional  newspaper. Ana holds a master’s degree in journalism and International Affairs from Columbia University, where she was a fellow at the Toni Stabile Center for Investigative Journalism.   David Adams is a journalist for Univision News.  Previously he worked at the Tampa Bay Times and Reuters.  ",,,
United States,BuzzFeed News,Big,Participant,Best data-driven reporting (small and large newsrooms),The Impersonators,10/03/19,"Investigation,Long-form,Open data,OSINT,Politics,Business","Scraping,Json,CSV,Python"," In 2017, the Federal Communications Commissions was bombarded with millions of fake public comments about its controversial plan to repeal “net neutrality.” It remains the most prolific known instance of political impersonation in US history. Our investigation — based on an unprecedented data analysis, court records, business filings, and interviews with dozens of people — revealed, for the first time, where 1.9 million of the fake comments originated, how they were generated, and the political operatives behind them. We also revealed that the operatives had worked on state-level campaigns that raised similar impersonation allegations. "," By revealing the precise way in which the comment process had been compromised, our reporting has provided a level of specificity that may help spur reform. In his <a href=""https://www.hsgac.senate.gov/imo/media/doc/2019-10-24%20Carper%20Opening%20Statement.pdf"" style=""text-decoration:none;""> written opening statement </a> to a Senate hearing on widespread problems in federal public commenting, Sen. Tom Carper — the ranking member of the Senate’s Permanent Subcommittee on Investigations — <a href=""https://www.buzzfeednews.com/article/jsvine/senate-report-impersonation-public-comments-fcc"" style=""text-decoration:none;""> called BuzzFeed News’ findings </a> “extremely troubling.”       In Texas, we unearthed new evidence regarding the role of LCX Digital in a controversial letter-writing campaign accused of impersonating local constituents. Stacy Hock, the chair of the school-choice group that commissioned the campaign, told BuzzFeed News that these findings were “alarming.” Hock told BuzzFeed News that the consulting firm that had hired LCX for the campaign — without her knowledge at the time, she said — had “launched an internal review” and was “demanding answers from LCX.” And depending on what those answers were, she said, “we will determine our future course of action, up to and including legal action.”       Our reporting also brought answers to people who knew they had been impersonated, but didn’t know by whom or how. One such person was Sarah Reeves; the political operatives had impersonated her mother — using her name, address, and email address — more than a year after she had died. They had also impersonated Sarah herself, attributing to her an opinion exactly the opposite of what she truly believed. "," We used the federal Freedom of Information Act to obtain all comments submitted via the FCC’s bulk-uploading mechanism. (We also used FOIA to seek server logs pertaining to other comments; we were denied, we appealed, and then had that appeal denied, 3-2 along party lines, in a vote by the agency’s five commissioners.)       Once we had obtained the data, we ran large samples of the email addresses associated with those comments through <a href=""https://haveibeenpwned.com/"" style=""text-decoration:none;""> Have I Been Pwned </a>’s <a href=""https://haveibeenpwned.com/API/v3"" style=""text-decoration:none;""> API </a> (Application Programming Interface), to determine the data breaches in which they had been exposed. We believe this is the first-ever journalism investigation to make such extensive use of that service, and to use it to identify widespread public impersonation.       To analyze the Have I Been Pwned results, as well as to analyze patterns across millions of comments uploaded to the FCC over the past five years, we used a combination Python, Pandas, Jupyter, Matplotlib, xsv, and VisiData. To obtain comments from prior FCC proceedings, we wrote web-scrapers using Python and Requests.        We used a similar set of tools to analyze the Texas letter-writing data, with the addition of WHOIS records to obtain information about the owners of the IP addresses in that dataset.       Our story also featured an explanatory interactive graphic, which demonstrated how Media Bridge generated the millions of “unique” comments it submitted to the FCC. To reverse-engineer Media Bridge’s comment-generation model, we used Python and Pandas. To build the graphic itself, we used the Svelte library for JavaScript.     We also used GitHub and the Internet Archive to publish to <a href=""https://github.com/BuzzFeedNews/2019-10-fcc-comments"" style=""text-decoration:none;""> the final code, methodology, and anonymized data </a> behind the FCC analyses. "," The presence of fake comments in the 2017 FCC proceeding attracted attention at the time. Virtually every major publication in the country covered it. None, however, managed to trace the fake comments back to their data source. BuzzFeed News was able to do so through a unique combination of data analysis, online sleuthing, and traditional shoe-leather reporting.       One set of challenges involved obtaining, structuring, and analyzing more than 40 gigabytes of data directly relevant to the findings. The scale of those records required us to carefully design data-processing pipelines so that we could quickly run new analyses in response to new discoveries in our reporting.       Another challenge: Piercing LCX Digital’s veil of secrecy. Before we began our investigation, the company had received virtually zero scrutiny and had left only a very light trace online. It presented itself online as an innocuous digital advertising outfit with cutting-edge technology — a self-portrayal that went unchallenged until we started digging deeper. To unravel LCX and its main owner’s many lies, BuzzFeed News interviewed former employees and business partners, scoured old versions of LCX’s website on the Internet Archive’s Wayback Machine, and combed through scores of business filings in a half-dozen states.       One major breakthrough came when BuzzFeed News discovered, in a previously-unreported court case in San Diego County, an extraordinary deposition by one of the company’s cofounders. That deposition not only accused the LCX’s main owner of extensive deception, but also claimed that the company was a “completely fraudulent” enterprise. The details of that alleged fraud bore remarkable similarities to patterns BuzzFeed News was beginning to uncover in the FCC proceeding, Texas, and South Carolina. "," In addition to overcoming the challenges described above, the investigation also demonstrated two novel techniques, which have applications beyond this specific story.       The first novel technique was to use <a href=""https://haveibeenpwned.com/"" style=""text-decoration:none;""> Have I Been Pwned </a>’s <a href=""https://haveibeenpwned.com/API/v3"" style=""text-decoration:none;""> API </a> to identify which email addresses have appeared in database breaches. BuzzFeed News ran a large sample of all email addresses in the FCC comments against the HIBP API, and then calculated the breach rate for all bulk-uploaders. One uploader — Media Bridge’s Shane Cory, who had worked on the comments with LCX — stood out as a massive outlier, and thus became a major focus of our reporting.       Digging deeper, the HIBP data allowed us to identify the  specific database breach  that contained the vast majority of the email addresses. Later, we were able to confirm that the personal data Media Bridge submitted to the FCC was exactly the same as it appeared in that breach, down to idiosyncratic spelling and punctuation.       The second technique was to reverse-engineer the Mad Libs–style algorithm Media Bridge used to generate the comments, and then present that algorithm as interactive graphic — as far as BuzzFeed News is aware, the first time a news organization has done something of that nature. To do so, BuzzFeed News wrote computer code to test hypotheses for how the algorithm worked, refine them, and <a href=""https://github.com/BuzzFeedNews/2019-10-fcc-comments/blob/master/notebooks/analyze-mb-comment-structure.ipynb"" style=""text-decoration:none;""> prove the completeness </a> of the reverse-engineered model. We ultimately calculated that the algorithm was capable of generating more than 9 sextillion “unique” comments.        Due to the unique goals of the interactive graphic, BuzzFeed News could not use any off-the-shelf tools to build it. Instead, BuzzFeed News developed an interactive graphic with JavaScript, HTML, and CSS to allow readers both to generate new comments, and also to cycle through all possible iterations of each phrase. ",https://www.buzzfeednews.com/article/jsvine/net-neutrality-fcc-fake-comments-impersonation,https://github.com/BuzzFeedNews/2019-10-fcc-comments,https://www.buzzfeednews.com/article/jsvine/senate-report-impersonation-public-comments-fcc,,,,,"Jeremy Singer-Vine, Kevin Collier", Jeremy Singer-Vine is the data editor for the BuzzFeed News investigative unit and is based in New York.       Kevin Collier is a national- and cybersecurity reporter. ,,,
Italy,"Brush&Bow, France 24",Small,Participant,Best visualization (small and large newsrooms),Shrinking Spaces: The question of return to Syria,12/05/19,"Explainer,Long-form,Cross-border,Podcast/radio,Illustration,Infographics,Video,Map,Audio,Politics,Lifestyle,Immigration,Human rights","Animation,Personalisation,QGIS,Adobe,Microsoft Excel","Our work consists of an interactive multimedia feature piece looking at the issue of Syrian refugees impossible return to Syria. This work is the result of over two years of reporting and multimedia information gathering in Lebanon's northern region of Akkar, 5km from the Syrian border, and in Berlin, working with refugee communities in northern Germany. Brush & Bow's correspondents based in Lebanon lived in the Syrian camps, establishing a r in Lebanon and Germany, two of the countries hosting the greatest number of Syrian refugees in the world. Over the past two years, Brush & Bow correspondents have been"," In Lebanon, our work has involved collecting testimonies from displaced Syrian and Palestinian communities and their Lebanese hosts, starting the ‘Radio Hakaya’ program which was broadcasted in Arabic and English through Lebanon, Jordan, Turkey and the UK by international and local radio stations. Brush&Bow members in Germany instead, were collecting information concerning the lives of Syrian refugees who have moved to Germany, their stories of migration and integration in Europe, and their opinions, fears and hopes of return to Syria. The project drove an intense and wide participation by a series of people without which not much of this project could have happened. It was truly a community work, run over a long period of time, but with a declared and stated aim from the beginning: tell the story of how and why is it so difficlut to imagine a return of the millions of Syrian refugees back to Syria in the forseable future.   The ultimate impact of the project is to create creative and informative material to inform an arabic and international audience about this issue, through the creation of a multimedia piece bringing together in a coherent, accessible and visually stimulating manner much of the information collected in these two years spent both living in Syrian refugee camps and witnessing the process of refugee integration in Germany.  "," Ours is a truly multimedia endevour, in which a variety of tools, techniques and technology were used to achieve the final project.   Brush & Bow's principles of slow and participative journalism were key technical guidelines we followed and developed over two years in Syrian refugee camps, living and sharing life with the Syrian inhabitants of the camp, witnessing life and memory in an organic way.    We used a great variety of tools to collect and give voice to stories less heard, those of individuals. We started by establishing a radio workshop in a tent of the refugee camp of Tel Abbas, running singing, storytelling and journalism workshops with adults and children, as well as a fully organised photography workshop with children that has since been exposed at the camp, in Beirut, Berlin and Glasgow.    Through the podcast show 'Radio Hakaya' many of the key interviews were collected, always with the full consensus and collaboration of the interviewees. Our sound editors, content editors and translators were all people from the camp's community.   Final materials include video interviews and audio recordings, a photography project run and developed with Syrian children from the camps of Tel Abbas, hand-drawn illustrations as well as written testimonies and official documents.   As techniques for the narration, we decided to use the scrolly-telling technique used by in long-feature first class journalism pieces, integrating photography and text with animation, illustration and audio, so as to enhance the reader's experience when engaging with the complex information provided. "," The hardest parts of the project were the consolidation of relevant information, together with the retreival, management and storage of great amounts of extremely sensitive data.   In Lebanon alone, Brush & Bow correspondents conducted 42 live, recorded and transcribed interviews, held over 50 workshops with adults and children alike, and produced the 'Radio Hakaya' whole podcast series, a photography course and exhibition, and a 7-piece article series for Al Araby al Jedeed, which later informed this project. Consolidating and filtering the relevant information out of this huge amount of material was itslef a work that took great time and effort, especially considering the very volatile situation on the ground.   Protecting the data in safe storage was key considering the great vulnerability of the interviewees and the risk of raids from both the Lebanese authorities and paramilitary groups in the camps. The material and information taken from the Akkar and Arsal areas in Lebanon, and from our trips to Homs in Syria, needed regular encryption, anonymyzation and safe transfer to our colleagues in Germany through regular updates.   Gaining the necessary confidence to do this work was also a real challenge. The three main collaborators working on this project had never worked with such a wide range of means in such a volatile environment. We had to learn how to constantly keep each other in check to avoid transfer of trauma, stress and burnout.    Finally, the most difficult but rewarding part of this project was to test the real power of a slow, non-intrusive style of journalism, where a real relation is created between reporter and interviewee, based on trust. Despite surely not all material was collected from people we knew, some of the most in-depth stories were made accessible to us because of our developed connections with the local communities.     "," This piece aims to provide a unique insight into the lived situation on the ground, one which is further reinforced by the personal relationships forged with refugee communities we have lived and worked with in both Lebanon and Germany.    As the war in Syria draws to an end, addressing the issue of refugee’s return is of growing importance for both Europe and Syria’s neighboring countries. Ours is a small but significant attempt to give space to Syrian voices to speak for themselves about their fears of return, shout out the rage for the destruction that has so far broken their lives, and voice the future hopes related to refugeehood abroad or return to a war-torn country.   We attempt to balance these voices by including opposite voices and opinions undiscriminatly, aiming for the readers to judge by themselves the relevance and strength of arguments made. Our reporting attempts to reflect the complexity of multiple realities, as in war no truth is alone. ",https://webdoc.france24.com/shrinking-spaces/,http://shrinking-spaces.surge.sh,https://raseef22.net/keyword/1000566-راديو%20حكايا,http://tmbth.com,https://www.alaraby.co.uk/english/author/2017/9/5/roshan-de-stone-and-david-suber,https://salonsyria.com/return-to-syria-a-proposal-from-syrian-refugees-in-lebanon/#.XjtJVi2cbVo,https://www.facebook.com/OPENMediaHub/videos/2544559572431239/?v=2544559572431239,"David Leone Suber, Roshan De Stone, Hannah Kirmes-Daly"," This project was conducted by a core team of three: David L. Suber, Roshan De Stone and Hannah Kirmes-Daly, co-Directors of the  Brush&Bow C.I.C  creative journalism collective, focused on challenging mainstream opinions, negative stereotypes and fears, using illustrations, articles and sound-recordings to explore the individual voices and stories of people within current social issues.   Individual biographies:    David L. Suber  is an italian journalist and researcher, working on migration, conflict and international crime. His research has focused on the socio-political changes in the Middle East and North Africa, border controls and international migration, as well as smuggling and trafficking networks of organised crime and terrorist groups. He is currently based at the UCL Jill Dando Institute of Future Crime (UK), but has lived and worked in Egypt, Tunisia, Lebanon and Turkey for the past 7 years. In 2017, he directed the animation “شهيلي Scirocco” on Europe's counter migration policies, and in 2018 he curated the Radio Hakaya podcast series from Syrian Lebanese camps in Lebanon. David is a co-Director of Brush&Bow.    Roshan De Stone  is a british-indian human rights advocate, currently working as a trainee lawyer for domestic violence victims and destitute asylum-seekers in the UK. In 2017-19 she worked as a journalist and illustrator with Brush&Bow in Lebanon, co-producing the Radio Hakaya podcast series whilst working as an international monitor against the incarceration and deportation of Syrian activists from Lebanon. For the last 6 years, Roshan has worked with asylum seekers and refugees in Europe and the Middle East, both in a legal capacity and through the arts, creating spaces of cultural celebration through music, dance, figurative art and mural workshops.     Hannah Kirmes-Daly  is a british-german reportage illustrator, working with art to document individual stories and collective narratives. Hannah is one of the two foudners of Brush&Bow, a creative journalism collective based in the UK but open to collaborators across the world. She specialises in using the arts to create spaces for conversations and facilitates people expressing their own stories through illustrations, murals and comic strips. She currently lives in Berlin as she is completing her Masters of Art in Context at the Art University of Berlin. Having lived and worked in refugee camps in France, Greece and Turkey, much of her work focuses on issues concerning migration, gender, development and refugees. ",,,
United States,"InvestigateWest, Indian Country Today, Crosscut.com, Spokesman.com; also Tribal Tribune in Nespelem, Washington",Small,Participant,Best data-driven reporting (small and large newsrooms),Driving While Indian,19/12/19,"Investigation,Explainer,Long-form,Database,Infographics,Chart,Map","QGIS,Adobe,Microsoft Excel,R,RStudio"," An exhaustive investigation of millions of traffic stops and searches of drivers by state patrol officers in the state of Washington (USA) exposes the phenomenon of ""Driving While Indian"": Native Americans are searched 5 times more often than white motorists by the Washington State Patrol, often just outside Native reservations. Yet white drivers are far likelier to be found with drugs and other contraband. Black, Latino and Pacific Islanders also are searched at higher rates than white motorists. Legislators expressed outrage at the findings. The State Patrol now is taking steps to correct bias. "," As a result of our two-year investigation, state lawmakers and tribal leaders expressed outrage at the findings; lawmakers said they will be looking at the issue during this year's legislative session, while the Washington State Patrol said it’s moving forward to address the problem. The State Patrol says it has put some Seattle-area troopers through an anti-bias training course that is more comprehensive than what cadets receive in the training academy, and it is in talks with university researchers to look into the issue.   One of our main findings was that the State Patrol had discontinued a series of studies by Washington State University researchers examining stops and searches of people of color by Washington troopers. Those studies stopped in 2007 even though researchers said indicators of “implicit bias” required more research and further attention by the patrol. Within days of publication of our investigation, the patrol said it was ""taking strides to jumpstart the research relationship with WSU."" A spokesman also said the agency was examining the anti-bias curriculum available through the Washington Criminal Justice Training Commission identified in InvestigateWest’s reporting; the agency had never before taken advantage of the training.       State Patrol chief John Batiste emailed all employees of the agency with a link to our investigation. “We must be clear we make mistakes from time to time ... but when we do we must admit our mistakes, we must apologize, and we must commit to improvements,"" Batiste wrote.  "," We used primarily RStudio, Excel and QGIS for processing, analyzing and visualizing the data - about 8 million police stops. Because of the size of the data set, we had to use statistical software, in this case RStudio, to customize the time range, create new variables for discretionary searches and police districts, and visualize the different cuts of data. We wanted to repeat and update the descriptive statistics that had last been done over 10 years ago by researchers at Washington State University. Ultimately, we used Excel for published visualizations, after experimenting with other programs like Flourish. We also used Excel as a database, starting with lists of stops exported from R. We then requested police reports from the state patrol, and processed Adobe narrative police reports (pdf files) into a file with driver demographics and stop information. We used this information to try to contact drivers. Finally, we used QGIS for analysis and visualization, ultimately publishing the heat map of Native American high-discretion searches, overlaid with counties and tribal lands.    "," As a small nonprofit, we did not have anyone ready to take on the data side of this reporting. Despite over ten years of applied research experience, Joy Borkholder had not learned some of the powerful open-source software used in this project, but was eager to do so. Over the course of working on this story, Borkholder attended hands-on workshops at <a href=""https://www.ire.org/events-and-training/event/3189/"">NICAR 2018</a>, including one with the team from the Stanford Open Policing Project. Stanford faculty and staff remained accessible for support and even script-checking throughout our reporting. Borkholder also took advantage of free online courses offered by the <a href=""https://knightcenter.utexas.edu/"">Knight Center for Journalism in the Americas</a>, including R (using RStudio), QGIS, and Data Journalism and Visualization. This project is an example of the data journalism community supporting skill development, and thus, stories with impact.   Aside from the data aspects of the project: There are always difficulties covering under-represented communities that have histories of poor representation in the news media. Tribal leaders wouldn't agree to meet with story co-author Jason Buch until we demonstrated our dedication to the story by assigning Buch to travel to Central Washington to visit their communities, pretty much speculating we could get interviews. Even then, tribal members remained guarded.   We had great difficulty finding drivers to talk to us: people who had been the subject of a high-discretion search, with contraband found, didn't want to talk. We combed through hundreds of police reports and contacted drivers, and reported on the ground in Central Washington, and still struggled to find people willing to go on the record about their personal experience. Our resultant story has drawn reaction. “Omg! Someone is finally reporting on the WSP near the Coville Rez,” read one email we received after the story published. “THANK YOU!!!!” "," You don't have to be a data scientist or IT wizard to learn new programs that handle millions of records and visualize them.  At first, Joy Borkholder learned and taught herself just enough RStudio to export slices of the data and figure out that there was a story there. She also did some basic work in QGIS, in both cases, starting with skills classes at the <a href=""https://www.ire.org/events-and-training/event/3189/"">IRE/NICAR conference</a> in 2018.  <a href=""https://knightcenter.utexas.edu/"">The Knight Center for Journalism in the Americas</a> offered in-depth R and QGIS classes as the reporting on this project continued. Some Knight Center classes remain publicly available as videos and tools for anyone to access, like this one: <a href=""https://learn.r-journalism.com/en/"" style=""color:blue"" target=""_blank"">https://learn.r-journalism.com/en/</a>. Additionally, the online community of R and QGIS users, like in Stack Overflow, is super helpful in figuring out glitches and scripts. Finally, programs like R and Python make transparency so simple; if you see a story with data crunching behind it, you might be able to click through to see the code, learn and copy.  ",https://www.invw.org/2019/12/19/driving-while-indian/,https://www.invw.org/2019/12/19/driving-while-indian-how-investigatewest-conducted-the-analysis/,https://newsmaven.io/indiancountrytoday/news/data-from-millions-of-traffic-stops-reveal-there-is-driving-while-indian-eDL9jC8MM0SdKAwn86ftyQ,https://www.spokesman.com/stories/2020/jan/02/report-washington-state-patrol-singles-out-native-/,https://crosscut.com/2019/12/native-american-drivers-are-more-likely-be-searched-washington-state-patrol,https://www.invw.org/2020/01/28/state-patrol-under-pressure-to-examine-bias-in-searches-of-people-of-color/,,Joy Borkholder," Joy Borkholder is a freelance researcher, aspiring to do more data and investigative journalism. Driving While Indian with InvestigateWest was her first project. She has a background in advocacy-oriented research, primarily as a researcher for labor unions. She has expertise in the health care and retail industries, as well as the tools of financial analysis, public-records acquisition and analysis, legal research, survey research, and public policy analysis. Now she considers herself a beginner at R, QGIS, Flourish and Google’s Data Studio. She has master’s degrees in sociology and social work, from the University of Virginia and University of Chicago, respectively. ",,,
United States,The Associated Press,Big,Participant,Best data-driven reporting (small and large newsrooms),America's Dams: A Legacy of Neglect,11/10/19,"Investigation,Long-form,Breaking news,Multiple-newsroom collaboration,Database,Open data,News application,Environment","Scraping,D3.js,Google Sheets,R,RStudio"," The Associated Press used public records requests across 50 states to compile an exclusive dataset on the status and conditions of dams in the U.S.  It found at least 1,688 high hazard dams in poor or unsatisfactory condition, and in places where failure is likely to kill at least one person. The AP shared its data -- and thousands of inspection and emergency documents -- with member news organizations so local outlets could do local stories using the AP's research.  "," The results were immediate. Newspapers across the country editorialized about the need for lawmakers to prioritize the problem. A series of local newspapers in Pennsylvania wrote: “You ignore infrastructure issues at your own risk. That’s why every Pennsylvanian, including the 450,000 folks in York County, should be more than a little concerned about a recent Associated Press report on the condition of the state’s nearly 3,400 dams.” In Iowa, The Daily Nonpareil wrote, “Given the growing frequency and intensity of storms and the potential loss of life associated with dam failures caused by those storms, Congress should begin working to develop national standards for dam inspections.” The Columbus Dispatch editorial noted the AP story and its own localization of the project in writing, “Ohio’s dam guardians have no time to waste.” Some states, including Arizona, began studying the condition of their dams and how to make them safer as a result of the AP’s investigation.Weeks after the story moved, U.S. Sen. Kirsten Gillibrand called for congressional action to provide more oversight of the nation’s dams and more money to fix them, saying, “We should not wait for a catastrophic dam failure or major flooding event to spur us to action.” "," To compile and analyze the original data set used in the dams project, we used a variety of data acquisition, analysis and visualization techniques. We FOIAed state submissions of dams databases to the National Inventory of Dams. In many cases, we were able to get access to them as CSV files, that could be interpreted by a database, but some states' submissions came in as PDF files which had to be parsed, with a first pass using Tabula, and then double checking by hand. A custom Ruby script was used to iterate through state data files, and standardize headers using a metadata spreadsheet we created in Excel by hand, and export state-specific csvs in an easy-to-combine format. A custom Python script was used to pull down data from the National Inventory of Dams in 2016, for comparison with what we received from the states, before the structured data was made available in a downloadable format in 2018.   R was our primary tool for analyzing data. We compiled state-provided csvs into one file, standardized values in individual columns accounting for different ways of encoding values, and encapsulated answers about the data received from states into structured data. We used R for analysis including basics such as sorting, counting and grouping dams, as well as graphing the data to visually explore outliers and surface reporting avenues.Mapping was an important component of the project. We used libraries in R, as well as the program QGIS, to map dams we wanted to focus on by coordinates, so we could look at our data geographically. We also compared dam location to those of past disasters, such as earthquakes and floods. Eventually, our partners at ESRI used ArcGIS Online to create an interactive map for the public of dam coordinates we focused on. "," The fundamental challenge of the project was also essential to its importance: because the National Inventory of Dams redacts key fields needed to assess the state of the nation's dams, AP reporters had to file records requests in every state for the information the federal government would not release. Initial responses from state officials necessitated additional rounds of FOIA requests, a process that took more than a year. And of course when the data was made available, the formats varied wildly. Analyzing the data, comparing it to what was available on the federal level and then vetting and organizing the state reports _some released only in hard copy form _ took months longer. As the project progressed, reporters and data journalists also had to monitor the states for updated information to keep the data fresh. Once the data was usable for AP analysis, it had to be packaged in a way that would be easily accessible to thousands of AP customers across the country for localizations.     "," This project exemplifies the power of collaboration to overcome data roadblocks. The redaction of key information that people need to understand their own environmental safety could have been seen as a dealbreaker for any comprehensive coverage of this issue. Instead, AP data journalists and reporters found a way around the roadblock, and by sharing that data with other news organizations pre-publication, we were able to create a groundswell of coverage and concern that led to real impact. Local news organizations were essential to telling the ground truth story for their communities, and the collaboration allowed them to compare the dams in their coverage area with the state of dams across the country -- providing essential context that is lacking from the occasional dangerous dam story. ",https://apnews.com/f5f09a300d394900a1a88362238dbf77,https://apnews.com/897e0a39b1a324e13aaa680013a2679e,https://data.world/associatedpress/us-dams,https://blog.ap.org/behind-the-news/ap-investigation-on-dangerous-dams-sparks-local-reports,https://interactives.ap.org/dams-legacy-of-neglect/,,,"Michelle Minkoff, David A. Lieb, Michael Casey, Tom Verdin, Troy Thibodeaux, Andrew Milligan, Allen Breed"," The AP team that took on the dams project included data journalists, state reporters, video journalists, members of the national state government accountability team and the graphics team. Michelle Minkoff, the lead data journalist on the project, has done data-driven reporting with an emphasis on environmental stories and interactive development at AP since 2011. Previously, she worked at PBS and the Los Angeles Times and taught data journalism at Northwestern's Medill School, of which she is a 2010 graduate. David A. Lieb is part of the AP’s State Government Reporting Team. He covers developments and trends in state governments across the U.S., as well as elections for governors and state legislators. He’s been with the AP since 1995 and is a graduate of the University of Missouri School of Journalism. ",,,
Netherlands,NOS,Big,Participant,Best data-driven reporting (small and large newsrooms),Followed on the internet: hundreds of websites violate your privacy,03/01/19,"Investigation,Infographics,Map","Scraping,Google Sheets,CSV,Python"," Both European and Dutch law forbid the placing of so-called tracking cookies on someone's phone or computer without explicit permission. That is why many websites display a cookie-message: if you agree, you give websites permission to track your online behavior through tracking cookies. Research by the NOS shows that more than 1300 in the Netherlands popular websites place tracking cookies at the first site visit without permission. "," Following our publication the Dutch Data Protection Authority (Dutch DPA) started an investigation looking into the permission requirement regarding tracking cookies and whether website owners obey this requirement. Some of the websites also stopped their practice of placing tracking cookies without permission, and so stopped violating the online privacy of their website visitors. "," For this investigation we wrote several computer scripts. First we created a list of all sites, around 10.000 in total, we wanted to investigate. Second Joost Schellevis wrote a script in PHP to visit each website 10 times. Hereby using the sandbox function from the Google Chrome webbrowser to make sure we would get a clear overview of cookies placed by every site. Using the sandbox function, we could gather placed cookies for every website in a seperate folder. Use of the sandbox function also guaranteed that we indeed simulated a first time visit to a website. For every visit we stored all cookies placed by the visited site in a different folder. This information then became the basis for our first dataset.    Next we created a list of all unique cookies websites had placed on our machines during the 100.000 first website-visites we simulated. For every cookie we checked if it was a tracking cookie or not. Part of this work was done low-tech, with help of several colleagues to check every cookie we came across   Using a Python script we combined this second dataset with the data from all site visits. We filtered all sites that placed tracking cookies on three or more first visits using the Python Pandas library.    We found over 1300 websites that placed tracking cookies without the users permission at the first visit. Among them Dutch political parties; insurance companies, popular websites for children and youngsters and many sites of media companies.  "," The hardest part of this project was making sure our findings for every website visit would be and stay isolated from both other visits to the same websites and visits to other websites all together. Since we decided to visit 10.000 websites all 10 times, this required some thought.    We ended up using the Google Chrome sandbox, creating a folder for every website and website visit. This way, we automatically isolated cookies placed by a website for every visit; making sure our final result would be trustworthy.     "," With the use of technology, comes responsibilty. This investigation shows that many organisations - among others companies that handle healthcare data and sites from political parties - aren't as ready as one might expect them to be when it comes to taking proper care of protecting the online privacy of their website visitors.    Journalists create the maps people navigate society by: since society digitalizes with godspeed, journalists need to map the digital world too. This publication shows that sometimes no exclusive technologies are needed to do so: our reporting relied heaviliy on the use of the Google Chrome sandbox, a tool available to all. ",https://nos.nl/op3/artikel/2273929-gevolgd-op-internet-honderden-websites-schenden-je-privacy.html,https://nos.nl/artikel/2274020-deze-websites-schenden-ongevraagd-je-privacy.html,,,,,,"Joost Schellevis, Winny de Jong"," <a href=""https://twitter.com/schellevis"">Joost Schellevis</a> (1989) is a technology journalist, covering the intersection of technology and society. Among other things he reports on algorithms, privacy, security and IT law. Working at the Dutch national news broadcast NOS, he appreciates the opportunity to tell technology related stories to the general public. While his audience might not even know how they are impacted by technology, they are affected by said technology.   <a href=""https://twitter.com/winnydejong"">Winny de Jong</a> (1988) is data journalist at the Dutch national news broadcast NOS. Specialised in data-driven journalism, she applies data investigative reporting methods to stories on all topics. Since not all data investigations need a visualisation, and numbers in radio and tv reporting are rare; Winny embraces the invisibility that sometimes comes with data journalism. The data and analysis stay mostly invisible yet both reach the general public.  ",,,
United States,The Associated Press,Big,Co-winner,Innovation (small and large newsrooms),AP DataKit: an adaptable data project organization toolkit,09/12/19,"Solutions journalism,Multiple-newsroom collaboration,Open data","R,Python"," AP DataKit is an open-source command-line tool designed to help data journalists work more efficiently and data teams collaborate more effectively. By streamlining repetitive tasks and standardizing project structure and conventions, DataKit makes it easier to share work among members of a team and to keep past projects organized and easily accessible for future reference. Datakit is adaptable and extensible: a core framework supports an ecosystem of plugins to help with every phase of the data project lifecycle. Users can submit plugins to customize DataKit for their own workflows. "," The AP open-sourced its project-management tool, DataKit, in September of 2019. Our data team has used it internally for two years now on every single analysis project we've done. Its purpose is simple, yet sophisticated: With a few command-line directions, it creates a sane, orgnanized project folder structure for R or Python projects, including specific places for data, outputs, reports and documentation. It then syncs to GitHub or Gitlab, creating a project there and allowing immediate push/pull capabilities. Finally, it syncs to S3, where we keep our flat data files and output files; and to data.world, where we share data with AP members.   DataKit's release came at ONA and attracted the attention of roughly 60 or so conference attendees, many of whom returned to their classrooms and newsrooms to try it out. It has been adopted by individual users, by the data analysis team at American Public Media, and is in use in some data journalism classes at University of Maryland and University of Missouri. We'll have another install party for interested data journalists at NICAR in March.   Interestingly, the project has also had several open-source contributions from the journalism community. Several journalists have built additional plug-ins for DataKit -- for instance, one coder wrote a plugin to sync data to Google Drive.   The impact of DataKit is fundamental: it allows us to move quicker and collaborate better, by creating immediate and standardized project folders and hook-ins that mean that no data journalist is working outside of replicable workflows. Data and code gets synced to places where any team member can find them; and each project looks and acts the same. It creates a data library of projects that are well-documented, all in one place and easy to access. "," DataKit is an extensible command-line tool that's designed to automate data project workflows. It relies on core Python technologies and third-party libraries to allow flexible yet opinionated workflows, suitable for any individual or team.   The technologies at the heart of DataKit are:   * [Cliff](http://docs.openstack.org/developer/cliff/) - a command-line framework that uses Python's native setuptools entry points strategy to easily load plugins as Python packages.   * [Cookiecutter](https://github.com/cookiecutter/cookiecutter) - a Python framework for generating project skeletons   Through the cookiecutter templates, DataKit creates a series of folder and file structures for a Jupyter notebook or an RStudio project. It also configures each project to sync to the proper gitlab and S3 locations, and loads specific libraries, dependencies and templated output forms (such as an RMarkdown customized to match AP design style).   The AP has built four plug-ins: for Gitlab and GitHub; for S3 and for data.world. Other open-source users have since built additional plug-ins to customize DataKit to their workflows, such as syncing to additional data sources (Google Drive) and outputs such as Datasette. "," The most difficult part of the project was creating clear, concise documentation that would help others use our open-source software. We had never open-sourced something so ambitious before, and were put in the position of anticipating others' uses (we created a GitHub plug-in despite our team not using GitHub regularly) and others' pain points in understanding, installing and using DataKit.   We created DataKit to scratch our own itch -- to make our team work better, faster and with more precision and control. Having DataKit means we spend less time every day handling the messy, boring parts of a project -- finding old files, creating working directories -- and more time on the serious data analysis work we need to be doing.   The AP is a collaborative news cooperative, and in that spirit, it made sense this year to fully open-source one of our team's most powerful tools to share it with others. One of our goals is to make data more accessible to other newsrooms, and DataKit we hope does this by taking away some of the barriers to getting to an analysis and sharing data. "," Creating standardized workflows across a data team leads to quicker, more collaborative and stronger work.   Data workflows can be notoriously messy and hard to replicate -- Where are the raw data files stored? What order do you run scripts in? Where's the documentation around this work? Is the most recent version pushed up to GitHub? Can anyone beside the lead analyst even access data and scripts? -- and DataKit was built to fix that.    The thing AP's Data Team would like others to come away with is that we don't all have to use these messy, irreproduceable and bespoke workflows for each project that comes across our desk. Creating a standardized project structure and workflows creates sanity -- through DataKit we at the AP now have an ever-growing library of data and projects that we can grab code from, fork or update when needed -- even on deadline. We can also dip into each other's projects seamlessly and without trouble: One person's project looks like another's, and files and directories are in the same places with standardized naming conventions and proper documentation.   DataKit simply lets analysis teams work better, and faster, together. One real-life example from 2019: When we received nearly a half billion rows of opioid distribution data this summer, and were working on deadline to produce an analysis and prepare clean data files to share with members, we had six people working concurrently in the same code repository with no friction and no mess. The AP landed an exclusive story -- and shared data files quickly with hundreds of members -- thanks to DataKit.     ",http://datakit.ap.org/,https://www.rjionline.org/stories/ap-datakit-intro,https://ona19.journalists.org/sessions/23627451/#audio,https://www.poynter.org/tech-tools/2019/data-journalism-solves-big-problems-but-its-an-organizational-mess-a-new-tool-from-the-ap-aims-to-fix-that/,https://github.com/associatedpress/datakit-core,,,"Serdar Tumgoren, Troy Thibodeaux, Justin Myers, Larry Fenn, Nicky Forster, Angel Kastanis, Michelle Minkoff, Seth Rasmussen, Andrew Milligan, Meghan Hoyer, Dan Kempton"," AP’s 12-person data journalism team brings the power of code and quantitative analysis to AP’s newsgathering and production, generating distinctive content across all platforms and providing our members and customers with greater capacity to tell their own data-driven stories. The team is distributed across seven cities in the United States, and its members are technologists, journalists, analysts and problem-solvers. Serdar Tumgoren, the team's former news apps lead, is now a professor at Stanford University, but continues to support and work on DataKit. ",,,
Argentina,Datos Concepcion,Small,Participant,Innovation (small and large newsrooms),Teach data journalis for the new journalist generation,03/01/19,"Investigation,Solutions journalism,Documentary,Database,Open data,Infographics,Chart,Map","Personalisation,Scraping,D3.js,QGIS,Json,Creative Suite,Microsoft Excel,Google Sheets,CSV,OpenStreetMap",<pre> The Datos Concepción Team (Argentina) presented its proposal for Training in Data Journalism for Universities in the framework of the National Meeting of Communication Careers (ENACOM). The Program is open to all Universities that want to incorporate Data Journalism to The training of your students. </pre>,"<pre> The training program seeks to incorporate Data Journalism into teaching in Communication Careers throughout the country and Latin America. - Teachers from 10 universities in Argentina have been trained. - At least one data journalism workshop has been developed with students in each of these universities - A total that exceeds 400 students hasn been trained in data journalism skills. - At the moment four Journalism career coordinators have been interested to include data journalism as a subject in the training of communicators. Datos Concepción showed the scope of these first steps of the Program in a paper entitled  ""Teach Data Journalism to train new Journalists.""  This participation is integrated into the actions to promote journalistic research based on the use of databases and the generation of stimulating experiences that show students new narratives, work with technology and the creation of interdisciplinary teams.</pre> <pre>  </pre>"," We train to teachers and students in:   - Search, download and clean datasets   - visualize data   - storytelling and interactive narratives   We use free tools like:   Flourish / Tableau / Qgis / openstreetmaps / Google spreadsheets / infogram, etc.              ",<pre> The hardest part of this project is to achieve changes in the training and perspective of teachers in journalism careers. Many of them are outdated or do not have basic technical skills for data journalism and that implies breaking strong resistance. The journey we have made through workshops in universities opens a very important door to drive these changes and establish training in data journalism</pre> <pre>  </pre>,"<pre> This experience must show that the path of transformations in the training of journalists is slow and begins little by little. First we give a talk, then a workshop, then a hackathon and later we try to generate a Content Unit focused on data journalism.</pre> <pre>  </pre>",https://datosyperiodismo.com.ar/ensenar-periodismo-de-datos-en-latinoamerica/,,https://docs.google.com/document/d/1kZ3uysA6d6sLyZLme79OlS2IG_wecjQMpbCZnpCjgmM/edit?usp=sharing,https://datosyperiodismo.com.ar/periodismo-de-datos-para-universidades-la-propuesta-de-datos-concepcion/,,,,Datos Concepcion, Adrian Pino - Data Scientist Jr. Data analytics Sr. Data journalist. Trainer in Data Jorunalim. Professor at Unviersity. Founder of Datos Concepción (Argentina)   Soledad Arreguez: Adrian Pino - Data Scientist Jr. Data analytics Sr. Data journalist. Trainer in Data Jorunalim. Professor at Unviersity. Founder of Datos Concepción (Argentina) ,,,
United States,The Wall Street Journal,Big,Participant,Best visualization (small and large newsrooms),How PG&E's Aging Equipment Puts California at Risk,29/10/19,"Investigation,Explainer,Long-form,Open data,OSINT,Infographics,Chart,Video,Map,Environment,Corruption,Business,Employment","Animation,3D modelling,Drone,Scraping,D3.js,QGIS,Canvas,Json,Adobe,CSV,R,RStudio,OpenStreetMap"," Dozens dead. Thousands displaced. Billions of dollars in damage. Major outages and high costs. After years of out-of-control wildfires,The Wall Street Journal undertook relentless reporting in 2019 that connected the deadly disasters in California to systemic failures by PG&E Corp. That reporting exposed a culture of neglect and prompted changes that will make millions of Californians safer.    Journal reporters flew drones and hiked along remote ravines to inspect PG&E power lines. They unearthed documents indicating when lines were last upgraded. This visual story explains the intersection of aging PG&E infrastructure and California’s elevated fire-risk areas.     "," Journal reporting revealed that PG&E’s equipment was starting more than a fire a day in California. PG&E was aware that hundreds of its lines had exceeded their life expectancy but repeatedly delayed safety work, including on the line that broke and started the November 2018 Camp Fire.   PG&E said it was considering permanently shutting off the line a day after the Journal reported that the company had delayed upgrades. "," Mapping the vulnerable equipment on top of California’s greatest fire-risk areas provided a birds’-eye view of the situation. Seamless use of drone video, animation and cartography allowed us to present this critical information to our millions of readers in a way that was both meaningful and memorable.   The Journal team used ArcGIS for compiling geographic data and digitizing missing data when necessary and Adobe Illustrator, Vanilla Javascript and d3.js to build our interactive maps. We also commissioned exclusive drone footage of the area to strengthen our reporting with the on-ground perspective. "," The most challenging part of this project was paring down a wealth of related geographic data to the most necessary. For three months, the team doggedly scrutinized data to pick the most impactful sets and relentlessly edited the material to provide a clear narrative.   Journal reporters flew drones and hiked along remote ravines to inspect PG&E power lines. They unearthed obscure federal documents and discovered when lines were last upgraded. And they tracked down hundreds of current and former PG&E executives, contractors and regulators.   Their reporting revealed:  <ul>    PG&E’s equipment was starting more than a fire a day in California.       The company repeatedly delayed safety work on the century-old transmission line that broke and started the November 2018 Camp Fire, killing 85 people.       PG&E was aware that hundreds of its high-voltage lines had exceeded their life expectancy—some were more than 100 years old—but never upgraded them.       The company planned to black out millions of customers in a desperate bid to stop sparking fires without sufficient consideration of the effects on health and safety.       California regulators were preoccupied with requiring the utility to purchase green power and slow to recognize how a changing climate increased the danger posed by its equipment.    </ul>"," This project is proof of the power of diligent data-driven reporting. The Journal mined hundreds of pages of the official reports to shed light on the causes of the deadly California wildfires. Using data, visual assets and intuitive navigation in a careful way the Journal turned an extremely complex topic into a very clear and story. The series, including this key visual story, had significant impact.   <ul>    Amid mounting lawsuits and growing financial pressure related to the company’s role in wildfires, PG&E’s CEO resigned and the company announced it would file for bankruptcy due to more than $30 billion in fire-related liabilities. Those announcements came within  24 hours of the Journal’s report that the company’s equipment was starting a fire a day.       PG&E said it was considering permanently shutting off the line that sparked the Camp Fire a day after the Journal reported that the company had delayed safety upgrades on it. PG&E eventually decommissioned the line forever.       All the while, the Journal’s team showed readers the human toll of what PG&E wrought. An article about the survivors trying to rebuild Paradise, the town turned to ash by the Camp Fire, quoted school superintendent Michelle John confronting PG&E executives visiting the city: “You ruined thousands of lives.”    </ul>  Erin Brockovich, the activist famous for her legal fight against PG&E for contaminating a town's drinking water, cited the Journal’s reporting while calling for a criminal investigation into the utility. “People are dead and it could have been prevented,” she told followers on Twitter, sharing the Journal’s report that PG&E had sparked at least 1,500 wildfires. ",https://www.wsj.com/graphics/california-wildfire-pge-drought/,,,,,,,"Yaryna Serkez, Renée Rigdon, Dave Cole, Russell Gold"," Yaryna Serkez creates interactive visualizations for the Journal. She is a full-stack data journalist whose daily responsibilities vary from data scraping and analysis to design and web development. Her projects were recognized by the SND, the GEN and Malofiej.   Renée Rigdon is an award-winning cartographer and graphics reporter for the Journal. She has spent the last 11 years making maps and infographics for the newspaper and website.    Dave Cole is a photo and multimedia editor for the Journal. He has spent the past two years creating visual projects for print and online. Before that, he was a producer building stories for the WSJ Snapchat Discover channel and on its Instagram Stories account. ",,,
United States,The Wall Street Journal,Big,Participant,Best visualization (small and large newsrooms),The Farm Belt's Miserable Year,25/09/19,"Explainer,Long-form,Open data,Map,Politics,Environment,Business,Agriculture,Economy","Animation,Scraping,D3.js,Canvas,CSV,R,RStudio", This engaging mobile-first interactive outlines how the trade war and bad weather are conspiring to create enormous challenges for U.S. farmers. ," This project is one of the first comprehensive analyses of current challenges for U.S. agricultural industry. This visual story outlines how commodity prices dropped, exports to China slowed, stockpiles rose, funding for farmers dropped and support for President Trump remained high. The package explains clearly and visually the economic factors at play. ", This project was mostly built with D3.js and canvas technologies. The Journal also used a mobile-first tapping approach which makes it easier to navigate information-dense interactives based on numerous charts. ," Turning more than a dozen relatively simple charts into an engaging experience was the biggest challenge of this project. The team aimed to maintain readers’ attention without introducing additional visual noise or flashy animation effects at the expense of visual clarity. To achieve this, the Journal introduced visual metaphors, for example a conventional stacked bar chart into animated rainfall. Smooth animation and a consistent set of visual cues were used throughout to lead readers through this complex story. "," Using a mobile-friendly format, in which readers tap through a series of slides, provides a better experience for readers. This format made editing extremely important. Telling a complex story in a limited number of visual frames concentrates the value and clarity of the story. It is important to reach readers where they are, on their phones, and to be respectful of their time. This visual story does both. It is a well-reported and important premise that is elevated with innovative visualization forms. The thought-provoking visual explorations help communicate abstract data in a clear and engaging way. ",https://www.wsj.com/graphics/us-farmers-miserable-year/,,,,,,,Yaryna Serkez," Yaryna Serkez creates interactive visualizations for the Journal. She is a full-stack data journalist whose daily responsibilities vary from data scraping and analysis to design and web development. Her projects were recognized by the SND, the GEN and Malofiej. ",,,
United States,The Wall Street Journal,Big,Shortlist,Best visualization (small and large newsrooms),How to Profit in Space: A Visual Guide,25/06/19,"Explainer,Long-form,Database,Open data,OSINT,Infographics,Chart,Economy","Animation,3D modelling,Scraping,D3.js,Three.js,Canvas,Json,CSV,R,RStudio,Node.js", An exciting journey through the constellations of Earth’s satellites and business opportunities in space. This project combines the realistic simulation of objects orbiting our planet with an in-depth look at how outer space is turning into a battlefield for startups and tech investors.   , This project is a thorough analysis of the space industry and its future trends. The Wall Street Journal scrutinized the Earth observation data market and investigated the potential threads of space debris and space commercialization. The project had high engagement and positive reader feedback for both desktop and mobile versions. ," Data-preparation process for this project included work with both R and Node.js. The Journal team has used R to merge Space-Track’s satellite catalog with available TLE (navigation) data and categorize satellites by type and purpose with UCS satellite database. The three-dimensional position of satellites, debris and orbit paths were calculated based on TLE data with javascript library satellite.js. In cases when TLE data wasn’t available, the approximate position was calculated based on the object’s inclination and apogee. "," This visualization is an advanced combination of d3.js and Three.js. To improve performance, the Journal team wrote a custom GLSL vertex shader with Tween.js logic under the hood. This made it possible to handle all calculations needed for chronological satellite animation by GPU and dramatically improve the overall user experience. Other charts were built mostly with d3.js and canvas. Camera transitions and zoom-in views at Lansat and Dove satellites were created using Tween.js. Designwise, the complexity of the subject required a sophisticated visual layout and a color palette that suggested space. "," Deeply rich data sets can be translated into clear and easily understood narratives, even on complex topics. This project is a great showcase of intricate design decisions which could facilitate complicated and information-dense layouts. The treatment simplifies the experience for the reader without simplifying the story. This data visualization successfully balances the need for complexity and the need for clarity. It is a complex macroeconomic story is told through a concise and accessible format.  ",https://www.wsj.com/graphics/new-space-race/,,,,,,,"Yaryna Serkez, Joel Eastwood, Robert Wall","<h3>Yaryna Serkez creates interactive visualizations for The Wall Street Journal. She is an award-winning full-stack data journalist whose daily responsibilities vary from data scraping and analysis to design and web development. </h3> <h3>Joel Eastwood is the graphics editor for the investigations team at the Journal.</h3> <h3>Robert Wall is the senior aerospace and aviation editor, The Wall Street Journal</h3>",,,
United States,The Wall Street Journal,Big,Participant,Best visualization (small and large newsrooms),Democrats and Republicans Aren't Just Divided. They Live in Different Worlds.,19/09/19,"Explainer,Database,Infographics,Chart,Elections,Politics,Culture,Economy","Animation,D3.js,QGIS,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,Python,Node.js"," This project explains how a post-Tea Party, post-Obama America is composed of different economic and demographic realities in predominantly Republican and Democrat areas, most prominently exemplified by the fact that despite geographic losses, Democrat-represented congressional districts have come to make up an ever-growing share of the U.S. economy. "," The project reinvigorated a discussion about economic and identity politics, with both the New York Times and Washington Post running news and editorial pieces based on the same data from The Brookings Institution in the wake of our publication.   In The Wall Street Journal’s newsroom, it has led to a series of articles that examine other aspects of how “Red America” and “Blue America” contrast with each other, both by reporters who worked on this project and others who were inspired by it.   In the longer term, the Journal team hopes that these trends mark the beginning of a data-focused way of examining political polarization and teasing out how partisans come to view America through two very different lenses. "," Having data to work with hinged on being able to map Census county-level data to Congressional district boundaries. To do so, the Journal team worked with Brookings to join the county-based data to Congressional districts via scripts and good old-fashioned Excel.    To begin outlining this visual story, the Journal team had to understand the basic shifts in the various statistics (i.e. income, employment, etc.) between Democratic and Republican districts, so reporters looked at the different ways to distill the summary points. Because mean-based averages were susceptible to outliers – frequently swing districts, undercutting the core party angle -- the Journal whittled its options down to median and mode-based analysis. Although the modal approach was considered, using median values gave the team a more authentic “typical” district and allowed it to show how that typical district shifted over a decade and won out.   When the team approached the stage of storyboarding the story and conceptualizing visualizations, it primarily used Illustrator. With the visualizations, reporters wanted to emphasize the divergence over the years between Democratic and Republican congressional districts, so they started with exploring visualizations for the summary figures.   However, to add depth to the story, the team decided to look at what chart the congressional-level district data could reveal, so it dove back into the scripts and Excel to see if there were finer points to explain with the district-level data. When it was confirmed that there was, reporters ideated on ways to chart all the congressional districts.    Once the design for the charts was determined, it was a mix between using D3 to chart the visualizations, and refining the storyboard in illustrator until we were able to focus solely on the web build. "," Distilling what the statistics show and presenting them in an accessible way were a big challenge. The initial version of the project was significantly more expansive because the divide in economies can be attributed to so many facets, and because reporters had a deep set of data.   However, to keep a concise and engaging story, the team had to carefully consider what points were most essential to the narrative, how the information was structured and designed, and how the text was worded.   To that end, the reporters cut. And cut. The team built on a template that gave readers a single panel at a time to streamline every point. The Journal structured the data and words to create an exploratory interaction, so that readers are guided through content that could easily be riddled with jargon and get too far into the weeds. "," “Democrats and Republicans Aren’t Just Divided” is a powerful testament to the importance of developing sources and working with them. The Journal team worked hard to join, clean and present the data, but a standing relationship with Brookings (and a library of other good data presentations) meant they trusted us with swaths of data.   This wasn’t just key for building an exclusive project, but also for affording the reporters the time for the back-and-forth with Brookings to develop the story and sharpen the narrative. ",https://www.wsj.com/graphics/red-economy-blue-economy/,,,,,,,"Danny Dougherty, Jessica Wang, Aaron Zitner, Dante Chinni"," Danny Dougherty is a DC-based data journalist for The Wall Street Journal working mostly in graphics reporting and interactive development.  Jessica Wang is currently a graphics editor at the Wall Street Journal, primarily covering U.S. news.Aaron Zitner, News Editor  Dante Chinni, National political and data reporter ",,,
United States,ProPublica,Big,Shortlist,Open data,Nonprofit Explorer Full-Text Search,06/06/19,"Database,Open data,News application,OSINT",PostgreSQL,"The IRS publishes millions of XML files with the full suite of information found on a nonprofit's tax filings, as long as they were filed electronically (which . But a reporter asked if we could search through them to find the names of specific people or companies, and we realized there were no free tools to search through their contents -- so we fixed that. We added the ability to search anywhere in the text of more than 3 million 990s, giving researchers, reporters and anyone else the ability to dig deep into these records and unearth hidden relationships between"," We've heard from journalists from all ends of the spectrum that this tool has helped them uncover hidden donors and <a href=""https://twitter.com/stevemistler/status/1147148138567876608"" style=""text-decoration:none;""> dark money in politics </a>. BuzzFeed used it to find information as disparate as nonprofits with <a href=""https://twitter.com/paldhous/status/1177367751423188992"" style=""text-decoration:none;""> connections to Jeffrey Epstein </a> tp a wealthy conservative whose private foundation <a href=""https://www.buzzfeednews.com/article/rosiegray/federalist-weekly-standard"" style=""text-decoration:none;""> lists an investment in The Federalist </a>, helping solve a perennial question. The real depth of the tool's impact isn't known, but as the only free tool of its kind, and as one of the most well-trafficked parts of a well-used news app, it is likely to be quite large. "," For a while, ProPublica has allowed people to search for company names and eventually the names of nonprofit employees from our free app. But at some point we decided: why not dump the entire text of the tax forms into Elasticsearch? So we did just that — took the files, stripped out the XML tags (which make up the bulk of the file size), and dumped them all into Elasticsearch for indexing.   It’s a simple solution, but deceptively powerful. We could have created more structured search engines: for grants, contractors or conflicts of interest. But in the end, giving people the ability to run searches across the whole set proved not just structurally easier, but more versatile. "," Processing the entire pile of 990s -- which is millions of files, tens of millions of individual forms, and gigabytes on gigabytes in size -- is no small task. It takes hours to reprocess from scratch, so formulating a way to create an additive search index (instead of destroying and recreating one, as many elasticsearch indexes do) was a challenge. We had to create a way to be sure that we had an index that was up-to-date at all times, and creating redundancies in case an indexing operation failed. "," I truly believe that the beauty is really in the simplicity: dumping a bunch of text into Elasticsearch is really exactly what it was meant for, and what better than to dump millions of government records that are otherwise not readily searchable? ",https://projects.propublica.org/nonprofits/full_text_search,https://www.propublica.org/nerds/new-search-full-text-of-3-million-nonprofit-tax-records-for-free,https://projects.propublica.org/nonprofits/full_text_search?boolean=true&q=%22pro+publica%22+OR+propublica,https://projects.propublica.org/nonprofits/,,,,Ken Schwencke," Ken Schwencke is the editor of our news applications team, which creates interactive databases and graphics. Ken has been with ProPublica since 2016, where he has worked on our award-winning <a href=""https://www.propublica.org/electionland/"">Electionland</a> project, ran our <a href=""https://projects.propublica.org/nonprofits/"">database of nonprofit data</a>, and reported on LGBTQ issues and white supremacists. Previously, he worked on The New York Times' interactive news team and the Los Angeles Times data desk. He has a journalism degree from The University of Florida. ",,,
United States,"ProPublica, The Charleston Gazette-Mail",Big,Participant,Best visualization (small and large newsrooms),A Guide to Every Permitted Natural Gas Well in West Virginia,03/06/19,"Explainer,Map,Satellite images,Environment","D3.js,QGIS,Canvas,Json,PostGIS"," Well pads are large gravel clearings where horizontal natural gas wells are drilled into the earth. In West Virginia, this often involves flattening hilltops, denuding forests and paving new roads. There can be multiple wells on a pad. Some have over a dozen. ProPublica obtained images of every permitted horizontal well pad in the state using data from the West Virginia Department of Environmental Protection and aerial imagery from the U.S. Department of Agriculture. "," This was part of a larger project called Powerless that published both in 2018 and 2019; the impact was to show the citizens of West Virginia the extent and contours of natural gas development in their state -- which is quite intrusive. One of the people we profiled in that series were plaintiffs in a case against a natural gas company, and afterward the court ruled that the company was in fact trespassing on their land. "," First, we grabbed all of the wells from the Department of the Environmental Protection, and used a clustering algorithm to locate the well pads. We then collected aerial images taken by the USDA's National Agriculture Imagery Program within a 250-meter radius of the center of those clusters. We fed that to Google Earth Engine to process the imagery, and then used built those into a front-end to let people explore them. We used d3 to create a locator map that shows you where you are in the scroll, and general javascript to power the site. "," Identifying the clusters of natural gas permits that make up well pads and turning that data into 1,400 pieces of high-resolution aerial imagery was a complex task involving new and innovative data processing pipelines, which I had never attempted before. By using this technique we were the first to show the extent of the environmental devastation wrought by natural gas development in West Virginia. ", Using computer programming to show the totality of an instance of harm can be a powerful rhetorical tool for influencing public opinion and policy decisions on a topic. Sometimes you don't know the full scale of a problem until you have collected and shown an image of every single instance of it together. ,https://projects.propublica.org/graphics/wva-well-pads,,,,,,,Al Shaw and Kate Mishkin," Al Shaw is a news applications developer at ProPublica. Equal parts designer, developer and reporter, he uses data and interactive graphics to cover environmental issues, natural disasters and politics.   Kate Mishkin was an environmental reporter for the Charleston Gazette-Mail, who now works for Neon Hum Media, a podcasting company.     ",,,
United States,Atlanta Journal-Constitution,Big,Participant,Best data-driven reporting (small and large newsrooms),Precinct closures harm voter turnout,13/12/19,"Investigation,Solutions journalism,Map,Elections,Politics","CSV,R,RStudio"," Using four years of data and statistical techniques, we discovered that after the U.S Supreme Court invalidated a key provision of the Voting Rights Act of 1965, leading to the closure of more than two hundred Georgia voting locations, an estimated 50,000 to 80,000 Georgia voters didn’t vote in the 2018 gubernatorial election because of their newly-increased distance to the polls. "," Our work was cited by the NAACP's Legal Defense Fund in their January 2020 document ""Democracy Diminished."" It was cited by presidential candidate Elizabeth Warren, former presidential candidate Julian Castro, former Georgia gubernatorial candidate Stacy Abrams, and Terry Sewell, D-Alabama, the House sponsor of a bill intended restore the provisions cut from the Voting Rights Act of 1965 by the Supreme Court in 2013. "," The analysis was primary done in R, however, we also used the Census geocoder. A great deal of spatial statistics was performed and vetted for the analysis, which required determining voters' home location, voters' precinct location, and the distance between the two. We used voter history data, demographic data from the voter rolls, shapefile data from the State of Georgia, and demographic data from the Census to create a model of how distance to the polls affect voter turnout.   Because voter behavior depends heavily on age, race, county, and prior voting history, we used voter history files and demographic information from the voter rolls to account for those variables.   We grouped voters by their age, race, income, county and prior voting history, calculating voter turnout in those groups for those near and far from their polling places, taking the difference between those rates to be the effect of distance on voting.   To say voting decreased because distance increased, however, is a causal claim—typically forbidden in the statistical sciences. We used a method for causal inference, called do-calculus, to determine which variables we needed to control for in order to properly estimate the causal effect of distance on voter turnout. We also consulted with political scientists and statisticians to verify and vet our methodology. "," Creating a robust methodology with easily understood results was by far the hardest part of the project.   The underlying analysis behind this project would fit perfectly in a research-level social science journal. Deciding on a methodology that would be scientifically accurate but comprehensible to a lay reader, to whom our work is ultimately tailored, was not easy. It meant simultaneously meeting the standards of professional science and professional journalism, which while overlapping, differ significantly. Originally, we tried more advanced statistical regression methods. However, we decided against them because they presented another obstacle for a reader to overcome in understanding the results and the story. Those models were, instead, used as sanity checks for our simpler model.   Months were put into the process of selecting a model that was both accurate and understandable, and we consulted with academic political scientists and statisticians to ensure model validity. Then, we spent months translating our scientific results into a story, supplementing the analysis with the stories of real people who had difficulty voting because of the changes we identified. We ended up with one story that fused two worlds, not two stories crammed into one.   Creating research-level statistical analyses aimed at lay people is a difficult task, but it's one some subset of data journalists are, correctly, doing more and more often. By selecting our work for the Sigma awards, investigative stories that originate in the statistical analysis of data will be more accepted in newsrooms around the world. "," Firstly, that the Supreme Court's 2013 Shelby v. Holder decision invalidating a key provision of the Voting Rights Act had the effect of dissuading voters from engaging in democracy, something the Supreme Court assured would not happen. Our project made clear that the Supreme Court's decision had a negative impact on voter turnout in Georgia, an important battleground state in the 2020 U.S. presidential election.   From a news-making perspective, others can learn that there is space for inference in a newsroom. While machine learning and exploratory statistics have been rightly accepted as useful tools for finding stories worth telling, inferential statistics largely has not. In part, due to the scientific difficulty of statistical inference, in part due to journalists' discomfort with publishing stories that cannot be entirely verified as correct (""all models are wrong, but some are useful""), statistical inference isn't used as the basis for news-heavy stories. However, inference has already been used by news-organizations in a variety of ways. Polls released in conjunction with polling agencies, the ""needle"" at the New York Times, and electoral probabilities at 538 are a form of inferential statistics. Investigative projects whose discovery of neglect and abuse comes from statistical inference are as useful as scientific results that stem from statistical inference. Our project shows that this work is viable in a newsroom.   Unfortunately, inference is, admitedly, difficult. Others can learn about one proper way to do spatial statistics from our project. The methodology behind our work is hosted on a GitHub page in Jupyter Notebook format, so that other newsrooms with interest in this kind of work can see what we did and how we did it. While their projects won't be identical, useful similarities will remain. ",https://www.ajc.com/news/state--regional-govt--politics/precinct-closures-harm-voter-turnout-georgia-ajc-analysis-finds/11sVcLyQCHuQRC8qtZ6lYP/,https://github.com/nthieme/voter_deserts/blob/master/Voter_deserts_final.ipynb,https://qz.com/1774916/10-of-decembers-best-investigative-reports/,https://www.naacpldf.org/wp-content/uploads/State-local-responses-post-Shelby-1.6.20.pdf,,,,"Nick Thieme, Mark Niesse"," Nick Thieme:Nick Thieme has worked at a data scientist and statistician for New America, NASA, Comcast, the NSF, and a variety of universities, while his writing has appeared in Slate Magazine, BuzzFeed News, Significance Magazine, and Undark Magazine. At the AJC, he designs statistical experiments and analyzes data to discover and improve stories about inequality, human rights, health care, and climate change.   Mark Niesse: Mark Niesse covers Georgia government. He previously covered DeKalb government and Atlanta schools. Before coming to the AJC, he worked for The Associated Press and The Daily Report. ",,,
United States,"ProPublica, The Times-Picayune and The Advocate",Big,Shortlist,Best visualization (small and large newsrooms),"In a Notoriously Polluted Area of the Country, Massive New Chemical Plants Are Still Moving in",30/10/19,"Explainer,Infographics,Map,Environment,Health","D3.js,QGIS,Canvas,Json,PostGIS,OpenStreetMap"," We created some of the most detailed maps of cancer-causing air in seven parishes in southeast Louisiana, at a time when there's an influx of new plants being constructed in the area. The project maps the toxic air down to the square kilometer level, and shows flaws in how industrial emissions are regulated in the area, and how much worse it could get when new facilities get built. "," This project was the first of its kind to show this sort of cancer-causing air toxicity, and to model the potential new pollutants from new industrial development in this area. We gave the people of south eastern Louisiana and the government the tools to evaluate the potential impact of cancer-causing chemicals on the community at a level never done before. "," We used the output of an obscure scientific model developed by the Environmental Protection Agency to map the toxic air and potential hazards to residents. We analyzed a billion-row database to show cumulative cancer effects of air pollution, and we processed that information into two parts. One, at the bottom of the piece, is a MapBox map and associated vector tiles that make up an exploratory lookup map that lets you see how toxic air near you compares to the rest of the seven parishes. The other is a set of data that we processed into a set of animated maps using canvas, d3 and vue.js to walk readers through different facets of the flaws in how industrial air emissions are regulated in southeast Louisiana.  "," Understanding and processing the data was a real challenge. We went back and forth with sources for months to understand how to use this database. Before we even got there, we had to transcribe and assemble a database of emissions permits for new plants and work with a modeler to model expected emissions. Rendering an animated map of 810 square grid cells was also difficult, to put it mildly. In order to even show the mapped gradients showing cancer-causing air toxicity, we had to develop our own methodology for quantifying the extent of these problems. "," Sometimes finding ways to visualize what would appear to be boring, wonky or scientifically dense data can be incredibly impactful. Also, the more granular you show information like this, the more illustrative and interesting it is. We could have shown this at the county level, but being able to see these plumes of toxic air wafting into the community from facilities gives a more visceral response. It literally shows the story. ",https://projects.propublica.org/louisiana-toxic-air/,,,,,,,"Lylla Younes, Al Shaw and Claire Perlman"," Lylla Younes is a news apps developer for ProPublica’s Local Reporting Network. She was previously a data reporter with New York Public Radio (WNYC) and Gothamist.   Al Shaw is a news applications developer at ProPublica. Equal parts designer, developer and reporter, he uses data and interactive graphics to cover environmental issues, natural disasters and politics.   Claire Perlman is a research reporter for ProPublica's Local Reporting Network. Before becoming a journalist, she was a senior investigator at the Mintz Group, a private investigations firm. ",,,
Nigeria,"Agency France Press, Aljazeera, Yahoo News, The Pulitzer Centre, DW",Big,Shortlist,Open data,MapMakoko,29/11/19,"Solutions journalism,Documentary,Open data,Crowdsourcing,Mobile App,Map,Satellite images,Economy,Human rights","Sensor,Drone,Scraping,Microsoft Excel,Google Sheets,CSV,OpenStreetMap","MapMakoko is an opendata-driven innovative project that empowers citizens in Makoko and its n The resulting open geodata will for the first time give community leaders, residents, planners, and development agencies exact intel on everything from schools and clinics, to waters sources, sewers, roads, markets and homes in Makoko. CfAfrica will make the data available in community gathering points to help residents use it for better planning or campaigns. CfAfrica will also proactively share the maps and data with emergency response and public health and service agencies in Lagos state to ensure they have the best available geo-data for planning","<ul>  Capacity building of 15 female drone pilots.   An interactive Map of Makoko Community on the OpenStreetMap (before now, Makoko has been a blank spot on any official Map) now exist.   Makoko now have a baseline data revealing a lot of missing social amenities and absence of government presence in the community for example the non existence of a secondary school in the community.   Visitors and citizens have access to the map/data for easy navigation within the community.   Small and Medium Enterprises have been captured in the map and this is hoping to cause a business growth and better the economy.  </ul>    ","  Drones  - Trained mostly female drone pilots on data collection for Mapping. Drones was used to collect geo-spatial data and Aerial footage of Makoko    Open Data Kit  - An Open source tool which was tailored to collect ground data and geo-spatial data of Points of Interests (POI) while navigating the waterways of Makoko on  wooden canoe . I trained the team of volunteers from the community main consisting of the Makoko dream girls. the complete ODK suite was used:    The ODK Collect  - This is an andriod mobile front-end interface for the data collecting volunteers to engage with while gathering data, itwas designed to collect geo-spatial data, multi-media, text and numerical data types.    The ODK Build  - This web based tool was used in designing and structuring the front-end user interface. Simply put, it was used to build the data template/forms/ instrument that runs on the mobile devices.    The ODK Aggregate Server - The app engine that resides on a Virtual Private Server to host an instance of forms and data base to store data collected from the field remotely.      Java OpenStreetMap  - A tool I used in uploading the Points of Interest to the Live Map on the Open Street Map (OSM) Platform, highlighting waterways, streets, roads, SMEs etc.    Spreadsheet  - I used it in cleaning the data collected, making it ready for upload.    Map.me  - I also used it to upload some POIs directly to the OSM platform.    Open Aerial Map  - This is an open source web platform used to host the stitched drone Imagery, in preparation for upload to the OSM platform.    Slack-  I created Private team channels to coordinate the team work on task and other project management in general.     "," Due to the unique location of Makoko (a floating slum on the Lagoon) drones was used in Mapping the community to get a high resolution imagery for mapping, also I spent days working remotely from thin balanced canoes made of wood, mapping the nook and crannies of this community, this is the first time a mapper would go physically to collect Points of interest from a canoe - Maintaining my balance while collecting data and geo-coordinates was a skill i learnt and perfected on the field simply to provide an open data.   Bringing teams from different geo-locations/ time zone together to work on the project and collaborate on task, adopting tools for work and achieve result remotely.     ","<ul>  That getting the community buy-in is key in projects such as this. and this is achievable by organizing town halls.   Availability of Baseline opendata is important and in collecting these, building a relationship with the community helps them connect with the project.    Communities should be made to take ownership of projects in their community for sustainability and lasting impact.    There are resources in slum communities, supporting these communities is a sure way of refining them for a greater impact.  </ul>    ",https://twitter.com/dbelaid/status/1200395066805805057,https://www.youtube.com/watch?v=Xu5_ryCL8Sw,https://twitter.com/AJEnglish/status/1211874874420944897,https://news.yahoo.com/drone-project-aims-put-floating-lagos-slum-map-045702823.html,https://www.hotosm.org/projects/code-for-africa-using-drones-to-map-makoko-one-of-africas-largest-slums/,https://pulitzercenter.org/reporting/last-french-speakers-lagos,https://web.facebook.com/watch/?v=973781016334126,"Celia Lebur, Jacopo Ottaviani, James Mark, Denis Irorere, Code for Africa, The Pulitzer Centre for Crisis Reporting, The Humanitarian Open Street Map, AFP media, Makoko Dream, Uhurulabs, African Drone"," Eromosele John is a Civic Technologist at Code for Africa where he manages data-driven projects with MapMakoko as one of his recent projects.  He is well experienced in opendata and shares his skills through the Code for Africa's Academy programme as a data Literacy trainner. John's wrangleing skills has been useful to the data and design team in Code for Africa as he contributes to solving data related tasks that confronts the team on a weekly bases.   A core team player and one time lead of the Edo State Open Data Programme- The First Sub-National Open Data Portal in Africa with datasets from 32 Ministries, Departments and Agencies (MDA) of Government. For 5 years John was involved in the entire process of collecting, processing and visualizing datasets from these MDAs for the open data portal.   His backgroung in Natural Science has made him understand and have affinity for projects that connects citizens with their environment as he has a first degree in Pure and Industrial Chemistry and an MSC in Industrial Chemistry, Environmental Option from two Federal Universities (Nnamdi Azikiwe University and University of Benin) resepectively. Trainings in project management, other ICT programmes and an impeacable team has made him a seasoned professional built to confront most challenges in the field.   Jacopo Ottaviani is an award-winning computer scientist and data journalist who manages Code for Africa’s (CfA) Knowledge portfolio, as Chief Data Officer (CDO).   The CfA data analysis team consists of data analysts, software engineers and graphic designers who transform often incomprehensible data into easily understood interactive visualisations and tools. Jacopo’s role with CfA is underwritten by a Knight International Fellowship, with the International Centre for Journalists (ICFJ) and supported by the Bill and Melinda Gates Foundation.   See his full biography- <a href=""https://datajournalism.com/contributors/jacopo"">https://datajournalism.com/contributors/jacopo</a> ",,,
Brazil,O Estado de S. Paulo,Big,Participant,Best visualization (small and large newsrooms),Simulation Shows Which Children Are Adopted (And Which Are Not) In Brazil,09/09/19,"Long-form,Human rights","Animation,Json,Adobe,Creative Suite,Google Sheets,CSV,Python"," In Brazil, couples that want to adopt children need to go through an extensive examination and backround check. After they are accepted into the adoption proccess, they also need to fill a form in which they declare which kind of child they are willing to take in. This is when characteristics such as age, existance of siblings, special needs, gender and race come into play.     "," With wide repercussion in Brazil, the article was quoted by Senator Kátia Abreu two days after publication – mentioning the newspaper's name directly in her speech. She was announcing the creation of a local branch for the National Association of Adoption Support Groups, at the state that she represents.   In the social networks, internet users emphasized the sensitivity of the subject, as well as the importance of the topic. "," The wannabe adoptive parents are looking for very specific traits. For most of the children and teenagers living at government foster homes, this makes the odds of moving in with a new family very low. Age is the most decisive factor: after one turns 10, the chances drop drastically. Along with that, having disabilities and siblings are also key factors.   To show how parental preferences impact the likelihood of adoption, we developed a simulator that displays how long it would take for specific children to be selected. Each individual child is represented as a plant that grows as time passes. "," The team responsible for the report had to develop its own simulator with three different databases.  The first one brings figures about the characteristics of the population of children and adolescents in Brazil and the second one, the National Registry of Adoption (CNA), which details what are the characteristics that the applicants seek when adopting.   The third database is the result of simulations made by  Estadão . Based on real numbers from the two sources above, we created an algorithm that generates children and suitors - and then checks if there was a ""match"" between them. The match happens when there is a child with all the characteristics that the parents are looking for.    An example.  According to the CNA system - in consultation made on August 10, 2019 - there were 42,546 applicants in Brazil. Among them, 15,694 accepted to adopt a child who had siblings. This indicates that approximately 37% of the applicants accept to adopt children with siblings.   When the simulator generates a new suitor, it makes a kind of crown or coin to decide if this suitor accepts children with siblings. However, unlike a normal coin, where there is a 50% chance of one thing or another happening, this coin has a 37% chance of falling on one side. If it falls on this side, this suitor accepts to adopt siblings. Now imagine that we have made 100 tosses of this coin. The number of times it fell on the face of the ""accepts to adopt children with siblings"" tends to be 37. This means that it should be close to 37, but it could be 40 or 35, for example.  "," In addition to the technical challenge, we had to think about how not to distance the readers from the report. Naturally numbers and descriptive statistics are cold, so we had to think about how to humanize the data. Our choice was to represent each child as a plant. If it has a ""v"", it has siblings and a flower, it means it has some kind of deficiency. ",https://arte.estadao.com.br/brasil/adocao/criancas/,https://github.com/estadao/simulador-de-adocao,,,,,,"Mariana Cunha, Bruno Ponceano, Vinicius Sueiro, Júlia Marques"," After two years leading a data visualization team, I have decided to join an innovation consultancy. My goal is to build stronger digital experiences, from UX and UI design to artificial intelligence & connected objects. Moreover, I’m really excited about creating physical-digital products, as they enable easier interactions and seamless experiences.    As a leader, I’ve helped a team of talented people in projects that combined data, design and web development at Estadão, a major Brazilian media company. Before that, I was a designer at a tiny agency, where I’ve worked directly with Veja, Ecovias, Via Varejo & Warner Bros.    I also have a thing for sharing knowledge. Recently, I have presented some of my work at Dataviz.Rio and conducted a workshop on how to get insights from data at Rede Globo. ",,,
Indonesia,Beritagar.id,Small,Participant,Best data-driven reporting (small and large newsrooms),Securing votes through religious sentiment,13/06/19,"Long-form,Chart,Elections","Google Sheets,CSV,R"," The story anlaysed correlation between tolerance and votes garnered by each of candidate in Indonesian Presidential election in 2019. It proves that there was religious sentiment in Indonesia during election. Prabowo Subianto who is oftenly associated with conservative supporters garnered higher votes in provinces with lower Index of Tolerance and mono-religion. On other hand, Joko Widodo who is associated with more liberal parties acquired higher votes in more tolerant provinces with high range of diversity.  ", People are more aware of the position of each of the presidential candidate and their ideology.  ," 1. I used R and online software to convert the data from dbf to csv   2. I used OpenRefine to clean the data   3. Google Spreadsheet to do the analysis from csv (Pivot Table, Correlation)   4 I used Datawrapper to visualise the charts.  "," I analysed the data from the scratch, it was raw data in dbf format from Statistics Indonesia. I collected the data and discussed with political scholar (Burhanuddin Muhtadi) to create an Index of Tolerance in 34 provinces in Indonesia based on three questions asked by the surveyor to respondents (head of family):   1. The willingness to have neighbours and allow their kids to have friends from other religion   2. The willingness to allow other religious activites in the neighbourhood   3. The willingness to allow establishment of religious places of worship   The score of index then divided into four parts: very low, less tolerant, tolerant, high tolerant. None all of them get extrem score, the score ranges are less tolerant and tolerant.    After creating the index, I did a correlation analysis with the votes garnered by each of presidential candidate to look for a pattern.  "," Data approach story can prove assumption that people talked about. Before this story revealed, journalists always associate Prabowo with conservative supporters and Jokowi with more liberal ones. However, they have never come up with a statistical proven story that strengthen the argument. The research done in this story makes the argument stronger.   ",https://beritagar.id/artikel/laporan-khas/mendulang-suara-lewat-sentimen-agama,,,,,,,Aghnia Adzkia," Aghnia is data journalist at BBC World Service for East Asia Hub and founder of <a href=""http://bit.ly/daftarjournocoders"">Journocoders Indonesia</a>, a data journalists and professionals community based in Jakarta aims to learn data analysis and visualisation. Prior joining the publication, she was a data journalist at Beritagar.id and covering various topics from legal to human rights. She was an intern at the Guardian’s data team in London right after completing her master degree in Digital Journalism at Goldsmiths College, University of London.    Previously, she won Thomson Foundation Fellowship in London in August-September 2017 and had a chance to learn data journalism, digital writing, video editing from worldwide experts and practitioners in the field. She also won One World Media Production Fund (London, UK) to filming a documentary of Islamic boarding school for transgender people in Yogyakarta, Indonesia. ",,,
United States,"NPR, The University of Maryland Howard Center for Investigative Journalism",Big,Participant,Best data-driven reporting (small and large newsrooms),Heat And Health In American Cities,09/03/19,"Investigation,Solutions journalism,Long-form,Multiple-newsroom collaboration,Open data,Podcast/radio,Infographics,Map,Satellite images,Audio,Environment,Health","Sensor,Scraping,D3.js,QGIS,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,R,RStudio,OpenStreetMap,Python,Node.js","  Through a complex data analysis corroborated by thorough on-the-ground reporting, NPR and the Howard Center showed how the most vulnerable people in dozens of major U.S. cities -  the urban poor, who are often disproportionately people of color - are exposed to far more heat day after day than their wealthier counterparts.   ","  Our stories were picked up by news services across the country and specifically cited in a House Oversight Committee hearing. The city government in Chesapeake, Va., used our data to inform decisions made by its community arborist. And after these stories ran, the Louisville city government considered an ordinance that would require developers to leave a certain percentage of trees in place.      The series won an innovative storytelling award from the National Press Foundation, and the Howard Center specifically won a grand prize from the Online News Association that will allow the center to help other news organizations build on our work next summer. The Investigative Reporters and Editors Philip Meyer Awards also recognized the series with an honorable mention.  ","  We discovered academic research by a team from the Science Museum of Virginia and Portland State University showing huge disparities in temperature between poorer and richer neighborhoods in Baltimore, and set out to discover the reason for the disparities. In 2018, Capital News Service applied for - and received - a grant to underwrite the construction of temperature sensors that we placed inside of homes in East Baltimore to collect minute-by-minute temperature readings the following summer.     In collaboration with the Howard Center, NPR analyzed 97 of the most populous U.S. cities using the median household income from the U.S. Census Bureau and thermal satellite images from NASA and the U.S. Geological Survey. NPR <a href=""https://github.com/nprapps/heat-income"">open-sourced</a> the computer code used to acquire and analyze the data, as well as the data itself. In more than three-quarters of those cities, we found that heat is not distributed evenly across a city, and where it's hotter, it also tends to be poorer.      The Howard Center for Investigative Journalism  did all of its work using QGIS, R, RStudio, Carto, Leaflet. The NPR analysis was automated in Python using Mapshaper, GDAL, GeoPandas and other libraries. To visualize the data, NPR used QGIS, Tableau Public, d3.js and Adobe Illustrator. NPR's data editor wrote more about the process on <a href=""https://blog.apps.npr.org/2019/09/27/heat-income.html"">their team blog</a>, and the University of Maryland gave a <a href=""https://cnsmaryland.org/interactives/summer-2019/code-red/behind-scenes.html#story"">look behind the scenes</a> on its project website.     Data sources we used   included:   <ul>     USGS Earth Explorer         Natural Earth         Census API         Census FactFinder         OpenStreetMap         Union of Concerned Scientists         OpenBaltimore         NAIP imagery via USDA         U.S. Forest Service         IOWA Mesonet Weather Network         NOAA and NWS     </ul>","  This series was a unique collaboration between student journalists at the University of Maryland and professional journalists at the University of Maryland and at National Public Radio.      We used more than a <a href=""https://howard-center-investigations.github.io/code-red-baltimore-climate-divide/"">dozen discrete data sets</a> to tell this story, each of which required extensive cleaning, verification and reporting to guide our analysis.  This included historical temperature and humidity data, data describing the urban heat island in Baltimore, U.S. census data, tree canopy data, redlining data, hospital visit data, emergency medical visit data, and more. The two most complex data challenges came in two areas: collecting data from our temperature and humidity sensors and determining the link between heat and poverty in nearly 100 U.S. cities.      Baltimore was the first city we mapped, and the first place we saw that troubling pattern. There, the hottest neighborhoods in that city can differ by as much as 10 degrees from the coolest. Those areas also had higher rates of poverty, according to the Howard Center's analysis of U.S. census data and air temperature data obtained from Portland State University and the Science Museum of Virginia.      But Baltimore is only the tip of the iceberg: At least 69 of the cities NPR mapped had an even stronger relationship between heat and income than Baltimore. That extra heat can have deadly health consequences, which we found confirmed in Baltimore's high rates of emergency calls when the heat index rose to dangerous levels and in the city's patterns of hospital admission rates.  ","  We aren't the only journalists who have covered the intersection between climate change and rising heat in urban environments. We were inspired by the work done by <a href=""https://www.kqed.org/science/1933708/how-hot-was-it-in-california-homes-last-summer-really-hot-heres-the-data"">KQED, which published on Oct. 29, 2018,</a> and focused on heat in Los Angeles; and work done by WNYC, which <a href=""https://www.wnyc.org/series/harlem-heat-project"">published a series</a> on heat in Harlem throughout the summer of 2016. Both outlets used sensors in telling stories about hot homes in their respective cities. Our innovation was using these techniques to show the disproportionate impact on people in the poorest areas of cities and to show that these trends were in evidence in cities across the U.S.  ",https://www.npr.org/2019/08/24/754044732/as-rising-heat-bakes-u-s-cities-the-poor-often-feel-it-most?live=1##dropdowns-container,https://www.npr.org/2019/09/04/755349748/trees-are-key-to-fighting-urban-heat-but-cities-keep-losing-them,https://cnsmaryland.org/interactives/summer-2019/code-red/neighborhood-heat-inequality.html,https://cnsmaryland.org/interactives/summer-2019/code-red/heat-health.html,https://blog.apps.npr.org/2019/09/27/heat-income.html,,,"Meg Anderson, Sean McMinn, Sean Mussenden, Nora Eckert, Nick Underwood, et al."," Meg Anderson is an assistant producer on NPR's investigations team.   Sean McMinn is the data editor on NPR's News Apps team.   Sean Mussenden, a former Washington correspondent, is data editor for the Howard Center for Investigative Journalism.    Nora Eckert is a graduate student at The University of Maryland.   Nick Undeerwood is a former intern at NPR. ",,,
Brazil,O Estado de S. Paulo,Big,Participant,Best visualization (small and large newsrooms),Basômetro: How much support does the governament have in the House?,13/06/19,"Open data,News application,Chart,Politics","Scraping,D3.js,Json,CSV,Python"," ‘Basômetro’ is a tool that monitors the votes of each representative in Brazil’s House of Representatives, with historical data ranging back from 2003 up to now. It then calculates a government support ratio for each politician and the values are also aggregated by party. "," The ‘Basômetro’ measures the governance of deputies and parties in the House. To do this, the tool calculates how many votes followed the government leader's guidance, in percent.   We consider that a vote in favor of the government is one that exactly follows the orientation of the leadership. For example, if the indication is ""yes"", only ""yes"" votes are considered pro-situation. All others (""no"", ""obstruction"" or ""abstention"") are considered votes against the government.   When the government does not register a direction and releases the bench position, the related data are discarded. Thus, the rates of government and attendance by parliamentarians are calculated taking into account only the votes in which the Executive has explicitly assumed a position "," In the tool, information made available by the Open Data Portal of the House of Representatives through the API (a kind of system that allows automation of data collection and publication) of the institution is used.    We created in Python a lib to constantly consume the House of Representatives API and format the data in a more readable way. Also in Python, the script groups the data by party and government, in addition to bringing other congressman data as voting attendance rate.  ", Among the difficulties are public data that is neither standardized nor organized. The scripts developed had to be designed to deal with these inconstancies. Another difficulty factor was to create a visually impactful but at the same time simple visualization so that the tool would be accessible and not restricted to researchers and political scientists - as it was in the first version of the Basômetro in 2012 , We believe that it is possible to reach several layers of the audience without necessarily being frivolous or too technical. The Basometer was the balance of these two strands. ,https://arte.estadao.com.br/politica/basometro/,https://github.com/estadao/basometro,,,,,,"Rodrigo Menegat, Bruno Ponceano","  Rodrigo Menegat.    I tell stories with data. Ever heard that talk about chasing news and bringing to light issues that shouldn't be hidden? I do pretty much the same, but before knocking on doors, I write some code and build some spreadsheets. There are stories in every corner and there are also stories to be uncovered behind the numbers. I seek all of them.   I worked in a big newsroom, did freelance gigs and collaborated with an university-based photojournalism team. I graduated from my hometown State University of Ponta Grossa and I have a post-bach certificate on data-driven journalism from Columbia University, in New York City. Currently, I work with the infographics team at Estadão, the most traditional Brazilian newspaper. ",,,
Argentina,Chequeado,Small,Shortlist,Innovation (small and large newsrooms),Chequeabot: Investing in technology that accelerates our impact,03/01/19,"Explainer,News application,Fact-checking,Elections,Politics","AI/Machine learning,Scraping,Google Sheets,CSV,Python"," Chequeabot, Chequeado’s fact-checking automation platform, relies on AI and Machine Learning to speed up the process of fact-checking without sacrificing quality to battle the growing amount of misinformation created and circulated at a much faster rate than fact-checks. Chequeabot does many things: it automatically scans over 30 media outlets all over Argentina, as well as all the speeches and conferences given by the president, identifies claims that can be fact-checked thanks to AI and Natural Language Processing (NLP), and indicates which of those claims are related to previous fact-checks. It also provides an open transcription platform and a text analyzer.  "," Firstly, Chequeabot frees a lot of Chequeado’s editors’ time that can then be used to produce better content, while reducing biases by being sure about its regional and media coverage. Over these two years and a half, Chequeabot has made its way into Chequeado’s newsroom meetings on Mondays, suggesting claims to check and helping our journalists find claims related to specific persons and topics.   The platform runs checkable claims with Chequeado’s database of statements previously checked, which allows to publish quicker in social networks when an already factchecked claim is being repeated, through a second function called “What’s already been checked”. This has already proven extremely useful at key moments, such as debates as it has allowed Chequeado to react faster and publish in social networks relevant content related to what was happening, and freeing the journalists to dedicate themselves to checking new information. We want to take this functionality to other editors as it relieves the work of journalists and help them avoid any omissions that might exist. Chequeado has also developed a tool to extract video transcriptions from YouTube: Chequeado's Transcriptor (chequeado.com/transcriptor, or chequeado.com/desgrabador in Spanish, is an open source application, based on Chequeabot's development. It also links every phrase to the exact moment in which is said in the video, to make its verification faster and easier. And it's free and open for everyone who need to speed up their work.    To complement this, we launched a microsite that integrates several automation tools in one interface, which allows users to submit text to be analysed searching for checkable statements, and relates those claims with previous fact-checks. Although is now restricted to Chequeado’s newsroom, it’s being improved so it can be released and open for every newsroom interested in implementing this tool. "," Chequeabot uses scraping techniques to automatically extract information from the selected media outlets, and afterwards, applies Machine Learning and NLP to identify fact-checkable statements and the relevant labels (such as the speaker or the context, or the media where it was reproduced). To do so, it uses Python libraries, such as Scikit.learn, nltk, and spaceit. The information is placed in an MySQL database, which is read by an app accessible and UX friendly to the newsroom, comprised mainly of non-technical professionals.  "," Artificial Intelligence is a powerful ally, especially in smaller newsrooms with limited resources. However, asking the AI to analyze large amounts of information blindly can become a bigger drawback than the solution it provides. It was hard for us to realize that whatever decision we chose to make could bias the algorithm, even those that seemed to be the ones that could foster and improve our work more efficiently, like asking the bot to prioritize claims from more relevant people. That, in terms of AI and Machine Learning, could have impoverished all subsequent results, neglecting those voices that are also interesting and necessary for the journalistic process. We need our Chequeabot to help us and to be better than us. From Chequeabot, we also need what it offers to be relevant to the newsroom. We need to be sure, for example, that Chequeabot’s results do not hide necessary information, and that it selects claims and fact-checkable with a criterion (that is learned), that reflects the will of the organization, specially when it comes to relevance, plurality and federal coverage. All in all, this kind of experimentation was challenging in several ways, that we could not have foreseen before, being a small newsroom.  "," The Chequeabot is an example that it is worth investing in innovation, especially when there are limited resources to be prioritized. Moreover, it shows that the greatest impact is achieved when the problem to be solved is chosen correctly, even if the solution demands too much effort in a first analysis. Failing to diagnose, or misassessing the organization's priorities, can lead to total failure. If the problem is well chosen, a small breakthrough, like the first demo of the Chequeabot back in 2017, is significant. If the problem is poorly chosen, even major developments can have no impact at all, and that, for a small organization, is critical. For us, Chequeabot was an hours multiplying tool. The investment was big, but the payoff, in the mid and long term, is enormous. ",https://chequeabot.chequeado.com/transcriptor/?,https://youtu.be/87RbE_W0k9I,http://chequeado.com/automatizacion,https://www.youtube.com/watch?v=O1PuOeYAGNw,https://twitter.com/chequeado/status/1123942647389929474?lang=es,http://chequeado.com/automatizacion,,"Laura Zommer, Pablo M. Fernandez, Mariano Falcon, Joaquin Saralegui, Matias Di Santi"," Laura Zommer. Executive Director and Editor-in-chief at Chequeado. Member of the Board of the International Fact-Checking Network and the Consultative Council of Sembramedia and the Brazilian site Gender and Number. Laura has a Bachelor's Degree in Communication Science at University of Buenos Aires and she's also a lawyer and access to information and transparency activist. She is Professor of Right to Information at the University of Buenos Aires and writes in the daily La Nacion. She worked at CIPPEC (2004-2012) as Director of Communications and at the Secretary of Internal Security of the Ministry of Justice  (2003-2004).    Pablo M. Fernandez. Director of Editorial Innovation in Chequeado. Assistant lecturer at the Data Department of Social Communication at the University of Buenos Aires and member of UBACyT research team on technology and media. Professor of the Master's Degree in Innovation in Digital Media at Torcuato Di Tella University. Author and founder of Jomofis, the independent workers' community. Blogger on Onlain. Co-lead the Diyitales podcast. Former editor of Lanacion.com (Technology and general update) and digital magazines of La Nación (Brando, Living, Lugares, Hola, Maru, Ohlala, Rolling Stone and Susana).   Mariano Falcon. Programmer in area of Innovation in Chequeado. Engineer in Information Systems of the National Technological University. Specialist in technologies for working with data, such as scraping and Natural Language Processing.   Joaquín Saralegui. Developer at Chequeado. Analyst in Information Systems, specialized in languages like Java, Python, c++, Javascript, and NLP and Machine Learning algorithms.   Matias Di Santi​. Editing and production coordinator at Chequeado and Reverso. He is a former reporter for Buenos Aires newspaper Diario Z, where he mainly covered politics and society. He helped to research and edit the investigative book El Enigma Perrotta. He was nominated for the Estímulo TEA award in 2014 and 2015. In 2016, he won the FOPEA award for Investigative Journalism. He mainly covers topics such as misinformation, official advertising and electoral financing. He is a regular commentator for Buenos Aires radio stations, as well as at TV programmes. ",,,
Mexico,PODER,Small,Winner,Open data,TodosLosContratos.mx,20/08/19,"Investigation,Explainer,Database,Open data,OSINT,Chart,Map,Corruption,Money-laundering,Business,Economy","AI/Machine learning,Scraping,D3.js,JQuery,Json,CSV,PostgreSQL,Node.js","TodosLosContratos.mx (All the contracts) is a data journalism project that has compiled almost 4 million public contracts made between 2001 and 2019 by the Mexican Federal Government. The project mixes journalistic reports that explain cases of corruption and bad practices in the mexican procurement sistem, with rankings based on algorithms especificaly designed for the mexican by the team.. The objective of the project is to promote accountability in the contracting process in Mexico, so we published all the data in QuiénEsQuién.wiki platform and API, opened the methodology of the analysis algorithms and published a guide on how to investigate with"," The publication of TodosLosContratos.mx together with the uploading of the data in QuiénEsQuién.Wiki has had three main impacts:  - Simplify the journalistic investigation of public contracts. The publication of the vast majority of contracts of the Mexican federal administration in a usable and reliable search engine has increase the productivity of the journalist, this has been expressed to us by journalists from Mexican outlets like Animal Político, Aristegui Noticias, El Universal, Cuestione, Proceso, among others, also local mexican online newspaper like Zona Docs, BI Noticias, Lado B or Cuestione, and International newspapers like AJ+ in spanish and El Faro (El Salvador).  - Promote the opening of public contracting data. Following our publication three government agencies have approached us to know how they can improve or upload new data to our platform. We have given them advice on how to improve their open data strategies; and once they publish we will update QuiénEsQuién.Wiki and our algorithmic analysis in TodosLosContratos 2020 edition.  - To increase the knowledge and interest of the citizens about the public procurement. As a result of the project, more people know how public contracting works and can easily consult it. Visits to the QuiénEsQuién.Wiki platform are increasing exponentially and every week we receive messages from people with doubts or clarifications about contracts or their participants. "," A project of this complexity has several processes and key technologies:  - Data Import: Based in the free software <a href=""https://nifi.apache.org/"">Apache NiFi</a> we have developed an importer and webscrapper orchestrator. This modular software allows us to have a simple setting for reusable components like the data cleaning module or the data update module.  - Plataform and API: QuiénEsQuién.Wiki is based on a mongoDB+node.js, all the data is hosted in a Kubernetes cluster of MongoDB databases and then exposed through a public API which is documeted both in Spanish and English. Plus a model <a href=""https://github.com/ProjectPODER/node-qqw"">client in nodejs</a> is usable with the NPM package registry. The website consumes the API and is compatible with desktop, tablets and mobile devices.  - Algorithmic analysis: Our ""groucho"" engine for analyzing open contracting data in the <a href=""https://www.open-contracting.org/"">OCDS data standard</a>. The engine is<a href=""https://github.com/ProjectPODER/OCDS_RedFlags""> published with a GPL license</a>, which makes it reusable and transparent. It's written in Node.JS.  - Data analysis: In order to fine tune the parameters of the algorithmic analysis engine we have combed through the data with the help of <a href=""https://kibana.quienesquien.wiki/"">Kibana</a>, an open source data visualization dashboard based on the ElasticSearch database engine,  which helped us to quickly recognize patterns and detect deviations.  - Data visualization: Our data is nicely presented using custom designed web-based interactive graphs and maps using primarily the D3.js library. "," For this project, our interdisciplinary team took the enormous task of automating the cleaning, compilation, transformation and analysis of 4 million contracts from 64 different tables of government-published data, a highlight of the hardest parts follows:   - Data cleaning: The mexican government does not have a practice of unifying the name of the suppliers, neither they provide a unique identifier. Our <a href=""https://github.com/ProjectPODER/lavanderia-empresarial"">""lavadora empresarial"" software (also GPL)</a> takes care of detecting duplicates with different spellings and other common errors, while avoiding to merge different but similar companies. For example, here's the page for <a href=""https://www.quienesquien.wiki/empresas/televisa-sa-de-cv"">Televisa in QuienEsQuien.wiki</a> showing all the 23 different spellings of their name across 535 contracts.   - Data transformation and compilation: Contracts from all sources are converted to the OCDS standard using specific mappings for each source, which can be very intricate with complex dependencies for the field values. 64 datasets are published in 5 different data structures, each of them requiring different pipelines in our Apache NiFi setup. These databases contain repeated contracts and several entries for the same contracting process which can only be compiled after they are transformed to OCDS standard.   - Data analysis in an interdisciplinary team: Creating work tools which can be used by both journalists, programmers and analysts took several months and several long meeting until agreements were reached on the best way to capture specific malpractices in contracts or on why we could or couldn't perform specific evaluations with the available data. "," Sharing our learned lessons is one of the main goals of the project, and encouraging others to emulate this kind of project.   As we have said all of our projects are based in free software solutions, our own code is published in GPL licenses, all of our data and methodologies is published in CC-BY licenses. And all our reports are properly quote their sources. Plus we have documented the usage of our tools in Spanish and English, making everything we've done entirely reusable.   We think the main takeaway is that it is possible to measure corruption based on public contracting data and we are starting to see the possibility of one day no longer relying on corruption perception surveys.   Having a team that is committed to making bold assumptions and running deep journalistic analysis based in data was a key asset to accomplish our impact goals and to highlight our organization as one of the most advanced in the latinamerican region.     ",https://www.todosloscontratos.mx/,https://www.quienesquien.wiki/,https://manualinvestigarcontrataciones.readthedocs.io/es/latest/#,https://api.quienesquien.wiki/v2/docs/,http://www.elclarinete.com.mx/mas-de-medio-millon-de-pesos-han-costado-visitas-de-amlo-a-aguascalientes/,https://www.m-x.com.mx/al-dia/el-chef-de-las-estrellas-era-el-favorito-de-pena-nieto,https://twitter.com/nayaroldan/status/1191900609164779520,"Eduard Martín-Borregón, Martín Szyszlican, Claudia Ocaranza, Fernando Matzdorf, Félix Farachala, Marisol Carrillo, Ricardo Balderas and Isabela Granados."," The Project on Organizing, Development, Education, and Research (PODER) is civil society organization whose mission is to improve corporate transparency and accountability in Latin America from a human rights perspective and to strengthen civil society stakeholders of corporations as long-term accountability guarantors. The main problem PODER seeks to address is state capture, whereby an economic and political elite controls public decision-making and effectively limits the realization of sustainable capitalism and democracy for the rest of society. PODER organizes its work in five programs: strategic research, community capacity building and accompaniment, transparency technology, advocacy, and strategic litigation. PODER works primarily in Mexico, though it also conducts projects in Argentina, Brazil, Chile, Colombia, and Peru.    TodosLosContratos.mx is a project lead by the Transparency Technology department who is in charge of all the data, journalistic and technology work of the organization. The department is actually conformed by seven people: three developers, two data and investigative journalist, one data analist and one manger of the team. The main projects of the department are:   -QuiénEsQuién.Wiki: Collaborative database and power map of Latin American companies and business elites. It contains data from 17 different countries and is speacilliced on Mexican public procurement data.   - <a href=""https://www.rindecuentas.org"">Rindecuentas.org</a>: An data and investigative journalistic blog that each week publish a repor on corporate transparency and accountability.   - Whistle-blowing platforms: PODER is a founding member of the <a href=""http://mexicoleaks.mx/"">MéxicoLeaks</a> alliance and has help to sep up <a href=""https://chileleaks.org/index.html"">Chileleaks</a> and <a href=""http://leaks.pe/"">Peruleaks</a>. ",,,
Philippines,Rappler,Small,Shortlist,Best news application,What inflation means for you,21/01/19,"Explainer,Open data,News application,Business,Economy","D3.js,Json,Microsoft Excel"," Through its two-part series, TheNerve team tried to make the concept of inflation more understandable to readers using data. The first part aims to help readers understand how inflation directly affects them as consumers using a calculator that compares the prices of basic goods in the Philippines from 1957 to 2011 with the prices of goods today. It also made use of both social and economic data to put in context the 2018 inflation rate hike. For the second part, TheNerve team conducted surveys to show how Filipino consumers and business owners are coping with the rising prices of goods. "," TheNerve’s inflation project puts the 2018 inflation rate hike in context using data from multiple sources – economic data, social listening, and surveys – and used innovative data visualizations to help readers understand the concept easier (ie. inflation calculator, topic mapping for news, etc).​   TheNerve’s inflation series was widely read on the Rappler website, generating 10,000 pageviews and 5,000 interactions on social media. The inflation calculator was also used more than 5,000 times. ", Tools and technologies used:  <ul>    Social listening tools to gather Twitter data.       Natural Language Processing to process news data.       Charts made using Flourish and Tableau.       Calculator built by TheNerve. Computation based on publicly-available data on inflation rates/prices of goods.     </ul>  Official data were sourced from government agencies. Data on inflation-related conversations from Twitter. Data on Inflation searches from Google Trends. The survey was conducted by TheNerve team. ," Building the inflation calculator was the hardest part of the project, requiring the team to comb through decades’ worth of economic data for accurate computations. Significant effort was also spent on visualizations and in making sure the news app is easy to use and understand. "," The project emphasizes the value of newsrooms working with the private sector, in this case a data company, to fill in vacuums in technological resources and skills, in order to deliver impactful data journalism. ",https://www.rappler.com/philippines-inflation-meaning,https://www.rappler.com/philippines-inflation-impact-consumer-business-spending,,,,,,TheNerve Team," TheNerve is a Manila-based consultancy, working with Rappler, that specializes in analyzing data to bring forth powerful insights and narratives. Believing that data can deliver real-world impact, the company enables its partners across a wide range of industries to cut through the clutter and extract value and meaning from various datasets. The insights guide partners’ business decisions and help them engage with their communities better. Composed of a team of data scientists, business strategists, award-winning storytellers, and designers, the company is on a mission to transform data science into data relevance. ",,,
Australia,Australian Broadcasting Corporation,Big,Shortlist,Best news application,Australia Talks news application,10/06/19,"Cross-border,Multiple-newsroom collaboration,Quiz/game,News application,Crowdsourcing,Infographics,Chart","Personalisation,D3.js"," With Australia Talks, the ABC built a sweeping portrait of Australians' attitudes and behaviours, and then created an immersive news application that allowed people to explore their own individual story through the lens of that data — helping them to better understand themselves, their neighbours and their country.   The user starts out by answering questions about topical issues (eg. climate change), and their own experiences (eg. sex, self-esteem).   Australia Talks then delivers a personalised story exploring how the individual's attitudes and experiences compare to other Australians — based on the results of an earlier, representative sample of 54,000 people. "," The <a href=""https://australiatalks.abc.net.au/"">Australia Talks news application</a> was completed by more than 450,000 people, and a subsequent impact study found that 30 per cent of Australians engaged with the project as a whole.    Individual impact    Australia Talks posed curly questions about hot-button topics (smacking children, climate change) and also dug into very personal areas (sex, happiness). Many people found that the very act of answering prompted them to think about their own opinions and behaviours in new and surprising ways.   Then, their personalised Australia Talks results page provided them with both a greater understanding of themselves and their fellow Australians, and also often a sense of belonging.   One young audience member told us she cried when she completed the Australia Talks tool because she found it so affirming and ""felt less alone"" when she learned how many other young Australians were also struggling with mental health concerns.   Another user said that being asked how much she would be willing to spend personally to slow climate change had prompted her to start buying carbon offsets.   Another wrote: ""From naked selfie to religion, as an Indonesian migrant, this Australia Talks by ABC News really helps me to know where I fit in modern Australia.""    Starting conversations    By exploring the views and experiences of the nation, Australia Talks was able to spark important conversations — about <a href=""https://www.abc.net.au/news/2019-12-17/what-youd-spend-to-halt-climate-change-and-what-you-could-get/11784704"">climate change</a>, <a href=""https://www.abc.net.au/radionational/programs/lifematters/why-young-adults-are-most-prone-to-anxiety-and-loneliness/11708966"">mental health</a>, <a href=""https://www.abc.net.au/news/2019-10-22/annabel-crabb-national-identity-what-makes-an-australian/11623566"">national identity</a>, <a href=""https://www.abc.net.au/news/2019-10-18/annabel-crabb-australia-talks-women-worried-more-than-men/11562860"">gender</a>, <a href=""https://www.abc.net.au/news/2019-11-15/sex-dating-and-prejudice-why-we-are-a-nation-sharply-divided/11694038"">prejudice</a>, <a href=""https://www.abc.net.au/news/2019-11-28/australia-talks-annabel-crabb-political-correctness-analysis/11742380"">freedom of speech</a>, <a href=""https://www.abc.net.au/news/2019-11-06/annabel-crabb-australia-talks-religion-insights/11674076"">religion</a> and much more.   The conversations played out both between individual citizens and on the national stage. They were held on social media, but also in the real world through dozens of community engagement events including outside broadcasts and town hall meetings.   They were also extended by media outlets across Australia, including the Guardian, The Australian, Daily Mail, Australian Financial Review as well as some international coverage. "," Australia Talks was an epic undertaking, two years in the making, conducted by the ABC in conjunction with social scientists and data scientists at Vox Pop Labs.   We started by crowdsourcing areas of concern for Australians through open-text surveys and interviews with thousands of Australians from across the political and sociodemographic spectrum. With an academic advisory panel, we then designed more than 500 questions and statements that tested the attitudes and behaviours of Australians.   After 18 months of crowdsourcing and survey design, these 500 questions were then put to 54,000 Australians in July 2019. We spent a month in August analysing the data with help from data journalists at ABC and Vox Pop Labs’ data analysts.   We then developed the Australia Talks news application as the central way for people to understand this huge new dataset and situate themselves within the broader narrative of modern Australia.   This involved the selection of a subset of survey questions to include in the tool, and the development of an algorithm to generate the personalised results pages. And when we say personalised, we really mean personalised. There were more than 574 sextillion possible results pages — well and truly more than one possible result for every individual Australian!   One key consideration was for each of those pages to have a clear storytelling arc, weaving together personal experiences, as well as views and hopes around a broad range of themes such as prejudice, happiness, sex, cost of living, climate change and identity.   We also wanted Australia Talks to engage with the widest range of Australians as possible, so made it available in four languages (English, Simplified Chinese, Arabic and Vietnamese).   The Australia Talks application was used by more than 450,000 people, and results cards were shared 7,000 times on social media. "," The project's aim is to get the Australian community talking about things that matter to them — and we want to include as many people as possible in that conversation. As a result, we decided to launch this ambitious news application not only in English, but also the three most common languages among non-English speaking Australians (Simplified Chinese, Arabic and Vietnamese).   Launching such a complex interactive digital experience in multiple languages was an enormous undertaking — and a first for us at the ABC. As we closed in on launch, translation seemed to pose a new, surprising challenge with the dawn of each day.   The ABC puts an enormous amount of effort into making sure our stories are accurate, fair and interesting for the audience. So when we committed to launching Australia Talks in three languages other than English, we were determined that those other languages would get the same high-quality experience as we offer our English-language readers.   Working with translators when all the key developers and editorial staff were English-only speakers meant we had to undertake additional rounds of quality testing. When you can't read the text, importing multiple languages into the back-end of the application is a fraught process.   Each individual language had its own unique challenges as well. Arabic, for example, is a right-to-left language which throws up all kinds of unexpected challenges — from the design of the data visualisations and page layout to how the software we used to simply edit the content handled each language. ","  1. How partnering across disciplines can extend your journalism    The ABC has a long history of delivering important journalism. But journalists are not trained in the latest methods of social science or public opinion research.   To develop such a sophisticated project, it was necessary to marry the ABC's journalistic prowess with specialists in social science and data science. Australia Talks proves that the combination of journalism with social science can deliver powerful results.    2. How to make data feel personal, and engaging    Taking a huge dataset — with literally millions of individual data points — and making it accessible and understandable for the audience is always a huge challenge. The power of the Australia Talks approach is that it puts the focus of the app experience on the individual user, and uses their own personal thoughts and feelings as the way to guide them through the dataset.   This is an extremely powerful framing, as it allows the user to be at the centre of the storytelling. While that could risk becoming narcissistic, the arc of the Australia Talks story uses the individual as simply the starting point, helping the user to look outward and expanding their understanding of others in the community. ",https://australiatalks.abc.net.au/,https://australiatalks.abc.net.au/results/2ba047c5-e507-42cb-add5-71a723645701 (indicative results page),https://www.abc.net.au/news/about/backstory/digital/2019-10-24/australia-talks-your-questions-answered/11608434 (How and why explainer),https://www.abc.net.au/news/2019-10-06/australia-talks-explained/11570332 (Launch explainer),,,,ABC News Story Lab and Vox Pop Labs," Australian Broadcasting Corporation is Australia's national public broadcaster. The ABC provides Australian stories and conversations across radio, television, online and mobile services throughout metropolitan and regional Australia and overseas through ABC Australia and Radio Australia. The ABC provides informative, entertaining and educational services that reflect the breadth of our nation.   Vox Pop Labs is a <a href=""https://bcorporation.net/about-b-corps"">B Corp</a> operated by academics and based at McMaster University in Canada. Vox Pop Labs specialises in the application of digital technology and data science to foster democratic participation and civic engagement.    At the ABC, the Australia Talks project was led by Matthew Liddy (editor, ABC News Story Lab), Julie Hanna (managing editor, Factual), Nick Hayden (managing editor, Entertainment) and Natasha Banks (projects lead, Content Ideas Lab). The Vox Pop Labs team was led by founder and CEO Clifton van der Linden. They worked with teams across the ABC, academic partners at the University of Melbourne as well as a multi-institutional panel of academic advisors to deliver Australia Talks. ",,,
United States,"The Marshall Project, published in collaboration with The New York Times's Upshot.",Big,Shortlist,Best data-driven reporting (small and large newsrooms),Is There a Connection Between Undocumented Immigrants and Crime?,13/05/19,"Investigation,Explainer,Infographics,Immigration,Crime","D3.js,R"," After The Marshall Project and the New York Times’s The Upshot published an investigation debunking the often-repeated idea that immigrants increase crime in the U.S., many readers asked: What about undocumented immigrants?        We knew we wanted to try to answer this question. The problem was that very little data exists about undocumented immigrants. So when the Pew Research Center released new undocumented population estimates across the country, for the first time it was possible to compare population changes to changes in crime in the last decade, and show that undocumented immigrants, too, do not increase crime. "," Advocates for immigration reform have used this work to rebut misleading narratives on immigration at White House news conferences where ICE and other law enforcement officials were speaking. The Washington Post published an opinion piece from their editorial board about our findings. In January, the report was used to motivate a bill passed by the Washington senate to curb discrimination against undocumented immigrants.   This report was shared and viewed widely, with hundreds of thousands of views on the Marshall Project website and our partner New York Times page. It was posted to social media by the thousands, including by many respected journalists, politicians, organizations and leaders such as Peter Baker, Glenn Thrush, Nicholas Kristof, Sam Vinograd, Sahil Kapur, Rep. Nydia Velazquez, the ACLU, the U.S. House Committee on Homeland Security, the Urban Institute, the Sentencing Project, Prison Legal News, and others. Flagg was interviewed on CNN on Michael Smerconish’s morning show, and on the radio on SiriusXM and NPR.   Dozens of other news organizations picked up or otherwise covered the analysis, including NPR, Politico, The Washington Post, The Atlantic, Rolling Stone, The Boston Globe, The Trace, the Southern Poverty Law Center, Documented NY, AZ Central, Daily Kos, Yahoo News, Splinter News, ThinkProgress, The Huffington Post, New York Magazine, Mother Jones, AM New York, and a range of Spanish and international outlets such as Univision, Diario De Noticias, Proceso Digital, Al Dia, The Brazilian Times, La Prensa, Radio Bilingüe, Gestión, El Sol de Mexico, El Diario NY and others.    Some localized the data we provided on request to produce stories focused on their own areas, including Patch.com’s series of local articles covering Chicago, Philadelphia, Boston, Charlotte, Providence, Nashua and Lubbock. "," To investigate the potential relationship between undocumented immigrants and crime, The Marshall Project downloaded and merged all types of violent and property crime data published by the FBI Uniform Crime Reporting program for the same areas and time period covered by the Pew undocumented population estimates that had just been released. Historical changes in the legal definition of rape and inconsistencies in how motor vehicle theft is recorded in different areas meant both of these types of crime had to be removed from analysis, so we used raw numbers of reported crimes and populations to produce amended rates for the rest of the categories.    After calculating 3-year averages and changes in crime rates, The Marshall Project fit regressions to model the relationship between changes in an area’s undocumented population and changes in violent crime, property crime, and their components of aggravated assault, robbery, murder, burglary and larceny. None of the models found evidence of a connection. This analysis was done in R.   We demonstrated our analysis visually, allowing viewers to see and understand the data directly for themselves. We used Illustrator and D3.js for this design and web development work. "," Our main challenge in terms of data analysis for this project was the scarcity of available data about undocumented populations. Thanks to the work Pew does, we were able to get estimates of these populations. But the estimates Pew publishes as part of their standard work are generally raw numbers of undocumented immigrants, and our analysis required percent change over time – nontrivial to derive due to the error margins in the estimate formulas. So Pew researchers generously worked with us to get the estimates in the form we needed for a robust analysis.   The other time-consuming part of this analysis was tracking crime rates by metropolitan area, the geographies of which change over time. Sometimes a metro would grow to encapsulate new area, sometimes it would divide into multiple smaller regions. Consulting technical documentation for the roughly 180 areas in our study by hand, we determined for each area when a changed geography could still be an appropriate match to the original, when smaller areas would need to be combined for a proper match, or when no accurate match was possible.   In editorial terms, our biggest challenge was to cut through the vast amount of misinformation and flawed data reporting on the subject of immigrants, and the fear that comes with such misinformation. Inspired by the specific needs our readers expressed, we did everything we could think of to encourage their trust by making our analysis process transparent and understandable, including presenting the information visually in a way that people could browse the data and make up their own minds. "," One thing we learned from the process of reporting this story was the value of listening to the questions and needs of our readers. By paying attention to readers’ voices, we were able to identify an opportunity to provide clarity on a question that was important to them, and we are so grateful to readers for giving us that opportunity. ",https://www.themarshallproject.org/2019/05/13/is-there-a-connection-between-undocumented-immigrants-and-crime,,,,,,,Anna Flagg," Anna Flagg is The Marshall Project's senior data reporter, covering criminal justice topics including immigration, crime, race, policing and incarceration. Her work has been recognized by the Global Editors Network’s Data Journalism Awards, the Society of News Design, and the Information is Beautiful Awards, and she was a finalist for a 2019 Deadline Club Award. ",,,
Brazil,O Estado de S. Paulo,Big,Participant,Best data-driven reporting (small and large newsrooms),These charts show the topics addressed in presidential inauguration speeches since Brazil's redemocratization,01/02/19,"Explainer,Chart,Politics","Scraping,Json,Google Sheets,CSV,Python"," This visualization compares the inauguration speeches of all Brazilian presidents since the return of the direct election system. It uses a method developed by a linguist and political communications researcher to classify each paragraph of the speeches into categories such as propositions"", ""beliefs"" and ""warnings"". By putting the texts and their classification side by side, the story tries to guide the reader through the different rhetorical styles of the political leaders of the country. "," The report had repercussions in several sectors of society, especially among digital influencers, political scientists and politicians of different ideologies. The report allowed the reader/user to find other angles, besides the one selected in the published content. The left, for example, focused on former President Lula and his legacy. The center-right focused on the period of Fernando Henrique Cardoso "," We used the research of a linguistics and political communication specialist to put in context how the Brazilian presidents chose to present themselves to the nation. The visualization allows the reader to easily compare both the length and the overall tone of each speech. The interactive graphic also allows the user to read each extract of the pronunciation, adding an exploratory dimension to the experience.    The piece was based on the work of João Bosco Bezerra Bonfim a Ph.D. in linguistics from the UnB (National University of Brasilia). He wrote the book ""Word of President"", published in 2004 and available in the digital library of the Federal Senate.    His work breaks down all the inauguration speeches in the history of the Republic, from Deodoro da Fonseca (1889) to Lula's (2003) first term: the author evaluated the pronouncements paragraph by paragraph and classifies each passage in the six categories previously mentioned. Based on the methodology developed by Bonfim, we classified the later speeches, from Lula's second term to Bolsonaro's, the most recent. "," The hardest part was to read and catalog all the speeches of the brazilian presidents since 1989. Because of the deadline, we wouldn't have time to try to create a script or similar, so, everything was done manually. We left the nine presidents - before 2019 - already ready. Bolsonaro - the tenth president since the ""new Republic"", would take office on January 1, 2019, so we had less than a day to transcribe, this was done in real time, and then categorize the newest president's speech de posso. On January 2nd, our report was already on the air. "," I believe that the greatest lesson we have had is to be able to extract data from not so well structured places and basically create our own basis to explore in different ways. Without a doubt, this is a challenge, even more with the rhythm of traditional journalism, but it is something unforgettable and enriching. ","https://www.estadao.com.br/infograficos/politica,graficos-mostram-temas-dos-discursos-de-posse-desde-a-redemocratizacao,955154",,,,,,,"Augusto Conconi, Rodrigo Menegat, Bruno Ponceano, Matheus Lara, Ariel Tonglet","  Rodrigo Menegat.    I tell stories with data. Ever heard that talk about chasing news and bringing to light issues that shouldn't be hidden? I do pretty much the same, but before knocking on doors, I write some code and build some spreadsheets. There are stories in every corner and there are also stories to be uncovered behind the numbers. I seek all of them.   I worked in a big newsroom, did freelance gigs and collaborated with an university-based photojournalism team. I graduated from my hometown State University of Ponta Grossa and I have a post-bach certificate on data-driven journalism from Columbia University, in New York City. Currently, I work with the infographics team at Estadão, the most traditional Brazilian newspaper.        Augusto Conconi.    I am 21 years old, have a degree in 'Multimedia Production' and 'Journalism', and I am now a post-graduate student in Digital Law. Currently I work as a visual journalist at O Estado de S. Paulo (Estadão) newspaper.  ",,,
United States,"NBC News, ICIJ, CBC, BBC, others",Big,Participant,Best visualization (small and large newsrooms),Secret Chinese documents reveal inner workings of Muslim detention camps: Beijing claims they're vocational centers. But a cache of leaked records show the sites were designed to be run like prisons.,24/11/19,"Investigation,Explainer,Long-form,Cross-border,Multiple-newsroom collaboration,Satellite images,Human rights","QGIS,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV"," The world has been learning through survivor testimony, government contracts, and satellite photos that the Chinese government has been holding hundreds of thousands of Muslim Uighurs in massive internment camps in the remote western province of Xinjiang.   But the International Consortium of Investigative Journalists acquired the plans documenting this system in the Chinese government’s own words: Secret documents that outlined how Muslim minorities in Western China were being targeted, and how that internment was designed to indoctrinate them.    NBC News was a key partner for this collaboration both in the reporting and visualization. "," NBC News produced a 4,000-word digital interactive with two accompanying video stories that aired on television — to 8 million viewers — and online, with over 700,000 viewers in the first 3 hours. Our story harnessed satellite imagery, classified Chinese government documents, and the harrowing accounts of two Uighur women who went through an ordeal most can’t imagine.   The impact of the China Cables was immediate, profound and is ongoing.   A week after publication, the House of Representatives overwhelmingly passed a bill calling on the Trump administration to sanction the Chinese officials responsible. Secretary of State Mike Pompeo called on Beijing to release the detainees. And government officials in the UK and Germany demanded that China allow international observers into Xinjiang.    Sens. Rick Scott and Josh Hawley wrote to NBC’s parent company, NBCUniversal, urging it to refuse to air the 2022 Beijing Olympics.   But the most important response may have come from China itself. Beijing denounced the China Cables as “fake news,” while Xinjiang officials began ordering subordinates to delete data, destroy documents, and tighten controls on information.   Two weeks after publication, the chairman of the Xinjiang regional government suggested China was moving to close the camps. He announced all detainees had now “graduated,” and the government would “normalize” the facilities. Whether this means the camps will close remains to be seen. But the announcement was evidence the China Cables leak is having a real effect at the highest levels, and might change lives. "," NBC analyzed satellite imagery for over 100 potential camps to build a map showing their prevalence across Xinjiang. The story highlights a specific camp that is representative of the system: a high-walled, heavily-guarded compound built where nothing existed just three years prior.    The tools used by NBC News included Google Earth Pro, ArcGIS and imagery from multiple sources, and were analyzed to reveal guard towers, high walls and even Potemkin basketball courts seemingly laid out for Western eyes, because for roughly two years they were courts with no hoops.    To analyze camp locations, NBC News used ArcGIS and Google Earth Pro. To display the camp locations, NBC News used QGIS.   Among the documents were an operations manual for the camps, personally approved by the region’s top security chief, and intelligence briefings, revealing the inner workings of China’s massive data-collection program. The leak contained a rare look at Xinjiang’s harsh justice system, with a court sentencing document that showed a Uighur man imprisoned for 10 years for urging his Muslim co-workers not to use profanity and to eat Halal.   The documents showed how camp personnel were instructed to prevent escapes, maintain total secrecy about the camps’ existence, and forcibly indoctrinate the detainees. They also laid bare the chilling nexus of technology and totalitarianism that collects vast amounts of personal information, and uses that data to select candidates for internment.   To display these documents and highlight key passages, NBC News used javascript and Adobe Illustrator with ai2html. "," The effort involved a painstaking process to verify the authenticity of the documents, while taking precautions to keep our very possession of the documents secret. At NBC News, we couldn’t involve our China-based colleagues for fear of endangering their safety. Other reporters in the collaboration asked not to be identified at all.    Our document verification effort drew on experts in Uighur language and Chinese government documents, and sources at two intelligence agencies. The documents were meticulously scrubbed to remove identifying marks that could be traced back to our sources.    In addition, NBC News analyzed satellite images of more than 100 probable internment camps,    To report the story, NBC News spoke to more than a dozen experts on everything from Chinese law to Uighur culture, employed six Uighur translators, and interviewed two survivors of the camps. These sources helped inform the backbone of a complicated and nuanced story in an invaluable way.   Reporting took place in Istanbul, Washington, D.C., New York City, and Virginia, and in 13 countries where our collaborators were based. We interviewed sources in Kazakhstan, Canada, and the UK. NBC’s production team consisted of more than 30 editors, cameramen, photographers, graphic designers, reporters and producers. "," The stakes for this story were high, and posed great danger to reporters at NBC News and other journalists that were part of this ICIJ collaboration. The verification effort was arduous, with great concern for not revealing the sources of the documents.   Security was a great concern for the documents. It may go without saying, but news organizations that obtain secret Chinese government documents should exercise great caution and plan carefully about publishing them.   Preparing the multimedia presentation took weeks of planning and coding. Teaming journalists with a variety of technical skills is crucial .News organizations will likely also find that projects like these require a dedication of time and people to make happen. ",https://www.nbcnews.com/news/all/secret-chinese-documents-reveal-inner-workings-muslim-detention-camps-n1089941,,,,,,,"NBC News, the International Consortium of Investigative Journalists", Kenzi Abou-Sabe is a reporter with the NBC News Investigative Unit. Andrew W. Lehren is a senior editor with the NBC News Investigative Unit. Didi Martinez is a researcher with the NBC News Investigative Unit. Kate Snow is a national correspondent for NBC News and the anchor of Nightly News on Sunday. ,,,
Singapore,Reuters,Big,Participant,Open data,The looming risk of tailings dams,19/12/19,"Database,Open data,Fact-checking,Infographics,Chart,Map,Satellite images,Environment,Business","Animation,3D modelling,Scraping,D3.js,QGIS,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,Node.js"," When a dam storing mine waste — called tailings — collapsed and killed over 240 people in Brazil, a spotlight was turned on the safety of such dams. The Church of England, an investor in mining companies through its pension funds, called for more data on them. Reuters gathered the firms’ public responses and visualized for the first time, tailings dams owned by 89 companies with mines in over 60 countries. "," At a time where there are currently no established global mining industry standards defining what a tailings dam is, how to build one and how to care for it after it is decommissioned, this project is a great example of service journalism as it provided a valuable first look at tailings dams on a global scale, increased awareness about these little-understood structures, and prompted further discussion among those working towards unifiying dams classification standards and preventing catastrophic dam collapses. This visualization was widely viewed and shared by industry experts, environmental groups, politicians, concerned parties and representatives of the Church of England's pension board as it illuminated the potential dangers of tailings dams and the company stakes in a clear and concise way.     "," The team made use of several data analysis tools to organize and geolocate the tailings dams. HTML, CSS, D3, Javascript, Adobe Illustrator and QGIS were used to produce the maps, graphics and the guided experience breaking down all 1,700 tailings dams. Lightwave was used to create 3D diagrams to explain the different ways a tailings dam can be constructed and why some are more dangerous than others. The team also analyzed a large amount of satellite imagery to identify distinct features in notable tailings dams, giving readers a bird's eye view of these little-understood structures. "," The hardest part of the project was organizing and factchecking the vast amounts of disclosures and data made available by the companies as they are often incomplete and/or inconsistent. The team spent months collecting and cleaning up the database to ensure that the data is accurate before we began analyzing and visualizing the information. After the data was organized, we had to figure out a way to structure the narrative so that the reader could easily digest the information about these little-understood structures and the potential danger they pose to communities around them. "," When visualizing large datasets, it is important to provide the reader with a guided experience so that key takeaways can be easily understood. During the course of the project, the team and others who have seen the project were constantly shocked by the scale of the structures. It was therefore crucial that we balance data with imagery and show these dams in the context of their environment. ",https://graphics.reuters.com/MINING-TAILINGS1/0100B4S72K1/index.html,,,,,,,"Moira Warburton, Sam Hart, Júlia Ledur, Ernest Scheyder, Ally J. Levine"," The Reuters graphics team publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Brazil,O Estado de S. Paulo,Big,Participant,Best visualization (small and large newsrooms),An AI-Guided Tour Of Museo Del Prado,19/08/19,"Long-form,Chart,Arts,Culture","AI/Machine learning,Scraping,D3.js,Canvas,Google Sheets,CSV,Python"," We used a neural network and a machine learning algorithm to measure visual similarity between 6,367 paintings at the Prado Museum, Spain's largest collection of visual arts. In other words, without such tech lingo, we offered a data-driven overlook of the museum that goes beyond the classic paintings by Goya and Caravaggio. We discovered unexpected curiosities, such as an impressive collection of still lifes or the work of Carlos de Haes, a master of realistic landscape painting who spent his life perfecting techniques to paint mountains, fields and forests faithfully. "," The article echoed Brazil abroad. Among the newspaper's readers, the article was well received and had low rejection rate. "," This material was produced using two artificial intelligence techniques. At first, a model capable of describing the content of an image was used to extract the features of each of the 6,367 works. In clearer language, this means that the computer extracted a mathematical description of the paintings.   These numerical values could be used by the program to, for example, detect that Carlos de Haes' La Canal de Mancorbo en los Picos de Europa shows a mountainous landscape. This last step, the prediction, was not realized in the report. Only numerical values were used - and that is where the second artificial intelligence technique comes in.   Using an algorithm called t-SNE, we compared the similarity between the numerical description of each work. Analyzing these values, the computer calculated the position that each image should occupy in a plane, so that images with similar characteristics would be close to each other. "," As in other reports made by the team of Data Viz do Estadão, there was no ready and pre-catalogue database. First, we created a scraper to download all the photos from the museum's website. At the same time, we asked the administration of Museo do Prado for permission to reproduce the works on the site of the newspaper. With the approval, we went on to create a Machine Learning application with TSN-e. After training and testing a few dozen times, it was time to process the whole base - and then produce the story. "," If I were to summarize, the greatest learning would be to think of solutions - and guidelines - out of the box, leaving aside limitations, whether technical or informational, for lack of datasets. In an unpretentious way, we created an explorer of works that guides the reader to one of the main museums in Europe ","https://www.estadao.com.br/infograficos/viagem,uma-inteligencia-artificial-vai-te-guiar-pelo-museu-do-prado-nossa-reporter-pela-cena-cultural-de-madri,1025741",https://github.com/estadao/museu-do-prado-tsne,https://arte.estadao.com.br/viagem/espanha/museu-do-prado/media/10000x10000.jpg,,,,,"Bruna Toni, Carlos Marin, Rodrigo Menegat, Bruno Ponceano","  Rodrigo Menegat.    I tell stories with data. Ever heard that talk about chasing news and bringing to light issues that shouldn't be hidden? I do pretty much the same, but before knocking on doors, I write some code and build some spreadsheets. There are stories in every corner and there are also stories to be uncovered behind the numbers. I seek all of them.   I worked in a big newsroom, did freelance gigs and collaborated with an university-based photojournalism team. I graduated from my hometown State University of Ponta Grossa and I have a post-bach certificate on data-driven journalism from Columbia University, in New York City. Currently, I work with the infographics team at Estadão, the most traditional Brazilian newspaper. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),EXCLUSIVE: Russian disinformation system influences PH social media,22/01/19,"Investigation,Database,Chart","Microsoft Excel,Google Sheets,CSV"," The story looked into links between the disinformation ecosystems in the Philippines and Russia. Among these is a supposed “geopolitical expert” named Adam Garrie. The Daily Sentry referred to Garrie in 41% out of its mentions of “experts” in its posts.​   Based on a diagram in a New Knowledge report, Garrie also contributes to websites with ties to GI Analytics, one of the Russian propaganda pages most promoted by Internet Research Agency ad accounts on Facebook. ", Some of the pages that were connected to The Daily Sentry were among the pages taken down by Facebook in early January 2019. , We used our own social media monitoring tool to determine the pages and websites connected to The Daily Sentry and related disinformation channels. We then visualized these networks using Flourish. ," The hardest part was collecting data on each of The Daily Sentry’s numerous posts and identifying which ones cited so-called “experts.” To speed up this process, we used a script to scrape the website for headlines and metadata. We then sorted the data according to keywords and manually looked at each article to check who the “expert” cited was. ", Disinformation networks in one country may be connected or at least helped along by foreign disinformation networks. ,https://www.rappler.com/newsbreak/investigative/221470-russian-disinformation-system-influences-philippine-social-media,,,,,,,"Gemma Mendoza, Vernise Tantuco, Michael Bueza, Glenda Marie Castro"," Members of the Rappler Research Team who worked on this project are team lead Gemma Mendoza and researchers Vernise Tantuco, Michael Bueza, and Glenda Marie Castro. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),Government offensive: Info operations attack media to manage SEA Games PR crisis,12/02/19,"Investigation,Long-form,Fact-checking,Chart,Map,Politics,Environment,Corruption","AI/Machine learning,Scraping,R,Python"," A week-long case study captured by Rappler that shows how information operations turned a potential disaster for the Philippine government's handling of the Southeast Asian Games (SEA Games) 2019 into a win, successfully manipulating public opinion and isolating “mainstream media,” attempting to provide cover for potentially corrupt or blatantly incompetent acts of government officials that resulted in logistical blunders, insufficient food for delegates, and workers racing to finish the construction of venues. "," The project underscores a playbook of the kind of systematic and sustained attack traditional journalists and news groups in the Philippines have been subjected to in the past 3 years. Enabled by social media platforms, it leaves journalists and news groups vulnerable to repeated attacks that incite anger and hate.   The story’s quick turnaround (published while the information operation was still ongoing), allowed for the public to be discerning of information they see online, and ultimately help stop the spread of disinformation about the games. The report also identified actors involved in the disinformation and government propaganda, and defend journalists and their coverage of the event. "," Producing the story within only days after abnormal online activity was detected was only possible with the help of proprietary tools made by Rappler, third party tools acquired specifically to monitor online disinformation in the Philippines, and the author’s mastery of social network analysis.   To monitor disinformation online, Rappler built a database and social media monitoring tool called the ‘Shark Tank,’ which records troll-like and propagandistic behavior online. The author used data from the Shark Tank and supplemented it with data on conversations about the games, scraped by third party social media monitoring tools. The author also used Natural Language Processing (NLP) to process thousands of social media posts as well as news articles published about the event.​   Using data from mixed sources allowed the author to map out the conversations online, identify the “content creators” driving the conversations, as well as the thousands of groups and pages that amplify government propaganda. "," Being able to publish the story as quickly as possible was the most challenging, yet most important part of this project.   The process of gathering, processing, and analyzing thousands of posts and news articles usually takes weeks to do, but the author made great strides for the story to be published while the topic was still relevant, in order to mitigate, if not reverse, the impact of the disinformation campaign about the games. Despite the quick turnaround, the story not only involved a detailed analysis of social networks, but used Natural Language Processing to process and visualize the different narratives being spread online. This was only possible with the existence of tools specifically acquired and builty by Rappler beforehand to monitor disinformation, as well as the author’s familiarity with data processing and analysis.   It’s also worth noting that after the publication of the story, the propaganda network identified in the report directed its attacks towards the author on social media. Just hours after publication, propagandist-bloggers doxxed and defamed the author, while their followers, in usual fashion, were sending threats and insults en masse. "," The project shows that there is merit in newsrooms investing in resources that would allow its journalists to monitor disinformation and produce quality data stories accurately in the shortest amount of time. Data stories on disinformation, which are difficult to produce, are usually published after weeks or months, when they’ve likely already lost their relevance and potential to stop the spread of false information. Data stories, if published immediately, can be a potent tool in the fight against disinformation online. ",https://www.rappler.com/newsbreak/investigative/246186-government-offensive-info-operations-attack-media-manage-pr-crisis-sea-games-2019,,,,,,,Don Kevin Hapal," Don Kevin Hapal works as a journalist for Rappler where he writes about disinformation on social media, technology, and overseas Filipino workers. He also heads Rappler’s Data and Innovations Team, where he helps the newsroom navigate its way through various challenges and turn crisis into opportunities using data and analytics. As a data journalist, Kevin has produced several investigative pieces and documentaries on disinformation and attacks against the press, some of which has led to hundreds of pages and groups involved in disinformation being banned by Facebook. ",,,
United States,The Associated Press,Big,Participant,Best data-driven reporting (small and large newsrooms),The Reckoning: Hundreds of Accused Priests Living Under the Radar with No Oversight,10/04/19,"Investigation,Long-form,Database,Corruption,Crime","Scraping,Google Sheets,CSV,R,RStudio"," The AP provided the first definitive picture of where priests credibly accused of sexual abuse are living since leaving the church, and revealed that many have access to children. Dozens have committed crimes, including sexual assault.   The hurdles to this project were high: Many of the credibly accused lists released by dioceses and religious orders lacked even basic information.    But after months of systematic, dogged work, the AP found almost 1,700 priests and other clergy members living with little to no oversight, with some teaching middle-school math, counseling troubled adults, working as priests overseas, volunteering at nonprofits and fostering children.      "," The impact of the project has been far-reaching and is still being felt. So far, more than 20 states have stepped in to double-check licensing lists or to fully change the way they do licensing background checks on educators, counselors and foster parents. A number of former priests have had their licenses revoked -- in Pennsylvania, 3 former priests had their teaching licenses pulled, and a dozen more are under investigation. A half-dozen child and family services departments responsible for licensing and screening foster parents -- including those in New Hampshire, Oklahoma and New Mexico – told the AP that they have checked the local diocesan lists to see if any former priests were approved foster care providers.   A number of dioceses added priests to their list after the AP found than more than 900 accused priests' names had been omitted from the credibly accused listings. And in New Orleans, the Saints NFL team has been accused of helping the city's archdiocese craft its list. Several states Attorneys General have said they have started to consider the role they could play in eliminating the gray area for people who have been credibly accused but not prosecuted.   Oregon’s attorney general, Ellen Rosenblum, has said her office is looking at appropriate next steps, particularly since one priest highlighted by the AP -- Roger Sinclair, convicted of abusing a vulnerable adult after he left the church -- lived in Oregon.   “This AP story … sheds light on an important aspect of clergy sexual abuse horrors—how easy it has been to continue the abuse in other states from where the initial offending conduct occurred,” Rosenblum said. “That is absolutely unacceptable, and the fact that the Sinclair case occurred right here in Oregon drives home the importance of addressing this issue.” "," This project relied on GoogleSheets for data collection, Google Forms to manage data entry from roughly a dozen reporters, researchers and freelancers, and then R and R studio to perform the data analysis. For the initial creation of the credibly accused lists, the AP scraped diocesan sites using Ruby and Python, or manually entered data collected from more than 170 diocese and religious order sites. To background the living people on the lists, the AP used Lexis/Nexis and a plethora of websites: from bishopaccountability.org to city assessor databases, PACER, state and city court databases, state licensing board databases, public pension databases, state and federal sex offender registry pages, social media platforms, church websites, individual employer websites, comment boards and blogs.   The AP hand-checked its master list of people named by dioceses and religious orders against lists in lawsuits, grand jury reports and on bishopaccountability.com to find that 900 priests who had worked in dioceses that had released lists of credibly accused clergy were omitted from those public statements, despite having accusations against them.   The AP made FOIA requests to dozens of law enforcement agencies seeking additional documents, and in one case got one priest's court documents unsealed.  "," Compiling the original list of credibly accused priests involved going to roughly 170 different dioceses and religious orders and hand-compiling vastly different data points that we then had to reconcile into one coherent document. This alone took months. Some dioceses listed only priests; others listed lay employees. Some provided details of allegations in big blocks of text; others, lists of parish assignments; others no information besides a name. Since many priests moved around the country, multiple dioceses listed the same priest -- the AP carefully de-duplicated this data and made sure to only have each clergy member (many have similar or the same names) once.   The AP then filtered out the 2,000 living priests for the backbone of a second database, aimed at answering questions about where each priest had lived and worked since leaving active ministry, and searching for any contact they may have had with children. We sent reporters to priests' houses and used public databases to track down employers, homes and social media accounts. This allowed the AP to report on trends with these largely unmonitored former priests: how many were charged for conduct while they were priests, how many committed crimes after, how many were still priests in the Catholic faith or others, how many had been licensed as teachers or counselors, how many had moved overseas and how close do they live on average to schools, playgrounds or places where children congregate.   A third database, which was also hand-built, compared the initial priests list with lists on bishopaccountability.org, in several grand jury reports and from several internal church documents to compare completeness and whether dioceses and orders had adequately reported all cases of abuse. The AP found roughly 900 priests that had been credibly accused but hadn’t been listed by dioceses. "," Our takeaway: A big idea is doable if you are organized, persistent and maintain a laser-focus on the big-picture question you're trying to answer and why that question matters to the communities you cover.   We knew we wanted to find out what had happened to all these credibly accused priests -- many of whom remained in the priesthood, but others who had left to be private citizens without a criminal conviction or notice to the communities they lived in. We also knew a sampling or an anecdotal look at a few named priests wouldn't do justice to the possible scope of the story, and the impact this issue was making in communities across the U.S.   But that meant backgrounding roughly 2,000 people -- many of whom didn't want to be found. It took months, a system of Google sheets and Forms, and a team of roughly a dozen reporters, researchers and freelancers to help dig into what accused priests had done in the time since leaving active ministry. We asked the same questions about each priest: things like their address, whether it was close to a school, if they held professional licenses, if they were on a sex offender registry or had a criminal record.   To do this justice, we were extremely organized, and everyone involved played a specific and important role. We held training sessions for backgrounding and kept lists of lists -- which dioceses had reported, which priests needed to still be backgrounded, which freelancers and researchers would be taking which people. We divided priests into types of cases -- those who had been laicized, for instance, and those we knew had left voluntarily -- to help organize our work. We relied heavily on public documents: bankruptcy cases, sex offender registries, professional licensing databases and assessors' property records.  ",https://apnews.com/6109dc3f9e744298ae3fd5fe607f0a3c,https://apnews.com/7d87d9c21e3d495ba68126bc4ceec01b,https://apnews.com/f6238fe6724bdf4f30a42ff7d11a327e,,,,,Claudia Lauer and Meghan Hoyer," Claudia Lauer is an enterprise and breaking news reporter for the Associated Press, specializing in criminal justice. She's based in Philadelphia. Lauer, who has a master's degree in journalism from University of California, Berkeley, previously worked at the Arkansas Democrat-Gazette.   Meghan Hoyer is data editor at The Associated Press, where she analyzes data and helps disseminate national data sets to reporters across the country, guiding them to find local stories in the numbers. She previously worked at USA TODAY and The Virginian-Pilot. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),Premeditated murder: The character assassination of Leila de Lima,12/06/19,"Investigation,Long-form,Fact-checking,Chart,Politics","Scraping,Microsoft Excel,Google Sheets"," Philippine Senator Leila De Lima is one of the most controversial opposition figures in the country. In just 3 years, her public image turned from dignified to dishonorable.   In this story, we traced her downfall – from being elected to being kept in jail for over 1,000 days – and how social media had a hand in all of it. We zoomed in on one Facebook channel that spread lies about the senator through a manipulated video, then looked at how its content were amplified by other disinformation channels, and vice versa. "," The project shows that politicking is now also done online. Rappler has published other studies that show how the current administration utilized social media in campaigning, but this is the first that showed how it can directly affect a person’s public image and personal life. "," After spotting Pinoy Republic’s manipulated video, we used our own social media monitoring tool to determine the pages and websites connected to Pinoy Republic. We then visualized the networks using the online tool Flourish. "," The hardest part was pointing out the main accounts and pages that spread false and harmful information about Leila De Lima.​   For about 3 years, Rappler has seen numerous dubious posts on social media attacking the senator. We’ve tracked the pages that shared them since 2016, but we couldn’t say for sure that there was a concerted effort to damage the senator’s image because the online accounts were scattered. Not until we spotted Pinoy Republic’s blatant act of deception (i.e. the manipulated video) did we start to piece things together. From there on, we had a focal point, and built our project around it successfully. "," That social media is powerful. Because it is accessible to almost everyone, the reach of information made available on the platforms is wide. The harm it can cause is not limited online – it can also ruin lives offline. ",https://www.rappler.com/newsbreak/investigative/246329-premeditated-murder-character-assassination-leila-de-lima,,,,,,,Pauline Macaraeg," Pauline works with Rappler’s research team. Her day-to-day grind includes fact checking suspicious claims, trying to make sense of data, and writing stories related to economy, environment, politics, and media democracy.   Pauline is a BA Journalism graduate from the University of the Philippines Diliman. She has been working in the field and has had experience both in print and online media since 2016. Prior to joining Rappler, she worked as a data journalist for magazines Forbes Philippines and Entrepreneur Philippines. In October 2018, she received the 8th Statistical Media Award for Online from the Philippine Statistics Authority. ",,,
Philippines,Rappler,Small,Participant,Open data,"Halfway through: Duterte's midterm, in charts",21/07/19,"Explainer,Open data,Chart,Business,Health,Crime,Economy,Employment","Scraping,Adobe,Microsoft Excel,Google Sheets,CSV"," To mark President Rodrigo Duterte's third year in office – halfway into his administration – the Rappler Research Team compiled various data to assess his midterm performance and presented it through charts and graphs all in one story. It covered 8 major areas: economy, his administration's ""war on drugs,"" criminality, infrastructure, employment, poverty, his approval ratings, and the Philippines' standing in certain world rankings. The data also included those recorded during previous administrations, for a more comprehensive comparison. "," The story showed that the Duterte administration's record so far is a mixed bag of successes and controversies. Experts interviewed by Rappler for this story also provided context into each metric, beyond the numbers and graphs. ", The data for this story were processed using Microsoft Excel and Google Sheets. The graphs were then created using DataWrapper and Flourish. A few sets of graphs were created using Adobe. ," The hardest part is getting historical data from previous administrations. Many of the datasets are from the 2010s and the 2000s, readily available, and thus easier to process. But some data especially during the 1990s and 1980s would take a longer time to request and process for inclusion into the graphs for analysis. "," Through this story, readers can glean how to properly compare any administration's achievements. It should not be just during the current administration or just with the immediately preceding one. As much as possible, wider time periods should be included, so that the current administration can be compared against previous ones more broadly. ",https://www.rappler.com/newsbreak/in-depth/charts-numbers-midterm-duterte-administration,https://www.rappler.com/rich-media/233500-duterte-year-3-halfway-mark,,,,,,"Michael Bueza, Vernise Tantuco, Pauline Macaraeg, GM Castro, Yusof Marohombsar, Addie Pobre"," Members of the Rappler Research Team who worked on this project are Michael Bueza, Vernise Tantuco, Pauline Macaraeg, GM Castro, Yusof Marohombsar, and Addie Pobre. ​Team lead Gemma Mendoza guided the group in this project. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),MAP: Major political families in PH after the 2019 elections,30/08/19,"Map,Elections,Politics","D3.js,Microsoft Excel,Google Sheets"," After the 2019 midterm elections in the Philippines, Rappler mapped the major political families that are serving at the same time at national and local levels, starting with the senators, representatives, and governors, and their relatives. Based on our count, there are at least 163 such families spanning the entire country. "," This is an attempt to map these political families, while also depicting their locations, sizes, and positions held. Political families are almost a staple of Philippine politics, and this visualization aims to depict the phenomenon.   Political families enjoy a great deal of influence in their respective localities. With them in power, checks and balances in governance tend to decrease, according to an election lawyer. To stem the tide of this culture, political pundits recommend the strengthening of political parties and reforms in the electoral system, among others. "," After collecting the data using Microsoft Excel, it was converted to CSV and plugged into D3.js. The base map for the Philippines was obtained from the PhilGIS website. "," The hardest part is manually putting in the coordinates of the dots on the map. Since one of the authors was a beginner in D3.js, he had to adjust some dots around manually, so that the collision algorithm still reflects a precise location of the group of dots (representing a political family) as much as possible. "," By seeing the map, readers would have an idea of the extent of political dynasties not just in their area but in other provinces as well. For fellow journalists, the conversation on political families should continue, as long as there is no enabling law yet to ban their proliferation, as mandated in the 1987 Constitution. ",https://www.rappler.com/newsbreak/in-depth/238673-map-major-political-families-philippines-after-elections-2019,,,,,,,"Michael Bueza, Glenda Marie Castro"," Michael Bueza is a researcher and data curator under Rappler's Research Team. He is an IT graduate who joined Rappler in 2013 after working for a top IT company. He usually works on data about elections, governance, and the budget. ​   Glenda Marie Castro is a graduate of Bachelor of Arts in Communication Research from the Polytechnic University of the Philippines-Manila in 2016. As a communication researcher engrossed with typical qualitative and quantitative studies, she shifted her focus and took great interest in the field of media research. Pursuing this newfound passion, she became a part of Rappler's faith, transportation, and media democracy and disinformation clusters. ",,,
Singapore,Reuters,Big,Participant,Innovation (small and large newsrooms),The last strunghold,09/04/19,"Investigation,Explainer,Map,Gun violence,Human rights",QGIS," After eight years of war in Syria and hundreds of thousands of deaths, the northwest corner is the only rebel stronghold that remains. President Bashar al Assad’s government and his allies have bombed front lines, along with markets, bakeries, schools and hospitals, the United Nations says. Hundreds of thousands have fled toward the border with Turkey and are now between the violent front line and a concrete border wall. This project relies on data to show how fires have been used to siege millions trapped in this corner of Syria.     "," The project had an important impact in our audience despite being published in August, a month traditionally low for news. It drove also a lot of attention on Syria in a moment in which there was little space in media for the Syrian civil war. "," The project waives a different set of media assets into a fluid scrolly narrative. Map based data visualization is combined with satellite imagery to make clear the main point the team wanted to explain: a siege is occurring and nearly three million Syrians have nowhere to flee. Photo and video assets drive attention to the harsh realities that strikes, and burns are having on civilians. QGis was used to plot the maps and Adobe Illustrator to edit those, then ai2html was used to generate the different art boards for the site. A front-end combination of html, js and css helped to waive everything into a fluid immersive narrative. "," Data storytelling was the core piece of the puzzle necessary to understand and drive the whole project, visually and narratively, while helping to contextualize the sense of urgency. "," Data can be used to tell human stories, not only by counting, measuring or locating assets on a grid, but it can also be a powerful tool to inform and communicate what peoples’ life look like. ",https://graphics.reuters.com/SYRIA-SECURITY-NORTHWEST/0100B251105/index.html,,,,,,,"Samuel Granados, Sarah Slobin, Matthew Weber, Ellen Francis"," The Reuters graphics desk publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),Midterm exodus: When political butterflies switch party alliances,05/11/19,"Chart,Elections,Politics",Microsoft Excel," The project explored the trend of ""political turncoatism"" or switching between political parties by Filipino politicians across 4 elections or since 2010. This politicians' maneuvers have been greatly observed in past elections, and this story is an attempt to visualize their movements. "," It showed in full view the patterns in party-switching in Philippine politics. During midterms, many of these politicians switch to the party of the incumbent President, to curry favor with the administration. However, they mostly remain in their current parties during presidential elections.​   Many camps have called for reforms in electoral politics to avoid such behavior. The commission that President Duterte formed to recommend revisions to the 1987 Constitution also came up with measures to prevent party-switching in its proposed federal constitution. "," The political data used were retrieved from their certificates of candidacy filed with the Commission on Elections. The data were collected in Microsoft Excel, and was then plugged in to Flourish to create Sankey diagrams, to illustrate the movements from one party to another between two elections. "," We've already processed the data for the 2010, 2013, and 2016 elections, so the hard part is processing the data for the 2019 elections, then summarizing everything to create the Sankey diagrams. This tracking and visualization of politicians' party affiliation across the years has arguably never been done yet before.​   Sadly, at the time of the story's publication, the authors does not have the capability yet to display movements across parties by individual politicians. Only the collective movements from one party to another were displayed. "," By visualizing the politicians' party-switching behavior, voters could be better informed about candidates' real intentions during elections. By giving this information to voters, they could hopefully be empowered to vote wisely in future polls. ",https://www.rappler.com/newsbreak/in-depth/229928-when-political-butterflies-switch-party-alliance,https://www.rappler.com/newsbreak/in-depth/229943-party-switching-perversion-of-political-system,,,,,,Michael Bueza," Michael Bueza is a researcher and data curator under Rappler's Research Team. He is an IT graduate who joined Rappler in 2013 after working for a top IT company. He usually works on data about elections, governance, and the budget. ​ ",,,
Mexico,La Data,Small,Participant,Best news application,THE DYNAMICS OF ROBBERY - CELL PHONES,09/09/19,"Investigation,Explainer",D3.js," This project is about the crime incidence that occurs in Mexico City. This particular piece addresses the problem to tica of the stolen cell and is one of the five cap í titles comprising the project.       The objective was to identify geospatial and temporal patterns (places and schedules) where cell theft occurs most frequently. The set data were obtained from the official source of the government of the City of M é xico and includes data from 2015 to 2019, which gives us confidence about the predictive and preventive capabilities have our tool. "," The true impact of this project revolves around people and their safety. The data contain information that we occupy or n very accurate, is to n georeferenced street level and have the exact time that occurred was or each offense. This enabled us or generate a visualization tool it or n data where we could, for the first time, show the level of Criminality of each delegation, street or colony of Mexico City; as the exact times in which these spaces become more dangerous. We believe this information or it n is extremely valuable for people, because you make changes in your daily life that will increase your safety and therefore their quality of life. "," For an to analysis of the data we use Phyton. For creating the map we ended up taking MapBox, what allowed or raise the quality of the design of our  tool. Aspects of design  were made in Illustrator and XD. ","     the hardest part was the tool design, we strive to generate something quite managed to be highly usable by the average user through the city. Several discussions took place in the office before they arrived to branches to a newer version or No to grant delegation level trends or n and at the same time geolocalizaci or n specifies for each offense.       In the design ñ ar this tool, also é n we had to consider a design ñ or could be reused for the 4 chapters of following project: Kidnapping, Robbery houses Theft Auto or robbery in public "," We liketo thonk the lesson to learn is about the potential of DataViz and data journalism to impact so r to ask and abrupt in our daily lives, in this case all term effort can choose a route safer to get to work or home, but in a same way there are countless changes and decisions that people could í do based on data. ",http://www.ladata.mx/ladinamicadelrobo_celulares/,,,,,,,"Oliver Morales Agiss, Wilt Gomari, Erandi flores, Daniel Gomez, Mariana Lopez"," The Data is a multidisciplinary team that brings together physicists, mathematicians, designers, sociologists and journalists to think and discuss the best way to communicate long databases. Data is a team of passionate about quality, innovation and data ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),Networked propaganda series: How the Marcoses are using social media to reclaim Malacañang,20/11/19,"Investigation,Long-form,Fact-checking,Chart,Politics","Scraping,Microsoft Excel,Google Sheets,CSV"," This 3-part series showed disinformation, coordinated amplification, and use of an extensive network of social media accounts as part of the Marcos comeback playbook.   The first part explored a network of websites, Facebook pages and groups, YouTube channels, and online influencers that produce and amplify propaganda and targeted disinformation favoring the family of a late dictator. The last two parts show their strategy: creating Facebook pages to bolster the Marcoses' image, and creating content that hypes up “achievements” of the Martial Law regime, deny its abuses, and vilifies rivals. It also exposes the network's false narratives to promote the Marcoses. ", The series focused on what appears to be a systematic campaign and the use of networked propaganda and false narratives to burnish the brand of a family that previously plundered the country while in power for decades. It raised awareness among Filipino voters on who they’re voting into power and how they might have been influenced into making that decision. The series also alerted social media users to manipulative content that they could be unknowingly consuming. , We used our own social media monitoring tool to determine the pages and websites connected to The Daily Sentry and related disinformation channels. We then visualized these networks using Flourish. , The hardest part of this project was collecting data on all the social media accounts that supported the Marcoses and spread misinformation or disinformation. It was also challenging to trace posts as far back as 2014 and to do historical research on what these politicians said in the past decade regarding their return to power. ," People who may potentially abuse their power can use a social media campaign in order to flip political narratives in their favor. These campaigns could be done years ahead of an election, which makes them more insidious. ",https://www.rappler.com/newsbreak/investigative/245290-marcos-networked-propaganda-social-media,https://www.rappler.com/newsbreak/investigative/245402-networked-propaganda-marcoses-rewriting-history,https://www.rappler.com/newsbreak/investigative/245540-networked-propaganda-false-narratives-from-the-marcos-arsenal,,,,,Gemma Bagayaua-Mendoza," Gemma is a journalist and a geek. After finishing AB History at the University of the Philippines in Diliman, she played around with databases, crashed a few computers and learned to put them back together again along the way. She received her web publishing training at the Stockholm University in Sweden as well as in multimedia journalism at the SEACEM in Kuala Lumpur, Malaysia.   At home with gadgets and gizmos, the instinct to tinker as well as the determination to manipulate the forces of technology to serve the ends of journalism continues to move her today. Gemma facilitated the transition of Newsbreak magazine from print to the World Wide Web. As Editor-in-chief of ABS-CBNNews.com, she played a key role in developing the online and social media components of the highly interactive Harapan series of ANC and ABS-CBN towards the 2010 elections. At Rappler, she serves as the bridge between the editorial and the technical.   She co-authored Newsbreak's latest book, ""The Enemy Within,"" on military corruption and civilian neglect. Her stories on governance & corruption, the security sector, disasters, and other social issues have won recognition in the Jaime V. Ongpin Awards for Investigative Journalism, the UNICEF-Philippine Press Institute Awards for Child-friendly Journalists, and the Asian Development Bank Institute’s Developing Asia Journalism Awards.   Offline, Gemma prefers action-suspense and romantic comedies over art films, and will eat any part of a chicken, except for the legs. ",,,
Mexico,La Data,Small,Shortlist,Best visualization (small and large newsrooms),Homicides overcome crisis of 2011,15/05/19,"Investigation,Explainer",D3.js," This project sought to analyze the evolution or n 20 to ñ os one of the problems m to s serious facing M é xico: the killings. We found that the level of violence nationwide to overcome the crisis that occurred in 2011 with the war on drugs, we discovered that see the big picture at the national level was not enough as í we decided to visualize trends homicide of the 32 states integrating M é xico. "," The impact of this project really is in the visualization it or n data, because we communicate effectively, simple and exploratory manner the evolution or n a problem to tica as complex and important as violence in M é xico.       On the other hand it is ñto note that this project had an impact on different pa mediatico í countries, thanks to which was retaken by the Gijn in its weekly Top 10 Data Journalism ", For an to analysis of the data we use Phyton; for the design ñ or employ the paqueter í to adobe illustrator and xd; for programming it or n of the visualization it or n use data D3 ," the hardest part of this project was the size of the  data that we work, the data set that deal was made up of hundreds of thousands of records and comprend í to infomarmacion infomarmaci or n data of the 32 states of M é xico for 20 to ñ os. Thinking and design ñ ar how could í masters so much data display was a big challenge which we are very proud to have faced. "," We like to think that with this project we show that journalism data seeks and must seek to understand and communicate the problem to ticas of sistem way to tica. M é xico all d í as news of killings abound, but rarely the subject comes from a glance m to s broader, deeper. What others can learn from this project is incre í potential ble having data journalism. ",http://www.ladata.mx/homicidiosenmexico_en/,,,,,,,"-Oliver Morales Agiss, Wilt Gomari, Erandi flores, Daniel Gomez, Mariana Lopez"," The Data is a multidisciplinary team that brings together physicists, mathematicians, designers, sociologists and journalists to think and discuss the best way to communicate long databases. Data is a team of passionate about quality, innovation and data ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),"IN NUMBERS: The freed 1,914 heinous crime convicts",09/11/19,"Investigation,Chart,Corruption,Crime","Microsoft Excel,Google Sheets", The story comprises of data kept from the public by the Philippine government with regards to convicts who have been released after serving time for heinous crimes. These freed men have been ordered rearrested by President Rodrigo Duterte on a whim. ," The story unveils the Philippine government’s faulty data-keeping in its campaign to rearrest released convicts who have served time for heinous crimes, which means the President’s order stood on wrong records.   We found that  not all of the 1,914 heinous crime convicts were released because of the law cited by the President as the reason for their mistaken release. We also found that despite the President’s anger about releasing the heinous crimes convicts, most of the releases were done under his own administration.   After the story’s publication, the government admitted using faulty data in following the President’s arrest order and after failing to validate their data, they suspended their massive arrest campaign. "," We used Google Sheets to arrange the data, and then Datawrapper and Flourish to visualize. "," The hardest part was obtaining the data, which contained damning information on the basis for President Rodrigo Duterte’s arrest order. "," Data on a systematic misstep is needed to assail a systematic misstep. Without data, stories cannot stand against questionnable policy decisions of the government. ",https://www.rappler.com/newsbreak/iq/239847-numbers-freed-heinous-crime-convicts-gcta,,,,,,,Rambo Talabong and Michael Bueza," Rambo Talabong covers the security, crime, and the capital Manila for Rappler. Even before completing his communication degree at the Ateneo de Manila University in 2017, he researched for Rappler’s multi-awarded Impunity series, which uncovered police abuses in the Duterte anti-drug campaign’s enforcement in Metro Manila’s poorest communities. Outside reporting, he writes about his travels and the queer experience in the Philippines.   Michael Bueza is a researcher and data curator under Rappler's Research Team. He is an IT graduate who joined Rappler in 2013 after working for a top IT company. He usually works on data about elections, governance, and the budget. ​ ",,,
Mexico,La DAta,Small,Participant,Best data-driven reporting (small and large newsrooms),Concessioned Beauty,04/04/19,Investigation,Animation," This project was about mining concessions that are within to natural protected areas (ANPS) of M é xico, situation it or n which contradicts the laws of conservation or n native of M é xico. We crossed the pol í gonos of all mining concessions with all pol í gonos of the ANP, it s to discover that the __% of them have mining concessions "," This piece of data journalism reached great diffusion it or n in the ecosystem of news M é xico. The complexity involving the management of pol í gonos of mining concessions and convert PNAs í to the problem that only one data journalism could í address. Near the date on which this piece was published, I discussed or at the Mexican Congress par to meter protection or n of these ANPS, we hope that the revelations obtained in this investigation or n have served this and other discussions environment the conservation it or n ANPs M é Mexico. ", For an to analysis of data deal Phyton and wolfram mathematica with them generate the first versions of the GR to ficas that would subsequently worked in Illustrator to finally be programmed interactively. , For the most part dif í cil was making the overlap of poligons ," This project we like í to the potential of data from official sources as sources of information is learned or n. Often traditional media are waiting for the big whistle or revelation or n someone them to create a great research or n, however, they do not realize that there are already the information or n enough to make big hip or thesis and substantial revelations. Journalism must begin to see the databases are not as complex, you can pay adorn their investigations, but as the true raw material from which you can begin to understand a problem to tica and start an investigation or n. ",http://www.ladata.mx/BellezaConcesionada/BiodiversidadMinada/index.html,http://www.ladata.mx/BellezaConcesionada/BellezaConcesionada/index.html,,,,,,"-Oliver Morales Agiss, Wilt Gomari, Erandi flores, Daniel Gomez, Mariana Lopez"," La Data es un equipo multidisiplinario que reúne a físicos, matemáticos, diseñadores, sociólogos y periodistas para pensar y discutir sobre la mejor manera de comunicar largas bases de datos. La Data es un equipo de apasionados por la calidad, la innovación y los datos ",,,
Switzerland,"SRF, Swissinfo",Small,Shortlist,Best visualization (small and large newsrooms),What people in Switzerland worry about,06/03/19,"Explainer,Open data,Immigration,Health,Economy,Employment","D3.js,R,RStudio"," What are the five biggest problems in Switzerland? Every year, several thousand Swiss residents are asked this question in a nationwide poll. The annual ranking of Swiss residents’ top concerns is regarded as an important policy tool to find out what’s on the electorate’s mind. As national elections approached, SRF Data visualized the development of Swiss worries over 25 years and consulted experts and and dozens of other data sources to find out, what drives those worries, how do politicians react to these concerns and how the political reactions influence the worries of the people "," The article was published in 9 languages and picked up by several news formats, both radio and TV. It was widely shared in Social Media. "," We used R-Studio to gather different datasets and find pattern to investigate. Once we had a rough storyline, we sketched the story in Sketch and wrote a first draft of the text. We then implemented the front-end with D3,js, React, animated SVG. For smooth transitions we used Flubber. ", We put a lot of focus on making sure the design is as clear and easy to understand as possible. Plus: A solid and reproducable documentation. , How to investigate data about feelings (worries) from different angles. ,https://www.swissinfo.ch/eng/2019-elections_what-people-in-switzerland-worry-about/44997722,https://srfdata.github.io/2019-06-worries/,,,,,,"Felix Michel, Angelo Zehr, Julian Schmidli, Tania Boa", Felix Michel (34 years old) is a datajournalist at SRF Data.   Angelo Zehr (28 years old) is a datajournalist at SRF Data.   Julian Schmidli (34 years old) is project-lead and editor at SRF Data.   Tania Boa (34 years old) is a designer at Interactive Things. ,,,
United Kingdom,The Economist Newspaper,Big,Participant,Best data-driven reporting (small and large newsrooms),UK General Election 2019,10/01/19,"Explainer,Breaking news,News application,Infographics,Chart,Map,Elections,Politics","D3.js,Json,CSV,R,RStudio,Node.js","  The Economist’s data-driven coverage of the December UK general election. This included our own poll aggregator; detailed analysis of YouGov’s “MRP” model of the election to look at the effect a surge of support towards the Liberal Democrats might on the election outcome; an innovated series of dynamic election maps; and election night live coverage, including a live forecasting model that updated through the night; plus additional data-led stories.  ","  We set out to provide as much insight and analysis of the snap UK general election as possible. We wanted to publish data-driven, visually stimulating stories that worked well in print and online, and that got noticed among the torrent of election coverage. Internally, we began from first principals: what information do we have at hand, and what is unlikely to be found elsewhere. We wrote a piece on the accuracy of polls that nodded to innovation in pollsters methods (MRP methods). We commissioned five high-quality constituency-level telephone polls with Survation, to inform on local-level reporting. Finally, we created a national poll tracker; innovative dynamics maps of historic electoral behaviour, and an election-night forecast model. Our coverage was lauded both internally and externally.   ","  As with much of The Economist’s data team’s project we used a wide variety of tools. Our data journalists rely on R and Rstudio, furnished by a suite of different packages. Our visual journalists work with R and Adobe Illustrator, along with some proprietary visualisation tools. Our front-end visualisers mainly use D3.js. We use cutting-edge and robust statistical techniques to make the best use of data, such as our own MRP analysis for the “Graphic detail” page.   ","  While a UK election had been rumoured for sometime our expectation was that it was likely to happen in the New Year. For that reason the timing and resource allocation was not wonderful, and we had to scramble a little bit more than we would have liked. Aligning the interests of different mediums, the tensions between print and online, both in scope and timing of what is possible always brings challenges. Yet, for example, our print-edition “Graphic detail” which made use of our internal MRP modelling, along with existing pollster analysis, helped inform our editorial endorsement of the Liberal Democrats.   ","  The project combined data-driven reporting from different perspectives. The local constituency polls alongside the national poll-tracker provided interesting insights. For the poll tracker, we scraped and hand-input national polls in order to analyse voting intentions by gender, education and by region. The project shows how these different prisms of reporting can be combined with innovative data elements—and deliver informative coverage of a pivotal political event in Europe.   ",https://projects.economist.com/uk-elections/2019/general-election-results,https://www.economist.com/britain/2019/11/07/how-britains-pollsters-have-changed-their-methods,https://www.economist.com/britain/2019/11/07/the-conservatives-are-struggling-to-win-a-crucial-midlands-marginal,https://www.economist.com/graphic-detail/2019/12/07/voting-lib-dem-could-hurt-the-tories-as-much-as-labour,https://medium.economist.com/forecasting-britains-election-in-real-time-bfcb8d395fa2,https://www.economist.com/graphic-detail/2019/12/13/britain-votes-resoundingly-for-boris-johnson,,"James Fransham, Martín González, Evan Hensleigh, Matt McLean, G Elliott Morris, Dave McKelvey, Dan Rosenheck, Marie Segger, Alex Selby-Boothroyd",  James Fransham is a data journalist at the Economist     Martín González is a visual journalist at the Economist.     Evan Hensleigh is a visual journalist at the Economist.     Matt McLean  is a visual journalist at the Econimist.     G Elliott Morris is a data journalist at The Economist.      David McKelvey is data journalist at The Economist     Dan Rosenheck is the data editor at The Economist     Marie Segger is a data journalist at The Economist     Alex Selby-Boothroyd head of the data team at The Economist  ,,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),Duterte gov't allows 'drug war' deaths to go unsolved,14/01/19,"Investigation,Long-form,Infographics,Chart,Map,Crime,Human rights","Adobe,Microsoft Excel,Google Sheets"," The story presents for the first time the number of drug war-related killings that have been investigated by government prosecutors and are being tried in court. The numbers, which came from the Department of Justice, are dismal, and belie the government's claim that they are investigating each and every death from the war on drugs.    Interviews reveal a systematic gap in prosecuting the deaths, as both cops and prosecutors pass the buck, leaving thousands of cases go unsolved. The investigation reveals a breakdown in the institutional process that leaves the kin of dead with little to no chance of justice. "," The story underscores what human rights lawyers say is a massive breach of a mandate of prosecutors to always be on top of killings. As the government denies accountability in the killings, this investigation charges them with failure in the administration of justice.    It also touches on a core legal issue surrounding Rodrigo Duterte's war on drugs – is the Philippines able and willing to prosecute the killings with its own system, or does the International Criminal Court have jurisdiction? ", The data was sourced from the Philippine National Police and the Department of Justice. We mostly used Datawrapper and graphic tools such as Photoshop to visualize the data through maps and charts. ," The story made use of statistics that have not been reported, therefore getting the data from the source was the hardest part of this project. It took months, countless letters, and unending follow-ups from various government agencies and sources before they approved our data request. The data was sourced from the Department of Justice and were correlated and contextualized with each other and via interviews with different authorities. Confronted with the conclusion of our analysis, they quickly passed the buck. These are all happening at a time when the administration is seen to be trying to cover up the mess that comes with the violent anti-illegal drug campaign, so we really had to be careful with our requests and the numbers that we had. "," In dealing with a recalcitrant government, data requests will take time – despite the existence of a Freedom of Information (FOI) mandate. Lots of patience and perseverance are also needed in constantly reminding concerned government agencies about requests, including types of legal documents or executive orders that support the right to requested information. ​   Another lesson here is the importance of viewing controversies in different ways. The drug war of Duterte has been ongoing for almost 3 years, reminding us about the importance of the public not turning a blind eye. We decided to focus on what the government does to show accountability over the drug war killings and to check on whether local justice mechanisms are still working. The answers were discouraging. ",https://www.rappler.com/newsbreak/in-depth/220595-duterte-government-drug-war-deaths-unsolved,,,,,,,"Lian Buan, Rambo Talabong, and Jodesz Gavilan"," Lian Buan covers the justice and corruption beats for Rappler. As such, she monitors the Supreme Court, the Sandiganbayan, the Court of Appeals, the Court of Tax Appeals, trial courts, the Department of Justice and all its attached agencies, the Office of the Ombudsman, the Commission on Audit and the Integrated Bar of the Philippines.​   Rambo Talabong covers the security, crime, and the capital Manila for Rappler. Even before completing his communication degree at the Ateneo de Manila University in 2017, he researched for Rappler’s multi-awarded Impunity series, which uncovered police abuses in the Duterte anti-drug campaign’s enforcement in Metro Manila’s poorest communities. Outside reporting, he writes about his travels and the queer experience in the Philippines.   Jodesz Gavilan is a writer and researcher at Rappler and its investigative arm, Newsbreak. She covers human rights and also hosts the weekly podcast, Newsbreak: Beyond the Stories, which aims to make sense of people, events, and controversies dominating the news cycle. She obtained her journalism degree from the University of the Philippines Diliman and joined Rappler in 2014. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),MAP: Negros killings since July 2016,28/08/19,"Investigation,Map,Crime,Gun violence,Human rights","Microsoft Excel,Google Sheets"," The project is a map of killings that happened in Negros province in the Philippines from 2006 to 2019, seen as a killing field under President Rodrigo Duterte. The incidents involved mostly human rights defenders and local farmers, among others, at a time when a culture of impunity dominates the country.   The project aimed to see where the epicenters of killings are and to provide details of each victim, including how they were killed, their occupation, and affiliation. Including these key information enabled me to draw a big picture of the situation in Negros. ", The history of Negros Island has been marred by violence for so many years. The victims are reduced to statistics. The project wanted to put a face behind each number to humanize the unfortunate situation. , The data was sourced via manual searching and tabulation of news reports and visualized using Datawrapper’s map tools. ," The main challenge that I faced in doing my work is the collection of information. For this, I consulted with a local human rights organization which tracks killings and added other vital details, for each victim. To complement this information, I went through several news reports, using tools to extract key information such as date of killing and other identifiable information. ​   Since I saw how hard it was to retrieve these data, I decided to place all the information I had gathered into a searchable table within the published story so other groups can also access them. No other media outlet in the Philippines has done a mapping of the killings in Negros. "," Killings are happening all over the country in the Philippines, but focusing on hyperlocal situations is highly important to put in context the situation. Mapping projects like what I did is useful in showing the magnitude of killings in Negros, which may not be all related to Duterte’s drug war but his other policy – the so-called anti-insurgency campaign which tagged many human rights activists as communists and enemies of the State. ​   Readily available data with extensive information when it comes to crime are vital and if these are severely lacking, other ways must be found. While going to each police station to get spot reports of each incident of killing was key, it would also eat up most of my resources (I am the only person on my team). So I made use of online archives of community and mainstream media and the individual news reports of each incident.    At the end of my work, I decided to embed all information I gathered publicly in my storypage so other groups will have access to them for whatever purpose they see. ",https://www.rappler.com/newsbreak/iq/238672-map-negros-killings-since-july-2016,,,,,,,Jodesz Gavilan," Jodesz Gavilan is a writer and researcher for Rappler and its investigative arm, Newsbreak. She covers human rights and also hosts the weekly podcast, Newsbreak: Beyond the Stories, which aims to make sense of people, events, and controversies dominating the news cycle. She obtained her journalism degree from the University of the Philippines Diliman and joined Rappler in 2014. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),IN NUMBERS: Public Attorney's Office under Acosta,11/04/19,"Investigation,Explainer,Open data,Chart,Crime","Microsoft Excel,Google Sheets"," The Public Attorney's Office poured resources to its forensic unit to file cases linking deaths of dengue patients to the dengue vaccine Dengvaxia despite no scientific proof. PAO was criticized for eroding public trust in vaccination. The story analyzes PAO's annual reports to show the office's priorities. Data showed that even though PAO was understaffed for the volume of cases, forensics cases shot up, and in the same period, the number of their clients who had to plead to a lesser offense just to dispose of the case increased three-fold. ", President Rodrigo Duterte withdrew funds from the PAO's forensic unit. , The data was culled from PAO's website. Relevant information from the annual reports drowned in numbers were extracted by keeping focus on what needed to be established in the story – what has the PAO been doing from 2017 to the present when it became busy for its Dengvaxia work? What has it achieved? Data was visualized using DataWrapper. ," The hardest part was to sift through all the numbers on the PAO's annual reports. To PAO's credit, it reported all kinds of numbers it could, but it is easy to drown in them because each data set stands on its own. We had to make the comparisons and relate the datasets to each other in order for them to make sense and bring the story forward. "," While Open Data is good, the government has a tendency to just dump unprocessed data. For other agencies like PAO, it does the extra step to process them into reports but even then datasets can still be confusing. It is tempting to just lift datasets as they would still be an accurate report, but it is important for journalists to filter relevant datasets and see how these datasets compare to one another in a given context. ",https://www.rappler.com/newsbreak/iq/244087-public-attorney-office-in-numbers-persida-acosta,,,,,,,Lian Buan," Lian Buan covers the justice and corruption beats for Rappler. As such, she monitors the Supreme Court, the Sandiganbayan, the Court of Appeals, the Court of Tax Appeals, trial courts, the Department of Justice and all its attached agencies, the Office of the Ombudsman, the Commission on Audit and the Integrated Bar of the Philippines. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),Corruption Red Flags series,14/07/19,"Investigation,Open data,Corruption","Scraping,Microsoft Excel,Google Sheets"," The 2-part series analyzed audit reports of 323 National Government Agencies (NGAs) and found that half of them had doubtful accounts. The Philippines is emerging from the pork barrel scam – the worst corruption scandal since Martial Law – involving fake receipts, yet 5 agencies were still found with fake receipts. President Rodrigo Duterte's anti-corruption messaging often centered on his pet peeves – excessive travels, yet 32 agencies were flagged for excessive and unliquidated travels, including the broadcast office that tails the president and airs his events and speeches. "," Some agencies found with audit red flags have overhauled their structure, especially the Department of Tourism, which had to change leadership due to anomalous transactions. Officials of the Philippine Coast Guard are facing charges. "," Audit reports were culled from open sources. The author identified red flag keywords, based on knowledge from covering the Commission on Audit and with insight from experts. After identifying keywords, the author and a team of researchers used the Find Key, and later on, a special file processing tool, to locate the red flags in the audit reports. After months of putting together this database, the author created another spreadsheet to track the red flags of the agencies, which helped her narrow down patterns. "," The hardest part was finding the time to sift through 323 audit reports to find the keywords. As it was not a dedicated investigating team, and the author had to carry on with her daily reporting duties, finishing the database took 6 months, and putting together the report took 2 weeks. "," Audit reports are rich materials for tracking the finances – and compliance with rules – of government agencies. Currently, audit reports make the daily stories if the agency is already controversial, or during the beginning of the audit season, when only one report is released a day, therefore reporters are not inundated and can focus on the report. As weeks pass by in the audit season, reporters lose track of the releases and the news cycle has already moved on. The project shows that with a dedicated unit, corruption can be tracked using public data. ",https://www.rappler.com/newsbreak/in-depth/235353-fake-transactions-doubtful-accounts-government-agencies-spending-coa-reports-2017-part-one,https://www.rappler.com/newsbreak/in-depth/235356-excesses-goverment-agencies-spending-travel-ghosts-coa-reports-2017-part-2,,,,,,Lian Buan," Lian Buan covers the justice and corruption beats for Rappler. As such, she monitors the Supreme Court, the Sandiganbayan, the Court of Appeals, the Court of Tax Appeals, trial courts, the Department of Justice and all its attached agencies, the Office of the Ombudsman, the Commission on Audit and the Integrated Bar of the Philippines. ",,,
Philippines,Rappler,Small,Participant,Best data-driven reporting (small and large newsrooms),Measles cases now found in nearly every province in PH,27/02/19,"Map,Health","Microsoft Excel,Google Sheets,CSV"," The story focuses on the measles outbreak in the Philippines in 2018. Its goal was to both measure and visualize how widespread the disease was after 5 of 18 regions were declared to have an outbreak of the disease. It also aimed to illustrate how contagious the disease was to communicate the necessity of immunization, especially due to a big vaccine scare from the prior year. The findings showed measles practically swept across the entire country with only 2 of the 81 provinces spared. "," In the few weeks after we published this report, the extent of damage the Dengvaxia dengue vaccine scare had on trust in Philippine health institutions became clear. There was no clear picture of this after the Dengvaxia scare and expects feared the worst with a resurgence in outbreaks of vaccine preventable diseases like measles. After we published this report, health authorities saw a continued increase in mass vaccination campaigns to respond to the disease. ​ ", The story made use of mapping techniques using Datawrapper for visualization of maps. Data was plotted on CSV files on Microsoft Excel. , Obtaining the data and presenting it in a manner that would best illustrate the status of the measles outbreak in the Philippines was difficult. Data was difficult to come by and required weeks of collating and sourcing to obtain. Health authorities were also having a hard time communicating to the public after the effects of the Dengvaxia scare but we decided to do this to show the public the toll the measles outbreak was taking on public health. ," In the Philippines, government agencies are notorious for poor data management and collation. While complex and rich data sets may not always be available, sometimes basic data can be compelling enough to illustrate an important finding. On the ground, after seeing the extent of the measles outbreak and how it spread to nearly the entire Philippines, vaccination rates slowly saw an uptick. ",https://www.rappler.com/newsbreak/in-depth/224434-measles-cases-found-nearly-every-province-philippines,,,,,,,Sofia Tomacruz," Sofia Tomacruz is a multimedia reporter at Rappler covering foreign affairs, elections, and labor.   Before that, she used to cover health and education. She also writes stories on the treatment of women and children. ​ ",,,
Brazil,G1,Big,Participant,Best data-driven reporting (small and large newsrooms),Violence Monitor: two years later,22/09/19,"Investigation,Open data,Illustration,Infographics,Video,Map,Crime,Gun violence,Human rights","Json,Microsoft Excel,Google Sheets,CSV"," This project is an unprecedented partnership among G1, the Center for the Study of Violence at University of São Paulo and the Brazilian Forum of Public Security. It focus on discussing violence in the country. To make it possible, G1 staff reporters all over Brazil kept track of violent deaths through the course of one week. There were 1,195 deaths in this period. All these stories have been cleared and written by more than 230 journalists spread throughout Brazil. Two years later, the same team joined to figure out how was the investigation of each crime.   The text: https://docs.google.com/document/d/1geBolBLwqd-zeTOgLzOqEUBeRgoiiKwD998N6hbZnmg/edit?usp=sharing "," The Violence Monitor marks the first time that a collective effort organized by a media outlet managed to record in detail and in an organized way all violent deaths occurred in a specific and substantial period of time (one week). This is especially due to the high number of cases (1.195). More than data, the project shows stories.   A task force was set up in the SP office to manage and coordinate 55 different teams in all the states of Brazil. There were more than 230 journalists involved. A database was assembled from scratch, containing information such as the victims' name, age, race, and gender. Also, the day, time, weapon used, and the exact location of the crime, among others.   The project demanded careful work on people management and data debugging. However, more important than the computer techniques applied was the work by this team to handle and manage information. The map, unprecedented and exclusive, became a reference.   Two years later, the same team managed to show the homicides clarification rate in all states, something never done before. And the stories were all updated.   The work had repercussions in various fields. In the media it was replicated by TVs, newspapers and news websites; it reached politics, as it was mentioned in a session in the Senate and in sessions held by State Legislative Assemblies; the impact on academia came in the form of studies and papers. The project also pressured the government to act and received important national and international awards. "," The map is based on html, CSS and javascript. All the content was placed, during the project, on a Google spreadsheet and transferred, then, to Carto – the data was collected through an API. "," During these two years, the data were collected by more than 230 reporters from Brazil, who obtained the information on the spot, through coroner's offices, morgues, victims' families and other sources.   To show how the investigation of each crime in 2019 was, the journalists had to find out with the police, prosecutors and justice all the details. It took months.   The information was then passed on to a team in São Paulo formed by editors from all areas (education, science, economics, data ...) prepared to put them on a spreadsheet and organize them properly.   All standardization was made so that a map could be put together, with specific filters to each category. The data were then analyzed and validated by teams specialized in public security issues (formed by researchers from USP and members of the Brazilian Forum of Public Security).   After the verification, the data was used to build the map and to update all the texts.   It was a thorough work, never done before in that scale in the world.   Here is the transcription of the text: https://docs.google.com/document/d/1geBolBLwqd-zeTOgLzOqEUBeRgoiiKwD998N6hbZnmg/edit?usp=sharing "," Violence Monitor is a living project: texts are updated as new information is discovered. And audacious: the idea is to shed light on the serious security problem and help reduce the number of violent deaths in the country.   After two years, it is already possible to see the impact that the dissemination effort had on brazilian public opinion and on the rulers. The issue of violence became the subject of a national debate.   The partnership with the academy also gave the project subsidies to make it statically verifiable and allowed analysis of the material to point out ways to combat the death epidemic.   Partnerships like this can and should be done by other media outlets. Mobilization involving a large number of journalists can also be carried out.   Violence Monitor is an example of a long-term project that does not leave stories behind, a project that pressures and forces public policies to be created. ",https://g1.globo.com/monitor-da-violencia/noticia/2019/09/22/monitor-da-violencia-dois-anos-depois-quase-metade-dos-casos-de-morte-violenta-continua-em-aberto-na-policia.ghtml,http://especiais.g1.globo.com/monitor-da-violencia/2017/uma-semana-de-mortes-violentas-no-brasil/?_ga=2.8008754.1611374156.1578319943-597399670.1567433879,https://g1.globo.com/monitor-da-violencia/noticia/2019/09/22/a-engrenagem-que-fortalece-as-tiranias-armadas-do-trafico-e-das-milicias.ghtml,https://g1.globo.com/monitor-da-violencia/noticia/2019/09/22/um-cenario-de-indiferenca-e-apocalipse-etico.ghtml,https://g1.globo.com/monitor-da-violencia/noticia/monitor-da-violencia-metodologia.ghtml,https://g1.globo.com/monitor-da-violencia/noticia/2019/09/22/monitor-da-violencia-delegados-e-promotores-elencam-os-desafios-na-investigacao-de-homicidios.ghtml,https://g1.globo.com/fantastico/noticia/2019/09/22/fantastico-revela-resultado-de-estudo-inedito-sobre-mortes-violentas-no-brasil.ghtml,https://docs.google.com/document/d/1q6xFK-8ySE_glnZhsXq8qeaEfDPh00bcUPy5F0isKdA/edit?usp=sharing," Biography of the main coordinator:   Journalist graduated from PUC-SP, with post-graduation in Documentary Cinema from FGV, Thiago Reis today coordinates a data core, fact-checking and special projects at G1, Globo's news portal. Previously, he worked as a reporter, chief reporting officer and assistant editor at the newspaper Folha de S.Paulo. Thiago has won, among others, the Data Journalism Awards, the AMB Award, the MPT Award and the Andifes Award. He received an honorable mention in the Vladimir Herzog Prize and the Journalistic Excellence Award of the Inter-American Press Society. ",,,
Switzerland,"Swiss Radio & Television's SRF Data (publication in German), Swissinfo.ch (publication in English and other languages)",Small,Participant,Best data-driven reporting (small and large newsrooms),How climate change affects your hometown,30/11/19,"Explainer,Multiple-newsroom collaboration,Open data,Chart,Video,Environment","Personalisation,Json,Google Sheets,CSV,R,RStudio"," Due to its diversity of valleys, hills and high mountains, climate change manifests itself differently in the various regions of Switzerland. The Swiss government’s CH2018 Climate Scenarios for Switzerland illustrate how climate change could affect each region. In this highly personalized explanatory piece, users must pick a Swiss town or city to get a glimpse of its future. Based on whether the user is rather optimistic or pessimistic, the modelled climate effects are presented in terms of easily understandable concepts like ""frost days"", both with auto-generated texts and simple line charts. There's even a dark mode! "," As it wasn't an investigative piece, the actual impact on the public is hard to measure. However, in terms of visitors, the project was a huge success, with more than five times more visitors than on our average performing articles. Also, the amount of comments on the piece is almost a record high for our articles, so that publication certainly stirred a debate. "," Based on the municipality entered and the scenario selected, users are shown thousands of different ""robot texts"". Here's how it works: We build up the texts, which refer directly to the own community, with templates which are located in a Google Doc (since often several journalists worked on the texts simultaneously). Important here are the numerous declensions (especially in German), these are additionally defined in a spreadsheet. Then we use <a href=""http://archieml.org"">ArchieML</a> and <a href=""https://www.npmjs.com/package/translate.js"">translate.js</a> to convert the GDocs and GSheets into JSON, which is then interpreted by the ReactJS code in the frontend and ""declined"" correctly. Good side effect of GSheets: Easy translatability to other languages (as used by Swissinfo). Certain parts of the text are simply played out based on the region the community belongs to and the scenario. For example, the sub titles, the texts on the regions and certain risks/opportunities in the pessimistic scenario. We use GSheets for these too. To switch between the two scenarios and thus from bright to dark and back, we use the theming functionality of the <a href=""https://github.com/emotion-js/emotion"">Emotion-JS-Library</a>. Since our app is not delivered in an iFrame, we can also influence styles of srf.ch and swissinfo.ch with it. Data preprocessing: The entire climate data on the municipality level, including climatic indicators and region, stem from the Swiss <a href=""https://www.nccs.admin.ch/nccs/de/home/klimawandel-und-auswirkungen/schweizer-klimaszenarien/klimaszenarien-verstehen.html"" target=""_blank"">National Centre for Climate Services (NCCS)</a> and are in netCDF format. They are converted from the original netCDF data to JSON within R. These data are then dynamically loaded from the CDN as needed. This conversion process was released as open source on our <a href=""https://srfdata.github.io/2019-11-auswirkungen-klimawandel/"">open data portal</a>. "," The hardest part was certainly making sure that all the text building blocks nicely fit together and that all the possible combinations of data result in syntactically and semantically valid prose. Basically all of the building blocks are purely data-driven, so it was also crucial to make them ""sound"" natural. Our goal was to present the reader with a highly personalized but still enjoyable article – ideally so he or she would not even notice that the texts were fully auto generated. After all, we chose this high degree of personalization because we really think that the modelled impacts of climate change can only be fully grasped and understood on that micro-level. Also for this reason, we mainly used variables that people could actually relate to (summer days, snow days, etc., instead of mere temperature and humidity). "," 1. A collaboration suite like Google Drive with Sheets and Docs in combination with an automated deployment environment (in our case Webpack with React) allows to quickly build, adapt and translate personalized texts, especially if more than one journalist works on the project. Therefore, try to use as few hardcoded text blocks as possible in your code and source everything from an accessible, easy-to-use collaboration tool.   2. Personalization is king, especially when it comes to relatively abstract topics like climate scenarios. As soon as the data comes with geospatial attributes (in our case, a netCDF grid), the data can be aggregated / filtered on the municipal – and thus on a personal – level.   3. For this project, we worked closely together with the data providers, the Swiss <a href=""https://www.nccs.admin.ch/nccs/de/home/klimawandel-und-auswirkungen/schweizer-klimaszenarien/klimaszenarien-verstehen.html"" target=""_blank"">National Centre for Climate Services (NCCS), </a>as we didn't want to aggregate or process the data in a wrong way. We always told them what we wanted to do with the data and asked them whether this was possible. This exchange is imperative when dealing with modelled and more complex data sets from science, so we'd recommend that in general. ",https://www.srf.ch/news/schweiz/so-ist-die-schweiz-betroffen-diesen-effekt-hat-der-klimawandel-auf-ihren-wohnort,https://www.swissinfo.ch/eng/switzerland-future-climate-change/45411758,https://srfdata.github.io/2019-11-auswirkungen-klimawandel/,,,,,"Timo Grossenbacher, Aline Metzler, Felix Michel, Angelo Zehr (SRF), Alexandra Kohler (swissinfo)", Timo Grossenbacher (1987) is a data journalist at SRF Data.   Aline Metzler (1997) is an intern at SRF Data.   Felix Michel (1985) is a data journalist and frontend developer at SRF Data.   Angelo Zehr (1990) is a data journalist and frontend developer at SRF Data.   Alexandra Kohler (1987) is a data journalist at Swissinfo. ,,,
Switzerland,Swiss Radio & Television's SRF Data,Small,Participant,Best data-driven reporting (small and large newsrooms),Here's what we found in the Collection #1-5 password leaks,03/06/19,"Investigation,Database,Politics,Crime","R,RStudio"," This was an investigation deep into the heart of the <a href=""https://www.troyhunt.com/the-773-million-record-collection-1-data-reach/"">“Collection #1-5”</a> password leaks that appeared in the web in early 2019. I showed that more than 3 million Swiss e-mail addresses and – more disquietingly – over 20’000 email addresses of Swiss authorities and providers of critical infrastructure appear in the leak. Among them also the Swiss army, of which I've found over 500 e-mail addresses and passwords in the leak. For this project, I used a so-called ""big data technology"", namely Spark, to sift through <a href=""https://www.wired.com/story/collection-leak-usernames-passwords-billions/"">the humungous amount of data</a>. "," The investigation showed that Swiss authorities still seem to have a problem with their employees using their business e-mail for third-party services that got hacked in the last years (think of Yahoo, LinkedIn, Adobe.com et al.). Interestingly, some institutions seem to be more affected than others: For example the Swiss army, which is grossly overrepresented among all federal e-mail addresses that can be found in the leak. Various media outlets from Switzerland followed up on this fact. Apart from that, I can only guess that some of the authorities affected by the password leaks might reconsider their security practices. While some might have been alerted before my investigation, our publication might have put even more pressure on them to pay more attention to their password policies in the future.  "," Actually I wrote a whole making-of of the project, <a href=""https://timogrossenbacher.ch/2019/03/big-data-journalism-with-spark-and-r/"">available on my blog</a>. In summary, I came up with a quite complicated data processing pipeline consisting of several R scripts. These would use sparklyr, the R wrapper package for the Spark big data technology. I chose Spark because it makes it easier to process data that does not fit into memory, so it abstracts away some of the problems that would arise when dealing with the data in ""plain"" R. In summary, the process was three-fold: In the first step, I made the thousands of files in the leak searchable by sanitizing file names. I then parsed the contents of the files which would often be CSVs, i.e. I primarily parsed the e-mail addresses into user, domains, subdomains, etc. In the second step, I filtered the data to only contain Swiss e-mail addresses (.ch top-level domain). In the last step, I made the usual aggregations and analyses on the nicely processed data (i.e. how many addresses per domain and subdomain, average password length, etc.). "," Certainly the large data volume and its variety in terms of various file formats. While it was quite straightforward to find and download the data, the problem boiled down to preprocessing, filtering and searching through a very, very large data set. This challenged me to learn to use a new technology (Spark) in conjunction with R. After all, it would have been easier to contact a company specialized in cyber security and ask them whether they can search the leak for Swiss E-mail addresses. Yet I think that tackling such problems wholly inside newsrooms has one tremendous benefit: You completely control what is done with the data, and you know exactly what can be interpreted into it – and what not. At the same time, since we're dealing with sensitive data and methods that can be used for malicious purposes, the approach cannot be made fully transparent. Thus, instead of publishing the source code, a <a href=""https://timogrossenbacher.ch/2019/03/big-data-journalism-with-spark-and-r/"">blog post</a> that explained the approach so that others can learn from it, was written in addition. This effort, undertaken by a single journalist, should certainly be considered. "," I think one of the biggest take aways of this project was to reduce the data volume early on so that it became gradually easier to work with the data. Secondly, investing into a robust data processing pipeline early on is key. That means that you should come up with an automated workflow that can run for days without crashing – and when it crashes, it should restart itself. Think of cron jobs and shell scripts that check log files for errors. Lastly, in this project I used R and sparklyr, which is a wrapper for Spark in R. Sometimes, I had a hard time finding good documentation for sparklyr. It might have been easier to work with Python, as the documentation for its Spark wrapper is better. So as a last advice I would argue that sometimes you should be willing to kill your darlings, e.g. use another scripting language than your favourite one, even though you might have to dig into it first. For more take home messages, consider <a href=""https://timogrossenbacher.ch/2019/03/big-data-journalism-with-spark-and-r/"">reading my blog post</a> about the project. ",https://www.srf.ch/news/schweiz/cyber-kriminalitaet-hunderte-schweizer-armeeangehoerige-tauchen-in-neuem-datenleck-auf,https://timogrossenbacher.ch/wp-content/uploads/2020/01/translation.txt,https://timogrossenbacher.ch/2019/03/big-data-journalism-with-spark-and-r/,,,,,Timo Grossenbacher," Timo Grossenbacher (1987) does data research, -analysis and -visualization for SRF Data, the data-driven journalism unit of Swiss Public Broadcast (SRF). Before that, he studied Geography and Computer Science at the University of Zurich and worked for the Zurich daily newspaper „Tages-Anzeiger“. He also teaches data journalism at the University of Zurich and is co-organizer of the Zurich chapter of HacksHackers, a get-together of journalists and developers. ",,,
Philippines,Palawan Daily News,Small,Participant,Innovation (small and large newsrooms),Vlog Talk,12/03/19,"Solutions journalism,Multiple-newsroom collaboration,Crowdsourcing","Animation,Adobe,Creative Suite,Google Sheets","Social media is something we cannot avoid in this kind of time, anybody can be a journalist, can be a reporter, no need to have a degree or seminar to deliver news, being a Digital content consultant of Palawan Daily News, we aim to gather and unify all the content creators in Palawan, to have a harmonious relationship and of course help each other in sharing knowledge and ideas, collaborate and produce inspiring content. Vlog Talk is a the first seminar we organize for free, to all those who want to learn the basic in creating digital content, at the"," It did build a strong connection to all the content creators in our province, we give them a platform and equipt them with skills on how to start in Vlogging, making them a responsible storytellers that create impact to the community. The Online community was formed after the event, as of this writing we have 70 people that are active in content creation and inspire them to be good in all the content they post online.  "," Basic techniques we use is sharing the knowledge, in a schoolroom type discussion the selected speakers discuss and educate the participants mostly from young generations, on how they will choose their brand, their skills, and talent, and inspire more people in creating a positive impact to our society.  "," Uniting the social media creators with the mainstream media is not easy, the traditional media peers in our locality didn't welcome the idea, they didn't support us in this project, but as a news publication both print and online, we believe in the new generation as our future, maybe not directly as a news writers, but a good storytellers that can create impact and positive vibes in the community.    Maybe we are doing some innovative approach in educating the young mind about the future of media in our province, a time that you don't need a degree but the training we provide is all that they need. The financial aspect is next in the hardest part, as a new player in the news industry in the City, we don't have many funds to this kind of event, we aim to make it free to the public that we have to shoulder some expenses from the venue to the snacks provided, and some prayers for some speakers to offer a free service, in which they did that make this knowledge sharing seminar a success.  "," The seminars aim to educate the almost 30 participants on how to create content, that is inspiring and can gain followers, we give them technical knowledge in video shooting and production, making them realize what is the best brand you can do in posting your videos online, and connect them to other community of content creators in the country today.  ",https://palawandailynews.com/event/palawan-daily-news-gives-birth-to-vlog-talk/,,,,,,,Joel Contrivida," Joel Contrivida is a broadcast journalist turn blogger in Palawan, he is popularly known for writing stories about lifestyle, travel, and entertainment. He is awarded Best Feature Writer in Philip Morris Agriculture Journalism award in 2007 for his story about cashew nuts, the famous delicacy in Palawan. He works with many brands, does successful coverages in and outside the Philippines and publish a coffee table about the Strategic Environmental Plan for Palawan. He is a past active national officer of Philippine Junior Jaycees, Inc.      ",,,
United States,Gizmodo,Small,Participant,Best data-driven reporting (small and large newsrooms),Goodbye Big Five,22/01/19,"Investigation,Explainer,Long-form,Documentary,Video,Business","Animation,Microsoft Excel,CSV,Python"," This months-long investigation into the big five tech giants—Amazon, Facebook, Google, Microsoft, and Apple—was a first-hand account using a custom data-blocking VPN of whether it’s possible to navigate the modern world without these companies, and how often we interact with them without realizing it. "," The series was widely-read (and watched), with most of the six entries attracting more than a half-million readers each. The series was published over three weeks, and many readers told Ms. Hill that they looked forward with anticipation to the publication of the next entry, following it much like they would a serialized tv show or podcast.    Published in January and February of this year, it has been referenced repeatedly in the antitrust conversation that has taken place regarding technology companies in 2019. It provided a better understanding of the extent to which consumers have no choice but to rely on these companies' services, and the control they exercise over other companies' apps and websites. Policymakers from the Senate and the FTC have told Ms. Hill that the series has been critical for policy makers seeking to understand the technical underpinnings of these companies' role in the economy and consumer experience.   The Verge’s Casey Newton called the series “brilliant” (<a href=""https://www.theverge.com/interface/2019/9/17/20869495/tech-backlash-nyt-rob-walker-antitrust-privacy"" rel=""noreferrer noopener"" target=""linked"">https://www.theverge.com/interface/2019/9/17/20869495/tech-backlash-nyt-rob-walker-antitrust-privacy</a>) in its demonstration of the monopolistic elements of some of these businesses: “To cut out the big five tech platforms from her life…required the use of special hardware, custom software, and an effort that could only be called Herculean. It’s little wonder that customers haven’t been fleeing the platforms en masse — it’s unclear how they even can, assuming they wish to continue using the modern internet.”    Hill has been asked to speak about the series around the world, from Kosovo to New Zealand. The series received widespread coverage from other publications, national public radio channels and podcasts, and has been cited by lawmakers (<a href=""https://nadler.house.gov/news/documentsingle.aspx?DocumentID=394012"" rel=""noreferrer noopener"" target=""linked"">https://nadler.house.gov/news/documentsingle.aspx?DocumentID=394012</a>) engaged in antitrust policy discussions in Washington, D.C. "," Reporter Kashmir Hill cut the big five technology companies from her life for a week each, and in her final week, stopped using all five. Not only did Ms. Hill boycott the companies’ products, she worked with technologist Dhruv Mehrotra who designed a special network tool that prevented her devices from communicating with the tech giants’ servers.The tool was a custom VPN through which Ms. Hill routed all her internet requests. The VPN blocks any traffic to or from an IP address controlled by a given tech giant, and in the last week of the experiment, the IP ranges controlled by all five tech giants. The tool kept track of how often her devices tried to send data to the companies, or receive it, to document the otherwise invisible ways in which we interact with these companies.   At the end of each week blocking a particular giant, Ms. Hill had a count of the number of times her devices had tried to interact with a company, despite her avoiding their products, which illustrated how deeply and invisibly Amazon and Google especially are woven into any internet experience. She used those numbers to augment her reporting, and make the investigation more powerful than her personal narrative of the difficulties of avoiding the companies.   Critics of the big tech companies are often told, “If you don’t like the company, don’t use its products.”  Ms. Hill did this experiment to find out if that is possible, and she found out that it’s not (with the exception of Apple). These companies are unavoidable because they control internet infrastructure, online commerce, and information flows.    At the end of the project, we explained our tech set-up in a separate DIY post: <a href=""https://gizmodo.com/want-to-really-block-the-tech-giants-heres-how-1832261612"">https://gizmodo.com/want-to-really-block-the-tech-giants-heres-how-1832261612</a> "," Reporters had ""boycotted"" tech giants before, but no one had tried before to do it technologically, and come up with a tool that would allow a reporter to detect and prevent ways in which they were invisibly interacting with a given company. The hardest part of this project -- beyond the inconvenience of spending months not using the tech giants' very useful services -- was coming up with a way to more thoroughly perform a boycott. The result was a far more robust understanding of the role these companies play in our lives, and concrete data about how we interact with them (when trying not to) that made the series more powerful than an anecdotal account alone.   There were many moving parts to the project. It required acquiring new gear (as documented in the story) and constantly reassessing the performance of the tool. In addition to the standard reporting and writing of the story, the experiment was documented by video producer Myra Iqbal; each week’s entry included a video, so that the series could be read or watched, or both. The videos are embedded at the top of each story or available here (<a href=""https://www.youtube.com/playlist?list=PLx1XbvvfIlc4zQgE5ohJA9EJ2NCcGc2QQ"" target=""_blank"">https://www.youtube.com/playlist?list=PLx1XbvvfIlc4zQgE5ohJA9EJ2NCcGc2QQ</a>).  "," Data flow monitoring continues to be a powerful technique for technology reporters. Ms. Hill got the idea for this project in part from a previous one. In 2018, she worked on a piece called ""The House That Spied on Me"" (<a href=""https://gizmodo.com/the-house-that-spied-on-me-1822429852"" rel=""noreferrer noopener"" target=""linked"">https://gizmodo.com/the-house-that-spied-on-me-1822429852</a>) that involved turning her apartment into a smart home in order to monitor the data that entered and left the house to understand the privacy trade-offs required by the Internet of Things. Making the invisible workings of the internet tangible and visible for readers helps to better grasp the nuances of the privacy debate, and what we are giving up in exchange for new products and services. ",https://gizmodo.com/c/goodbye-big-five,https://www.youtube.com/playlist?list=PLx1XbvvfIlc4zQgE5ohJA9EJ2NCcGc2QQ,https://gizmodo.com/want-to-really-block-the-tech-giants-heres-how-1832261612,,,,,"Kashmir Hill, Dhruv Mehrotra, Myra Iqbal","<h2> </h2>  Kashmir Hill is a tech reporter, now at The New York Times. She writes about the unexpected and sometimes ominous ways technology is changing our lives, particularly when it comes to our privacy.   Dhruv Mehrotra is an investigative data reporter for Gizmodo. He applies technology unconventionally to issues concerning social justice and accountability.    Myra Iqbal is a creative producer at Gizmodo. ",,,
United States,KXAN,Big,Participant,Best data-driven reporting (small and large newsrooms),Bargaining the Badge: How Hundreds of Accused Texas Officers Avoid Prison,22/05/19,"Investigation,Long-form,Open data,Video,Politics,Corruption,Crime",Microsoft Excel," Across Texas, hundreds of law enforcement officers have permanently surrendered their peace officer licenses in the past four years. A KXAN investigation of 297 of those surrenders uncovered nearly all the officers were accused or charged with a crime. In almost every case the officers used their license as a bargaining tool by agreeing to surrender it as part of a deal to avoid jail or prison. Those officers’ offenses included sexual assault of children and women in custody, abusing prisoners and arrestees, taking bribes, dealing narcotics to prisoners, lying about the circumstances of a police shooting and destroying evidence.     "," State law limits the Texas Commission on Law Enforcement’s (TCOLE) authority to permanently revoke an officer’s license, unless he or she has been convicted of a felony or certain misdemeanors. As the law currently stands, the high bar of obtaining a conviction puts district attorneys in a tough position if they know they want to get the officer out of law enforcement, according to experts. The law is forcing district attorneys to make plea bargains.   We discovered Texas has the most licensed peace officers of any state, yet other states like Florida and Georgia decertify far more officers per year than Texas. Those states allow their police licensing boards broader discretion to punish officers not only for the conviction of a criminal offense but also for the commission of a crime.   Texas lawmakers could give TCOLE more power in deciding to revoke a license. If the state licensing board could revoke a license based on evidence of misconduct or the commission of a crime, rather than a conviction, the number of license revocations would go up, according to experts.   Now, a Texas lawmaker’s office is conducting its own investigation into what KXAN uncovered. Rep. Jessica Gonzalez, D-Dallas, and her staff began requesting information from TCOLE during our investigation. Gonzalez sits on the House Criminal Jurisprudence Committee, a panel with oversight of TCOLE licensing rules. Gonzalez acknowledged there is a disconnect when state law allows licensing boards to hold other occupations, such as doctors, lawyers, plumbers and cosmetologists, accountable for conduct that doesn’t amount to a criminal conviction. The lawmaker raised the possibility of introducing legislation in 2021 to give TCOLE more authority to punish peace officer license holders for the mere commission of a crime. "," In the eight months it took to complete this project, KXAN uncovered the system of deals by analyzing records obtained from more than 100 public information act requests filed at all levels of state and local governments, including the Texas Commission on Law Enforcement, county and district courts, as well as local, county and state law enforcement agencies. Peace officer licenses are issued and maintained by TCOLE. All law enforcement officers at the municipal, county and state level, except for state corrections officers, must be licensed. KXAN reviewed 297 permanent surrender cases in Texas from 2015 through mid-2018. In nearly every case, the peace officers were accused of or charged with a crime.   We built a database of the 297 officers that permanently surrendered their license. We paid about $250 for the TCOLE records. Many of the requests we made were ultimately free because we spent time negotiating and narrowing our requests to avoid cost estimates. We completed that database, which included charges against the officers, the nature of the offense, criminal case outcomes, jail time, and the officer’s previous department, using records obtained through public information requests filed with district and municipal courts, police and sheriff’s departments, as well as cities, counties and state agencies.   The data did not exist in database form. We had to create the database from scratch using all types of records obtained through the Texas Public Information Act. Some records printed and sent in the mail, other records were transmitted by email.  We had to digitize and read everything, and we manually inputted the data ourselves. The TCOLE records we used were scanned documents in PDF form that, in many cases, were handwritten. The KXAN investigative team conducted all data analysis for this project. "," In many cases investigations into police officers were closely guarded by their former departments. We encountered many departments, especially in rural areas, that kept poor records and had little to no understanding of the state’s public records laws. Many departments and district clerks blocked our requests with prohibitive cost estimates and by sending the requests to the Attorney General’s office for rulings that took months to return.   The only indication KXAN could find of possible misconduct in many of these cases was noted in a portion of the file TCOLE maintains for each delicensed officer titled “summary of the reason for permanent surrender.” It’s a sheet each officer is supposed to provide, but that does not always happen. It is not always clear what level of punishments those officers may have faced. It is also more difficult, if not impossible, to obtain records of police misconduct when charges are not filed against the officer. KXAN found more than two dozen cases in the last four years in which police officers or jailers permanently surrendered their licenses to avoid prosecution or to end investigations into possible misconduct.   The digital project featured the full series, along with several embedded videos of extended police camera footage and video reports produced exclusively for digital audiences in some of these cases. We also highlighted specific examples in separate case pages, giving users access to documents and more details to better understand the issue. We also included interactive maps and data components to explore the scope of this statewide problem. "," It is important to note the huge effort made in requesting and analyzing records. We filed more than 100 record requests for this project, and they were not boilerplate requests. Each request was unique and many were submitted by fax and handwritten letter. We did not write a few template record requests and blast them out by email across the state. These requests were made individually and often required ample time with records custodians on the phone to explain how records were kept and to negotiate costs.   Every expert we spoke with was surprised by the data and trends we uncovered. We have found no other reporting that has amassed these records, discovered the common trend and dug into the origin of the problem. ",https://www.kxan.com/bargaining-the-badge,https://www.kxan.com/bargaining-the-badge/violent-crimes,https://www.kxan.com/bargaining-the-badge/sex-crimes,https://www.kxan.com/bargaining-the-badge/corruption,https://vimeo.com/338054331,https://vimeo.com/340064775,,"Jody Barr, David Barer, Ben Friberg, Josh Hinkle","  Jody Barr    Jody Barr graduated from the University of South Carolina in 2005 with a Bachelor of Arts in broadcast journalism. He served as an investigative reporter in Cincinnati, OH and Columbia, S.C. before joining KXAN in July as an investigative reporter, 2017. Most recently, his reporting in Texas led to legislation filed to provide more oversight of the state’s top oil and gas regulators.    David Barer    David was born and raised in Houston. He received a bachelor’s degree from Santa Clara University and a masters of journalism degree from The University of Texas at Austin. David has reported for Reporting Texas, KUT News, Austin American-Statesman, Associated Press, Texas Climate News and Community Impact Newspaper.    Ben Friberg    Ben Friberg is an investigative photographer at KXAN. A Texas native, his work often focuses on statewide stories, like documenting the controversy surrounding the regulation of the oil and gas regulation or the ties to mental illness many people who shoot and kill police officers may have.    Josh Hinkle    Josh Hinkle is KXAN’s Director of Investigations & Innovation, leading the station’s duPont and IRE Award-winning investigative team on multiple platforms. He also leads KXAN’s political coverage as the executive producer and host of “State of Texas,” a weekly program focused on the Texas Legislature and elections, seen in 14 markets statewide. ",,,
India,The Wire,Small,Shortlist,Best news application,The Election Game,05/03/19,"Explainer,Quiz/game,Open data,News application,Infographics,Chart,Map,Elections,Politics","Scraping,D3.js,JQuery,Json,Microsoft Excel,Google Sheets,CSV,Python,Node.js","India, the world's largest democracy, is a multi-party system. Dozens of parties compete with each other to form the government, and most national parties tie up with each other or smaller regional players to boost their chances of a win. Such tie ups can dramatically alter public sentiment and therefore the election. This interactive empowers the user to play around with the results of the 2014 general election, and to see how shifting alliances may impact the results of the 2019 election. The goal is NOT to predict the election, but to help users understand the possible impact of coalitions"," The interactive offers a deep dive into many prevailing terms and methods used by Indian psephologists, such as coalitions, percentage point change and vote swings. A lot of feedback recieved expressed gratitude for clearly documenting the methods used, and for an articulate visual explanation of phenomnenon that was being popularly discussed by the media ecosystem at that point of time. The interactive also offers a hexmap of the indian 'electoral college' that is unprecedented for an ecosystem that still mostly relies on land-area maps. This hexmap was open sourced and shared with members of the indian news community to remix and use. "," Python and Node were used to scrape and consolidate the data from the Election Commision of India's website.   CSV spreadsheets were used to validate the data   The Adobe Suite, d3.js, and Observable were used to construct the hexmap   Vue.js supplmented by dozens of libraries including d3.js, was used to build the app itself "," One of the hardest parts of this interactive was non-technical research into prevailing methods of psephology in India, such as vote swings. These methods are popularly used but rarely documented so it is hard for a non-psephologist to replicate a psephologist's findings. After a lot of work we found a process documented offline, refined it and used it to make the interactive itself. The most important part of the interactive is that we document our method at the end of this interactive—for transparency and posterity.   The hexmap was also built painstakingly and manually, by consulting the electoral map of india at each turn and manipulating hextiles in the Adobe suite. All this hard work was open sourced to the community for the betterment of the ecosystem. "," Learning is the raison d'etre of this project. We hope that, through the medium of articulate visual journalism, this project enables a better understanding of the complex workings of the Indian democratic system as well as the prevailing methods in Indian psephology. We believe in the power of learning through playing, and towards that end, the visual interactions in this game reflect immidiate colorful changes. The user is fully engaged in this learning process and is encouraged to play around and experiment.  ",https://thewire.in/theelectiongame,,,,,,,Aditya Jain," Aditya Jain is an award winning Creative Technologist who was the Creative-Technologist-in-residence at the Wire in the summer of 2019. He has previously worked at several distinguished organizations that include CQ Rollcall, the Library of Congress, the Center for Strategic and International Studies and Fast Forward Labs. He is currently pursuing a Masters degree at the Tisch School of Arts at NYU in New York City.  ",,,
Brazil,Sistema Verdes Mares,Big,Participant,Best news application,Cidade: História Edificada,15/12/19,"Long-form,Database,Podcast/radio,Chart,Audio,Arts,Human rights","Microsoft Excel,Google Sheets"," O projeto Cidade: História Edificada é um produto multimídia, publicado nas versões digital e impressa do jornal Diário do Nordeste e na rádio Verdes Mares. A  série de matérias ""Cidade: História Edificada"" fala sobre os imóveis de Fortaleza, capital do Estado do Ceará no Nordeste brasileiro. A reportagem obteve por meio da Lei de Acesso à Informação (LAI) um conjunto de dados do cadastro imobiliário oficial de Fortaleza, por meio da Secretaria Municipal de Finanlas. No material são detalhadas, dentre outras, a quantidade de imóveis por bairro e o ano estimado das construções e os desbobramentos dessas informações no cotidiano.              "," A série tratou de questões como o impacto da ausência de dados consistentes sobre a idade dos imóveis da cidade, ausência de registros, imóveis centenários. Tudo partindo da base de dados públicos e conectanto essas informações a ""cidade real"", materializada em histórias. A série teve repercussão junto aos leitores, principalmente, por abordar questões referentes as próprias moradias. Teve também impacto junto ao poder público em debates e repercussões em redes socias e instituições como Câmara Municipal.      "," As ferramentas foram utilizadas, sobretudo, na apuração. Utilizei o E-Sic (sistema para obter informações públicas via Lei de Acesso à Informação) para obter as informações que vieram em formato de planilha. Os dados foram compliados em programas de planilha, como excel.  "," Houve dificuldade no acesso aos dados, pois, além do acesso ao banco de dados do cadastro imobiliário da cidade que tem 871.689 imóveis registrados, distribuídos em 121 bairros. Mas também no conflito entre a disponibilidade desses dados e o acesso a informações que são consideradas de interesse particular. Como o endereço dos imóveis centenários na cidade. Que também tive acesso, mas após uma série de embates junto à gestão pública.  ", A utilização de dados públicos para discurtir questões referentes ao planejamento e cotidiano da cidade. A aplicação desse dados em um produção que evidencia os rumos que a cidade toma a partir da produção de informação.  ,https://diariodonordeste.verdesmares.com.br/editorias/metro/falhas-em-cadastro-geram-incerteza-sobre-a-idade-real-dos-imoveis-em-fortaleza-1.2187950,https://diariodonordeste.verdesmares.com.br/editorias/metro/capital-tem-268-imoveis-com-mais-de-100-anos-segundo-cadastro-1.2188512,https://diariodonordeste.verdesmares.com.br/editorias/metro/fortaleza-tem-predominancia-de-imoveis-construidos-nos-anos-1990-1.2188039,https://diariodonordeste.verdesmares.com.br/editorias/metro/familias-lidam-com-dilemas-e-proveitos-de-habitar-casas-centenarias-em-fortaleza-1.2188511,https://diariodonordeste.verdesmares.com.br/editorias/metro/casas-sem-papel-criam-areas-informais-com-deficit-infraestrutural-1.2188956,https://diariodonordeste.verdesmares.com.br/editorias/metro/jose-walter-teve-a-maior-construcao-de-imoveis-nos-ultimos-7-anos-1.2188957,,"Repórter: Thatiany Nascimento, editoras: Karine Zaranza e Dahiana Araújo. Diagramação impresso: Louise Anne Dutra"," Formada em Comunicação Social - Jornalismo - pela Faculdade Integrada do Ceará (FIC) desde 2010. Especialista em Políticas Públicas e Seguridade Social pela Faculdade Cearense (FAC) e Mestre em Comunicação pelo Programa de Pós-Graduação em Comunicação da Universidade Federal do Ceará (UFC). Minha pesquisa de mestrado versa sobre transparência pública, democracia e jornalismo, analisando especificamente como os jornalistas do Nordeste do Brasil incorporam em suas rotinas produtivas as ferramentas de transparência como fonte de informação. Atuo em redação de jornais desde 2009. Neste intervalo de tempo, tenho realizado coberturas jornalísticas factuais e especiais vinculadas, sobretudo, a editoria de Cidade/Cotidiano. Atualmente, sou repórter da editoria de Metro do jornal Diário do Nordeste, veículo que integra o Sistema Verdes Mares de Comunicação no Estado do Ceará, Nordeste do Brasil. Tenho aptidão para cobertura que envolva o trabalho com dados bem como a produção de narrativas aprofundadas. Dentre outros grandes temas, tenho aptidão para a produção de matérias relacionadas a: desenvolvimento urbano, patrimônio, mobilidade, habitação, questões de gênero/orientação sexual, direitos humanos e saúde mental. ",,,
Brazil,"Jornal O Globo, Revista Época",Big,Participant,Best data-driven reporting (small and large newsrooms),"In 28 years, Bolsonaro clan has named 102 people with family ties in their offices",08/04/19,"Investigation,Long-form,Breaking news,Database,Infographics,Elections,Politics,Corruption,Money-laundering,Crime","Animation,D3.js,Json,Adobe,Microsoft Excel,Google Sheets,CSV,R,RStudio"," After an investigation into 28-year public data, we found that President Jair Bolsonaro and his three parliamentary sons have had 286 advisers since 1991. In total, 102 had a family relationship. From the Bolsonaro family alone, 22 named relatives were discovered. We also found that of this total, 37 were phantom employees who received public money without working for years. They were nannies, housewives, retirees. The revelations fueled official investigations by the authorities into two of Bolsonaro's sons who raised suspicions that they were getting their salaries in a big money-laundering scheme. "," The report has given another dimension in the story that is known in Brazil as the ""Queiroz Case"", the main event of corruption involving the Bolsonaro family. Until that time, the Rio de Janeiro Public Prosecutor's Office was investigating nine advisers of Flavio, Jair Bolsonaro's eldest son, and many suspicions that Flavio was charging ilegally his employees part of their salaries. The report has shown this modus operandi was biggest and widespread in the Bolsonaro family offices.   The publication has been divided between the two vehicles of the group: newspaper O Globo and magazine Época. The initial presentation of data of the 102 named advisors came out at O Globo, and the magazine article presented the impact of R$ 65.2 million on public coffers, showing to the public the dimension of this crime that, even though is common in Brazil, is seen as a minor corruption.   With the data revealed, authorities have deepened the investigation into Flavio and found that 10 of his relatives used to withdraw up to 99% of their salaries every month, always on the payday of the Legislative House. At the same time, came the suspicion that this money was laundered with real estate purchases.   Prosecutors also opened an official investigation into Carlos, another Bolsonaro's son, for of the same suspicions, and the basis for the opening was the allegations published in the reports. Even though he was a city councilor for the city of Rio de Janeiro, people living in other towns and states advised him, which is against the law. Even an ex-wife of the president has become investigated.   Bolsonaro threatened the press the day after the reports were published. He reduced one of the sources of funding for media companies. After this, he also began to make changes in investigation institutions.  "," With the immense amount of information investigated in three Legislative Houses and official newspapers, the reporting and infographics team have built a unique database that was unavailable to the public - much data of three decades was documented only on paper and was lost in the archives. This material has become relevant not only for gathering information about the President's family but also for his 28 years of public life.     Then, the team decided to present and draw these family links in a digital environment showing the kinship relations between the advisors in the four offices. With the aid of algorithms of a programming language and statistics (R), the connections were organized and compiled in a format that allowed the construction of an interactive network diagram (made with the D3 package, in JavaScript / HTML / CSS) that the reader can consult at ease. A real database was made available to anyone who wanted to see, with information about the profession, salaries, and kinship of advisors.   An interface (also with React / JavaScript / HTML / CSS) was designed for individual consultation of each advisor, optimized for smartphones. In this way, we made public a variety of information that should appear on websites of Legislative Houses, but were not available, in violation of Brazil's law on access to information. The infographic also allowed the readers to access the report as they pleased.   We also recorded a podcast explaining all the process of the reporting and using all of the recordings we made in interviews with these ghost employees. This way the public could hear some of the confessions that this people made when they were asked about the reality that they did not worked for Bolsonaro's family. "," We have requested Information using a properly law in Brazil for acess of public data to be able to know the list of all Jair Bolsonaro staff in the House of Representatives since 1991, his first term. In those requests we also asked the periods in which these people supposedly worked, positions and salaries. All information is public by law in Brazil. The same request was made to the Rio de Janeiro Legislative Assembly and to the Rio de Janeiro City Council - where the president's son were elected.   In the House of Representatives and the Legislative Assembly the requests were fulfilled within the deadline of the law, which is 20 days. Many documents were delivered in print or PDF format. This way, about a thousand pages of information had to be converted into a spreadsheet so that the data could be further analyzed. The gathering of all information was done by the entire team of reporters.   The Rio de Janeiro City Council ignored the requests for data. Given the lack of response, 3 reporters decided to do the survey personally because this data was at least registered in printed papers on the City Council archive. The only online information available is from 2017. We needed data from back 2001 until 2018. So we manually searched all the papers of 18 years to find all the names of the employees, positions and periods.   After all the data was analyzed extensive investigated 37 people to prove that they were ghost workers. We made several trips to other cities, consulted several databases and interviewed several people. Most employees declined to interview, many were afraid of police investigated for homicides who had relationships with Bolsonaro family employees. Three people, however, even confessed that they never worked. "," To Brazil we created an important database that did not exist. This work was done in a concentrated manner over 3 months, but we began to investigate almost a year earlier.   This data was scattered in three legislative houses with lack of transparency and, in some cases, were even lost in the archives of these institutions. With the investigation we made this information available online, what should have been done from the beginning by the authorities themselves.   The interface created to provide queries to the database allowed the reader to see the story in different ways. It is posible to search for a unique person or for an entire family. The readers can also see the connections these people have with Bolsonaro's family. There are 286 online individual files and the advisers who are being investigated are detailed on these.    This was a huge learning process because it enables multiple readings and this is something that reporter teams need to start thinking about when working with large databases. The general data is often not as interesting as the unique stories inside them.    This was a huge learning because it enables multiple readings and this is something that reporter teams need to start thinking about when working large databases. The general data is often not as interesting as the unique stories.   We also combined a large investigation to have the data only  to create the very data base we needed for the special reporting. Only after that we went to the street to do the research on the ghost workers.   I think this work shows that data-driven research will not always be from a ready-made database, but one that needs to be built with a lot of persistence and only then can software be used for data analysis work. ",https://infograficos.oglobo.globo.com/brasil/os-lacos-familiares-do-cla-bolsonaro.html,https://oglobo.globo.com/brasil/em-28-anos-cla-bolsonaro-nomeou-102-pessoas-com-lacos-familiares-23837445,https://oglobo.globo.com/podcast/ao-ponto-caminho-para-encontrar-as-relacoes-familiares-nos-gabinetes-do-cla-bolsonaro-23854754,https://epoca.globo.com/integrantes-das-32-familias-que-assessoram-clabolsonaro-receberam-65-milhoes-desde-1991-23864535,https://oglobo.globo.com/brasil/reportagens-do-globo-revelaram-distribuicao-de-cargos-para-parentes-em-gabinetes-da-familia-bolsonaro-24145433,,,"Juliana Dal Piva, Juliana Castro, Rayanderson Guerra, Pedro Capetti, Marlen Couto, Bernardo Mello, João Paulo Saconi, Daniel Lima, Gabriel Godoy, Fernando Lopes, Ana Luiza Costa"," Juliana Dal Piva is a reporter at newspaper O Globo and Época magazine. Studied at the Federal University of Santa Catarina and holds a master's degree from the Getulio Vargas Foundation Center for Contemporary History Research and Documentation in Brazil. She was a reporter for Folha de S.Paulo, O Dia and O Estado de S.Paulo and one of the founders of Agência Lupa.   For the report on the disappearance of Deputy Rubens Paiva, she won the Embratel and Libero Badaró awards for print journalism in 2014 and also got honorable mention of the Vladimir Herzog Award for Amnesty and Human Rights in 2014. In 2019, she was a finalist for the International Humanitarian Coverage Award. of the International Committee Red Cross (ICRC) and, for the investigation of Bolsonaro's family, won with the Globo/Epoca team the Freedom of Expression Award offered by the Inter-American Commission on Human Rights that belongs to the Organization of American States.     ",,,
France,DISCLOSE,Small,Winner,Best data-driven reporting (small and large newsrooms),Made in France,15/04/19,"Investigation,Long-form,Open data,OSINT,Illustration,Infographics,Video,Map,Satellite images,Human rights","Microsoft Excel,Google Sheets,CSV","<h2>Following six months of investigation, Disclose reports on how french made weapons sold to Saudi arabia have been used against the civilian population in the Yemen war. Disclose used an unprecedented leak of secret documents and used OSINT research and data analysis to establish French responsibility for the war in Yemen. An investigation combining both human sources, secret documents and open source information, using satellite imagerie to track French weapons in Yemen and their impact. </h2> <pre>  </pre> <pre>  </pre> <pre>  </pre>"," The investigative story was published simultaneously on five media in France. The project has placed the question of France's arms sales to Saudi Arabia at the center of the political and civil debate. The Minister for the Army and the French Minister for Foreign Affairs were heard by the parliament. The information has demonstrated the lies of the French government on the ongoing arms exports to Saudi Arabia. Dozens of NGOs have called on the government to stop arms deliveries to Saudi Arabia and several public demonstration take place in France againt arms deliveries. A month after the revelations, the government, under pressure from public opinion, had to cancel two arms deliveries to Saudi Arabia, for the first time since the Algerian war. In January 2020, the government suspended the delivery of bombs to Saudi Arabia.  <pre>  </pre> <pre>  </pre> <pre>  </pre>","  We used satellite images to prove the presence of French weapons used in the Yemen War. We watched dozens of videos found on official social accounts, which we then geolocated using satellite views. So we were able to prove the presence of French military equipment in Yemen. We use open data from the Yemen Data Project to know in order to know the number of civilian victims in the firing range of French hotwizer and by calculating their range from public information given by the manufacturing companies. With this information, we were able to find possible evidence of civilian deaths related to these weapons. We used satellite images, webcam and data from Marine Traffic to retrace the course of a boat carrying arms from France to Saudi Arabia. We have also analysed the details of 19,278 aerial bombing raids recorded between March 26th 2015 and February 28th 2019.The results: these show that 30% of the bombing raids were against civilian targets. The intent of the coalition was clearly to destroy infrastructures that are essential for the survival of Yemen’s population of 28 million people. We geolocated all this bombing on map and find evidence on social network of the bombing.       <pre>  </pre>     <pre>  </pre> <pre>  </pre> <pre>  </pre> <pre>  </pre>"," The ""Made in France"" project had for finality to investigated a sensitive topic covered by military secret in France and whose investigation on the ground was made difficult or even impossible due to the ongoing conflict. The objective was despite these problems to conduct an investigation into the sale of weapons and their use in the war in Yemen with public data and open source information. The hardest part of this project was to verified and published this secret documents. We want not only publish a secret document but use the same intelligence tools used by the French military to prove the implication of our weapons in the war in Yemen. The hardest part it was to disclose the route of arms deliveries by boat, the information of which is nevertheless classified as military secret. We wanted to show that only with open source information we could investigate hidden matters. ""Made in France"" project is an unprecedented multi-long format that brings datajournalism to one of the most difficult areas of investigative journalism.      <pre>  </pre> <pre>  </pre> <pre>  </pre> <pre>  </pre> <pre>  </pre> <pre>  </pre>"," This project is a demonstration that we can investigate on arms deliveries only with public data, that we can investigate war grounds from a computer screen. But datajournalism is not a dehumanized journalism, because journalism need sources and whistleblower to have informations. Data journalism can be a powerful means of investigation also on the more sensitive topics like war and arms trade.       <pre>  </pre> <pre>  </pre> <pre>  </pre>",https://made-in-france.disclose.ngo/en/,https://youtu.be/BKUi1HmaJL0,https://theintercept.com/2019/05/17/france-takes-unprecedented-action-against-reporters-who-published-secret-government-document/,,,,,"Mathias Destal, Michel Despratz, Lorenzo Tugnoli, Livolsi Geoffrey, Aliaume Leroy"," Mathias Destal is a journalist and cofondator of Disclose, a new non-profit media of investigative journalism. He have work on far right movement in France during five year for the weekly news  Marianne.  He is the co-author of the book ""Marine est au courant de tout"", published in 2017 about the the funding of the far right party of Marine Le Pen by Russia during the presidential campaign.   Michel Despratx is a freelance journalist and director. He worked at Canal +, Les InRocKs, France Télévisions. He has also collaborated with L’Expres, Le Canard enchaîné, Le Monde diplomatique. He is also an advisor on several cinema scenarios and co-scriptwriter of a feature film.   Lorenzo Tugnoli, born in Italy in 1979, is a photographer based in Beirut. His work has been published by The New York Times, The Wall Street Journal, Le Monde, Newsweek, Time Magazine, Wired, The New Republic, The Atlantic, Der Spiegel, LFI - Leica Fotografie International. He is a regular contributor of The Washington Post. In 2014 he published The Little Book of Kabul, a book project that depicts a portrait of Kabul through the daily life of a number of artists who live in the city, in collaboration with writer Francesca Recchia.      Geoffrey Livolsi, is journalist and co-fondator of Disclose. He has work for several media like Mediapart, France Inter, l'Express. He is the author of several documentary on tax evasion and financial corruption.       <pre>  </pre>",,,
United States,Personal website,Small,Participant,Best data-driven reporting (small and large newsrooms),Points of Light: Protest in America,12/12/19,"Long-form,Open data,Infographics,Chart,Map,Elections,Politics,Environment,Corruption,Culture,Women,Immigration,Health,Gun violence,Human rights","D3.js,Adobe,Creative Suite,CSV,R,RStudio"," Points of Light explores street protest in the United States from January 2017 through October 2019. The project shows how protests rise and fall in response to current events, the importance of organizing, and the topics that inspired protest in different places. Through stories about guns, the environment, immigration, education, and more, Points of Light argues that protest does not belong to coastal cities or liberal causes. Rather, protest is a tool that all Americans can use to push for change.   The project uses protest data from Count Love. I created the visuals independently as a personal and class project. "," Points of Light helped to bring attention to the incredible work done by Tommy Leung and Nathan Perkins at <a href=""http://countlove.org"">Count Love</a>. The project was featured in AnyChart's <a href=""https://www.anychart.com/blog/2019/12/20/visual-data-analytics-dataviz/"">DataViz Weekly </a>alongside stories by Bloomberg, the New York Times, and Reuters. The project was also featured in <a href=""https://warninggraphiccontent.tumblr.com/post/189772506496/20-december-2019"">Warning: Graphic Content</a> and David Napoli's newsletter. Points of Light inspired a series on <a href=""https://spreadsheetjournalism.com/2019/12/23/political-protest-data-part-1-a-few-demonstrations/"">Spreadsheet Journalism</a>, considering the pros and cons of recreating the project in Excel. The project will also be discussed on an upcoming episode of the Data Viz Today podcast, and a chart from the project will be included in Jon Schwabish's upcoming book,  Better Data Visualizations . "," I used R to analyze the data and create preliminary graphics. Andrew Ba Tran's muckrakr package was invaluable for working with the Count Love data, and I used ggplot to create the rough visualizations. The only exception was the streamgraph, which I created in RawGraphs. I polished and annotated the static graphics in Illustrator. I created the interactive graphics with D3.js. This was my first D3 project, and I found Amelia Wattenberger's  Fullstack D3 and Data Visualization  incredibly helpful. I wrote more about the creation of the project at my blog, <a href=""https://dataanddragons.wordpress.com/2019/12/12/making-points-of-light-protest-in-america/"">Data and Dragons</a>. "," There were two main challenges in this project. The first was balancing my own curiosity with providing interesting, relevant information to the reader. There were many avenues of analyis that, while interesting to me, would have required too much work from the reader for too little return.   The second (and most difficult) challenge was learning to do mundane work about intense topics. It felt wrong to focus on caption alignment and font size when writing about mass shootings and human rights violations. However, making that information accessible is part of honoring the victims. I had to learn that patient, steady, un-dramatic effort is the best thing I can do, even in the face of very dramatic feelings. "," I hope that the site itself helps readers to understand that protest doesn't just happen: it requires organization and commitment. I also hope that readers walk away understanding that protest isn't always effective, it isn't futile either. While protests about guns didn't lead to lasting policy change, healthcare and education protests made a material difference in U.S. law and West Virginia labor conditions.   From my making-of posts, I hope that others learn that figuring out new technologies and datasets is possible, even if they seem insurmountable at first glance. I also hope that I can pass on the data profiling process I learned at the University of Virginia to others. ",protests.space,https://dataanddragons.wordpress.com/2019/12/12/making-points-of-light-protest-in-america/,https://dataanddragons.wordpress.com/2019/09/10/profiling-protests-or-what-i-did-on-my-summer-vacation/,,,,,Alyssa Fowers," Alyssa Fowers studies data visualization and interactive media at the University of Miami. She is a Data Science for the Public Good graduate fellow and cofounder of the University of Miami Visualization and Infographics Consulting Service. Before going to graduate school, she worked in data analysis and management for five years. Her professional career focused on making data accessible, reliable, and interesting to businesses and nonprofits in Washington, D.C. She received a B.A. in psychology from Duke University. ",,,
Brazil,G1,Big,Participant,Open data,The votings of the Brazilian Congress,05/11/19,"Database,Open data,Chart,Elections,Politics","Scraping,Json,Google Sheets,CSV,R,RStudio"," The project simplified the access to the results of 74 votings in the Brazilian Congress in 2019 and 61 votings in the past legislature. The database of the project has more than 37.000 rows of votings. This includes the Social Security Reform (2019), the impeachment of former president Dilma Rousseff (2016), the Labor Reform (2017) and bills about firearms legislation (2019). The data of the project refers to bills voted in the Brazilian Congress since 2015 and is still updated whenever a new voting happens. "," Our tool displays these data in a very easy way to navigate and get pieces of information about each Brazilian congressman/congresswoman and about each bill voted in the past years. Each bill voted in the Congress has its own page on the project and presents the decisions and views of each congressman/congresswoman about that particular subject.        It also indicates if the congressman/congresswoman has voted with the majority of his/her party or not. Another advantage is that the tool has filters which allow the citizen to see the result by party, state or position.        The homepage of “G1”, the biggest news website in Brazil, related to the TV channel “TV Globo”, always has a link to the project when this happens. On the day that the tool was released, on the 11th of May 2019, the project has reached over 350.000 pageviews. The project has also been well received by political scientists, researchers and readers, who highlight the relevance of accessing this data more easily.       The main idea is to approach citizens and politicians, so they can see if their interests and promises are followed in the Congress. If the congressman/congresswoman was absent during the voting (or voted different of what the citizen expected), for example, the citizen can also send him/her a tweet or an email to question this behavior. "," The programming language R was necessary to scrape, extract, clean, join and organize the data of the votings in the Brazilian Congress. The codes for downloading all the pictures of the deputies and senators were also written in R. Some of the data were also inside PDFs, and the team managed to get the content with OCR technology.       For updating the project, the leader of this project and data journalist Gabriela Caesar has coded some scripts in R, which get the data and automatize the processes, so all the content can be published in less than one minute after the voting. All the data team knows this language and is able to run these codes on RStudio.       After this step, all the data needs to be uploaded to Google Spreadsheet, because the system gets the new content with Google Spreadsheet API and PHP. With just one click the system updates the project, so a programmer’s assistance is no longer needed.        The website was developed with HTML, CSS, and Javascript, with the framework React. The layout of the project was designed considering that nowadays users access the project with desktop as well as mobile devices. "," The hardest part of this project was to learn the programming language R (mainly the collection of R packages “tidyverse”) and be able to code scripts that scrape, extract, clean, join and organize the data of the voting in the Brazilian Congress. The data of the Senate and of the House of Representatives are published in different formats and different websites, and the scripts needed to run for both.        Since open data isn’t a reality in Brazil, it would be very tricky for each citizen to get the data on the official website of the Senate and of the House of Representatives. The results of the votings are kept hidden and have unfriendly formats for the general public, such as DBF and PDF. Our tool displays these data in a very easy way to navigate and get information about each Brazilian congressman/congresswoman and about each bill voted in the past years. "," Others can learn from this project the fact that public data should always be open and accessible so that the general public can access and understand them in a simple way. The possibility to search for the name of a deputy or a senator makes it easier to monitor the Brazilian politicians’ activities. There is also the possibility to search for the bill.       If the bill was approved or declined by the Congress, media outlets can not only report the factual news but also display the data and show how each congressman/congresswoman has voted. Although the Brazilian citizens vote every four years for the Senate and for the House of Representatives, this doesn’t mean that the citizen should get information only during the election. There are ways (and this project is one of them) to keep in touch permanently and know their work better in the long term.       The project is also a very powerful tool to remind votings of past years and to help in the decision of the next voting for the Senate and for the House of Representatives. According to the results, the citizen can decide, for example, not to reelect a congressman/congresswoman.        The project is also very important because some issues related to politics are hard to understand and the project translates them in a simple, straightforward format. Politics should be reachable and understandable for the general public.       The reporters who cover the Congress and work in the Brazilian capital, Brasília, help the project with this part. For additional information, we call sources who work in the Congress. ",https://drive.google.com/drive/u/0/folders/1VcApFuO0VhLZuzGou6dRlAwHZAWwi4Dc,https://especiais.g1.globo.com/politica/2019/o-voto-dos-deputados/#/projetos/pec-da-reforma-da-previdencia-1-turno/,https://especiais.g1.globo.com/politica/2019/o-voto-dos-deputados/#/deputados/abilio-santana,https://g1.globo.com/politica/noticia/2019/05/11/o-voto-dos-deputados-e-dos-senadores.ghtml,https://g1.globo.com/retrospectiva/2019/noticia/2019/12/23/as-votacoes-do-ano-no-congresso.ghtml,,,"Gabriela Caesar, Antonio Lima, Thiago Reis, Clara Velasco, Rodrigo Cunha, Alexandre Mauro, Guilherme Gomes e Rogerio Banquieri"," The news website “G1”, related to the TV channel “TV Globo”, was launched in September 2006, since then reporting on important Brazil and world events. More than 70 journalists work for the news website based in the city of São Paulo, but there are also several other journalists working throughout the country. The data team of “G1” was created in September 2013.       Nowadays the data team at “G1” has five journalists: Clara Velasco (data journalist), Felipe Grandin (data journalist), Gabriela Caesar (data journalist), Roney Domingos (fact-checking reporter) and Thiago Reis (data and fact-checking editor). The project was also supported by the technology/art team at “G1”: Antonio Lima (full stack developer), Alexandre Mauro (designer), Guilherme Gomes (designer), Rodrigo Cunha (art and technology editor) and Rogerio Banquieri (full stack developer).       The leader of this project is data journalist Gabriela Caesar, 27, who joined the team in October 2017. Gabriela Caesar has been coding with the programming language R for two years, while learning R on her own and on Datacamp as well, since her graduation is on Journalism at the Pontifical University in Rio de Janeiro, in 2015.        Before launching this project, she has lived in the capital of Brazil, Brasília, where she used to report about the Congress and got to know better all the available data as well as all the procedures of bills in the Senate and in the House. While at the data team she started using technology such as R to help and simplify her work as data journalist, mainly in this project.  ",,,
United States,Nature Biotechnology,Small,Participant,Best data-driven reporting (small and large newsrooms),Innovation' Nation,29/10/19,"Investigation,Explainer,Long-form,Chart,Business,Culture,Health","Adobe,Creative Suite,Microsoft Excel,Google Sheets"," The article investigates China's fast-growing healthcare biotechnology field and reveals the country is still dependent on the western world's drug industry and universities for talent and inspiration, contrary to most media reports. The article also highlights the steps China would need to take in order to achieve true innovation, and it points to areas where the country does indeed have unique strengths.  "," The article has helped re-align investing trends in China, and pointed to its lack of an efficient technology transfer system at its universities.  "," Analysis of all recent biotech IPOs and private financings, determing ties to the west. Also generated a historical timeline of all new modalities in biotechnology, and determined where the initial science came from, showing that innovation has almost exclusively come from the academic research centers in the west (where free speech is cultivated). Sifted through hundreds of thousands of patents to determine country of first filing, showing trends in biologial innovation.  "," Gathering accurate information on China -- a government that is not particularly forthcoming. The language barrier is one issue, but the Chinese often does not release public information on clinical trials, the number of biotechs, etc. This means that many media outlets looking at 'innovation' in China missed the truth. It took more than a year of reporting and multiple visits to the country in order to get a more accurate picture of what is actually happening on the ground, and in the academic labs.  "," What China may truthfully excel at in the life sciences in the coming years, and why.  ",https://www.nature.com/articles/s41587-019-0306-9,https://www.nature.com/articles/s41587-019-0306-9.pdf,,,,,,"Brady Huggett, Andrew Marshall, Laura DeFrancesco"," Brady Huggett received a Bachelor of Science degree in biology from Wake Forest University, a master's in journalism from the University of North Carolina at Chapel Hill, and a master's in creative writing from The New School. Prior to joining  Nature Biotechnology  as business editor in 2007, he was managing editor at the biotech daily news service BioWorld, and he has also worked as a staff writer for a university research magazine. He has written articles covering all aspects of biotech, and has participated in or moderated panel discussions both in the United States and abroad.   Andrew Marshall obtained his PhD and postdoctoral experience in molecular biology and microbiology at King's College London. Before joining  Nature Biotechnology  in 1996, he was Editor of Current Opinion in Biotechnology for four years. He has written numerous articles and editorials on science and technology for the popular media, including The Economist, and for trade publications. As well as frequently speaking on biotechnology issues at international meetings, he regularly organizes conferences and symposia for Nature Research in the biotechnology area.   Laura DeFrancesco received her PhD at the University of California, San Diego in cell biology and did postdoctoral work on mitochondrial nucleic acids at California Institute of Technology. After doing research for 10 years at the City of Hope National Cancer Center and the University of California, Los Angeles on various topics in animal and cell science, she became the founding editor of the LabConsumer section of The Scientist. Following that, she was managing editor of the web-based life science portal BioResearch Online, and later went on to write for several publications, including  Nature Medicine , Analytical Chemistry, and BioIT World.      ",,,
United Kingdom,Gair Rhydd,Small,Participant,Best data-driven reporting (small and large newsrooms),Domestic abuse statistics revealed for England and Wales,12/09/19,"Open data,News application,Infographics,Chart,Women,Crime","Adobe,Microsoft Excel,Google Sheets"," The project is an article that I wrote for the Cardiff University student newspaper  Gair Rhydd . After seeing a dataset released by the ONS on the latest domestic abuse figures for England and Wales, I began cleaning and analysing the data. After ciphering through the data, I was able to piece together a story revolving around an important topic. Being a student paper, I made sure to convey the latest figures for students, as well as domestic abuse figures in a wider context. I also accompanied the article with the two infographics created in Tableau. "," Domestic abuse is a sensitive topic, and that applies to whatever demographic impacted. I was able to produce a project that was perfectly suited to the student audience of the university newspaper. The figures showing the number of students who were victims of domestic abuse was alarming, and this kind of data-driven reporting is something I feel is very important. Sensitive themes like domestic abuse need to be discussed because seeing if progress is being made is essential in resolving the issue on a nation-wide scale. Being able to speak to the university was also a great help because it accompanied the data and graphics really well. One of the most important bits of data I was able to discover is that over four in five victims of domestic abuse do not report it to the police. This has to change and my project outlines all the necessary services. The data I analysed and the trends I identified are clearly presented both in written and graphic form, and this piece is a great example of data-driven reporting that would very much be suited to a small newsroom. I hope my article had an impact on any domestic abuse victims to encourage them to speak out. "," The first tool I used was Microsoft Excel. I had to clean the data a bit so I could narrow down exactly what I was looking for. After that, some quick sort and filter actions meant I could see certain trends forming and would make notes on my findings in a separate spreadsheet. I then exported what I had discovered into the graphics software Tableau, which is where I created the two infographics. One of the things I highly value when it comes to visualisations is simplicity. Sometimes, graphs can be difficult to read. I made sure my tables were clear, precise, and showcased exactly what I needed them to. In terms of the article in the newspaper, I designed the page myself using Adobe InDesign. Embedding the infographics distorted the layout of the page which is why both of the tables are at the bottom of the page so they were more visible and easier to configure. "," Unsurprisingly, one of the most challenging aspects of this project was the data analysis. I was dealing with a large dataset, and whilst I have done this kind of work before, I am still very new in my data journalism career. A lot of my data analysis skills are self-taught, so filtering through this specific dataset took a considerable amount of time. Being a university newspaper, we have a particular audience and I was able to write an important piece that included students in what is a pertinent issue in society. Keeping the student focus is important, and I was glad that I was able to produce something that was aimed at people at the same stage of their life as mine. This project shows a variety of skills that go into making an effective data-driven story. These include analysing a raw dataset, identifying relevant trends, producing concise infographics and being able to transfer data findings into a news article form. "," I think for other fellow students in the same position as me, they should use my project as motivation to continue to write about important topics such as domestic violence. Even if we are student publications, we can be platforms for this kind of journalism, and it should be encouraged. We have an opportunity to make a difference in being able to write regularly. I for one will certainly be looking to produce similar projects whilst I continue with my studies. Another thing I would say others can learn from my project is that it shows some of the possibilities you have working with data. Starting off with just a dataset, my project demonstrates how you can use your findings to create multiple infographics and formulate an entire project that consists of your work.  ",https://issuu.com/gairrhydd/docs/1147_book/4,https://mygwjournalism.wordpress.com/2019/12/09/domestic-abuse-statistics-revealed-for-england-and-wales/,https://cardiffstudentmedia.co.uk/gairrhydd/domestic-abuse-statistics-revealed-for-england-and-wales/,,,,,George Willoughby," My name is George Willoughby and I am a final year journalism student at Cardiff University. I have been writing for several years, starting off by creating my own football fan page and writing other articles in my spare time. It wasn't until my second year of study at Cardiff where I was introduced to data journalism in a module I thoroughly enjoyed. Since then, I have maintained my passion and drive to make a career in the data journalism industry. I have been trying to improve my visualisation skills and data analysis anywhere I can. When I produced this project is was a good marker to show how far I have come from not really knowing a couple of formulas in excel to create a full project using both analysis and graphic creating software. I am the deputy editor of Cardiff University's student newspaper  Gair Ryhdd  which is where my article was published. I am determined to excel in the data journalism profession and hopefully I can keep writing meaningful data-driven articles. ",,,
Egypt,Almanassa,Small,Participant,Best data-driven reporting (small and large newsrooms),Train accidents and compensations: Live like death - حوادث قطاراتنا وتعويضاتهم: حياة تشبه الموت,27/10/19,"Investigation,Illustration,Infographics,Video,Human rights","Creative Suite,Google Sheets","The data driven story evaluates the value of the mendatory compensations for train accidents victims in case of death or complete disability which was 20.000 L.E (1253.42 $) from 2002 until February 2018 when the Railroad Authority increased it by 50%. t The stabilty of the total amount for a long time lead me to the idea of evaluating the value of the compensation during the perion from 2002 to 2019, to answer the question of the value of the number for a family that has lost its provider or need to pay for health care in case of complete"," The project made a good echo, Al hurra Tv Channel interviewed the victim we interviewed and featured in the story after The published our story and The Global Investigative Journalism Network mentioned the story and appreciated it. "," I used Google sheets to analyze data I gathered from The Egyptian central bank, The Central Agency for Mobilization and Statistics and other official organizations to answer 3 Questions in order to give a clear picture about the economic value of the compensation:   1. How the value of the compensation has changed against the value of Dollars from 2002 to 2019?   2. How many months a mandatory compensation would last if we used it to feed a family with the necessary food basket?   3. How many months a mandatory compensation would last if we used it to provide for one person near the poverty line?   I also used the information available from different sources to explain to the audience how the compensation system works, and the legal limitations to the mandatory compensation estimation in the Egyptian judiciary system for a train accident.   Also, My colleague Yusef Al araby used Adobe illustrator to illustrate the 4 infographics were produced to visualize the insights that that where found.  "," The hardest part was gathering data and understanding the compensation system, although A lot of press articles spotted the problems with the railroads system in Egypt, but the information about compensations and it's regulations are either incomplete or inaccurate, It took me a lot of time to investigate the right information, and gather the credible data to use. "," I think they can learn the importance of accuracy, When you say the mandatory compensation is devalued, you have to give the audience a way to understand and recognize that. ",https://almanassa.com/ar/story/12596,,,,,,,Me," Hager Hesham, a multimedia journalist who recently works for a pan-Arab Newspaper called Al Shark Al Awsat, I used to work for several news websites like Al Manassa and Sasa post. I was awarded with The second place at the best data driven story category in the ADJN Data Journalism Awards. ",,,
Kenya,ROYAL MEDIA SERVICES,Big,Participant,Best data-driven reporting (small and large newsrooms),THE LUR DILEMMA,27/03/19,"Explainer,Solutions journalism,Long-form,Documentary,Women,Health","Animation,Personalisation,Adobe"," The Lur Dilemma is a multimedia story incoroporating both video, still images and text.It is an explanatory narrative that explores the burden of infertility in Kenya.It draws  the narrative from the available data, and tells a story from the perspective of two women who can never have children of their own. "," The Lur Dilemma has begun an important conversation on the burden of infertility.It has explored Infertility,  its scale and scope and also focuses on the economic,politicial, health and personal nuances.It explores the problems and also offers the solutions . With the hashtag #LurDilemma, the feedback helped get feedback from our viewers on what needs to be done. The personal stories helped people connect to their own experiences. ","  1.PageFlow . It is a digital storytelling tool that incorporates audiovisual material: videos, still photographs, audio, text and hyperlinks.    2.Animation/ Adobe Premiere .   This was for the animation of the numerical data.     ", Incorporating the data .It took a month to prepare and rewrite the project.The host site also requires additional payment for more data visualization.There are limits to the content for each story. ," Data is not the entire story.People can make sense of data when they see the people represented by it. Good stories are made of multiple elements,  and the most important is ""People."" If data can cast light on the human condition, it is central to storytelling. ",https://dorcaswangira.pageflow.io/the-lur-dilemma#194458,https://www.youtube.com/watch?v=8-tj2K1U9_M&t=233s,https://www.youtube.com/watch?v=ImYCqfNZAzs,https://www.youtube.com/watch?v=4zludGNbktk&t=38s,https://www.youtube.com/watch?v=MNCdrKKfkvg,https://www.youtube.com/watch?v=hIEEWfdVoy8,https://www.youtube.com/watch?v=ymzkP1fJ09o,DORCAS WANGIRA," Dorcas Wangira is an early career journalist passionate about science and human interest features. She is a Features reporter working with Citizen Television, Kenya’s leading TV network. She has previously worked as a Special Projects reporter and news correspondent for KTN NEWS, Kenya’s only 24-hour news network.    She believes in the power of the human spirit and amplifying the voices of those often left behind and at the fringes of society. She produces Your Story, a weekly segment that airs every Sunday on Citizen TV 9pm Prime Time News. Her work explores social issues and she is often tasked with explaining complex science issues in a simple way, putting a human face to every story.    For her work, she has received the 2019 Michael Elliott Award for Excellence in Storytelling and the AJEA(Media Council of Kenya) 2019 I.C.T. T.V. Reporting Award, the 2018 Upstream Oil and Gas Journalist of the Year Award and the Zimeo 2017 Award for Climate Change and Conservation reporting. She has also been shortlisted for the UK Foreign Press 2017 Young Journalist Award and the 2015 Mohamed Amin Africa Media Award People’s Choice Award.     An avid reader, Dorcas is passionate about African literature and how the oral and written tradition is central even to news writing. Her vision is clearly spelled out as “disturbing the comfortable; comforting the disturbed.” ",,,
United States,Philadelphia Inquirer,Big,Participant,Best visualization (small and large newsrooms),Philadelphia pharmacies loved OxyContin — until suddenly they didn't,12/12/19,"Investigation,Explainer,Infographics,Chart,Health,Crime","D3.js,Python","This graphic shows how a single moment (August 2010, when Purdue Pharma altered OxyContin to make it harder to abuse) reveals how much of Purdue's business in Philadelphia was directly fueling abuse and supplying illicit drug markets. It relies on newly unsealed federal data, th showing which pharmacies immediately abandoned OxyContin when it became harder to abuse (and instead quickly moved into other potent opioid pills, and how the much of the drug had been supplied by those few pharmacies. This graphic accompanied an article that focused on what we know about that moment and the consequences for the entire"," It's hard to say what the full impact of this work is at this point. We are continuing to report on the pharmacies (one came under federal investigation shortly while reporting on this story) and litigation involving Purdue Pharma, the Sackler family, and other drugmakers is ongoing. We hope this reporting has provided some context for stakeholders, litigants and the general public. "," This project was built on analysis of a massive trove of federal data using a mix of UNIX commands and Python scripts (primarily the pandas and numpy libraries). I used both the raw data that was ordered released by the court and a filtered version that was published by the Washington Post.   This ""ARCOS"" data, weighing in at >100GB, was too large to deal with on a local machine in its raw form. So UNIX commands (mostly grep, really) were used to slice the larger dataset of drugs into categories by state, county and active ingredient. Those were fed to Python scripts that produced comparatively managable datasets of the monthly orders made by each pharmacy in the U.S. for the types of drugs I was interested in. That aggregated dataset formed the basis for much of the analysis in the story and graphic.   The graphic itself is a ""scrollytale"" built with d3.js. It leans heavily into maintaining discrete objects for the reader to hold onto as the story moves along. The main way this was accomplished was by making each ""drug"" the object that is manipulated, either by filtering data to certain pharmacies or between certain dates. One useful piece of technology that made that easier was Flubber, a javascript library that makes interpolation between SVG shapes much cleaner.    Further below, I checked ""desktop"" as the superior platform for viewing this project, but some may find the experience on mobile more compelling.  "," Setting out, we knew we were going to do a story on OxyContin's role in the region, it quickly becoming clear that OxyContin made up a huge portion of Philadelphia's opioid market, and at a rate that far exceeded most other areas. I sliced and diced maps, charts and metrics around those particular findings dozens of times, even drilling down to the pharmacies that ordered the most of the drug.   But the story seemed a little hollow, stating particular factoids about OxyContin that weren't particularly interesting (to me, at least).   It took a few weeks of reporting to get a breakthrough around which I could build a coherent story. We knew and were interested in Purdue's decision to ""reformulate"" OxyContin (as they put it) in 2010, but my analysis had been too coarse to see how it actually played out. As I learned more about the drug and spoke to experts in opioids, I decided to throw out a lot of previous work to zero in on that moment when the abuse-deterrent drug hit the market.   As soon as the data was in good enough shape, much more shocking findings came forward: pharmacies abandoned the most potent opioid on the market as soon as it became harder to crush. The effect was immediate and so startingly obvious (once we knew to look for it) that we were surprised to find many were still in operation. "," To drive in the previous response:  Look for inflection points  — they can be moments in time, or boundaries between geographies where something changes. The edges represent something  real,  something that changes and begs explanation. Follow up with reporting to get to the bottom of it. What makes these discrete little moments so useful in data journalism is that instead of looking at broad periods or regions    Another useful point: we had a perfect case study in Northeast Pharmacy, largely because it was caught up in a larger federal case years back. Many pharmacies had very similar histories of oxycodone orders. But Northeast's pharmacist had pleaded guilty to directly supplying drug traffickers, and the ensuing trial provided sworn testimony that provided plenty of color and context for how these schemes were operated.  Dig deep on entities  that your analysis and algorithms surface, and find the one you can hang your hat on. ",https://www.inquirer.com/news/inq/philadelphia-opioid-crisis-oxycontin-interactive-graphic-sales-reformulation-20191212.html,https://www.inquirer.com/health/opioid-addiction/philadelphia-opioid-crisis-purdue-pharma-oxycontin-sales-reformulation-20191212.html,,,,,,Nathaniel Lash," Nathaniel Lash is a reporter on the Philadelphia Inquirer's investigations team, where he writes stories and code to help the public better understand how government agencies and the powerful operate. ",,,
Spain,El Confidencial,Big,Participant,Best data-driven reporting (small and large newsrooms),The Spanish Urban Exodus,27/09/19,"Investigation,Explainer,Long-form,News application,Infographics,Chart,Map,Immigration,Economy,Employment","D3.js,Canvas,Json,CSV,R,RStudio"," The Spanish Urban Exodus project did a microdata analysis of Spain’s internal migration data from 1988 to 2018, based on data from the Spanish Statistics Institute. After compiling more than 51 million records and analyzing more than 15 million inter-provinces migrations, this data-driven project details how the two biggest cities of Spain, Barcelona and specially Madrid, attract high-skilled young people from mid-sized Spanish cities in a brain drain process linked to the 21st century phenomenon of the metropolization of Spain. "," The project shows an under-reporting new phenomenon in Spain: the internal brain drain capitalized by Madrid and Barcelona of getting high skilled young workers from mid-sized cities, impoverishing these towns and provinces. This is mostly known as capital effect: national capitals and big cities are a pole of attraction of skilled labor because big companies and high-tech jobs are located in these areas. Unlike the rural exodus, where cheap labor left small villages going to cities and towns where factories and industries were located, the urban exodus attracts high skilled workers from these towns, decapitalizing these areas, employing them as high skilled workers in services companies.   The seven pieces of the project shows the extent of this recent phenomenon in Spain, specially in Madrid. Nevertheless, the arrivals of new skilled workers to Madrid and Barcelona have a side effect: the rise in housing prices and the expulsion of low skilled workers to the neighboring provinces, where homes are more affordable, making commutes more difficult. The project also shows two exceptions to this capital effect, Basque Country and Málaga, who have found a way to retain and even attract skilled workers.   The impact of the project was huge, both quantitatively and qualitatively. The seven pieces reached nearly 500,000 views since their publication, and all the articles reached at least the top 3 most read articles of every day of publication. Qualitatively, we were able for the first time to compile 31 microdata files in one file to explore more than 51 million migration records of the last three decades (the R scripts with the process are published in our GitHub). Also we could show the inter-province migration flows of every Spanish city with more than 10,000 inhabitants in an interactive dashboard. "," The seven pieces of the Urban Exodus project combines a great number of journalist techniques: data analysis, interactive and static infographics, maps, interactive dashboards, reporting trips, photographs, layout and design.   The compilation and the analysis of the microdata files published every year by the Spanish Statistics Institute were made with R due to the large size of the database (the final csv file has more than 51 million records and 15 columns and has a size of 3.3 GB; the R script of the compilation process is published in our GitHub: <a href=""https://github.com/ECLaboratorio/unidad-de-datos/tree/master/proyectos/migraciones_espana"">https://github.com/ECLaboratorio/unidad-de-datos/tree/master/proyectos/migraciones_espana</a>). We also produced several csv files, tables and infographics with R to help us to identify reporting leads for our series.   The interactive dashboards, infographics and maps were made with D3.js and Javascript and rendered with Canvas to improve the data loading on desktop and mobile. "," The project has two main difficulties: the compilation of the 31 microdata files published by the Spanish Statistics Institute (one per year between 1988 and 2018) and the data loading in the interactive dashboards and infographics. The data compilation were made with R and, due to the large size of the files and the lack of enough computer memory, the process was very slow and restarted back and forth. The solution found was to split the compilation process to obtain a file for every five years and then to join these five year files in an only one. With this original raw file, we did the data analysis, categorizing the migrations in three types: intra-province, inter-province and foreign migration. We focused on the inter-province migrations because the main reason for a person to change his residence to another province is for work reasons, so this type of migration depends mainly on work opportunities.   To solve the data loading problems in the interactive dashboards and infographics, we used D3.js and Javascript and rendered then with Canvas to improve the loading on desktop and mobile. This process was a great success, especially in the map that heads the overview (<a href=""https://www.elconfidencial.com/economia/2019-09-27/exodo-urbano-espana-migraciones-provincias_2240119/"">https://www.elconfidencial.com/economia/2019-09-27/exodo-urbano-espana-migraciones-provincias_2240119/</a>) and in the interactive dashboard which shows the urban migration flow of every Spanish city above 10,000 inhabitants (<a href=""https://www.elconfidencial.com/economia/2019-09-27/exodo-urbano-espana-migraciones-ciudades-buscador_2251515/"">https://www.elconfidencial.com/economia/2019-09-27/exodo-urbano-espana-migraciones-ciudades-buscador_2251515/</a>). Nevertheless, to improve the data loading we also selected the 396 main flows between provinces (15% of the total) which represent more than 10.77 million migrations since 1988 (71% of the total). "," The first thing others can learn from the Urban Exodus project is the potential of the microdata published by National Statistics Institutes all around the world. These microdata files requires a hard work to compile them in an unique file or database to explore the data in all its extension because National Statistics Institutes used to publish the microdata yearly, so the first step to work with microdata must be to compile and combine them. The big advantage of this process is that once you have combined them, you can upgrade it every year with new data releases by National Statistics Institutes. And also these big databases offer journalists leads to several reports and news articles. ",https://www.elconfidencial.com/economia/2019-09-27/exodo-urbano-espana-migraciones-provincias_2240119/,https://www.elconfidencial.com/economia/2019-09-27/exodo-urbano-espana-migraciones-ciudades-buscador_2251515/,https://www.elconfidencial.com/economia/2019-09-27/exodo-urbano-espana-llegadas-madrid-ciudades_2240155/,https://www.elconfidencial.com/economia/2019-09-28/exodo-urbano-espana-cataluna-madrid-poblacion_2240171/,https://www.elconfidencial.com/economia/2019-09-29/exodo-urbano-espana-migraciones-malaga-sevilla_2240195/,https://www.elconfidencial.com/economia/2019-09-30/exodo-urbano-espana-migraciones-pais-vasco_2240187/,https://www.elconfidencial.com/economia/2019-10-01/exodo-urbano-espana-salidas-madrid-burbuja_2240163/,"Javier G. Jorrín, María Zuil, Jesús Escudero, Laura Martín, Antonio Hernández, Pablo López Learte, Luis Rodríguez, Pablo Narváez"," This project is the result of the team work of the Data Unit and Storytelling team along with Javier G. Jorrín, a reporter from the Economics section, and María Zuil, a data reporter. Besides, Jesús Escudero led the data compilation and analysis of the microdata; Antonio Hernández, Luis Rodríguez and Pablo Narváez developed the data interactivity and the data loading both on desktop and mobile, and Laura Martín and Pablo López Learte were the designers and produced the graphic line of the project. ",,,
United States,The Pudding,Small,Participant,Innovation (small and large newsrooms),Data-driven videos with D3,28/03/19,"Explainer,Infographics,Chart,Video,Sports,Politics,Culture,Women","Animation,AI/Machine learning,Scraping,D3.js,Json,Adobe,Creative Suite,Google Sheets,CSV,R,RStudio,Python,Node.js"," At The Pudding, we're known for our data-driven visual essays. At first, that was synonymous with scrollytelling where we would interweave text and graphics, but in 2019 we also expanded into video storytelling. None of us were video editting experts, so we created a command line interface tool that allowed us to program in Javascript and D3.js and export to video — no extra skills required. "," The tool allowed us to build our first-ever gallery installation in partnership with the Smithsonian's National Potrait Gallery. For the 100 anniversary of women's suffrage, we examined the number of women-related words in political party platforms. The result was <a href=""https://www.youtube.com/watch?v=-DXKDw8l0wY"">this video</a> (displayed in person at the gallery exhibit in DC), and an accompanying online Spanish translation <a href=""https://www.youtube.com/watch?v=LExGIC9iaRA"">video</a>.    We have since used it to build two more videos — one on the <a href=""https://pudding.cool/2019/05/three-seconds/"">NBA's 3-second rule</a> and one on <a href=""https://www.youtube.com/watch?v=fzwrpAfMdKw&t=31s"">yearbook hairstyles</a> — and we will continue to use it for future projects. "," Our <a href=""https://github.com/russellgoldenberg/render-d3-video"">open source CLI tool</a> was built off of previous open work by <a href=""https://roadtolarissa.com/d3-mp4/"">Adam Pierce</a> and <a href=""https://github.com/veltman/gifs"">Noah Veltman</a> — a real testement to the industry's collective power. The tool generates videos from a locally running server using D3.js to control time. Javascript animations are captured, saved as individual jpegs, and then stiched together in a video using ffmpeg. "," We began building his tool in tandem with building our first video around women's issues in political party platforms. It was a trial by fire experience and we often had to rework and retool the infrastructure of the build to accomodate how we wanted to animate different elements. While that sometimes created frustrating situations where we felt like we were duplicating work, it also gave us the space to really design for all use cases. The end result is a battle-tested tool with clear documentation and examples. It's given us a new storytelling technique in our arsenal and we hope it has empowered others to wade into areas that they once thought weren't a match for their skillset. "," We're a group of journalist-engineers who are constantly trying to push the boundaries of storytelling. We know that what makes for a good text-driven story might work as a photo-driven story, or a data-driven story, or a video-driven story. This was an exercise in pacing and audience attention for us. While scrollytelling requires the user to take a somewhat active approach to digest content, videos are a more passive experience. Scrollytelling and video has a lot of overlap — in both you're leading the audience through a linear experience. With scrollytelling that often means presenting one element at a time, while in video the motion and presentation is much more overlapped. For the videos, we also had to consider things like how sound interacted with motion and how to calculate optimal reading time. We have the technical tools to seemlessly switch from medium to medium, but we should also build out our thoughtfulness in other areas. ",https://github.com/russellgoldenberg/render-d3-video,https://www.youtube.com/watch?v=-DXKDw8l0wY,https://pudding.cool/2019/05/three-seconds/,https://www.youtube.com/watch?v=fzwrpAfMdKw&t=31s,,,,"Russell Goldenberg, Jan Diehm"," Russell Goldenberg is an Editor at The Pudding. He is a fan of local currencies, and thinks you should follow his cat Smokey on <a href=""https://instagram.com/smokeylama"">Instagram</a>.   Jan Diehm is a Journalist-Engineer at The Pudding. She appreciates the finer things in life: LEGO sets, southern delicacies like pimento cheese and fried green tomatoes, and vintage Britney Spears. ",,,
Brazil,Correio Braziliense,Big,Participant,Innovation (small and large newsrooms),Correio nas Escolas,11/01/19,"Explainer,Database,Open data,Crowdsourcing,Video,Map,Satellite images,Audio","JQuery,Adobe,Google Sheets,OpenStreetMap,Node.js"," The project “Correio nas Escolas” map and tell stories of all the 234 high schools in Brazil's Federal District. The project brings together an interactive map with information on the schools; crowd-sourced accounts, photos, audios and videos of readers; timeline with reports from Correio Braziliense newspaper about education since 1960; in addition to 20 reports. I envisioned the site by participating in News Corp Media Fellowship for Global Journalists administered by the International Center for Journalists (ICFJ) in partnership with The Wall Street Journal, which is why I won a News Innovation Grant of US$ 1.000. "," The project “Correio na Escolas” has had impacts on the newspaper where I work, the education of the Federal District and the readers. This was the first time the Correio Braziliense newspaper has produced a major data journalism project and the first time it has used various data visualization tools, including Google Maps. After I produced the project, other teams in the newspaper where I work dedicated themselves to data-driven work and used some of the same tools I used.    There is a lot of official data on schools (from Brazil’s Ministry of Education and from DF’s Secretary of Education), but without an organized platform, this information is inaccessible and remote from most of the population. On these interactive site, stakeholders can identify and search by region, neighborhood, school, performance, and more. Thanks to the project map, parents were able to assess more carefully the schools where their children will study. It is interesting to highlight that the community was able to get to know high schools better through both official data and readers input: students, alumni, parents and teachers provided input and feedback on the schools.    This project turned out to be relevant for a number of reasons: Firstly, it is a great source of advice for families who need to choose where to put their child to study. The site also helped the community get to know schools better. Thus, make more appropriate choices for their children. Second, the ""Correio nas Escolas"" project generated greater community engagement with each school (which translates into the amount of reader input received). This is very important for the community itself to asks for improvements and quality in school management and also for the community itself to help to improve the schools (for example, by doing volunteer work, donating, or otherwise contributing). "," The project consists of 20 stories (16 reports on prominent public schools, using scrollytelling feature in the desktop version; eight of which were made by me, and the other eight edited by me; and 4 stories about high school challenges); an interactive map in which all 234 high schools in the Federal District were mapped and arranged with information about them (data include number of students and teachers, student income, scores on educational indicators, whether or not schools have sports fields and laboratories, among other information; the data was collected by me and the freelance journalist I hired, but the map was all generated by me); a second map highlighting the 16 schools we visited and about which we produced stories about; contributions of 58 readers in texts, photos, audios and videos (<a href=""https://correionasescolas.correiobraziliense.com.br/relatos-dos-leitores/"">https://correionasescolas.correiobraziliense.com.br/relatos-dos-leitores</a>); a timeline, gathering 85 stories from the Correio Braziliense newspaper from 1960 to the present day:<a href=""https://correionasescolas.correiobraziliense.com.br/linha-do-tempo/""> (https://correionasescolas.correiobraziliense.com.br/linha-do-tempo</a>).    The US$ 1.000 grant I received was used to hire a freelance journalist and a web developer to help me with the project. However, resources were limited. So I looked for free tools that could be used. Google Maps was used to display the result of the Excel tables that I spent two months producing, concentrating data on each of DF's 234 high schools. The Northwestern University Knight Lab's StoryMapJS highlighted the 16 schools visited by the reporting team, portrayed through text, photos and videos. Finally, Timeline JS, also from The Northwestern University Knight Lab, was used to display 85 educational reports published by Correio Braziliense. The site was developed on the WordPress platform. Reports on the 16 public schools considered good examples were displayed using the scrollytelling feature. "," I faced many challenges to put the project into practice. Education data for each school was not concentrated in one place. We had to analyze a lot of worksheets from the Ministry of Education, the Department of Education and even collect some data school by school. The 16 best public high schools were discovered thanks to the analysis of the data collected to rank schools and thus show the public what works well in the public school system. We were showing good examples, yet there was resistance from schools to receive reporting staff and resistance from other schools to provide us with data. Another challenge is that I didn't have much experience with large databases, but that was good because I could learn a lot during the process. The reporting part also required a lot of effort.   The lack of manpower is one of the biggest challenges in the newspaper Correio Braziliense and in many others. People are missing, so it is difficult to the newspaper to produce cool and relevant projects. For the website's web design, I ended up relying solely on a freelance professional hired under the funds of the News Innovation Grant, since the newspaper's developer team said it was too busy and couldn't help me. So the site was made by the web designer I recruited thanks to the grant, with my help. This project was only possible because all the time I used for his production was my free time. For eight weeks, I was doing my normal work (at least 7 hours a day and at most 9), during which I dedicated myself to the obligations I already have in the newspaper. So, out of office hours, I spent six to seven hours a day more so I could dedicate myself to the project. "," The eight weeks I used to produce the project were very intense and seeing the project ready is rewarding. I learned a lot about data collection and organization. In addition, I discovered several open tools that allow journalists to produce interesting and interactive material for internet. Most of all, I learned that data journalism and a great journalism project are possible, even in a newsroom and a team with few resources, when you commit and do your best to make it happen.     ",https://correionasescolas.correiobraziliense.com.br/,https://drive.google.com/file/d/1aDY9A6ZPzKN-EKCAg7rnX5iYVSSlOjui/view?usp=sharing,,,,,,"Ana Paula Lisboa (creation, coordination, editing, reporting, data collection and development); Thays Martins (reporting and data collection); Adalberto Sampaio (web design and development)"," Ana Paula Lisboa, author of the project ""Correio nas Escolas"", is a Brazilian journalist, graduated from the University of Brasilia. She has been working for the newspaper Correio Braziliense since 2012, where she is a sub-editor, working mainly on education, childhood and the labor market. She has won seven Brazilian journalism awards and has been a fellow of two programs at the International Center for Journalists (ICFJ). From February to April 2020, she will work as an international correspondent for the German journal Der Tagesspiegel with a grant from the International Journalist's Program (IJP).   Thays Martins has been a reporter for Correio Braziliense newspaper since 2019. Prior to that, she was an intern at the newspaper for two years. Won a Brazilian journalism award. She studied journalism at the University of Brasilia, where she is currently pursuing a degree in audiovisual.   Adalberto Sampaio holds a degree in advertising from the University of Brasilia. In addition, he is Webdesigner, copywriter & front-end developer. Has worked at Lacuna Software since 2018. ",,,
United States,The Pudding,Small,Shortlist,Best news application,The Language of Congress,13/09/19,"News application,Infographics,Chart,Politics","AI/Machine learning,D3.js"," We fed thousands of Congressional tweets to a machine learning algorithm powered by Salesforce's Einstein AI in order to recognize political issues. The tweets are categorized into 15 topic areas include environment, guns, jobs, and social issues, and then visualized nationally for <a href=""https://congress.pudding.cool/person/SenateMajLdr"">members of Congress</a>, and <a href=""https://congress.pudding.cool/issue/Health/year"">issues</a>. The project updates every day of the 116th congress, from January 3 2019 through January 3, 2021. "," Twitter is designed so that you come across one tweet at a time — often breaking news, reactive rants, and unfiltered spur of the moment thoughts — and are never exposed to larger patterns or trends. This application allows people to dig into the issues and see which issues Congress as a whole prioritizes and which issues their representatives personally favor. It uses big data to put the power in the hands of average people. "," We sought to use an out-of-the-box machine learning model to make predictions—one that could run in real-time and update each day.   This analysis builds on new deep learning, advanced language models. For this project, we used the <a href=""https://einstein.ai/products/custom-intent"">Einstein Intent API</a> to train a model to predict what issue a member of Congress’ tweet pertains to. This model was trained on approximately 3,000 tweets that were manually classified into issues by our team (i.e., a training process). Afterwards, it develops a probability that a tweet falls within a given issue.   Tweets were obtained via the Twitter API for all current members of Congress with active Twitter accounts.    The front-end is built with Javascipt and D3.js. "," The hardest part of a continually updating news application is making sure that you build out all the base infrastructure to handle as many of the future unknowns as you can. We are working with a massive and ever-growing amount of text data so it's important to make sure the framework is robust and flexible. Luckily we we working with two strong and structured APIs: the <a href=""https://einstein.ai/products/custom-intent"">Einstein Intent API</a> and the <a href=""https://developer.twitter.com/en/docs/api-reference-index"">Twitter API</a>. "," The project is able to provide real-time insights into what issues are pushed into political and public discourse by Congress. After the Global Climate March in September, we were able to see how Congress <a href=""https://www.instagram.com/p/B2mpFXOhbOz/"">responded</a> and map their tweets to news events. ",https://congress.pudding.cool/,https://www.salesforce.com/company/news-press/stories/2019/10/100419-WhatCongressTweets/,https://www.instagram.com/p/B2mpFXOhbOz/,,,,,Charlie Smart," Charlie Smart is a journalist, designer, and web tinkerer. He likes good food and good music. ",,,
United States,The Pudding,Small,Participant,Best news application,The Millennial Question,25/09/19,"News application,Culture","Scraping,D3.js,Json,Google Sheets,Python"," We’ve seen countless stories about what millennials have supposedly killed. From napkins to marriage to Applebees, just looking at headlines you’d guess that for the past decade the millennial generation's been on a rampage. But we wanted to dig deeper: how does popular media report on millennials more broadly? We combed through 12,500 to find out. "," We'd seen countless listicles of what millennials had killed, but no definitive, overall picture of how the media covers millennials. We wanted to create something that both confirmed and challenged the sterotypes the media constructs about millennials — and put that info directly in the hands of millennnials themselves. "," We used the Event Registry API to scrape news articles about Millennials. The query filtered on news articles with the word “Millennials”, “millennials”, “Millennial”, or “millennial” in the headline published between June 15, 2015 and June 15, 2019. This query yielded nearly 38,000 articles. We obtained article metadata, including the URL, title, body, and publishing date from the query. Sometimes, multiple news outlets in the same media family publish the same article; removing these duplicates yielded a total of 26,565 articles.   We used the Spacy Python package to part-of-speech tag the headline text. Part-of-speech tagging identifies each word’s part-of-speech in the sentence (e.g., a noun versus a verb versus an adverb). We filtered on articles headlines in which Millennials perform an action (“Millennials are killing the napkin industry'”, for instance). Narrowing our focus made it easier to identify the focus of their love and/or destruction. Using the newly tagged headlines, we subsetted the main dataset on headlines where “millennials” is the subject noun of the sentence, yielding 12,500 articles. Of these articles, we also removed articles with less than five sentences in the body.   The objects you can explore are the noun chunks Spacy identified as the first direct object in the headline. We opted to look at noun chunks instead of just nouns to get a complete picture of the items Millennials are interacting with. Noun chunks include adjectives plus nouns, such as “second home” instead of “home”. This method left us with about 4,000 unique nouns and 2,000 unique verbs.    We used javascript and D3.js to develop the front-end word wall experience. "," Often when people see our final projects, they assume that the development is the most consuming and most difficult part, but it's almost always the behind-the-scenes data work that you don't see that's the most challenging. Please see our answer to the tools and techniques question for a step-by-step rundown of how we wrangled all the headline text — by far the hardest part of this project. "," Millennials are media darlings. It's easy to fall into the tropes of millennials killing retail and loving avocados, but there's a tendency to filter out the headlines that don't conform to our confirmation bias ",https://pudding.cool/2019/09/millennials/,,,,,,,"Alexandra Saizan, Ilia Blinderman, Jan Diehm"," Alexandra Saizan is a data analyst living in Washington, D.C. She enjoys cats, comics and long reads on the beach.   Ilia Blinderman is a Journalist-Engineer at The Pudding. He was first an academic, then a writer, and finally, joined those two occupations in holy union through data-driven storytelling.   Jan Diehm is a Journalist-Engineer at The Pudding. She appreciates the finer things in life: LEGO sets, southern delicacies like pimento cheese and fried green tomatoes, and vintage Britney Spears. ",,,
United States,The Pudding,Small,Participant,Best visualization (small and large newsrooms),The Hipster Summer Reading List 2019,22/07/19,"Open data,Infographics,Arts,Culture","D3.js,R,RStudio"," It’s officially summer and you’re looking for your next read. But you don’t want to read what everyone else is reading. No, you want something more obscure. You’ll take the books that no one is reading. Better yet, how about the books no one has touched in years?   We (programmatically) sifted through over 100 million <a href=""https://data.seattle.gov/dataset/Checkouts-by-Title-Physical-Items-/3h5r-qv5w"" target=""_blank"">checkout records</a> from the <a href=""https://www.spl.org/"" target=""_blank"">Seattle Public Library</a> to find fiction books that haven’t been checked out in over a decade<a href=""https://pudding.cool/2019/06/summer-reading/#methods"">*</a>. Sounds like they’re just your speed. "," This project put a new spin on a publicly available dataset. Instead of showcasing the MOST checked out books, we looked at the LEAST and hopefully highlighted some forgotten gems within the Seattle Public Library's collection. "," Both <a href=""https://data.seattle.gov/dataset/Checkouts-by-Title-Physical-Items-/3h5r-qv5w"" target=""_blank"">checkout</a> and <a href=""https://data.seattle.gov/Community/Library-Collection-Inventory/6vkj-f5xf"">inventory</a> data are publicly available thanks to the <a href=""https://www.spl.org/"">Seattle Public Library</a>. You can view our processed data and the R scripts used to process the data <a href=""https://github.com/the-pudding/data"" target=""_blank"">here</a>. All rating data available comes from <a href=""https://www.goodreads.com/"" target=""_blank"">Goodreads</a>.   The front-end was built using Javascript and D3.js. "," The biggest challenge for this project was working within the data caveats, detailed below:    All books discussed in the article are fiction books that have appeared in the Seattle Public Library’s physical book inventory for the entire span of time between September 2017 and May 2019 (the earliest and latest dates available for these data) and are still available, but have not been checked out any time between September 2005 and May 2019.   Since we don’t have inventory data from 2005 - 2017, there is a chance that some of the books may have entered the library’s collection during that time span. We excluded any books that were published after 2005 to minimize this likelihood. At the absolute minimum, all the books on our list have been in the inventory and have gone unchecked out since September 2017. ", A data story doesn't have to include any traditional charts or graphics to make it successful. You can communicate data with whismy without losing integrity. ,https://pudding.cool/2019/06/summer-reading/,,,,,,,"Russell Goldenberg, Amber Thomas"," Russell Goldenberg is an Editor at The Pudding. He is a fan of local currencies, and thinks you should follow his cat Smokey on <a href=""https://instagram.com/smokeylama"">Instagram</a>.   Amber Thomas is a Sr. Journalist-Engineer at The Pudding. She spends time away from her keyboard exploring the outdoors around Seattle and hanging upside down in aerial silks. ",,,
United States,The Pudding,Small,Shortlist,Best visualization (small and large newsrooms),The Gyllenhaal Experiment,22/02/19,"Quiz/game,Crowdsourcing,Infographics,Chart,Audio,Culture","Personalisation,D3.js,Json,Node.js"," Building off of work by <a href=""https://twitter.com/HalfEatenScone"" target=""_blank"">Colin Morris</a> who <a href=""https://kottke.org/19/01/visualizing-dubious-spelling-with-flow-diagrams"" target=""_blank"">explored</a> the difficulties of spelling by identifying Reddit comments with (sp?) next to words, we wanted to visualize the variations of how people spell celebrity names like Jake Gyllenhaal and Matthew McConaughey. We later adapted this project for NBA stars.   Consider these spelling bee quizes with sankey-ish twists. "," We captured 337,492 users attempts to spell 15 celebrity names and 25,123 users attempted to spell 15 NBA star names. ", The front-end was built using Javascript and D3.js and the data backend was hosted on Firebase. ," The hardest part was building out a sankey-ish diagram that could handle the spelling permutations. Since the data was user generated, we didn't know what shape it would take and how that would effect the end visualizations. "," 1. Spelling is hard. For example, there are at least <a href=""https://www.texasmonthly.com/the-culture/matthew-mcconaughey-spelling/"">864 ways</a> to spell ‘McConaughey.’    2. People enjoy the immediate feedback they get from quizes and the ability to compare themselves to others. They enjoy it even more if there's something new, like sankey-ish spelling diagrams, waiting for them at the end. ",https://pudding.cool/2019/02/gyllenhaal/,https://pudding.cool/2019/03/nba-spelling/,,,,,,"Russell Goldenberg, Matt Daniels"," Russell Goldenberg is an Editor at The Pudding. He is a fan of local currencies, and thinks you should follow his cat Smokey on <a href=""https://instagram.com/smokeylama"">Instagram</a>.   Matt Daniels is a Journalist-Engineer and Business lead/CEO at The Pudding. He first experienced Internet fame in 2014 and has been chasing that feeling ever since. ",,,
United States,The Pudding,Small,Participant,Best visualization (small and large newsrooms),How many high school stars make it in the NBA?,27/02/19,"Infographics,Chart,Sports","Animation,Scraping,D3.js,Canvas,Node.js"," Remember NBA great Donnell Harvey? Neither do we. Despite being the #1 high school recruit in 2000—something that you think would indicate future NBA stardom—he put together a meager career in the league, averaging around 5 points per game over 5 years. On the flip side is LeBron James—the #1 high school recruit just a few years later—and we all know what he’s been up to.   This disparity got us thinking, are these top 100 recruit lists any indication of making it to the NBA, let alone becoming a star? "," The project sprouted from a central question: How many high school stars make it in the NBA? We used repitition and small multiples to address answer it once and for all from multiple angles:  <ul>  What about Top 100 High School Players?   What about Top 10 High School Players?   What about Straight-to-NBA HS Players?   What about Drafted Top & Unranked HS Players?  What about Players from Kentucky, Kansas, Duke, and UNC (top schools)?  </ul>"," Data was scraped from <a href=""https://www.basketball-reference.com/"" target=""_blank"">Basketball Reference</a> using node.js. The front-end was built using Javascript, D3.js, and Canvas. "," The most challenging part was building out the bouncing ball animations.     We wanted to make sure that the animation wasn't so overwhelming and that you could follow the dots from level to level.   We wanted to allow users to explore each dot, but only after the animation finished.   We wanted to attach some names to dots so that you could have some wayfinders.   And we wanted the animations to play, replay, and stop in an intuitive way.   ", Sometimes you don't need a traditional chart to produce compelling data viz — sometimes you just need to following the bouncing ball. ,https://pudding.cool/2019/03/hype/,,,,,,,"Russell Goldenberg, Amber Thomas"," Russell Goldenberg is an Editor at The Pudding. He is a fan of local currencies, and thinks you should follow his cat Smokey on <a href=""https://instagram.com/smokeylama"">Instagram</a>.   Amber Thomas is a Sr. Journalist-Engineer at The Pudding. She spends time away from her keyboard exploring the outdoors around Seattle and hanging upside down in aerial silks. ",,,
United States,The Pudding,Small,Participant,Best data-driven reporting (small and large newsrooms),The Rise of Hyphenated Last Names in Pro Sports,13/05/19,"Investigation,Illustration,Infographics,Chart,Sports,Culture","Animation,Scraping,D3.js,CSV,Node.js"," Last winter I was watching an NFL game featuring one of the league’s most memorably named players: <a href=""http://archive.jsonline.com/sports/packers/get-to-know-packers-safety-ha-ha-clinton-dix-b99368534z1-278873391.html/"" target=""_blank"">Ha Ha Clinton-Dix</a>. But it wasn’t his first name that caught my attention—it was his last.   The list of players whose names arch over the numbers on the back of their jerseys goes on and on: Clinton-Dix, <a href=""https://fansided.com/2018/06/29/shai-gilgeous-alexander-longest-name/"" target=""_blank"">Shai Gilgeous-Alexander</a>, <a href=""https://www.mlive.com/loons/2015/04/reid-foley_brothers_square_off.html"" target=""_blank"">Sean Reid-Foley</a>, <a href=""https://www.latimes.com/sports/usc/uscnow/la-sp-usc-juju-smith-schuster-20150810-story.html"" target=""_blank"">JuJu Smith-Schuster</a>, <a href=""https://www.sny.tv/uconn/news/kml-carries-name-to-honor-her-mother/148617506"">Kaleena Mosqueda-Lewis</a>. So I wanted to investigate: Are double-barrelled last names getting more common in professional sports? And what about overall? "," Hyphenated names are hard to study. Although athletes proudly wear their last names on their jerseys, most names are personal. The US Census collects last names, but to preserve the anonymity of individuals, only names appearing 100 or more times are <a href=""https://www.census.gov/topics/population/genealogy/data/2010_surnames.html"" target=""_blank"">released</a>. So, you get names like Smith and Johnson, but never names like Smith-Johnson. Laurie Scheuble, a Penn State professor who researches names told me that this was a first-of-its-kind analysis and that the work should be published in an academic journal. "," Player names were scraped from Baseball Reference, Basketball Reference, Football Reference, Hockey Reference, MLS, and NWSL using Node.js. Names that included “-” were tagged and manually vetted. Korean names, where the last name appears before the first name, were not tagged as hyphenated names. Players were grouped into decades by the season in which they played in their first professional game. When seasons spanned multiple years (i.e. 1979-1980), the last year was used as the decade. The front-end was built out using Javascript and D3.js. "," The hardest part of the project was figuring out how to display the names in the data viz. I knew that the names themselves needed to be there centerpiece, after all, seeing the names was what attracted me to this project in the first place. I went through several iterations with the names as data: a histogram, a network diagram, but finally settled on the animated wall of words you see in the final. ",   Something as simple as a passing thought while watching TV can turn into a data project that rivals academic research.    Sports stories don't have to be game stories or numbers-based.   Charts don't have to look like charts at all.   Important cultural trends can be hidden in everyday life.   ,https://pudding.cool/2019/05/hyphens/,,,,,,,Jan Diehm," Jan Diehm is a Journalist-Engineer at The Pudding. She appreciates the finer things in life: LEGO sets, southern delicacies like pimento cheese and fried green tomatoes, and vintage Britney Spears. ",,,
United States,The Pudding,Small,Participant,Best data-driven reporting (small and large newsrooms),Where Do Adoptable Dogs in Your State Come From?,15/10/19,"Investigation,Illustration,Infographics,Chart,Map,Culture","Personalisation,Scraping,D3.js,Adobe,Creative Suite,CSV,R,RStudio"," If you’re looking to add a new furry friend to your family, you may be encouraged to “adopt not shop”. That is, to find a new dog at a local shelter or rescue organization rather than a pet store or breeder. But where do adoptable dogs come from? We looked at the PetFinder profiles of all 58,000 dogs available for adoption across the US on a single day and found 2,460 dogs whose travel was described in enough detail to follow. This piece explores the trends and patterns. "," This project revealed trends that we didn't know existed – that adoptbale dogs often move from the south to the north. The most rewarding projects often come when you not only share something with the audience, but also when you learn something new too. This project also expanded our thinking about what you can get from traditional API calls. "," Using the PetFinder API, details about all 58,180 dogs available for adoption in the 50 US states and Washington DC on September 20, 2019 were collected. Since PetFinder does not provide an entry field for an animal’s location before arriving at its current organization, we parsed the text of each pet’s “description”. We started by limiting text to anything that came after the word “from” but before the word “to”, or after “located in”. We then analyzed the remaining text using entity recognition from the spacyR <a href=""https://cran.r-project.org/web/packages/spacyr/index.html"" target=""_blank"">package</a>. We manually checked the data for anything mislabeled. The front-end experience was built using HTML/CSS, Javascript, and D3.js. "," The data collection was the toughest part of this project. The APIs weren’t really set up to do exactly what we wanted to do, so we needed to figure out some workarounds. Then it was a process of programmatically filtering through the text (described more in the answer about tools and technologies) and manually filling in where the computer couldn't. Data work often needs that tag-team approach of computer + human, but that means the process can take longer.  "," Sometimes a simple question (Where Do Adoptable Dogs in Your State Come From?) can yield some surprising answers. We didn't expect to find such a stark geographic pattern when we first started this analysis, but you can clearly see that northern states import more does and southern states export more dogs. ",https://pudding.cool/2019/10/shelters/,,,,,,,"Amber Thomas, Sacha Maxim"," Amber Thomas is a Sr. Journalist-Engineer at The Pudding. She spends time away from her keyboard exploring the outdoors around Seattle and hanging upside down in aerial silks.   Sacha Maxim is a Seattle-based designer, musician, and proud pom-mom. ",,,
United States,The Pudding,Small,Participant,Best data-driven reporting (small and large newsrooms),Colorism in High Fashion,29/04/19,"Investigation,Infographics,Chart,Arts,Lifestyle,Culture,Women","Animation,D3.js,R,RStudio,Python"," To find out how women of all shades were represented in Vogue magazine over the last 19 years, we programmatically calculated how light a model's skin tone looked in each photograph. At a glance, you could argue that  Vogue  covers are diverse, or at least that they have gotten more diverse in recent years.But when we really look, it’s easy to see that the majority of the black women are light-skinned and the majority of dark-skinned women are actually a single person. "," The project made waves on social media, was shared by The Guardian, and spurred conversations around colorism, tokenism, and representation in the fashion industry. "," The covers were downloaded from the Vogue archive and then fed through a python <a href=""https://github.com/malaikahanda/vogue_data_collection"">script</a> that identified female faces. For each face, several k-means clustering models were fit. The clustering models varied in terms of which features were used (some combination of the rgb and hsl color values) as well as how many clusters were formed (two or three). Since the style of the covers varied so differently, <a href=""https://twitter.com/malicodes/status/1065419416136048640"" target=""_blank"">different clustering models</a> did a good job at identifying the skin. The script filtered out just the pixels that were determined by the computer model to contain skin, and calculated and stored the median rgb color, as well as the corresponding lightness value.   Prototypes for the graphics were developed in R and the final build was designed in HTML/CSS, Javascript, and D3.js. "," The most challenging part of the project was making sure that we didn't gloss over the more nuanced findings. Overall, Vogue models' skin tones have gotten more diverse, but we needed to effectively communicate that a single person, Lupita Nyong'o, was pulling the trend toward the darker end of the spectrum and that you still saw curious overlaps within skin tone ranges. For example, Anne Hathaway's darkest tone overlapped Rihanna's lightest tone. "," This project is a great example of what The Pudding does best — take a common occurrence like magazine covers and turn it into a thoughtful and thorough data-backed critique on deeper cultural issues. It's also proof that data viz by women and for women is important, powerful, and deserves space in this industry. ",https://pudding.cool/2019/04/vogue/,,,,,,,"Malaika Handa, Amber Thomas, Jan Diehm"," Malaika Handa is occasionally a data scientist and occasionally a software engineer. She likes ggplot, higher order functions, and rewatching The Great British Bake Off.   Amber Thomas is a Sr. Journalist-Engineer at The Pudding. She spends time away from her keyboard exploring the outdoors around Seattle and hanging upside down in aerial silks.   Jan Diehm is a Journalist-Engineer at The Pudding. She appreciates the finer things in life: LEGO sets, southern delicacies like pimento cheese and fried green tomatoes, and vintage Britney Spears. ",,,
United States,The Pudding,Small,Participant,Best data-driven reporting (small and large newsrooms),The Sexualized Messages Dress Codes are Sending to Students,13/02/19,"Investigation,Long-form,Infographics,Chart,Culture,Women","Scraping,D3.js,CSV,R,RStudio"," In early 2017, writer Dana Schwartz posed a <a href=""https://twitter.com/DanaSchwartzzz/status/846065368997212161"" target=""_blank"">question</a> to the Twitter-verse: ""Ladies, when was the first time you were made to feel embarrassed and sexualized for what you wore?"" In our analysis of 481 school dress codes, we found 37 banned items that accentuate the body. 57% are <a href=""https://pudding.cool/2019/02/dress-code-sexualization/#methods"">primarily marketed in stores</a> to girls, 38% to any gender, and only 5% are marketed primarily to boys. Rules that prohibit specific items due to their perceived sexiness (e.g., ""short shorts"", ""sheer clothing"" etc.) impact students wearing clothing marketed to girls more than their peers. ", This project was shared widely by high schoolers on social media. We were even contacted by one student who had started a national campaign and survey to address dress code sexism. This project provided data evidence to confirm the lived experience of most female or female-identifying students. ," Using the National Center for Education Statistic's (NCES) <a href=""https://nces.ed.gov/ccd/schoolsearch"">search function for public schools</a>, we collected a list of over 8,000 public high schools in the US. We limited the resulting schools to just those that had a web address listed in NCES. Accessing the homepage web content from each site, we searched for words like ""handbook"", ""dress code"", and ""code of conduct"", filtering my list of schools to just those that contain one of the above phrases. Then, we manually visited the resulting (2,000+) websites in an attempt to find the dress code. Dress codes were disqualified from analysis if they: had a uniform policy, were not from the 2018-2019 school year, or represented magnet or boarding schools (according to the NCES).   Once the list of dress codes was compiled, we manually recorded every rule listed in each dress code, the words used in the dress code's rationale, as well as any listed sanctions for breaking the dress code. Only items and body parts that were explicitly listed were included in this analysis. Some minor subjectivity came into play when combining similar phrases from different handbooks (ex. ""midriff shirt"" = ""crop top"", ""shirt with open sides"" = ""muscle shirt"" etc.).   The data was then analyzed in R and the front-end was built out using HTML/CSS, Javascript, and D3.js. ", The hardest part was the data collection. Much of it was manual because school handbooks are not uniform and not easily accessible online. ,"   If you put in the work, you can create a robust dataset where one has never existed before.   Don't be afraid to tackle large data projects with a mix of code and manual labor.   If you're dealing with a complex topic, but in the research to make sure you are presenting it responsibly.   ",https://pudding.cool/2019/02/dress-code-sexualization/,,,,,,,Amber Thomas, Amber Thomas is a Sr. Journalist-Engineer at The Pudding. She spends time away from her keyboard exploring the outdoors around Seattle and hanging upside down in aerial silks. ,,,
United States,"The Pudding, The Marshall Project, The Chicago Reader",Small,Participant,Best data-driven reporting (small and large newsrooms),The Kim Foxx Effect,24/10/19,"Investigation,Multiple-newsroom collaboration,Infographics,Chart,Politics,Crime","Animation,D3.js,R,RStudio,Python"," This project provides the first detailed look at the more than 35,000 cases that flow through Cook County Prosector Kim Foxx’s office every year. We found that since she took office she turned away more than 5,000 cases that would have been pursued by previous State’s Attorney Anita Alvarez, mostly by declining to prosecute low-level shoplifting and drug offenses and by diverting more cases to alternative treatment programs. ", The project was widely shared throughout the social justice community (the ACLU and Deray McKesson tweeted it). The piece reached the target audience and proved that data journalism can be a catalyst for action and polocy change. ," We began this project with a simple question: what is the human impact of electing a prosecutor who runs on reform? To try to assess this, we focused on two measures—charges and dismissals—that we can examine through the unprecedented <a href=""https://www.cookcountystatesattorney.org/about/case-level-data"">case-level data published regularly by the Cook County State's Attorney's Office</a>.    The data was analyzed using both R and Python. The front-end was built using HTML/CSS, Javascript, and D3.js. "," The hardest part of this project was making sure that our data work was backed up by reporting. When analyzing a data set about people, you want to make sure you treat it with rigor and respect and not lose sight of the fact that there are people behind the numbers. Since this was the first detailed analysis of this data, we had to get it right. We had several experts check the data work and methodology and partnered with topic and local experts like the Marshall Project and The Chicago Reporter to co-publish. ",   There are robust stories to be told with public data.   Go in with a clear driving question and rely on the expertise of others to inform your reporting.   Clearly communicate the data caveats and shortcomings.   ,https://pudding.cool/2019/10/prosecutors/,,,,,,,Matt Daniels, Matt Daniels is a Journalist-Engineer and Business lead/CEO at The Pudding. He first experienced Internet fame in 2014 and has been chasing that feeling ever since. ,,,
United Kingdom,BBC News,Big,Participant,Best data-driven reporting (small and large newsrooms),100 Killings,03/08/19,"Investigation,Explainer,Solutions journalism,Long-form,Multiple-newsroom collaboration,Database,Fact-checking,Illustration,Infographics,Crime","Google Sheets,CSV"," Throughout 2019 a team of journalists recorded details of every single homicide in the UK as and when they happened by creating and maintaining a comprehensive and detailed database. The aim was to find out why the murder rate was on the rise and what were the factors behind each death. Interwoven with testimony from trials, interviews with the families and friends of victims and killers, with police, lawyers, criminologists and politicians was data gathered through Freedom of Information requests. It presented a disturbing picture of violence in the UK and explodes the myths supporting so much contemporary stereotypes. "," No other media outlet, or indeed the Office of National Statistics, has captured data of this kind from across the UK. With data at the heart of all the stories we combined it with powerful reporting on cases and crimes which often attracted too little attention in a news agenda dominated by Brexit in 2019.   Each part of the project featured on many parts of the BBC including the flagship radio programme 'Today' and also made the 1800 and 2200 national news bulletins. Various regional packages were made for different parts of the UK to bespoke the data and cases to each area. The data and stories were often picked up by other national newspapers and broadcasters.   According to Chartbeat, the online articles maintained strong engagement and, by the BBC's own Telescope metrics, accrued over 3m unique page views. The work attracted praise from criminologists and comments from senior UK government officials including the Home Secretary and then Policing Minister.   In December 2019 the project recieved a <a href=""https://www.pressgazette.co.uk/british-journalism-awards-2019-ft-wins-top-prize-for-second-year-in-triumphant-end-for-departing-editor/"">British Journalism Award in the Crime and Legal Affairs category</a>. The judges said: “The subject area is one that lends itself to stereotypical responses, like the idea that this was mainly people in their teens stabbing each other. This comprehensive data analysis paints a far more in-depth picture of what is happening.” "," A Feedly account was set up which gathered every press release from the UK's 46 police forces and this was then filtered to display only the cases which included the words murder, manslaughter or infanticide in. Details were logged in a separate Excel spreadsheet and then a Google spreadsheet which had formulas which made it easy to see how many homicides there had been, details of every victim, their age, the police force involved and the causes of death as well as information about subsequent arrests, charges and trial dates.   As this was a unique dataset we needed to get information about previous years statistics, so FOIs were sent to each police force to get this historical data. Again, details were split into the causes of death, location and whether domestic violence or gangs were a factor in the homicide. That information would then be put into a separate spreadsheet and provided the 2019 with an official comparison for each part of the project when needed.   Locations of each death were also taken and coordinates were gathered and put into the Google spreadsheet. This allowed for an interactive map to plot each homicide and gave the reader a far better understanding as to where the homicides were occurring. Photos were also collected from each police force of each victim and used to create the interactive 'Facewalls' which visualised the stories of the people to be killed in 2019 in the UK.   The project did not just draw on raw data journalism but it also benefited from old-fashioned courtroom reporting which allowed for us to explain how complicated the circumstances were behind many of the 2019 homicides. "," The project was split up into different parts, the first 100 homicides, the first 100 fatal stabbings, domestic violence killings at a five-year high and the end goal of a big piece reviewing and analysing the whole of 2019 - although this was published in early 2020. The database also regularly provided many parts of the BBC with up-to-date homicide statistics - often bespoke to certain regional areas.    All that considered, it gave us our main difficulty as we knew that we would find that the status of a homicide investigation can change over time. For example some turn out not to be homicides - and police forces tend not to proactively put this information out. So, it meant in the lead up to every part of the project about to go to broadcast - the data would have to be forensically double-checked with each of the 46 police forces to make sure the figures, names and homicides we had recorded were correct, if any had been downgraded or indeed if any had been missed.   Some police forces were more helpful than others and it often took weeks to get the entire dataset checked. It involved tireless efforts from members of the team to call and email police forces who had not responded as well as managing the editorial team to make sure graphics, maps and information were all up to date.   The team's working was so rigorous that once each of the parts of the project were published and broadcasted - not once did any police force complain or say any of the published information was incorrect. "," The biggest thing this project achieved was that it changed public perception in that not all murders in the UK were linked to gangs, drugs and teenagers. London often dominates the headlines, but the project showed that other parts of the UK have just as complicated issues to deal with when it comes to stopping the rise in homicides.   With original raw data gathering it adds real strength to any public perceptions which could be challenged or indeed proved. Whether that be politics, American shootings, transport, climate change or housing - public issues can all benefit from up-to-date raw data if there is a team which is forensic enough to stick with it.   But, while the data is importantly gathered it is crucial that any data is double-checked so to be sure not to mislead. Advice from the team would be to think way ahead of any broadcast dates so that nothing is delayed.   While, the gathering of the data was crucial for all aspects of the project, but what it also benefited from was getting out of the office to listen to the real circumstances behind the deaths. The team found dozens of trivial disputes with tragic outcomes brought about by clouded, spaced out decision-making - which again was only possible by members of the team covering for each other at times and being given the time to not be behind a computer screen.   The comprehensive data analysis provided a breakdown for each police force too and we could see which areas were seeing a drop in homicides - not just an increase. We were able to reveal what some forces were doing differently to others and what some have since taken onboard. ",https://www.bbc.co.uk/news/uk-47476217,https://www.bbc.co.uk/news/uk-49459674,https://www.bbc.co.uk/news/uk-48186035,https://www.bbc.co.uk/news/uk-england-48208208,https://www.youtube.com/watch?v=h0fl01WILGM,https://www.bbc.co.uk/news/uk-50855299,https://www.dropbox.com/s/xcwq3ze1arikd0f/100%20Killings%20Radio4.mp3?dl=0,"Thomas Mackintosh, Steve Swann, Jodi Law, Danny Shaw, Tom Symonds, Sarah Lee, David Brown, Wesley Stephenson"," Thomas Mackintosh - 26-year-old journalist working for BBC London. Thomas co-led the project with Steve Swann and spent three months on a sekondment to the specalist Home Affairs department. Thomas was responsible for the data gathering and attending dozens of court trials across the UK in the summer.    Steve Swann - Senior investigations producer with the BBC's Home Affairs department. Steve co-led the project with Thomas and was responsible for gathering and recording interviews which were used alongside the data to tell each story. Steve also attended dozens of court trials across the UK.   Danny Shaw - Home Affairs correspondent who was responsible for providing analysis and packages for the BBC's flagship Today programme.   Tom Symonds - Home Affairs correspondent who was responsible for presenting the TV packages which led the main BBC News bulletins. Tom interviewed most case studies for the packages.   Wesley Stephenson - Journalist with the BBC's Visual Journalism Department, Wesley designed the interactive Carto map in the first two online articles and also designed the graphics for online and television which illustrated the findings.   David Brown - A journalist with the BBC's Visual Journalism Department, David helped design the interactive 'Facewall' which was used in the first '100 homicides' piece and the 'first 100 fatal stabbings' online article.   Jodi Law - Journalist based in Nottingham helped maintain the databse at the start of 2019 and before the launch of the first 100 killings online article. In April Jodi went on maternity leave.   Sarah Lee - 27-year-old journalist with BBC London who helped out with the fact checking shortly before launch of the piece into domesic violence and also helped maintain the database towards the end of 2019 and attended a few court trials in the summer. ",,,
Brazil,"Infoamazonia, ICFJ, The Intercept Brasil",Small,Participant,Best data-driven reporting (small and large newsrooms),Mined Amazon: Brazilian agency is allowing extractive activities within protected areas,21/11/19,"Investigation,Multiple-newsroom collaboration,Database,Open data,Infographics,Map,Politics,Environment,Corruption","Scraping,Json,CSV,Node.js"," Mined Amazon is a data project sponsored by the International Center for Journalists (ICFJ) and published by Infoamazonia. The real-time map shows illegal mining requests within protected areas of the Amazon.   Every day, Mined Amazon bot cross-check data from the National Mining Agency of Brazil and ICMBio, the Brazilian national parks service, and then tweet on @amazonia_minada when a new request is made by a big mining company.   Until 21st November of 2019, the Mined Amazon database has founded 441 records os illegal mining applications within 31 protected areas in the Amazon.   "," The Intercept Brazil published a history using the Mined Amazon database. The report tells who are the owners of these illegal mining requests: big companies, offshores, public employees and politicians want to explore the Amazon soil.     Two weeks after we publish our history, Brazilian prosecutors filed a lawsuit condemning the severity of illegal mining within protected areas and on indigenous lands in the Amazon.      ", We use mining requests data in GeoJSON and cross-check with the integral protected areas of the Amazon. The results are shown in a real-time map. And we created a Twitter bot to inform in Portuguese and English new illegal mining requests.  ," The hardest part of the Mined Amazon project was to find a way to monitor real-time mining records in protected areas of the Amazon. Our bot analyzes more than 50,000 records daily and shows about 500 that are effectively illegal under Brazilian environmental law.   The Twitter Profile @amazonia_minada is the way we found to simplify the data obtained and to inform the population and journalists daily about new illegal mining requests. "," With Mined Amazon, citizens, researchers and journalists can daily track illegal mining records and charge Brazilian authorities to cancel these records.   Mined Amazon on Github: https://github.com/InfoAmazonia/amazonia-minada   Mined Amazon on Twitter: https://twitter.com/amazonia_minada ",https://infoamazonia.org/en/2019/11/mined-amazon-brazilian-agency-is-allowing-extractive-activities-within-protected-areas/,https://theintercept.com/2019/11/21/vale-politicos-minerar-areas-proibidas-amazonia/,https://twitter.com/amazonia_minada,https://www.icfj.org/our-work/2019-news-innovation-grant-projects,https://github.com/InfoAmazonia/amazonia-minada,https://infoamazonia.org/pt/news/#!/map=49,,"Hyury Potter, Rodrigo Brabo, Juliana Mori, Gustavo Faleiros and Amara Aguilar."," I 'm a journalist since 2005. I studied journalism at the State University of Pará, in the Amazon region of Brazil. I worked with television news production and press reporting. Since 2011 I live in the south of Brazil. In my latest job, on the Diário Catarinense newspaper, I won the ""Investigative Journalism Award 2015 RBS Group"" for a series of reports that shows public spending in the state parliament of the state of Santa Catarina. In 2017, I made another special report of data journalism on this topic, analyzing more than 67,000 political trips and servants of the Legislative Assembly. In recent months, I have also written investigative reports on the implementation of ""Car Wash Operation"" and the Chapecoense airplane crash.   In 2018 I took a professional internship at Deutsche Welle, in Bonn, where I stayed for six months and had the opportunity to write about the Brazilian presidential elections. Since my return to Brazil in November 2018, I have worked as a freelance reporter and published reports on BBC Brazil, DW Brazil, and The Intercept Brazil.    In October 2019 I won with Mined Amazon Project one of the five News Innovation Grants of News Corp Media Fellowship for Global Journalists, organized by ICFJ and sponsored by The Wall Street Journal. ",,,
Taiwan,Apple Daily Taiwan branch,Big,Participant,Best data-driven reporting (small and large newsrooms),Land Injustice: Overpriced Farms in Taiwan,03/11/19,"Investigation,Long-form,Multiple-newsroom collaboration,Infographics,Video,Map,Environment,Agriculture","Animation,D3.js,JQuery,Json"," By compiling the big data of over 120,000 farmland transaction records and doing grand scale field survey, Apple Daily's exclusive report has discovered the truth of farmland speculation in Taiwan. We found that Taiwan's average farmland price in 2018 is as high as 1.6 million US dollars per hectare, which is 14 times higher compared with Japan and 157 times compared with the United States of America. As a result, the farmlands in Taiwan have been dubbed as “the diamond farms,” because they are too expensive to buy for young farmers. "," The report has raised the public's awareness to this issue. Several Taiwanese environmental protection groups stressed that farmland should not be a real estate commodity. They emphasized that the farmlands are indispensable to keep Taiwan a food self-sufficient country.   After the report was published, the president of the council of Agriculture, Taiwan's Executive Yuan, was forced to confess that the farmland price in Taiwan is“the most expensive around the world, which makes it impossible for people to buy them.”He has promised to fix the problem by regulating the farmland transaction, and to provide more lands for young farmers to rent. "," The four main tools we used in this feature story are Animation, D3.js, JQuery, and Json. We used them to construct the primary structure of the website, the map of Taiwan and the farmland price data we wanted to highlight.   Besides, we designed a small interesting quiz by using the tools. It’s an interactive one, which readers can try to pick a right answer and get response. ","  First of all, it is not an easy task to compile and analyze the big data of   120,000 farmland transaction records. An engineer helped us to gather the necessary information from the official database, and analyzed which cities and townships have the highest growth of farmland price. Then, we highlighted these places on the map, in order to let the readers understand the issue clearly.      After that, we had to figure out why the prices soared so much in those areas. However, it was an arduous job for us to do field survey, since our subject was related to the farmland prices around different places in Taiwan.      To find out the truth about farmland price in Taiwan, six investigative journalists of Apple Daily have interviewed more than 30 farmers and senior agricultural scholars in four main areas in Taiwan, including the north, the central part, the south and the east coast.      By interviewing people around Taiwan, we were able to figure out the reasons behind the phenomenon. To sum up, the most difficult part of this feature story is that we had to compile big data and do massive field survey to discover the truth.   "," By reading our feature story, the readers would be able to learn that the farmland prices in four areas change with respective reasons. For example, the farmland price rise in Northern Taiwan is driven by the local government's land acquisition, but in central Taiwan, it is driven by the construction of illegal factories on the farmland. Additionally, in the east coast of Taiwan, the high price is driven by the constructions of lavish cottage.    Also, the readers could search information on a map we have compiled. The map shows every township's farmland price change, so the readers could check on his or her hometown.   ",https://tw.inv.appledaily.com/farmland/,https://www.youtube.com/watch?v=x-ZjYrGcL1Q&feature=emb_title,https://www.youtube.com/watch?v=dS913QNVmsI&feature=emb_title,https://www.youtube.com/watch?v=TmdJQD2YPeg&feature=emb_title,,,,"Po Chun Ho, Wei Chou Chen, Liang Ju Hou, Yi Jing Wu, Ting Jen Chen, Huan Cheng Lin, Charles Wang, Kristi Hu, King Hsueh, Tiffany Keng"," We are a multi-newsroom team with 10 people that focuses on issues such as human rights, environment, and land justice in Taiwan. Six investigative journalists are responsible for doing big data research and field survey. By traveling to every corner in Taiwan, we are able to discover the truth of different issues. In addition, three visual designers and one software engineer from another news-desk help us to produce interactive feature story website.   As a result, the team can present news contents to readers through various multimedia tools such as video, small quiz or game, and interactive infographic. The team has won several international awards in recent years, including Asian Digital Media Awards, SOPA awards, and Human Rights Press Awards.             ",,,
Netherlands,RTL Nieuws,Big,Shortlist,Innovation (small and large newsrooms),ADAM: The Automatic Data Article Machine,14/11/19,"Open data,News application,Chart,Map,Environment,Crime","Personalisation,JQuery,Json,Microsoft Excel,Google Sheets,CSV,Python"," RTL Nieuws is a national news broadcaster. We bring stories that are engaging and relevant to our audience. To help us do this, we developed our own news robot, ADAM, the Automatic Data Article Machine. ADAM can generate many stories from a single dataset, for every city, neighbourhood, school etc. So far, we have used ADAM to write about traffic accidents and crime. ADAM generated 5.000 unique – and well-read – stories, customized for the 2.462 towns in The Netherlands. This allowed us to reach our audience on a local level and has already directly impacted local politics. "," The articles that we have created with ADAM so far, have been customized for the almost 2.500 towns in The Netherlands. Each time, within 24 hours after publication, we see that almost all of these articles have been read, from large cities to tiny villages. We see readers sharing the results that ADAM has generated for the place where they live and deeply engaging with this information. Readers are seeing confirmation of their own observations and even use this to influence local politics. Council questions have been submitted in several municipalities referring to an article that was written by ADAM.   ADAM has also been of use to local news outlets, who have shared our findings and created their own follow-up stories. By visiting dangerous crossroads we’ve found, trying to find explanations for an increase in crime or questioning their local government. While many local outlets do not have the capacity of hiring specialized data journalists, through ADAM we can create data stories on a local level. This way, we’re not competing with these outlets, but supplementing and aiding their work. "," Every story we publish is the result of a larger data investigation, which is usually done using Excel or Python. For the actual ADAM project, we developed a backend and user interface (UI) to enable journalists to publish large sets of data driven articles.   The UI enables journalists to upload datasets and to write templates for their stories. Data journalists use Python code within the text of their templates, to write conditions and make calculations. Pandas is used to manipulate the data within the UI. Furthermore they can use the UI to design and create unique infographics for there stories. Under the hood, we use Python to build a backend to process and analyse the data and to convert those into a collection of unique articles, including graphs, tables and images.   Both the backend and UI run in the cloud and are easily scalable. After the template is created, the text and required data visualisations for each story are generated and saved on an ElasticSearch server. Our API connects the generated content with our CMS, our website and our app to display these stories. For the end user, an ADAM story looks just like any other article we publish, including fonts, graphs, images and tables. "," A strong prerequisite in this project was creating a tool that goes beyond simple fill-in-the-blanks templating. With ADAM, titles, graphs and entire paragraphs can be changed based on our source data. This ensures all generated articles are truly unique and relevant, while looking and feeling like any other human-written article.   In the end, the success of ADAM is defined by how useful it is to our audience. The simpler and more seamless ADAM seems to a reader, the better we can serve this goal. But this requires a lot of advance thinking, both during the data analysis and while writing templates.   We also wanted to create a reusable system that we can continually use to distribute the results of our data investigations.   A very important issue for us was figuring out the journalistic ethics of automated journalism. It is crucial to warrant the accuracy of every single generated story. To achieve this, it’s essential to us that journalists that use ADAM were involved in the development, so they understand how it works and what the limitations are. This way we can avoid a ‘black box’ and stay in complete control of the output. We’re also transparent in our use of ADAM, explaining what it is and how it works in every article written by it.   We have researched the use of AI technologies such as Natural Language Generation in ADAM, but even a  nearly  perfect algorithm is not good enough for this purpose. "," We are not the first news outlet to experiment with automated journalism. We have been able to learn from what others have tried and hope to bring the development of this field another step forward.   The value of RTL Nieuws lies in a strong connection with our audience. ADAM shows how powerful it is to tell news stories that are close to the lives of readers and viewers. This can be a lesson for many journalists, especially with regard to data. Numbers can feel very abstract or complicated. The best way to make people care is by translating data to stories that hit close to home – literally, in the case of ADAM.   ADAM is a demonstration of how technology can help journalists with limited time and resources reach and serve large audiences.   We have received a lot of interest from other media in The Netherlands and abroad. We have given interviews in national newspapers and given presentations to other journalists, who are keen to learn from our experiences developing ADAM. ",https://translate.google.com/translate?hl=en&sl=nl&tl=en&u=https%3A%2F%2Fwww.rtlnieuws.nl%2Fnieuws%2Fartikel%2F4914671%2Fadam-robot-nieuws-journalistiek-innovatie-rtl-nieuws-google-dni&sandbox=1,https://www.rtlnieuws.nl/nieuws/artikel/4959941/criminaliteit-adam-bedreiging-mishandeling-inbraak-diefstal-drugs-wapens,https://www.rtlnieuws.nl/nieuws/nederland/artikel/4917371/adam-robot-verkeer-kruispunten-woonplaats-zoek-ongelukken,https://www.rtlnieuws.nl/sites/default/files/content/documents/2020/01/16/www.rtlnieuws.nl_zoek-het-op_datavisualisatie_2019-criminaliteit-misdrijf-inbraak-drugs-wapens-politie-veiligheid_0300fb6debd43ef2b34a64e5e1973e23_arnhem.pdf,https://www.rtlnieuws.nl/sites/default/files/content/documents/2020/01/16/www.volkskrant.nl_columns-opinie_de-robotjournalist-schrijft-stukjes-maar-kan-gelukkig-nog-niet-denken_b7783fc7_%20%282%29.pdf,https://translate.google.com/translate?hl=en&sl=nl&tl=en&u=https%3A%2F%2Fwww.villamedia.nl%2Fartikel%2Frtl-nieuws-robot-adam-maakt-van-data-lokaal-nieuws,,"Jasper Bunskoek, Wouter van Dijke, Rana Klein, Lara van Zuilen"," Jasper Bunskoek and Wouter van Dijke are data journalists on the investigative team at RTL Nieuws. They work on data investigations on a broad range of subjects, such as crime, food safety, housing, education and finance. Together with the reporters on the team they produce these investigations for TV and online.   Jasper launched - together with RTL's lead data scientist Daan Odijk and former chief digital Mireille Derks - the idea for creating ADAM. To develop these plans, RTL recieved funding from the Google Digital News Initiative.   Rana Klein is the data scientist on our team. She joined RTL specifically to help create ADAM and has programmed ADAM from scratch. Together with colleagues form her team (Data Intelligence) she deployed ADAM on the cloud and developed the API that links ADAM to our regular CMS.   Lara van Zuilen is a web developer and together with her team (multi-site publishing team) she build the connection from the API to the CMS, the RTL website and our apps (ios and android). ",,,
Sweden,Altinget Sverige,Small,Participant,Open data,Legislative Decision Chain: Beslutskedjan,19/03/19,"Solutions journalism,Database,Open data,Illustration,Politics","AI/Machine learning,Scraping,JQuery,Json"," Via open data from the Swedish Parliament, and scraping of the Swedish government homepages, ""Beslutskedjan"" collects, reformulates and package the entities of the Swedish legal desicion chain in a single view in a constantly updated visualisation and easily navigable taxonomy. The data is organised on a timeline, that is incorporated in all the journalism about legislation in the organisation (and vice versa). "," ""Beslutskedjan"" is a visualisation of the entire legal process in Sweden from the research stage in the government to the final adopted law. No other place in Sweden you will find this entire process on display digitally. Beslutskedjan gives every law proposal its own ""chain"" or timeline. The timeline also includes the journalism of Altinget and connects every other editorial elements that is relevant for the specific law proposal or ""chain"".    The impact of Beslutskedjan is that it is bringing the Swedish democratic machine room closer to our readers; readers who are working with og very interested in politics. Since Beslutskedjan is autoupdated with new data, it will be helpful for legislators and political actors - as well as interested citizens - an ease their engagement in democracy continuously.     "," The technical process wasn't anything cryptic. The challenge of the project was mainly to understand the open data and to find a logic method where the complex legislative proces in Sweden match an automated technical structure, and to visualise it in a logical way, where readers easily can navigate and decode what is going on.   We started by mapping the data and the legislative process and hence we identified the best suited way to tap the open data, RSS feeds or scrape government pages. From here we started making autogenerated content. Then we tried to connect the dots partly with machine learning, and then visualise it through html and javascript.   Tools: Machine learning, Webscraping, regex, RSS feeds, C#, SQL, html, javascript. automation. "," Firstly, it has taken a great deal of work to map and interpret the available data. Even if the data is open (at least for the Swedish Parliament) it does not always fit into the imagined purpose of the project. At som stages - mostly in the government data - there were some missings in the data, that made it necessary to make complex scraping coding to get the relevant information. And we have lobbied the Swedish parliament data-team.   It was also a challenge to develop a logical visualisation to display this complex data. Finally, every law proposal got its own vertical ""chain"" or timeline with all autogenerated legislative elements from the open data on the left side of the line. The timeline also includes the journalism of Altinget and connects every time other editorial elements is relevant for the specific law or chain. These are displayed to the right. All elements have a fixed date on the timeline to secure chronological overview.   Since Beslutskedjan is connected to all relevant articles at Altinget, we also had to find a visualisation that could be embedded in a simple display. This format did not at all fit to the concept of the vertical chains/timelines. Therefore, we had to develop a separate horizontal view to be embedded in the articles.   Another challenge is that the data is not stable. In government and parliament, people replace each other regularly and the whole setup changes every 4th year. But it’s worth the effort - at least to us. Beslutskedjan is highly appreciated among our readers and ask for more functions to be included. Next, we will introduce notifications on each chain, for the readers to stay oriented on a certain issue. Now we are developing a Danish version too.  "," Working with open data on live basis can make very valuable journalistic products, but it is important to plan maintenance costs of the systems as data sources change over time.    Another learning is that this type of content gives special value when it is broken down into smaller elements or views to be incorporated in the regular journalism. I. e. when a story on a certain law also contain the entire legal process as an ""illustrative element"" give great editorial value - and also contributes to a wholistic interpretation of the media as the article is more than the displayed text or content. - It's also part of something bigger. In our case a ""chain"" in ""Beslutskedjan"" (helps to both acquisition on open content and retention on closed).     ",https://www.altinget.se/decisionchain,https://www.altinget.se/artikel/beslutskedjan-verktyget-som-foljer-lagstiftningen,https://www.altinget.se/decisionchain/oversyn-av-mojligheterna-till-foraldraledighet-for-statsraad#proposition,,,,,"Altingets project team: Kasper Kaasgaard, digital editor, Christopher Bjerre, developer, Kristoffer Hecquet, Head of Projects, Amanda Bergland, project developer, Martin Lyngbæk Olsen, data journalist. J++ team: Leonard Wallenting, data journalist."," Kristoffer Hecquet, Head of Projects: Project management   (At Altinget since 2010 in project development)       Kasper Kaasgaard, digital editor: Koordination and editorial develoment   (Journalist at Altinget and fellow at Constructive Institute)       Christopher Bjerre, developer: Technical implementation of this prokect   (Developer of many different projects at Altinget. Background in machine learning)       Amanda Bergland, project developer: Reseach, datamapping, projekt develoment, copywriting   (Working with project development and translation at Altinget since 2017)         Martin Lyngbæk Olsen, data journalist: coordination and project develoment   (Working with datajournalism and voting advice application at Altinget since 2017)       J++ team: Leonard Wallentin, data journalist: helping out with datamappng, scraping and concept development   (Data journalist and concept developer at the Journalism ++ network) ",,,
Dominican Republic,Diario Libre,Big,Participant,Best data-driven reporting (small and large newsrooms),Patrulla Letal (Lethal Patrol),11/12/19,"Investigation,Explainer,Long-form,Database,Illustration,Infographics,Chart,Video,Map,Audio,Corruption,Crime,Gun violence,Human rights","Animation,Scraping,D3.js,JQuery,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets"," Patrulla Letal (Lethal Patrol) is an investigation that strips the hidden part of the deaths in the so-called shooting exchanges between police and civilians in the Dominican Republic.  It includes an unpublished database, prepared from scratch for the investigation. In this, almost 2,000 cases of people who since 2004 were killed by the Dominican police and other security forces are organized and presented, with their credentials to details of any judicial process, if any. The data gave rise to 33 cases of victims whose deaths were questionable. Likewise, details of police corruption in this regard are revealed in the investigation. "," When presenting the database and the findings in a multimedia special, the impact of the research has been overwhelming. In a society with individuals who support the “hard hand” of the police to eliminate criminals, the evidence presented to question many of these cases is overwhelming and allows us to estimate in its magnitude a problem that is usually taken as a response from the authorities to crime.  For the first time in our small country, a media deepens and presents in a special the magnitude of the problem of human rights violations in many deaths reported as shooting exchanges. Likewise, for the first time a former police chief reveals how the corruption chain operates inside the Police to organize promotions and police themselves tell how they killed offenders instead of making them available to justice.  The families of the victims found with the investigation a way to demand justice and to forget the cases of their relatives killed by a system with serious weaknesses.  Former deputy and political leader Minou Tavárez Mirabal dedicated an <a href=""https://www.youtube.com/watch?v=OD0Bfaj7wsM&amp=&feature=youtu.be"">editorial in a radio program</a> demanding response from the authorities to the results of the investigation.  The team of journalists was invited to a television program to talk about the findings. Also, the National Human Rights Commission reported that it will use it for its annual report and as a mechanism for continuous consultation, and cited it in a <a href=""https://translate.google.com/translate?rlz=1C1CHBF_enDO748DO748&um=1&ie=UTF-8&hl=es&client=tw-ob&sl=es&tl=en&u=https%3A%2F%2Fwww.diariolibre.com%2Factualidad%2Fciudad%2Fcomision-de-los-derechos-humanos-registra-80-muertes-extrajudiciales-en-2019-EK15791423"">statement</a> sent to the press deploring extrajudicial executions.  The investigation was republished by the Colombian newspaper <a href=""https://www.elespectador.com/noticias/el-mundo/cuando-la-policia-mata-el-caso-de-republica-dominicana-articulo-891559"">El Espectador</a>, the Mexican media <a href=""https://translate.google.com/translate?hl=es&sl=es&tl=en&u=https%3A%2F%2Faristeguinoticias.com%2F1411%2Fmundo%2Fcuando-la-policia-mata-el-caso-de-republica-dominicana%2F"">Aristegui Noticias</a> and <a href=""https://www.connectas.org/especiales/patrulla-letal/"">Connectas.org</a>. ","In the absence of an official database with the more than 3,000 people that the police report as dead since 2004 by their agents a in service, we set out to prepare one with the information published by the press. Thus, for four months we worked hard to build a database in Excel with different fields, ranging from the date of death, credentials of the victim, until there was any judicial process and conviction against the agents. Once the cases were compiled from 2004 to August 2019, we proceeded to share the Excel with a programmer and web designer who took care of bringing the database to a way that could be visualized and navigated using filters. Simultaneous to this, we took 33 cases from the database that merited investigation and would be a representative sample of how the police operate. In each case a video was made with interviews with the families and details of the judicial proceedings, and a video gallery was created. We innovate with a comic that recreates scenes told by interviewed policemen in which they explain how they killed suspects and reported the fact as an exchange of shots. The special was published in November 2019 with 1,844 victims in the database. A few weeks later we added more cases and the number increased to 1,872. The objective is to continue updating the database as new deaths are reported and the project continues. Soon will be an update. From the database we extracted a secondary with more than 100 victims wanted for committing a crime against a policeman and develop a story with this approach. The tool has preponderant places in the special web and is public. It becomes a contribution to society, which can be consulted for personal interest, for academic work or country"," The most difficult part of this project was to review hundreds and hundreds, if not thousands, of journalistic publications since 2004 and tabulate them based on the criteria we established for the database. Also track if there were videos of the event reported.  It was a very hard job that initially took four months and involved dedicating work hours, free time in our homes, weekends and holidays. It was very tiring.  This is followed by a risk field work that involved finding relatives in 33 specific cases and finding their files in justice. This led us to move to violent neighborhoods in the Dominican capital and cities in the interior of the country. In these tours, looking for information, we are in the middle of microtraffickers who sold or packaged drugs for marketing. Also, an individual threatened us with a knife, and several people claimed in a threatening attitude why we inquired about certain victims.  We also had to apply source protection techniques, since we were dealing with a sensitive issue that involves people with power or who were being accused of crimes.  We can also mention that it was complex for web programmers to ensure that the database could be viewed in its entirety and navigated in its entirety, but we made it!    We overcome the risks and fatigue to present the magnitude of a harsh reality that many citizens take as normal, but that involves abuse of power, corruption and violation of human rights.  <h4>Due to the importance of the project, it was republished by foreign media such as the Colombian newspaper El Espectador, the Mexican media Aristegui Noticias and our allies Connectas.org. This became a boost to publicize the documented social problem internationally.</h4>"," This project can teach other people how to create an extensive database from scratch, with credibility, and what criteria to be important when dealing with victims of the security forces.  In addition, it serves as a lesson to discriminate how to convert tabulated data into stories, and how to publish in a striking way when there is a lot of material to explain a topic.    It allows to exemplify how when there is no official information, with dedication and responsibility, databases can be created that can become a public service, which in the end that is the essence of journalism.  It is also another example of how to proceed to protect sources, families and the same journalist against the risks involved in covering topics like this.   This project is an example of how the use of the right digital tools allow the reader to immerse themselves in the stories and consume the resource that interests them (text, videos, comics, database, infographics...).  <h4>A relevant aspect taught by this project is how teamwork is vital for extensive research. In this worked three journalists, seven photojournalists, an illustrator and different vehicle drivers. Two people edited about 37 videos, five others were responsible for programming, design and web support, one helped with the social media posts...  In short, all a rigorous work that was delegated to different people that ultimately resulted in a microsite.</h4>  If we talk about some motivational aspect, this project is an example of perseverance and courage, since dedicating more than six months to a sensitive topic and free time is a challenge. ",https://patrullaletal.diariolibre.com/,https://translate.google.com/translate?hl=es&sl=es&tl=en&u=https%3A%2F%2Fpatrullaletal.diariolibre.com%2F,https://translate.google.com/translate?hl=es&sl=es&tl=en&u=https%3A%2F%2Fpatrullaletal.diariolibre.com%2Flos-asuntos-internos-de-los-supuestos-intercambios-de-disparos%2F&sandbox=1,https://translate.google.com/translate?hl=es&sl=es&tl=en&u=https%3A%2F%2Fpatrullaletal.diariolibre.com%2Fejecuciones-disfrazadas-de-intercambio-de-disparos%2F,https://translate.google.com/translate?hl=es&sl=es&tl=en&u=https%3A%2F%2Fpatrullaletal.diariolibre.com%2Flos-policias-que-se-creyeron-jueces%2F,https://translate.google.com/translate?hl=es&sl=es&tl=en&u=https%3A%2F%2Fpatrullaletal.diariolibre.com%2Fla-muerte-persigue-a-los-sospechosos-de-agredir-a-policias-dominicanos%2F,https://translate.google.com/translate?hl=es&sl=es&tl=en&u=https%3A%2F%2Fpatrullaletal.diariolibre.com%2Fpatrulla-letal-comic%2F,"Mariela Mejía, Tania Molina, Suhelis Tejero","<ul>  <a href=""https://do.linkedin.com/in/mariela-mej%C3%ADa-3162033a""> MARIELA MEJÍA </a>  </ul>  Journalist and Dominican university professor with 14 years of experience. She specializes in investigation and multimedia reports. She has worked on local and regional collaborative projects. For her work she has been recognized in her country and abroad.      <ul>  <a href=""https://do.linkedin.com/in/tania-molina-856a6467?trk=public_profile_browsemap_profile-result-card_result-card_full-click""> TANIA MOLINA </a>  </ul>  Journalist. With an accumulated experience of 15 years in the journalistic and teaching exercise. She works for the Dominican newspaper Diario Libre, where she works as a senior editor dedicated to reporting and investigation topics.      <ul>  <a href=""https://do.linkedin.com/in/suhelis-tejero-puntes-2331b91?trk=people-guest_profile-result-card_result-card_full-click""> SUHELIS TEJERO </a>  </ul>  Venezuelan journalist based in the Dominican Republic. With more than 20 years of experience in economic journalism, she has collaborated with Venezuelan, Spanish and Ecuadorian media. She currently works for the Dominican newspaper Diario Libre. ",,,
Canada,Radio-Canada,Big,Participant,Best visualization (small and large newsrooms),Ce qu'il faut retenir du débat des chefs,10/11/19,"Explainer,Infographics,Chart,Elections,Politics","D3.js,Canvas,Json,Google Sheets,CSV,Python,Node.js"," One of the highlights of the 2019 Canadian election were the two political leaders debates, organized by a consortium of national media outlets. Since Canada is a bilingual country, one debate was in French and the other one was in English. For the French one, we produced this data visualization during the night, broken in three parts, to explain what happened to the voters.    "," This project was the front story of our national website the morning after the debate. At first, the plan was to have a simple text article to summarize what happened during the debate. We proposed this project instead. The stakes were high because the leaders' debate is a very critical event in Canadian elections. But we’ve been able to demonstrate that it’s possible to publish a complex data visualization story very quickly. The public responded very well to it with great engagement time and many positive comments on our work. We were the only media outlet to have a data-driven story published only a few hours after this live event on TV, which really put us in the spotlight. "," During the debate, four people were in charge of filling a Google Sheet with the details of each intervention by a leader. They worked on a recorded version of the debate, to be able to listen several times to some interventions or to ask second opinions from someone on the team if needed.   For the analysis and the data visualizations, many things were coded before the debate. A Python script connected to the Google Sheet and calculated several indicators (number of attacks, most talked topic by the leaders, etc.) to help to point to the most interesting facts of the debate. This Python script also outputted a JSON file directly usable for the data visualizations.   The design and coding of the data visualizations were made before the debate. The visualizations were coded in JavaScript with D3 and rendered into canvas elements, for performance reasons, especially for mobile devices. The coding of the data visualizations was made to be very flexible. Our scrolly telling template, based on React, was built with the same goal.   We decided on the order of the story elements and we wrote the story itself right after the debate.    "," The pressure was very high for publishing this project. The debate was scheduled to start at 8:00 pm and to end at 10:00 pm. Our story had to be published before 6:00 am the next morning. We had roughly 10 hours to make an entire transcript of the debate, to identify each topic, each attack, to write the story and to code complete data visualizations.   To succeed, we needed to prepare as much as possible in advance. For example, for the Python analysis and the JavaScript data visualizations, we worked on the previous leaders' debate (four years ago) to test our concepts on some relevant data. In JavaScript, all of the code was built around very flexible functions, to allow us to change easily and quickly the order of the elements or their animations.   Since the leader debates is a critical part of Canadian elections, we did a general rehearsal a few days before the French debate to test our plan and our code. We actually failed that night, unable to publish anything before the deadline! We made many changes in the way we worked, which made us faster and more efficient in our tasks.   This whole planning and rehearsal process was key to our success.    "," I hope other data reporters will learn that it could be very interesting to plan a data-driven visualizations coverage for some scheduled events (other than elections). We know in advance that some events are critical and will be talked about. It’s an opportunity to push forward an innovative work that will have great visibility.   To do that, planning is key. From the editorial, design and coding aspects, everything has to be flexible for last-minute changes. You need to automate as much as you can to keep your focus on what is really important in the end: content. It’s a lesson that we try to apply to all of our projects from now on.    ",https://ici.radio-canada.ca/info/2019/elections-federales/debat-chefs-partis-francais-canada/,,,,,,,"Nael Shiab, Melanie Julien,"," We are the digital innovation team of CBC/Radio-Canada, the bilingual Canadian public broadcaster. We work for the French services of the company, but we often publish in both French and English. Our team is multidisciplinary, with data reporters, designers and developers working together to produce exclusive data-driven analysis published in innovative forms. Since more and more readers consume information on mobile devices, we specialize in high-quality experiences on small screens, with innovative designs and very efficient code. But all of our projects work very well on big screens too, of course!    ",,,
Canada,Radio-Canada,Big,Participant,Best data-driven reporting (small and large newsrooms),2019 Canadian Election Coverage,09/04/19,"Explainer,News application,Crowdsourcing,Illustration,Infographics,Chart,Elections,Politics","AI/Machine learning,Personalisation,Scraping,D3.js,Canvas,Json,Google Sheets,CSV,Python,Node.js"," In 2019, Canada went through a general election. As the digital innovation team of CBC/Radio-Canada, the Canadian public broadcaster, we decided to cover this very important democratic process with a data-driven perspective and in innovative visual forms.   It was a big challenge since the electoral campaign lasted just six weeks. You will find in this submission the result of our very intense journalistic sprint: five projects, all covering different aspects of the election with different technics, including data analysis, data visualizations, machine learning, cartography, and simulations.    "," Canada is a bilingual country. Some stories were published just in French, while others were published in both French and English.   We kicked off the electoral campaign with the project  Set up to fail: Why women still don't win elections as often as men in Canada . By analyzing 15 years of electoral data and 10 years of financial data, we demonstrated that women candidates had fewer chances to be elected for two main reasons: they are more often placed in less winnable districts and they receive less money from their party to finance their campaign. We illustrated this issue with data visualizations in  scrolly-telling , including one with small multiples.   Then we published  Laissez-nous deviner pour qui vous allez voter  which allows you to build a character (gender, age, province, income, religion, etc.) and visualize which political party is more likely to get its vote. To calculate this probability, we fed a machine learning algorithm with the answers of 387 671 voters, who were asked about their demographics and which party they would vote for. We also identified the biggest factors influencing that decision.   For the project  Ce qu’il faut retenir du débat des chefs , we worked all night after the leaders' debate to publish a data analysis with visualizations of each leaders' interventions.   Finally, during the vote night, we published two projects. We presented an analysis of the results in  Voici où s’est jouée l’élection en carte , with maps animated in  scrolly-telling . And we also made a simulation of the results with different electoral systems. This is the project called  How would proportional representation have shaped this election's results?    In the end, we provided hundreds of thousands of Canadians with crucial information, published in innovative formats.    "," We created a  scrolly-telling  template in JavaScript, with the React library. For the visualizations, all were coded using D3. They are rendered into canvas elements instead of SVG elements for performance reasons, especially on mobile devices.   When a data analysis was needed, we did it in Python, with the Pandas module.   For the machine learning algorithm, we partnered with the non-profit Vox Pop Labs. The algorithm itself was based on three models: one multinomial logistic regression, one support vector machine, and one extreme gradient boosting.    "," The electoral campaign is very short in Canada. The last one was only six weeks long. Therefore, it was a real challenge to publish many customized projects, with many different technics. We standardized as many things as possible. For example, our React template helped us to be more efficient. But the real key is a great multidisciplinary team, with members trusting each other. "," We think this project showcases very different ways to adopt a data-driven approach, but always with a journalistic purpose. All the projects are about the electoral process, but with different angles, different visual forms and different experiences for the user.   We hope other reporters, designers, and developers will be encouraged to be more creative after reading our stories. Part of our job is to find and publish quality information, but it’s also to constantly find the best way to communicate this information.    ",https://ici.radio-canada.ca/info/2019/elections-federales/femmes-hommes-probabilites-vote-egalite-chateaux-forts/index-en.html,https://ici.radio-canada.ca/info/2019/elections-federales/vote-chances-electeur-boussole-influence-choix-parti/,https://ici.radio-canada.ca/info/2019/elections-federales/debat-chefs-partis-francais-canada/,https://ici.radio-canada.ca/info/2019/elections-federales/resultats-cartes-vainqueur-perdant-partis-circonscriptions/,https://ici.radio-canada.ca/info/2019/elections-federales/mode-scrutin-proportionnelle-mixte-compensatoire/index-en.html,,,"Nael Shiab, Mélanie Meloche-Holubowski, Laurianne Croteau, Valérie Ouellet, Francis Lamontagne, Charlie Debons, André Guimaraes, Mathieu St-Laurent, Melanie Julien, Éric Larouche, Martine Roy."," We are the digital innovation team of CBC/Radio-Canada, the bilingual Canadian public broadcaster. We work for the French services of the company, but we often publish in both French and English. Our team is multidisciplinary, with data reporters, designers and developers working together to produce exclusive data-driven analysis published in innovative forms. Since more and more readers consume information on mobile devices, we specialize in high-quality experiences on small screens, with innovative designs and very efficient code. But all of our projects work very well on big screens too, of course! ",,,
Canada,Radio-Canada,Big,Participant,Best visualization (small and large newsrooms),Forget the snowy winters of your childhood,03/05/19,"Open data,Illustration,Infographics,Chart,Elections,Politics,Environment","Scraping,D3.js,Canvas,CSV,Python,Node.js"," In nordic countries with harsh winters like Canada, it's sometimes hard for the public to realize the Earth is warming. That's why we decided to do a story on how the coldest season is warming up too. We collected and analyzed snow ground data. The resulting publication is a project with four data visualization and stories of people working in industries impacted by the decreasing snow cover. "," Winter is an important part of Canadian identity and culture. The Canadian public responded strongly to this story, published in both French and English (Canada is a bilingual country). This project is one of our most-read of the year.   This project also fills a gap in the academic literature. While doing our research, we noticed that many studies were focusing on snow precipitation, but almost none talked about the snow depth on the ground. On an everyday basis, it’s really hard for people to evaluate the variation in the quantity of snow falling from the sky. However, it’s very easy for them to notice the amount of it on the ground. Snow depth on the ground was a way better way to show the impact of global warming on Canadian winters, to Canadians.   This story managed to provide a rigorous analysis while vulgarizing the results in an interesting visual form at the same time.    "," Environment Canada is a federal agency managing more than thousands of weather stations in the country. To collect the data, we coded a web scraper in Node. For each weather station, the script checked if enough data was available for a long period of time (at least 40 years). When it was the case, the data was downloaded. We ended up with a file of 16 million rows.   We used Python to do our data analysis, with the Pandas library. We cleaned the data and did a regression analysis to find the trends. We shared all our results with one of the top Canadian expert on snow, Ross Brown, whom we also interviewed for the story.   Then we did the interviews and wrote the story. We coded the data visualizations in JavaScript, using D3. Some of the visualizations are in SVG elements, while the most complex ones are rendered into a canvas element for better performance, especially on mobile devices. All of them were integrated into a scrolly-telling template that we coded as well.    "," For the temperature and precipitation data, Environment Canada provides easily downloadable files for the whole country. It’s not the case for the snow cover data that is scattered through thousands of different web pages. You need the technical skills to gather and standardize the data, which may explain why almost no experts analyzed it.   But you also need a very strong methodology with precise indicators to calculate the trends. We read academic articles to build our analysis and asked the Canadian Consortium for Regional Climatology and Adaptation to Climate Change to review it.   One of the biggest challenges was to choose indicators easily understandable by the general public while relevant for experts and to translate them visually.    "," While working on the data visualizations, we realized that we couldn’t explain all aspects of the story with just one chart, even animated. We needed several data visualizations to explain different things. We often see data-driven projects with one big main data visualizations, followed by a long text. It’s not always the best way to communicate our information. We hope other data reporters and designers will learn from this project that, often, several simpler data visualizations are better than a complex big one.    ",https://ici.radio-canada.ca/info/2019/03/neige-accumulation-hiver-quebec-environnement-meteo-gel-degel/index-en.html,,,,,,,"Nael Shiab, Francis Lamontagne, Eric Larouche, Melanie Julien."," We are the digital innovation team of CBC/Radio-Canada, the bilingual Canadian public broadcaster. We work for the French services of the company, but we often publish in both French and English. Our team is multidisciplinary, with data reporters, designers and developers working together to produce exclusive data-driven analysis published in innovative forms. Since more and more readers consume information on mobile devices, we specialize in high-quality experiences on small screens, with innovative designs and very efficient code. But all of our projects work very well on big screens too, of course! ",,,
Canada,Radi-Canada,Big,Participant,Innovation (small and large newsrooms),Laissez-nous deviner pour qui vous allez voter,15/10/19,"Crowdsourcing,Illustration,Infographics,Chart,Elections,Politics","AI/Machine learning,D3.js,Canvas,Json,CSV,Python,Node.js"," This project was produced during the 2019 Canadian electoral campaign. It allows you to build a character (gender, age, province, income, religion, etc.) and to visualize which political party is more likely to get the vote of the character you created. To calculate this probability, we fed a machine learning algorithm with the answers of 387,671 participants of an online survey.  ", This  ," The illustrations are made in SVG.  For the machine learning algorithm, we partnered with the non-profit Vox Pop Labs, which also in charge of the online survey gathering the data. The algorithm itself was based on three models: one multinomial logistic regression, one support vector machine, and one extreme gradient boosting. "," Our team has to produce projects that work perfectly on both desktop and mobile. Since the user has to build its voter, there’s a lot of interactions involved. The hardest part was to find a design that would let the user make the choices easily, without friction, on the small screen of a phone. We based our design on the height and width of an iPhone 5, which is an old device, with a very small screen. We thought that if it worked on an iPhone 5, it would work on any screen. "," With Artificial Intelligence (or machine learning), an amazing new set of tools becomes available. But it is sometimes hard to find a journalistic purpose for new technologies. We hope this project will give ideas to other teams to apply machine learning to concrete journalistic projects. ",https://ici.radio-canada.ca/info/2019/elections-federales/vote-chances-electeur-boussole-influence-choix-parti/,,,,,,,"Nael Shiab, Francis Lamontagne, André Guimares, Mathieu Saint-Laurent, Melanie Julien, Martine Roy"," We are the digital innovation team of CBC/Radio-Canada, the bilingual Canadian public broadcaster. We work for the French services of the company, but we often publish in both French and English. Our team is multidisciplinary, with data reporters, designers and developers working together to produce exclusive data-driven analysis published in innovative forms. Since more and more readers consume information on mobile devices, we specialize in high-quality experiences on small screens, with innovative designs and very efficient code. But all of our projects work very well on big screens too, of course! ",,,
Spain,Outriders,Small,Participant,Innovation (small and large newsrooms),Vaccines & Fake News: Analysing Measles Outbreaks 2019,11/06/19,"Long-form,Multiple-newsroom collaboration,Database,Infographics,Health","Json,Adobe,Creative Suite,Google Sheets,CSV,Node.js","  ‘<a href=""https://outride.rs/en/vaccines-fake-news/"" target=""_blank"">Vaccines & Fake news’</a>  is an interactive story of Outriders  written by  journalist  Lola García-Ajofrín ;  designed by  designer  Anano Sharashenidze  (ForSet), Art Director  Irakli Chumburidze (ForSet)  and data Visualisation:  Mariam Gamkharashvili (ForSet);   developed by Piotr Kliks  and  edited by Grzegorz Kurek.    Even though it is a vaccine-preventable disease, measles kills over 100,000 people every year. Worldwide cases tripled in the first three months of 2019. The aim of this interactive story is analizying vaccine skepticism and trying to give answer to the main concers using an attractive design.We wanted to listen both sides of story and give answers.     ","  ‘<a href=""https://outride.rs/en/vaccines-fake-news/"" target=""_blank"">Vaccines & Fake news’</a>  is an interactive article that have five sections: 1. Measles inmunization: Are we protected? 2. Measles Outbreak Globally. 3. Vaccine Skepticsm & Polulism. 4. Case Study: France, The Most Vaccine Seceptic and 5. Experts answer 23 concerns about vaccines.   We consider a must to explain who inmunization works and giving answer to concerns because during the interviews we realizaed that even most of the poupulation was vaccinated against several disseases, most of them don't know which vaccines they have, how do they work and why are they important.   The article was published in English and in Polish. Poland had the most significant drop in confidence between 2015 and 2018, according to <a href=""https://twitter.com/ProfHeidiLarson"" style=""color:blue;text-decoration:underline"" target=""_blank"">Heidi Larson</a> from the London School of Hygiene & Tropical Medicine who runs <a href=""https://www.vaccineconfidence.org/"" style=""color:blue;text-decoration:underline"" target=""_blank"">The Vaccine Confidence Project</a>.   The article was published also in two versions: a larger version for desktop and an insta story for mobile version.   One of the main parts of the article is the section: Extperts answer 23 common concerns about vaccines. Here the reader can chose and click a virus and reply the main concerns about vaccines like: What do vaccines contain? Do vaccines contain aluminium? Why? Do vaccines contain mercury?  a vaccine contains pork gelatin, can I be vaccinated even if my religion doesn’t allow me to eat pork/meat?  Do vaccines have side effects? Can measles vaccine cause autism?   For us, it was specially important to listen skeptic parents, understanding their concerns and trying to use facts to reply that concerns.   We used a showy design of virus because we wanted as much as possible people read some information that they would not read in reports.     "," The interactive artcile include a fifth part with 'clickable' viruses where the reader can choose and click a virus and answer main concerns about vaccines: https://outride.rs/en/vaccines-fake-news/experts-answer-common-concerns-about-vaccines   We think this part is specially innovative because the design makes the reader want to continue clicking viruses and reading answers.   We also believe that it is a brave article where controversial worries are not hidden or avoided.   Experts from Institut Pasteur in Paris, World Health Organization and The Vaccine Confidence Project reply the common concerns that fake news have expanded around the world regarding the components of vaccines, side effects and their compatibility with religious beliefs.   The result is an interactive article with interactive virus that you can click and reply your main concerns on vaccines. "," This article is a work of months analyzing information, talking with people and writing an article that tells scientific data in a simple manner.   Firsly we wanted to show that vaccines skepticism was not a new phenenomenon but we found there was already demostrations at the end of the 19th century when between 1840 and 1853, compulsory vaccination was established by the government of England.   Secondly we wanted to explain complex data in the simpliest way. That it was specially important to explain inmunization and the so called ""herd protection"", when a group of people (the “herd”) can avoid a disease infection by ensuring that enough people are immune so that a chain of transmission is broken.   After interviewing several vaccine-skeptic partents in France we found that some parents living in a wealthy places and socio-economic situations think that if their children follow hygienic life patterns with healthy eating they don't need vaccines. This is a common mistake and we spent several days discussing how to explain the ""herd protection"".   Thirdly we wanted to explain that vaccines are not an individual issue but a social issue or what Frederick Tangy called: “Vaccines are social justice”. For this reason   we mapped Measles inmunization around te world and we mapped ""vaccine trust"".   And finally, in one of the key parts of this story, we replied the main concerns that vaccine skeptic families have. We consider specially important this part because we found than most of the article critize skepticism without listeing the concerns and parent who want the best for their children need reasons.   We would appreciate that jury understand that every part in this story was designed to give answers.     "," It is interesting that although vaccines have saved millions of lives and are part of the lives of each of us, most people do not know what vaccines they have, what vaccines are mandatory or recommended in their countries, which components vaccines have and how they work. This article gives a simple answer to all these questions. ",https://outride.rs/en/vaccines-fake-news/,https://outride.rs/en/vaccines-fake-news/measles-immunization,https://outride.rs/en/vaccines-fake-news/measles-outbreak-globally,https://outride.rs/en/vaccines-fake-news/vaccine-skepticism-and-populism,https://outride.rs/en/vaccines-fake-news/case-study-france-the-most-vaccine-skeptic,https://outride.rs/en/vaccines-fake-news/experts-answer-common-concerns-about-vaccines,,"Written by journalist Lola García-Ajofrín; designed by designer Anano Sharashenidze (ForSet), Art Director Irakli Chumburidze (ForSet) and data Visualisation: Mariam Gamkharashvili (ForSet); developed by Piotr Kliks and edited by Grzegorz Kurek.","  Written by journalist Lola García-Ajofrín;   designed  by designer Anano Sharashenidze  (ForSet),  Art Director Irakli Chumburidze (ForSet) and data Visualisation: Mariam Gamkharashvili (ForSet);  developed by Piotr Kliks  and  edited by Grzegorz Kurek.      <ul>    Lola García-Ajofrín   (Madrid, 1984) is a Spanish journalist  based in Asia. She is currently  Outriders newsroom manager  working on interactive journalism. Lola holds a  Bachelor´s degree in Journalism  from Complutense University of Madrid (2006) and a  Master’s Degree in Television Journalism  from Rey Juan Carlos University (2007). She is the author of  ‘Gigantes de la educación’ (FundaciónTelefónica, 2017 ), a book which systematically documents strategies utilised by teachers within eleven countries seeking to prepare their students for the demands of the twenty-first century.  In 2015, Lola received the “Manuel Alcántara” award, which is an international prize for reporters for  her article '<a href=""https://www.elconfidencial.com/mundo/2014-04-05/viaje-al-corazon-de-la-poligamia-elegi-el-cielo-tras-compartir-a-mi-marido-50-anos_112280/"" target=""_blank"">A tryp to the heart of poligamy in U.S'</a>.   She has published with many of the major news outlets which include  El Pais, El Mundo, El Confidencial, La Información, Telecinco and Cuatro TV  from different international environment like  Afghanistan, Brazil, Cambodia, China, Egypt, Ethiopia, Indonesia, Sri Lanka, Tunisia and Venezuela.      Anano, Irakli and Mariam  work with ForSet,  a non-governmental, non-profit,  civic tech organization from Georgia  specializing in effective communications for social issues using data, design, and technolog.  </ul>   <ul>    Piotr Kliks   is  Chief programmer at Outriders . He does  all the magic that is behind the design of the website and makes the website function  as the user wants it to function. He studied marketing and management at Poznań University of Economics and Business, but he was always keen about programming and new technologies. He has been working as a Web Developer since 2006.     Grzegorz Kurek   is responsible for t ranslating the articles and interactive reportages and takes care of the English language version of the website.  He is also an editor of Outriders Brief. English teacher, translator and interpreter. He studied English Studies at Tischner European University in Krakow, and now lives in Nowy Sącz. He co-wrote the RealMadrid.pl website.  </ul>  ",,,
Taiwan,Apple Daily Taiwan branch,Big,Participant,Best visualization (small and large newsrooms),The Cancer Town,07/05/19,"Investigation,Multiple-newsroom collaboration,Video,Map,Environment,Health,Human rights","Animation,3D modelling,Three.js,Canvas,JQuery,Json"," Apple Daily journalist spent one and a half year investigating the serious pollution and health issue in Shulin New Village, a small town near Guanyin Industrial Zone in Taoyuan City, Taiwan. We found that in the last eight years, as much as 17 people have died of cancer, and nine of them were younger than 55 years old. Due to the air and water pollution, cancer has become the most common death reason, with a high percentage of 77%, so even the local people call the village “Cancer Town.” "," The investigative journalists visited the village multiple times to talk with the local residents and the relatives of the people who died of cancer. Many have lost their dear husbands or wives, while an old man has lost his young son, who died of kidney cancer at the age of only 52. Each story is heartbroken.   Since the feature story was published in July 2019, it has raised the public’s awareness about the environment and health issue in Taiwan. It also forced Taoyuan City government to promise to improve the air and water pollution of that area, and to start planning relocation solution for the local suffering residents. "," The main tools our engineer used in this feature story are Animation, 3D modelling, Three.js, Canvas, JQuery, and Json.   In order to present our investigative results and the local environment, Apple Daily journalist cooperated closely with the visual designers and the software engineers to make an interactive news site. On the site, the most important innovation is the “virtual village” we created by using the 3D modeling technique.   At the beginning, we took the photos of the local environment by flying a drone. Then our 3D image modeler started to “built up” seven residential houses, a temple, a school and a small restaurant. He carefully depicted the details of those buildings and finally recreated the village on the 3D map. After that, another software engineer designed the interactive website for us.   Thus, the village is totally replicated and visualized on the 3D modelling map. The readers can “rotate” or “zoom in” on the map to look at the village closely by using the mouse. "," The hardest part of this project is the performance optimization. It’s a 3D modelling map and there are several clips, so the engineer had to try her best to do the performance optimization, in order to make it run smoothly when the readers are surfing on the page.   "," When surfing the news site, readers can see the surroundings of the whole village, and where the seven residents are living. They can also see the details of the temple and the school. If readers click the 10 icons on the map, they can read the articles and watch the videos about the residents and the places, including their life stories and longtime suffering.   Besides, when surfing the 3D map, readers can see how close between the village and the industrial zone. Readers can also see the violation and punishment records of the nearby factories. The data shows that at least 180 factories have been fined because of violating the environmental protection regulations. ",https://tw.inv.appledaily.com/cancertown/?allowww=1,https://www.youtube.com/watch?v=8qN8qAG4pL8&feature=emb_title,https://www.youtube.com/watch?v=U6x3ezzMgDQ&feature=emb_title,,,,,"Po Chun Ho, Wei Chou Chen, Liang Ju Hou, Yi Jing Wu, Ting Jen Chen, Huan Cheng Lin, Charles Wang, King Hsueh, Tiffany Keng, Wei Neng Yu"," We are a multi-newsroom team with 10 people that focuses on issues such as human rights, environment, and land justice in Taiwan. Six investigative journalists are responsible for doing big data research and field survey. By traveling to every corner in Taiwan, we are able to discover the truth of different issues. In addition, three visual designers and one software engineer from another news-desk help us to produce interactive feature story website.   As a result, the team can present news contents to readers through various multimedia tools such as video, small quiz or game, and interactive infographic. The team has won several international awards in recent years, including Asian Digital Media Awards, SOPA awards, and Human Rights Press Awards.  ",,,
Uganda,Monitor Publications Limited,Small,Participant,Open data,Birding can rake in more revenue than mountain gorillas,11/02/19,"Explainer,Environment",Adobe,"    Birdwatching.  Atleast every Ugandan has seen different species of colourful birds flapping around and singing, especially in the morning. But not many people believe tourists can actually come just to watch birds.In fact, when the word tourism is mentioned, many think of animals. Did you know that birding is a great way to connect with nature but also has the potential to generate revenue for Uganda?   ","   In its efforts to promote birding on the international scene, Uganda Tourism Board (UTB) is hosting 10 international birders on a familiarization trip to different birding destination around Uganda.       UTB is the marketing agency for the country as a preferred tourism destination. Uganda is home to 1, 080 birds. The trip will culminate into the African Birding Expo (ABE) that is slated to take place from December 6 to December 8.       The event will be hosted at Uganda Wildlife Education and Conservation Centre (UWEC) and Entebe Botanical Gardens, both birding destinations too.       The theme of the three-day expo is “celebrating birds and people to conserve Fox’s Weaver”.       “The objective of hosting the buyers is to offer them the birding experience in Uganda so that they can share their personal Ugandan experiences to their respective clients and interest them into visiting Uganda,” Herman Olimi, a marketing officer with Uganda Tourism Board (UTB).   "," I used basic tools as a journalist, thus a pen, notebook, recorder and digital camera. I used the notebook to capture notes as i received information during the research and fact-finding process. I used the recorder to assist me in take record of voices of sources i spoke to. The recorded voices helped me play back and listen as i compiled and pieced together the story. I used the digital camera to capture the photographs and videos during the process of gathering the infromation for the story. ", Birding is both challenging and fulfilling. The terrain in many of the tourism excursions is difficult but not impassable. I would say the challenhes gave me the opportunity to experience what tourists and birders experience on expeditions. I would like to take this as a plus for someone who is cutting their teeth in the tourism and travel field. , The lesson is to explore beyong our usual boundaries of reporting line. I am a features writer who has taken interest to explore travel and tourism as a new ground in my line of reporting as a way of stretching my abilities and fortifying my skill set. ,https://www.monitor.co.ug/artsculture/Travel/Birding-can-rake-in-more-revenue-than-mountain-gorillas/691238-5334458-qykwa0z/index.html,https://www.monitor.co.ug/artsculture/Travel/Birding-can-rake-in-more-revenue-than-mountain-gorillas/691238-5334458-qykwa0z/index.html,https://www.monitor.co.ug/Business/Prosper/2020-will-better-year-tourism-UTB--/688616-5417208-hru3pbz/index.html,https://www.monitor.co.ug/Magazines/Life/Kitagwenda-the-undiscovered-haven-in-the-west-/689856-5360380-1246l2xz/index.html,https://www.monitor.co.ug/Magazines/Life/shoebill-wildlife-prized-sight-Mabamba-Swamp/689856-5386274-bue7x6z/index.html,https://www.monitor.co.ug/artsculture/Travel/The-drive-to-save-the-four--big-cats-/691238-5414764-ayv8kxz/index.html,,Edgar R. Batte," My name is Edgar R. Batte and I have been doing journalism since my teenage life. I started out in school where I produced hand-written articles for my peers in lower high school. Apart from  Daily Monitor  my works have also been published by  The East African - a leading regional weekly, African Review,  Marimba Media , Kenya,  Daily Metro , UK’s  Eyecon Magazine , Rwanda’s  The New Times , and UK’s ecomedia.com.   I am winner of Afrika Divas Awards as Best Entertainment Writer of the year 2011. I have been a beneficiary trainee of African Synergy during in February, 2011 at the Sauti Za Busara (Sounds of Wisdom) Festival in Stone Town, Zanzibar.   I have also won an award as Best Arts and Leisure Writer of the year (2007) at the Golden Pen Journalism Awards. I have been voted by peers and editors as Daily Monitor overall Features journalist of the year for 2012.   I am also a two-time nominee for Young Achiever’s Awards’- 2010, 2011. I have worked with Daily Monitor newspaper on a full time basis since 2004 to date as a Features reporter with a bias towards development. I love writing, reading, traveling, making friends, photography and watching movies.     ",,,
Switzerland,"Independent entry, self-published on Towards Data Science / Medium.com",Small,Participant,Best data-driven reporting (small and large newsrooms),Mount Mid-Life-Crisis,24/12/19,"Investigation,Long-form,Open data,Fact-checking,Chart,Sports",Python," I performed an in-depth analysis of the Himalayan Database to seek answers to why there were such crowds on the summit of Mount Everest this year. I followed two main threads. First, looking at the changing ascent and summit patterns over last decades. Second, exploring how the expedition character shifted towards one composed of older members, more thoroughly assisted by the hired staff, and prevalently ending with a successful summit bit. The data tell a story of impressive technical progress in high-altitude mountaineering, but also one of commercialization of an activity once considered available only to highly skilled explorers. ", Providing new insights to the mountaineering community and empowering them to develop better policies aiming to deal with the problem of overcrowding. ," I used Python and Pandas for data extraction from the Himalayan Database, as well as subsequent cleaning and manipulation. I used Matplotlib with Seaborn to create custom data visualizations. "," The Himalayan Database is a very unique and unfortunately quite overlooked dataset. It contains records of most of the mountaineering activities in the Himalayas since 1910 (all expeditions excluding the Pakistani area). It has been created and maintained by a handful of passionate people and the amount of detailed data contained there is invaluable and unique on a global scale. However, due to the closed nature of the project (it was open-sourced only in 2017) and somewhat obscure way to access it (the authors provide only a proprietary software built on top of an SQL database), it hasn’t gotten the analytic attention it deserves. There has been some analysis done already, however not in-depth enough to tell a meaningful story - mostly simple aggregate statistics with a relatively shallow commentary to illustrate a more qualitative article. I have approached the dataset from a perspective of a data scientist and carved a more detailed and multifaceted story, tightly connected to the data and not stereotypes. Diving into the ecosystem of mountaineering blogs and mainstream articles, I identified the main questions and statements about the topic the public is discussing right now, such as: it is very dangerous to climb Everest nowadays due to the crowds, crowding on the summit is a very recent phenomenon, who are the people taking part in the expeditions. I confronted them with the data, and in some cases I confirmed the common knowledge present in the community, but in other, I invalidated it or shed new light on the topic. After publication, I have received many feedbacks from the community, thanking me for the in-depth insights and their balanced interpretation. "," In a way, the story hidden in this dataset is a story of commercialization of adventure. Only 70 years ago summiting the Everest was an unbelievable achievement at the limits of human skill and endurance. Even in the 80s, it was mostly professional mountaineers who came to the Himalayas.  Now the crowds on the Everest are composed in a big proportion on older men who can afford coming there time- and money-wise, no matter their country of origin, and the rise in arrivals is driven consistently by ever older population groups. This behavior is not exclusive to mountaineering: a very similar phenomenon has been described in the cycling world as the rise of MAMILs (Middle Aged Men In Lycra). While this has nothing to say about the motivation or passion of the expedition members, the data shows that the summit of Everest, at the 80% summit rate, is currently pretty much available to everyone that can afford coming (there are even “full-service” expeditions for people outside of their best fitness are available for fees of around 100 000 USD). Interestingly, what the data also hints to, this shift has also been possible because of the accumulated knowledge about the mountain and great technical improvement of mountaineering gear, ranging from axes and boots, to lightweight insulation and oxygen masks. This makes it a nice illustration of an intimate coupling between nature, technology and culture, visible only through the analysis of this unique dataset. ",https://towardsdatascience.com/frostbite-stories-part-1-mount-middle-life-crisis-97df574d58d7?source=friends_link&sk=0a9c3ded0801dbfedf06a11d5176011e,,,,,,,Teresa Kubacka," I am a data scientist, critical thinker, musician. Born in Poland in the 1980s, I have been living in Switzerland since 2011. In my previous life I was a physicist, but after having earned a PhD from Swiss Federal Institute of Technology I gave up on lasers and hamiltonians and moved on to the fascinating world of data science. For a few years I have worked setting up artificial intelligence solutions in an insurance company, after which I have switched to freelancing as an independent data scientist, data visualization and visual analytics expert, and AI consultant. Writing data stories is my passion and I am not backed up by a dataviz agency or a data journalism newsroom. More about me and my other work on www.teresa-kubacka.com ",,,
Japan,The Chunichi Shimbun,Big,Participant,Innovation (small and large newsrooms),Election analysis with natural experiment approach,06/01/19,"Explainer,Open data,Map,Elections,Politics","Scraping,D3.js,RStudio,OpenStreetMap,Python"," In the national election coverage we used natural experiment approach to the turnout analyses. We found 1) costly early voting stations in the shopping malls do not improve overall turnout, just replacing Sunday vote with early vote, by denying the relationship between the distance to the newly created station and the turnout, 2) pig's year effect, long cherished by political pundits in Japan, has no ground by comparing turnouts between with-local-election municipalities and without-local-election ones. ", The story reached an above-average level of engagement on social media. ," To show the relationship between the walking distance to the voting station and voter turnout, we used QGIS network analysis with 100m estimated population mesh and OpenStreetMap. The main graphics of road network is made on d3.js and webGL to explain the intention of the analysis. In pig's year effect story, data analysis and some charts are done on RStudio. All charts are screen-width responsive. "," Precinct map is created manually, with help of reverse geocoding, because it is defined not by geographical area but by address in Japan. Local election data is collected manually by hitting database because it is not collected by local governments. Both data are the most time-consuming tasks but is the starting point of the projects.  "," Natural experiment approach is one of the compelling ways for the reporters to write causal inference stories by analyzing observational and/or public data while avoiding selection bias and statistical cynicism. Although this approach can not always be applied to the questions you want to know, we have still many policy-making arguments that we can check anew and debunk.  ",https://www.chunichi.co.jp/ee/feature/data/early_voting_stations_en.html,https://www.chunichi.co.jp/ee/feature/data/pigs_year_effect_en.html,,,,,,Isao Matsunami,"<pre> Isao Matsunami is digital editor for Chunichi/Tokyo Shimbun, daily newspaper, in Japan. His main interest has been in digital visualization applied in journalism. He has been organizing, with support from Japan National Press Club, computer-assisted reporting (CAR) workshops in Tokyo. He is also a contributor to OpenRefine, translator of Mailvelope and documentary of ""Noting to Hide"". </pre>",,,
Japan,The Chunichi Shimbun,Big,Participant,Best visualization (small and large newsrooms),Why are solar eclipses so rare?,11/07/19,"Explainer,Satellite images","VR,D3.js,RStudio", A visual explanation of the mechanism of two solar eclipses on 26 Dec. 2019 and 21 Jun. 2020.  , The story reached an above-average level of engagement on social media. ," Positions of celestial bodies are partly calculated by solving Kepler's law on the fly, partly obtained from NASA JPL's HORIZONS database. They are drawn by using webGL and the shadow on the Earth is calculated in the shader program. The interactive feature makes full use of the magnetometer in the mobile device to simulate the eclipse in the reader's environment.  "," Data about celestial events, if drawn to scale, can easily break the boundary of single-precision floating-point number on which webGL works. To make smooth transition of viewpoint along the story, the origin of coordinates is altered seamlessly inside the javascript program. "," Sometimes we have to reconcile ourselves to using deformed charts to explain the subjects in which the very ""scale"" matters as it is impossible to draw to scale on print media. Dynamic graphics make it easy to explain not only scientific subjects but also environmental issues or economic phenomena in which small number for each reader should be translated to large number for the whole. ",https://www.chunichi.co.jp/ee/feature/eclipse2019/eclipse_en.html,,,,,,,Isao Matsunami," Isao Matsunami is digital editor for Chunichi/Tokyo Shimbun, daily newspaper, in Japan. His main interest has been in digital visualization applied in journalism. He has been organizing, with support from Japan National Press Club, computer-assisted reporting (CAR) workshops in Tokyo. He is also a contributor to OpenRefine, translator of Mailvelope and documentary of ""Noting to Hide"". ",,,
Taiwan,Apple Daily Taiwan branch,Big,Participant,Innovation (small and large newsrooms),GPS Investigation: Recycling Secrets,01/11/19,"Investigation,Multiple-newsroom collaboration,Video,Map,Environment","Animation,Canvas,JQuery,Json,OpenStreetMap"," To discover the truth about recycling in Taiwan, the reporters stuffed nine GPS trackers into nine recyclable materials. Next, the reporters gave those recyclable materials to the government recycling collectors in three northern cities. After that, we started to track their movements by checking the real-time geological locations sent back by the GPS trackers.   We found that some materials were taken by recycling companies to illegal places, such as a recycling site on farmland in Taoyuan. The site is not only harming our precious farmland, but also poses a big threat to public safety. "," Taiwan’s official recycling policy has been running for 20 years with a recycling rate as high as 58% touted by the government. However, not many officials or people really care about where those recyclable materials go.   After being published, the report raised the public’s awareness about the problematic recycling system in Taiwan, and forced the government to confess their mistakes and promise to improve it. "," The five main tools we used in this feature story are Animation, Canvas, JQuery, Json, and OpenStreetMap.   When it comes to interactive news, the use of maps is a crucial element. Apple Daily’s in-depth feature story “GPS Investigation: Recycling Secrets” combines Geographic Information System with articles, photos, and videos to show readers “a complete map of Taiwan’s recycling industry.”   The reporters, visual designers, and software engineers worked together to create an interactive news site to present our investigative results. Every GPS’s positions and movements are “visualized” on the news site, so the readers can see all recyclable materials’ flows after being collected. By surfing the page, the readers can watch short videos to understand the complete “recycling circle” and its flaws. "," The digital presentation and visualization of our investigative results is the most difficult part of this project.   Besides, the latitude and the longitude information of GPS trackers are not one hundred percent correct, so the engineer had to find the errors of it and “adjust” the trackers’ routes manually. "," This interactive feature story combines data compilation with field survey, and it reveals the truth about recycling system to the public. It is also a multimedia report with articles, videos, and infographics to show readers the movements of every recyclable material.   In addition, we wrote in-depth reports to analyze the recycling problems in three cities. At the end of the page, there is a map of “legal and illegal recycling sites in Taiwan” for readers to check the locations of those sites by clicking the map. ",https://tw.inv.appledaily.com/gpstrack/,https://www.youtube.com/watch?v=H1YhMoB559Q,https://www.youtube.com/watch?v=jAb8a2zD6W0&feature=emb_title,,,,,"Po Chun Ho, Wei Chou Chen, Liang Ju Hou, Yi Jing Wu, Ting Jen Chen, Huan Cheng Lin, Charles Wang, King Hsueh, Tiffany Keng, Wei Neng Yu"," We are a multi-newsroom team with 10 people that focuses on issues such as human rights, environment, and land justice in Taiwan. Six investigative journalists are responsible for doing big data research and field survey. By traveling to every corner in Taiwan, we are able to discover the truth of different issues. In addition, three visual designers and one software engineer from another news-desk help us to produce interactive feature story website.   As a result, the team can present news contents to readers through various multimedia tools such as video, small quiz or game, and interactive infographic. The team has won several international awards in recent years, including Asian Digital Media Awards, SOPA awards, and Human Rights Press Awards.  ",,,
Greece,"AthensLive (English), The Press Project (Greek)",Small,Shortlist,Best data-driven reporting (small and large newsrooms),"Whose Home Is This? At Least 55,625 Properties Under the Hammer in Real Estate Auctions — and Counting: A data-driven research into housing financialization in Greece and the restructuring of the country by the markets, accompanied by an open dataset.",26/09/19,"Explainer,Long-form,Database,Open data,Illustration,Infographics,Chart,Video,Politics,Economy,Human rights","Scraping,Adobe,Google Sheets,CSV,Python"," <a href=""http://mail01.tinyletterapp.com/data-is-plural/data-is-plural-2019-11-27-edition/15752366-medium.com/athenslivegr/whose-home-is-this-f3b45d878b0b?c=196fc3c8-9884-4caf-a568-a338269c9356"" target=""_blank"">For an article in AthensLive</a>, <a href=""http://mail01.tinyletterapp.com/data-is-plural/data-is-plural-2019-11-27-edition/15752370-twitter.com/sotsideris?c=196fc3c8-9884-4caf-a568-a338269c9356"" target=""_blank"">Sotiris Sideris</a> collected <a href=""http://mail01.tinyletterapp.com/data-is-plural/data-is-plural-2019-11-27-edition/15752374-github.com/2109sot/eauction_data?c=196fc3c8-9884-4caf-a568-a338269c9356"" target=""_blank"">data on properties up for bidding</a> through <a href=""http://mail01.tinyletterapp.com/data-is-plural/data-is-plural-2019-11-27-edition/15752378-www.eauction.gr/?c=196fc3c8-9884-4caf-a568-a338269c9356"" target=""_blank"">eauction.gr</a>, Greece’s official website for auctioning real estate seized from over-indebted borrowers. The dataset includes 45,918 lots listed between mid-November 2017 (when the website launched) and September 1, 2019. For each lot, the dataset specifies the auction date, property characteristics, starting bid, total debt, debtor, hastener pursuing the auction, links to additional documentation, and more. "," Picture the ranks of people losing their homes or small businesses in auctions because they couldn't pay back their debt to the bank: families with children sofa-surfing, at least for a while, with friends and relatives; homelessness; the social stigmatization associated with unpaid debt which can lead to an increase in mental health problems.       Before starting our research into private property auctions, we didn't know how many private properties have gone under the hammer, how often or for how much debt. Most importantly, we didn't necessarily understand the disorienting reality for so many of living like this.   As part of our research, we talked to people who have to cope with such a dire situation and <a href=""https://www.facebook.com/AthensLiveGr/videos/1306135379564263/"">we followed activists</a>, who have been trying to block the auctions of primary residencies, on the ground. Also, we presented the findings of our research at data conferences and academic events both in Greece and abroad.    There were many insights throughout the process but the stand-out was trust. The media market in Greece is characterised by a lack of trust and poor use of traditional media (including a weak newspaper market) alongside some of the highest use of social media and digital-born outlets. At AthensLive we create a climate where journalists can regain public trust and the public’s interests can be represented in journalism again through democratic participation and transparency.  "," To collect data on properties up for bidding we scrapped eauction.gr using the Beautiful Soup library and Selenium framework in Python. The first round of scraping gave us basic information about each auction but most importantly the URL of each entry. Following those URLs we looped through all 45,918 entries in a second round of scraping in order to collect relevant data for each lot, such as the auction date, property characteristics, starting bid, total debt, debtor, hastener pursuing the auction, links to additional documentation, and more. Then, in the final round of scraping, we looped through every entry again, this time to download the court decision for each unique online auction in PDF and Word Doc format, in Greek language. Among the individual debtors several have multiple properties against their names. On August 31, at least 30,134 individuals had more than one property listed. Some of the individuals clearly held large property portfolios. In one instance, a single person is named against 87 lots. Therefore, there are 22,119 items of documentation in our database instead of 45,918 - which is the exact number of electronic auctions within our study period.   After we collected all the relevant information from eauction.gr, we converted our lists of results into a dataframe in the Pandas library in order to start the cleaning and then the analysis. We cleaned our dataframe using regex and then we analysed it in Pandas. When it comes to the visualization of our results, we used the Matplotlib plotting library for Python and then exported our graphs to edit them in Adobe Illustrator. "," To better understand how this platform accelerates the processing of auctions, we wanted to look in some detail at every property that has appeared online. However, unsurprisingly, no data is available online; nothing from the Notary Association website; nothing from Athens Bar Association website. The only way to collect this data was to loop through every property on eauction.gr to collect the relevant information and put it into a dataset of 45,918 lots that can then be analysed. Challenging, but totally worthwhile.   We managed to map 26,976 properties by regional unit. This number, however, represents 58.7% of all the properties in question as real estate type and location information are not available for auctions with posting date before September 24, 2018. This kind of information, simply put the exact address of the real estate, is only available in the documentation of each auction in our database. Mapping every single auctioned property would allow us to find hot sports of gentrification, eviction, and displacement.   When it comes to bidders, we don’t really know who bought what and for how much. That’s how financialization works; it serves to allow anonymity for the rich. On the other hand, land - and the devastation caused to the lives of individuals and communities who have lost it - is personal and the resulting wave of displacement can lead to an increase in mental health problems.   If we want housing to be a public good and not a vehicle for investment, we need more democratic control, not less. We must increase the visibility of the problem and advocate for those who are at risk of losing their homes, knowing that the struggle against the seizure of private property is a struggle for human rights. "," Undoubtedly, the online auction platform has dramatically expedited the rate of auctions. Essentially, though, it has de-linked real estate and place by making the intrinsically local and fixed nature of real estate into something liquid and therefore tradable on global financial marketplaces. “By using the online auction service, you can participate in online auctions without your physical presence using a computer.” That’s the first thing you read when you enter the platform.   In terms of prices, there is great disparity in the starting bids for individual lots, with €52,000 being the median. At the top of the scale are the production facility of the solar power generation company HelioSphera in Tripoli, Arcadia (€37,449,000); the facilities of IRIS printing company in Koropi, Attica, one of the largest in Southeast Europe (€35,183,140). Also at the top is ‘the most expensive house in Athens,’ a six-storey luxury residential tower minutes away from Syntagma Square (€35,000,000), whose owner, arms dealer Konstantinos Dafermos, was arrested in 2015 for illegal payments given to secure a contract to supply Kornet anti-tank missile launchers to the Greek armed forces. At the bottom of the price range are storage rooms for €900, cars for €300, individual parking spaces, even coffee tables.   Journalism in Greece has been in a dismal state for years. Overall, the project is a contribution to the growth of independent journalism projects in a multicultural and international community. ",https://medium.com/athenslivegr/whose-home-is-this-f3b45d878b0b,Open dataset and explanatory notes [https://tinyurl.com/s9t454v],Greek version of the article [https://tinyurl.com/rdzcla9],Video of activists blocking an auction in Athens [https://tinyurl.com/tyhj2fr],Reference and video interview by CNN Greece [https://tinyurl.com/ughe65m],,,"Sotiris Sideris, Olga Souri","  Sotiris Sideris :   Sotiris is of the co-founders of AthensLive and a member of its steering committee. As a 2019/2020 Bertha Fellow, he is now investigating Greece’s housing market and the inequities impacting the most vulnerable in society. He is also serving as a teaching assistant in the Department of Communication and Media Studies at the National and Kapodistrian University of Athens. He has worked at the NGO Network for Children’s Rights as the coordinator and editor-in-chief of Migratory Birds, the first newspaper in Greece made by refugee, migrant and Greek youth, and as a producer and researcher for the national TV documentary series 28 Europe. Sotiris holds a Bachelor's degree in Communication and Media Studies from Panteion University of Athens and a Master’s degree in New Media and Digital Culture from the University of Amsterdam. In the summer of 2017, he was granted a scholarship from Stavros Niarchos Foundation to participate in Lede 12, a post-bac certification program on coding and data analysis from Columbia’s Graduate School of Journalism and Department of Computer Science in New York.        Olga Souri :   Olga is an artist, who works with both traditional and digital practices, and the designer of AthensLive. Her body of work varies but its common element is the use of text art. She believes that art is a dialogue and she chooses to communicate her work with the actual use of language. Her themes mainly explore social life, anxiety, the digital self, social media, and how all of these affect contemporary life in general. Olga graduated from University of the Arts London in 2014. She has participated in group exhibitions in London and Athens. In 2019 she co-founded NOUCMAS, a gallery and artist studio space in the centre of Athens. ",,,
Switzerland,Neue Zürcher Zeitung; www.nzz.ch,Big,Participant,Best data-driven reporting (small and large newsrooms),Where do the young Swiss still speak grandma's dialect?,18/01/19,"Long-form,Crowdsourcing,Infographics,Video,Culture","CSV,R,RStudio,PostgreSQL"," We addressed dialect change through data analysis and reporter work.  We analyzed crowdsourced data: Swiss German speakers had submitted pronunciation, age, gender and local dialect. We picked extrema: Fribourg, the place with most linguistic change where the youngest generation speaks most differently from the oldest generation. And the place with the least linguistic change: Baar.  In both places we visited a family and discussed their dialect.  We tell the story with graphics showing how much (Fribourg) or how little (Baar) the linguistic landscape has changed between the oldest and youngest generation, and through dialogues between grandchild, mother and grandmother.    "," The story addresses a wide audience: Dialects are the prestige linguistic variety in Switzerland, as opposed to Standard German; the latter is mostly used in written or in more formal oral contexts. Furthermore, German-speaking Switzerland exhibits strong variation in a relatively confined area. Therefore, the Swiss typically show a strong interest for dialect variation and change.     We measured success using several metrics such as article views and engaged time, as well as the resonance of the article in social media and the real world. The article had a wide audience on our news site (www.nzz.ch), as measured by our analytics tools, as well as on social media. A high number of spontaneous, personal messages from the general public as well as from experts in the academic field of linguistics showed us that it conveyed the topic equally well to experts and to the general public. "," Our methods of data analysis combine novel data, analytical methods for analyzing linguistc change as well as different statistical methods for measuring variation in categorical variables.    The project is based on a dataset crowdsourced by a mobile app that predicts people's dialect based on how they pronounce a set of words. This work was done by a group of Swiss researchers, one of which is the first author of the present article. This method for crowdsourcing linguistic data was, at the time (2013), novel, and a dataset of this size is still an exception in linguistic research. We fetched the data from a database via SQL code and analyzed it in R. For data analysis, we mainly used custom code that we wrote in R, along with methods used in linguistic research.   The basis for the visualizations was created in R using the library 'ggplot2'. These plots were then processed further in Adobe Illustrator.   The dialogues in the story can be read in Swiss German as well as in Standard German. The interactive tool that allows users to switch from one variety to the other was built by the Editorial Tech Team of the Neue Zürcher Zeitung.   The final article was adapted for wide screens, for mobile screens (the graphics, in particular), as well as for the printed newspaper (parts of the story, particularly around the graphics, were rewritten). "," The hardest part was the fact that the data analysis showed many interesting micro-level patterns (strong linguistic change in some places, almost none in others) buth that linguistic change did not correlate with any of the variables that we had tested (e.g. urbanization, population density, medium age, ...). We therefore used the data analysis to pick «pars pro toto» places and explain dialect change and the reasons for it with a much more personal note than first planned.  "," We believe that the story is innovative: It revolves around people and their language, it features dialogues and video snippets. It is data journalism in which the graphics do not take the center stage, yet the story is completely data-driven.    We like this approach where data journalism is about much more than data and graphics. Also, we think that this is a good example of how one can turn a story around and publish something completely different from what was pitched (but that turns out to be, in this case, better than what was pitched).  ",https://www.nzz.ch/gesellschaft/schweizerdeutsch-viel-wandel-im-freiburger-dialekt-kaum-in-baar-ld.1390868,https://twitter.com/mjKolly/status/1086138106754056194,https://twitter.com/davidbauer/status/1086191555705212929,,,,,"Marie-José Kolly, Alexandra Kohler, Stefanie Hasler, Anna Wiederkehr"," I have been a datajournalist at Neue Zürcher Zeitung, a Swiss media outlet, from 2016 until the end of 2019. During this time, I have worked on very different topics, namely banking regulation, dialect change and weapon exports.   I am a linguist / mathematician by training. I have completed a PhD in phonetic science, during which phase I have conducted research in the fields of speaker-specific features, foreign accent recognition and dialectology. I also conducted several science communication projects, e.g. co-developed several smartphone apps that recognize their users' dialect and crowdsource data.  Since January 2020, I am a datajouralist at Republik Magazin, a digital Swiss media outlet. ",,,
Switzerland,Neue Zürcher Zeitung,Big,Shortlist,Best news application,Cook spaghetti bolognese with us: We'll show you how climate-friendly your recipe is,12/06/19,"Explainer,Quiz/game,Illustration,Chart,Environment,Lifestyle,Agriculture","Animation,Personalisation,Google Sheets,R,RStudio"," This interactive story lets our readers pick the ingredients they use when they cook spaghetti bolognese. We show them, how climate-friendly their recipe is as compared to all the possible combinations of ingredients and compare its ecological footprint with a car ride. In the second part of the article, we go through all the ingredients, show a graphic of the ecological footprint of all their variants (e.g. Swiss tomatoes in the summer, Swiss tomatoes in spring, Italian tomatoes, Spanish tomatoes, canned tomatoes) and explain the reasons behind variation.     "," The story addresses a wide audience: Everyone has to eat. And many people like spaghetti bolognese. The playful way to address environmental impact appealed to our users: The piece was very successful, as measured by article views and engaged time. There was also a wide resonance of the article in social media and the real world. A particularly high number of spontaneous, personal messages from the general public as well as from experts in the academic field of life cycle assessment showed us that it conveyed the topic equally well to experts and to the general public.    "," Data were modelled for us by a team of researchers from Zurich, after we had told them what ingredients we need to show, provided transport distances for e.g. tomatoes from Tuscany (IT) and packaging weights e.g. for canned tomatoes. We had also asked different supermarket chains which products they provide in which month, in order to display a realistic Swiss consumer situation. The researchers then used the software Simapro and the databases Ecoinvent und Agrifood.  We processed their data further using R. Interactive elements were programmed within a JavaScript environment.   The final article was adapted for wide screens, for mobile screens (the graphics, in particular), as well as for the printed newspaper (parts of the story were rewritten). "," The hardest part was to keep all of the information and the involved people together and manage the project, besides all the other tasks that we had as a journalist / graphic designer / developer in a newsroom. Communicating with different external stakeholders required time and communication. The project was published later than planned, because several election projects got in the way. However, in the end, everyone was proud of and happy with the result. "," What we believe: How to create an interactive piece that is interactive where it should be, and non-interactive where the added value would be narrow. How to be very precise and thoughtful on what to include in the story / the analysis, on how work with data and explain the methods behind it. ",https://www.nzz.ch/wochenende/nachhaltiges-essen-wie-klimafreundlich-ist-ihr-spaghetti-rezept-ld.1525404,https://twitter.com/mjKolly/status/1203668337814843392,,,,,,"Marie-José Kolly, Balz Rittmeyer, Manuel Roth, Christoph Ruckstuhl"," I have been a datajournalist at Neue Zürcher Zeitung, a Swiss media outlet, from 2016 to the end of 2019. During this time, I have worked on very different topics, namely banking regulation, dialect change and weapon exports.   I am a linguist / mathematician by training. I have completed a PhD in phonetic science, during which phase I have conducted research in the fields of speaker-specific features, foreign accent recognition and dialectology. I also conducted several science communication projects, e.g. co-developed several smartphone apps that recognize their users' dialect and crowdsource data. Since 2020 I work at Republik Magazin, a Swiss digital media outlet. ",,,
China,Qingyue Information Service Center,Small,Participant,Open data,Qingyue Open Data Center of Environment,01/01/19,"Database,Open data,Environment","JQuery,Json,CSV,PostgreSQL,Python,Node.js"," We collect environmental data that exist on many independent government web site, and open to all(including media) in Open Data format(csv) via web download and API.  It already supported more than 2000 projects, including media such as Caixin, Southern Weekly. "," In China, Government only open realtime detailed environment data,  not open detailed history data, such as air quality. But for some study or report, we need detailed history environment data, so can do more complax computing or find more hidden rules. This project matched this request well. Till now, it is the most famouse open data web site in china for envrionment data, we supported many media for their unique report, such as Caixin, Southern Weekly, China Business News, China Data Jouralism Competation....  Also we supported more than 2000 R&D projects, please refer to http://data.epmap.org/projects for details. "," We use python crowlers to collect data, ruby on rails to build the website,  postgresql to store the data, restfulAPI to open in API mode. "," It is the only website in China that provide DETAILED(hour level) environment data, and already supported more than 2000 projects. "," They can use the environment data to find some hidden rules, or compute them with other data, such as health data, communication data and so on. ",http://data.epmap.org,http://data.epmap.org/projects,http://data.epmap.org/api,,,,,Liu Chunlei," Liu Chunlei, worked in Huawei, Lucent as Project manager for more than ten years, created the project in 2014, to help the environment protection of China via Open Data.   If more people can use environmental data more freely, it is sure that more power can be involved to envrionment protection with their unique technology and resources. ",,,
France,Mediapart,Big,Participant,Best data-driven reporting (small and large newsrooms),Allô place Beauvau,24/01/19,"Investigation,Breaking news,Database,Open data,Fact-checking,OSINT,Crowdsourcing,Infographics,Video,Map,Politics,Human rights","Scraping,D3.js,JQuery,Json","  Allô Place Beauvau  tracks events of police violence against the Yellow Vests movement in France. Each case is first reported to the <a href=""https://twitter.com/davduf"">Twitter account</a> of freelance journalist David Dufresne, which is then automatically integrated into a database that generates an immediately updated datavisualization. Each case is documented (photos, videos, medical certificates, complaints). This database is then deployed on the Mediapart investigation site and offers several levels of readings: a collection of maps, a database by type of injury caused, type of weapon, location, law enforcement involved.  "," After a month of underground work,  Allô Place Beauvau  has truly made the news in France, forcing the media, politicians and NGOs to break out of their silence. The datavisualization, thanks to its graphic strength and continuous updates, has largely contributed to the success of the project. The Twitter account of the journalist behind the project, David Dufresne, went from 14,000 followers to 81,000 in one year. The monthly impressions of his tweets, which amounted to a few tens of thousands in November 2018, have risen to several million. From January 2019 to January 2020, the website  Allô Place Beauvau  reached 400k+ unique visitors.    On the political level, the project leader of  Allô Place Beauvau  was heard:  - in Paris, on 28 January 2019, by Dunja Mijatović, Council of Europe Commissioner for Human Rights, for her report of 26 February 2019 (<a href=""https://rm.coe.int/memorandum-sur-le-maintien-de-l-ordre-et-la-liberte-de-reunion-dans-le/1680931add"">report</a>);  - in Strasbourg, at the European Parliament on 13 February 2019, where he was invited to give a press conference (<a href=""https://www.youtube.com/watch?v=k1RqaSSMpsc"">video</a>, <a href=""https://www.humanite.fr/le-parlement-europeen-condamne-les-violences-policieres-contre-les-manifestants-667970"">interview</a>);  - in Paris, on February 20, 2019, by the Senate Law Commission, which wanted to table a law against the use of certain weapons in law enforcement (<a href=""https://www.publicsenat.fr/article/societe/les-senateurs-communistes-presentent-leur-proposition-de-loi-pour-interdire-les-lbd"">source</a>).   His work was also quoted by the UN High Commissioner for Human Rights, Michelle Bachelet, on 6 March 2019 (<a href=""https://www.lemonde.fr/societe/article/2019/03/06/l-onu-demande-a-la-france-une-enquete-sur-l-usage-excessif-de-la-force-pendant-les-manifestations-de-gilets-jaunes_5432222_3224.html"">source</a>).   In terms of media coverage, the  Allô Place Beauvau  project has received extreme attention from almost all French media (television, radio, print media), as well as from the US ( <a href=""https://www.nytimes.com/2019/01/28/world/europe/france-yellow-vests-police.html"">New York Times</a> ), English ( <a href=""https://www.theguardian.com/world/video/2019/mar/07/my-hand-was-hanging-from-my-wrist-gilets-jaunes-protesters-mutilated-by-police-weapons?CMP=share_btn_tw"">The Guardian</a> ), Canadian (<a href=""https://www.youtube.com/watch?v=yYeUy7OU_68"">Radio Canada</a>), Swiss (<a href=""https://www.rts.ch/play/tv/mise-au-point/video/lbd-40-enquete-sur-une-arme-suisse?id=10207881"">RTS</a>), Italian and Spanish press.    Allô Place Beauvau  became the  Grand Prix du Journalisme 2019  at the French <a href=""https://www.journalisme.com/""> Assises internationales du Journalisme </a>. "," The project being based on current events, happening every week, and the amount of reports to handle being tremendous, we needed a platform that could be used by multiple people at the same time to access, verify, de-duplicate and enrich the database of reports. We chose to use Airtable for this.    The starting point for each report is a tweet by @davduf, so we built scripts that automatically retrieve the latest tweets and append them to the Airtable. We developed other scripts to parse the content of the tweets and extract as much data as possible (date, city, weapons used...). Then, the contributors were able to manually add more info, such as first name, age and description of the victims, police forces involved, etc.   To build the website, we quickly elected to use a static site generator, for performance, security and ease of hosting. We picked Middleman which is a Ruby-based, mature tool. As Airtable provides an API for each data table, we were able to build a automated pipeline that: adds the latest tweets, enriches them, downloads the whole database from Airtable, uses this data to generate the static website, then deploys the update to the hosting platform (Netlify). The source code is hosted on Gitlab, where this publishing pipeline is automatically run every hour, thanks to Gitlab CI.   We built the website using web standards – HTML, CSS and JavaScript. The diagrams and maps were build with SVG, some statically with Illustrator, and other dynamically with D3.js. The maps make use of D3’s various layouts (geo and force layout) to make sure that everything is properly visible and labels don’t overlap.     "," On such a sensitive subject (police violence), transparency is necessary. Readers therefore have access to both our data and working methods. Each report possesses at least one documented trace: presence of at least one video or photo, medical certificate, complaint filed, and/or reliable link (local press, etc.).   Since the project began to have success, our reports are mainly based on direct testimonies from victims, their relatives or lawyers, but also on the many testimonies and twitter alerts received by email or direct messages on Twitter. About ten Twitter users have spontaneously begun sending possible reports, and about thirty do it in a more irregular way. The @davduf twitter account has received up to 7,000 notifications per week, including a very large number of duplicates.   As the cross-checking work is particularly delicate, we have published a leaflet indicating how victims and witnesses can help us, in particular by sending copies of their medical certificates, ID papers, and as precise descriptions as possible of the circumstances, dates and places of the violence. "," The project innovates on several levels: it is based on both urgency (violence every Saturday) and long time (six months of daily updates).    Allô Place Beauvau  is based on contemporary uses of the Internet: the census is available on Twitter, with free access, and datavisualization, which provides real added value, on the  Mediapart  paid site. We had to develop a specific algorithm for the map layout, in order to make sure that the city labels (inferred automatically from the database) did not overlap.    Allô Place Beauvau  is 80% automated, reducing the time between the event and its review.   Using open sources (mostly Twitter and Facebook videos),  Allô Place Beauvau  pays tribute to social networks as a possible source of information. The rigorous journalistic treatment, case by case, makes it possible to validate, or not, what is circulating there.   The questioning is of a civic, journalistic and political nature (the Place Beauvau is the seat and nickname of the Interior Ministry). At first it was aimed at breaking the silence of the media about police violence. A silence that was coupled with political denial, covering the greatest repression in 50 years, in France, of a social movement. ",https://alloplacebeauvau.mediapart.fr/,,,,,,,"David Dufresne, Valentin de Bruyn, Maxime Zoffoli, Philippe Rivière, Hans Lemuet, Nicolas Boeuf, Michael Hajdenberg, Donatien Huet"," <a href=""http://www.davduf.net/?lang=fr"">David Dufresne</a> is an <a href=""http://www.davduf.net/david-dufresne-s-awards"">international award-winning</a> independent writer and filemaker. Producer and <a href=""http://opendoclab.mit.edu/?s=dufresne"" rel=""external"">MIT OpenDocLab Fellow</a>, David Dufresne is the creator of interactive works including  DADA-DATA  (2016, with Anita Hugi and Akufen for Arte) and the multiple-award winning documentary  Fort McMoney  (2013, Toxa/Arte/NFB), acclaimed by  The New York Times  as an innovative hybrid “where film marries video game.” He also co-directed the web documentary  Prison Valley  (World Press Photo 2011 award for Best Interactive Non-Linear Work).   David Dufresne was a long-time reporter for  Libération ,  Mediapart  and managing editor of iTélé, France’s 24-hour newscast. He has also published a dozen investigation books. He was also one of the first Internet players in France (he published the first webzine,  <a href=""http://www.davduf.net/larafale"">La Rafale</a> , in 1995). Today, David lives in Paris, France.   Valentin de Bruyn and Maxim Zoffoli are designers at <a href=""https://www.etaminstudio.com/"">Etamin Studio</a>.   <a href=""http://visionscarto.net/"">Philippe Rivière</a> is a cartographer.   Hans Lemuet is a web developer and a UX designer at <a href=""https://www.etaminstudio.com/"">Etamin Studio</a>.   <a href=""http://twitter.com/nicolas_boeuf"">Nicolas Boeuf</a> is a freelance datajournalist.   Michael Hajdenberg and Donatien Huet are editors at  <a href=""https://www.mediapart.fr/en/english"">Mediapart</a> , an independent French online investigative and opinion journal which has 160k+ subscribers. ",,,
Netherlands,"Pointer, Reporter Radio and Follow The Money",Small,Participant,Open data,Zorgcowboys (healthcare cowboys),26/06/19,"Investigation,Documentary,Quiz/game,Open data,Video,Corruption,Health","D3.js,Google Sheets,R","  Pointer, Reporter Radio and Follow The Money used open data in the investigation of so-called zorgcowboys (healthcare cowboys): healthcare companies that make excessive profits at the expense of their clients. Before our investigation, publications on this subject were incidental. With our data analysis and reporting, we could structurally pinpoint what companies were suspicious and how much money is involved in this fraud.     We collaborated to make three TV broadcasts, four radio broadcasts, 38 articles and a journalistic game. We also published two datasets with suspicious companies.  ","  Thanks to our publications, we could identify almost hundred health care companies where the profits couldn’t be explained through normal practices. We used a very complicated open data set with all financial statements of healthcare companies. To give something back to the public, we wanted to give a data set with comprehensible information. We made a data set with all companies that made more than 10 percent profits: a very high and unusual profit margin in the forms of health care we researched.     The data was available for everyone, but insurance companies, municipalities and inspection services haven’t done any analysis with the data. After our publications, we heard that the Dutch health care inspection NZa used our method of analysis to do their own research from now on.     With releasing the data, we found that other media outlets could do their own findings: at least 71 publications were made by other newsrooms.     Our own investigation led to parliamentary questions, local political questions, insurers claiming money back from companies, and a few health care companies who had to stop their practices. We also won the national open data prize (Stuiveling Open Data Award) in the Netherlands for our efforts.  ","  We used open data from the ministry of Public Health, Welfare and Sport. The analysis was done in R. We analysed the financial statements of 2017, and later 2018.     Our first research focused on profit margins higher than 10 percent. The list of health care companies was 174 companies long, and we checked all reports for logical explanations for these high margins. For 97 companies the profits couldn’t be explained through legal means.     In our publications and broadcasts, we explained the systems at work in this complicated fraud. Most often, companies will write down a lot more hours than they delivered in care. But we also found a few more sophisticated forms of fraud: declaring housing costs through a shell company, paying yourself more salary through sister companies, etc.     In our second research we looked at companies who were making high profits in consecutive years. In this analysis, we found again some smart schemes. And during this phase, we collected a lot of research which we couldn’t use in our regular reporting. This is why we made a journalistic game. The game was made in Javascript.  ","  We encountered two difficult phases in our research. The first one is related to the data. Open data in the Netherlands is mostly ‘just publish it online, no one cares’. Our data had no documentation, and even the persons responsible for the data could not explain how the data was structured. Through trial and error (and a lot of cross-referencing with the reports on paper) we could reverse-engineer how this data could be used.     The second part was finding persons on record to talk about our findings. Employees of these companies are afraid to lose their jobs, although they see a lot of neglect of clients. And these clients don’t want to get stuck with no care at all: sometimes a little bit of bad health care is even better than none.     This is why we wanted to be as open and transparent as possible: to build trust with these persons. We asked for tips constantly during our research. And after publishing our first data set, we got a lot of good tips for our follow-ups. Building trust and being clear on the way we worked was the key to our publications.  ","  We proved that investigative journalism isn’t reliable on the monopoly of information. The data we used, was available for at least 4 years before we did our research. In the past, most publications were on incidents: a single company who was caught in the act. With our analysis we found a lot of companies that were under the radar.     Our research displayed how data analysis and classical reporting can enhance each other. We collaborated with our colleagues of Reporter Radio and financial experts of Follow The Money: without this collaboration, none of these newsrooms could have published anything. We were stronger than the sum of our parts.     By publishing our journalistic game, we also searched for other ways of telling our story. This is a challenge: how to make a game that is fun to play, but also does justice to our findings? We found a nice result in a game in which you can decide how to start up your own company. Do you want to be an honest entrepreneur? Of do you make high profits for your own financial well-being?  ",https://pointer.kro-ncrv.nl/zorgcowboys,https://pointer.kro-ncrv.nl/artikelen/terugkijken-zo-komen-deze-zorgbedrijven-aan-zeer-hoge-winst-zorgcowboys-deel-1,https://pointer.kro-ncrv.nl/artikelen/miljoenen-aan-winst-bij-97-zorgbedrijven-en-dat-wordt-zelden-gecontroleerd,https://pointer.kro-ncrv.nl/artikelen/slechte-zorg-leveren-maar-toch-enorme-winst-maken-hoe-gaan-zorgcowboys-te-werk,https://pointer.kro-ncrv.nl/artikelen/zorggame,https://www.nporadio1.nl/onderzoek/17294-miljoenen-aan-winst-bij-97-zorgbedrijven,https://pointer.kro-ncrv.nl/artikelen/terugkijken-hoe-worden-de-directeuren-van-deze-zorgbedrijven-rijk-zorgcowboys-deel-2,"Jerry Vermanen, Dirk Mostert, Jolien de Vries, Eelke van Ark", Jerry Vermanen - Datajournalist at Pointer   Dirk Mostert - Reporter at Pointer   Jolien de Vries - Reporter at Reporter Radio   Eelke van Ark - Reporter at Follow The Money ,,,
Netherlands,Pointer (KRO-NCRV),Small,Winner,Best visualization (small and large newsrooms),Danish scam,07/12/19,"Investigation,Database,OSINT,Illustration,Video,Audio","Animation,D3.js,Google Sheets","  One day in 2019, we received an obvious spam email in which we were asked to publish a guest blog on our website. Normally we would delete this, but after a follow-up email we became curious on how this scam works. We decided to find out for ourselves.     With the information in the email, we searched and found an elaborate network of two Danish scammers and at least 134 persons whose identities were stolen. We made an article in which we put you in the driver seat of our lead investigator.  ","  After our first publication and visualisation, we made a TV broadcast 4 months after the fact. We translated our online production to TV, instead of making an online production from our programme. In the TV broadcast, we also filmed our investigator’s screen and tried to do everything from behind our laptop.     During this second investigation, we discovered that the Danish guys improved their scam. They AI generated faces to fake reviews, contact persons and sell their content. So we made a second visualisation in which we explain how you can recognize this more sophisticated scam.     We tried to contact as much victims as possible. Most of them didn’t knew their identities were used for this scam.  ","  We didn’t want to tell this story in a familiar way: the most exciting part is discovering the answers step by step. So we searched for a way to translate a research on desktop to your mobile screen.     We used OSINT techniques like reversed image search, Wayback Machine searching, Google Dorks, searching in chambers of commerce, digital forensics to find outgoing url’s, etc. to reveal the intricate and complicated network behind this scam.     We also made our own database of persons whose identities were stolen. We needed to know how many persons were involved, and if they knew anything about this scam.     The most difficult person to find was Martyna Whittell, the fake identity of our emailer. She used photos of an existing person. We found the real ‘Martyna’ (her name is Mia) by geolocating her photos: we found a photo on a campus in Aalborg through a Starbuck coffee cup and a concert photo through the background of a Take That reunion tour. We eventually used face recognition in Yandex to find her friend on a group photo, and searching her friend list for a photo that looked like Mia.  ","  The hardest part of our research was finding Mia. We could find a lot of breadcrumbs online to reveal the scam(mers), but finding our main victim was difficult.     Also, making a visualisation that works on mobile and puts you in the seat of our investigator was a real challenge. We could make a direct analogue with a desktop computer, because of the orientation of your screen. Forcing users to rotate their screens would be a step in which most persons would back-out and quit.     We found a way in which we made our own screens with illustrations. This also works great in this example, because we needed to anonymize almost everyone. We translated the story to English because this story is not only interesting for Dutch readers.  ","  The most important lesson is never to take anything for granted: a good investigative story can hide itself in an ordinary spam email you get every day.     Also, making your own databases and being well-versed in digital research techniques is an essential part of modern investigative journalism.     The translation from desktop to mobile was a succesfull, in our opinion. We found that a lot of readers scrolled to the end of our story.  ",https://pointer.kro-ncrv.nl/artikelen/het-verhaal-achter-een-identiteitsroof#lang=en,https://pointer.kro-ncrv.nl/artikelen/dozens-of-identities-stolen-by-danish-online-entrepreneurs,https://pointer.kro-ncrv.nl/artikelen/our-tricks-for-identifying-victims-of-identity-fraud,https://pointer.kro-ncrv.nl/artikelen/online-oplichters-gaan-vaak-vrijuit,https://pointer.kro-ncrv.nl/artikelen/terugkijken-hoe-een-mysterieus-mailtje-ons-leidt-naar-online-oplichters-in-de-filipijnen,https://pointer.kro-ncrv.nl/artikelen/hoe-ontmasker-je-een-online-oplichter-in-7-stappen#lang=en,,"Peter Keizer, Wendy van der Waal, Marije Rooze, Jerry Vermanen, Wies van der Heyden", Peter Keizer - OSINT researcher at Pointer   Wendy van der Waal - Designer at Pointer   Marije Rooze - Developer at Pointer   Jerry Vermanen - Data journalist at Pointer   Wies van der Heyden - Data journalist at Pointer ,,,
Brazil,Colaboradados,Small,Shortlist,Open data,Colaborabot,17/02/19,"Solutions journalism,Database,Open data,Politics,Corruption","Google Sheets,Python"," @colabora_bot is a robot created by the brazilian vehicle of open data and government transparency Colaboradados. @colabora_bot is the interlocutor of a project of verification of the Brazilian governmental transparency. Through a large data base with all transparency portals belonging to Brazilian governmental agencies, the robot monitors which of these portals have been presenting downtime issues or have been deleted. On Twitter and Mastodon, he warns his followers, and also the government agency responsible for the site, that the government information and warns the bodies of the irregularity. "," As an open source project, the initiative becomes 100% transparent and replicable. Nevertheless, by being a collaborative effort, this project can make be approached by anyone in the civil society. In this manner we can say that we achieved success every time someone inputs a new portal for us to keep checking, or makes a request on our codes that makes the robot more efficient. Success, for us, is to keep the society as the guardians of transparency. @colabora_bot is a monitoring tool for Brazilian government transparency. Unfortunately, it is not uncommon that public data under the administration of the Brazilian government add up without prior notice. Journalists, researchers and those who cherish government transparency are often confronted with the lack of preparation of public bodies in the delivery of public information. @colabora_bot is not only a monitoring tool for society, but also for the enforcement agencies of the Law that governs access to public data. Its innovation is precisely this: to be an informational and also research tool, in search of democratization and the advance in Brazilian governmental transparency. "," @colabora_bot is developed in Python and its data is collected and aggregated in an Google Sheets. With the help of a collaborative network of people that submit links of governmental transparency portals, Colaboradados feeds a database of links to the robot so it can do its work. @colabora_bot then accesses each URL of the database and finds out if they are online. If some of the government portals are not accessible at the moment, @colabora_bot reaches out to its followers on their Twitter profile and Mastodon social network, asking them to mention the agents responsible for administering the sites for clarification. On a separated database, @colabora_bot logs each of the failed attempts. In this way, the bot produces its own database so that it can be accessed by journalists, the government and society in general to analyse the data. "," It’s not a simple task to specify one such thing. Various parts of the project were considered a challenge of its own. If I have to choose only one, though, I’d elect the difficulty to deal with Twitter’s platform in order to notify the municipalities responsible for keeping the transparency portal online. In various occasions we were suspended without a prior notice and clear reasoning and we would only could guess what we did to receive this treatment. The problem-solving part of this challenge, without knowing exactly what “is” the problem was very difficult. We had - in a example - to change our code establishing a various frequency of activity, and in another cases we had to change how we reached to the profiles responsible. So, in order to maintaining our program working, we had to deal with a bug we weren’t responsible to and neither knew what it was causing it, really. "," We hope that others can learn that we have more power to inspect public power than ever before. Only one person can, in a matter of fact, with the help of a computer and a little bit of knowledge, perform the same task of checking the availability of such an important service as the governmental transparency websites as would an entire team of employees in another time. ",https://twitter.com/colabora_bot,https://botsin.space/@colaborabot,http://colaboradados.com.br/bot_colaboradados.html,https://g1.globo.com/economia/tecnologia/noticia/2019/03/01/regras-do-twitter-afetam-robos-que-monitoram-politicos-e-orgaos-publicos.ghtml,,,,"Judite Macedo Cypreste, João Ernane, Ana Paula da Silva Mendes"," Judite Macedo Cypreste is a Data Journalist and Investigative journalist at UOL. She is Co-founder and Director in Colaboradados and Podcaster in Coluna7, the brazilian podcast about Data-Driven Journalism. She has participated as keynote in events such as the 14th ed. of Python Brasil, and also in Brazilian regional events: Python Nordeste and Python Cerrado. She believes that the match between journalists and programmers can bring enormous benefit to the society.   João Ernane de Paula Barbosa is a law school graduate that believes that the usage of data science in legal procedures and governmental transparency should be more a more widespread practice and not simply a far away fantasy. He seeks the concretization of this reality by playing the role of Director of Innovation of Colaboradados and as the main editor of the Coluna7 podcast, which aims to raise the public awareness of data driven journalism in Brazil.   Ana Paula da Silva Mendes is a Computer Science student at Universidade Federal do Piauí and have technical background in Software Development at Instituto Federal de Ciência e Tecnologia do Piauí. She is currently working as Full Stack Developer at Quiploy and as a volunteer Software Developer Technical Lead in the open source project, Colaboradados. In academic life she's a researcher at the Artificial Intelligence Laboratory, LINA, in the Computing department of the Universidade Federal do Piauí. ",,,
China,Guangzhou Green Data Environmental Service Center,Small,Shortlist,Open data,Green Data - Establishment and Application of China Environmental Database,01/01/19,"Database,Open data,Mobile App,Map,Environment","QGIS,CSV,PostgreSQL,Python"," Green Data, an Environmental Protection NGO, has established the largest environmental open database in China. Most of the environmental data are already published in local governments' websites, and we pushed them to publish if they didn't. However, there are more than 3000 local governments in China. Green Data collects and standardizes the environmental data from different websites, and then publishs them in the website of our own. Therefore, all the environmental open data can be visited in one website. Besides of analyzing the data by ourselves, the database is totally open and free to all web visitors, medias and others. "," Several famous medies, such as Caixin, Southern Weekly, The Paper, cooperated with us to report environmental data news related to air, water and soil, and carry on deep investigations on environmental damage. Therefore, the public could know serious situation we were facing, and public opinion often push environmental problems to solve.   Besides, we have developed a regular data opening program by using API port. In 2019, 23 companies/orgarnizations were getting environmental data from us to develop different applications. Amap, the most popular map APP in China, developed an 'Environmental Map' by using our data. According to Amap's statement, 0.7 billion users have viewed environmental data by entering 'Environment Map' from June 2018 to June 2019. Sogou Map, another popular map APP in China, developed a 'PM2.5 Map' by using our data. Through the coorperation with companies such as the two above, we solve the problem of how to spread environmental information to public.   Tianyancha and Qixinbao, two largest credit investigation companies in China, put environmental administrative punishments of companies getted from us into their database. This made many companies lose opportunities of financing or bidding a government's project, which often requires a company not receiving administrative punishments in recent years. The environmental administrative punishments are always not heavy enough for companies. However, losing opportunities of financing or bidding hurts them much more, and they will surely seek for better environmental performance afterwards. In one case, a company even persuaded the local Environmental Protection Bureau to illegally overturn a punishment, in order to let us and credit investigation companies delete its punishment record. The punishment was finally retained after we reporting to higher administration.   The data was also provided to several local Procuratorates for supporting commonweal litigation, and to several governments for supporting their administrative information system.     "," We built a website, where environmental data are shown in the map, and this makes the public easier to understand. The data was obtained mianly by IT technology, such as crawl.  "," The hardest of this project is to firstly establish the database with enough finacial support. Green Data was founded in 2015, and we spent 3 years to establish the database. Thanks to the trust from our donators, we can operate smoothly in the first 3 years.    It's not easy to run a NGO in China. Regarding our mission to reduce industrial emmision, it's much harder. Fortunately, we found a way to use environmental data to make changes. This is useful because we are in the era of Mobile Internet. The effects of data can be brought into full play. We think our experiences may be useful to other fields and areas. "," It may be a solution to carry out environmental protection by using data and IT tech in other countries and aeras. We are willing to help other people/organizations/governments to establish their own environmental database, in order to prevent and control the environmental pollution, as well as mitigate the climate change.    Besides, for small organizations, it's better to focus on the field you are good at, and try to be the best. You can find win-win partners who are top players in their field. Then the cooperation will result in double results. Don't try to do everything by yourself for small organizations. ",www.lvwang.org.cn,http://science.caixin.com/2019-04-25/101408460.html?s=070b69ea9b66bda8bf373b317c0fd8f89c6dd3f36864734521c0489d8ba4aecd84d41c9bb6ff81aa,http://magazine.caijing.com.cn/20161226/4216869.shtml,https://www.thepaper.cn/newsDetail_forward_1878834,http://www.bjnews.com.cn/news/2018/06/05/489827.html,http://www.alijijinhui.org/content/17044,http://xxgk.pingxiang.gov.cn/szbmxxgk/shbj/fgwj/qtygwj/202001/t20200117_1876999.htm,"Chun Xiang, Lei Guo, Hailong Lu, Tianran Fu, Geng Wu, Xiaochen Wei, Xin Luo, Suiying Xian, Wenzhang Ding, Yuan Zhang, Rongjie Zhang, Longfei Zhuang, Shuying Qiu, Xin He, Jiajun Su, Guohua Huang, Zhikai Niu, Huateng Wang"," Guangzhou Green Data Environmental Service Center (Green Data)，founded on March 2015，is a non-profit environmental organization. It aims to promote public involvement on environmental issues, to improve policies on environmental protection, and to protect our environment as well as public health through the analysis and application of the environmental data.   Green Data has developed a comprehensive environmental database including data of environmental impact assessments(EIAs), pollution sources, and environmental qualities. The life-cycle environmental information of a company, from its establishment to closure, is all collected.   Our database not only enables the public to check the environmental data around their living areas, but also helps the bank as well as brands to check the environmental impact of a company or supply chains.   Based on the collection and analysis of environmental data, Green Data investigates the macro impact of EIAs and the sources of pollution on environmental qualities. Besides, we attempt to promote the development of environmental policies, increase the efficiency of environmental management. ",,,
Singapore,Reuters,Big,Shortlist,Best visualization (small and large newsrooms),Drowning in plastic,09/04/19,"Explainer,Infographics,Environment","Animation,3D modelling,Adobe,Creative Suite"," It’s one thing to say the world is drowning in plastic. It’s another to show it. Around the world, almost 1 million plastic bottles are purchased every minute and they are having a devastating effect on the environment. That equates to 480 billion bottles per year. The numbers are mind boggling. But what do they look like?    This project puts those numbers into perspective by visualising them in real time, as well as piling the bottles up next to landmarks to give a true sense of scale. ", The project was instantly picked up and shared by prominent environmental groups including Greenpeace. It was tweeted by influential figures ranging from a U.S. House Representative to a Bollywood movie star. The piece grabbed headlines in other media outlets and online platforms which wrote articles on how the visualisation brought home the shocking scale of plastic waste. The piece is still shared widely as a vivid explanation of the world’s addiction to single-use plastic.      ," The main piece of software at the heart of this project was Cinema 4D. The team made a 3D physics engine in order to allow the bottles to drop, collide, and roll in a realistic space. The first inputs into the model were the mass and other properties of a single bottle from a vending machine in our Hong Kong bureau.   An early projection showed that the almost 17,000 bottles consumed every second would quickly bury first the human figure and the garbage truck we placed for a sense of scale.   Adobe After Effects was used to add final polishing to the animation.    NOTE: Viewing this project is seemless on any device. "," Building a 3D physics engine! As the team continued to work on the models and renderings, they hit a snag: the computer was not powerful enough to execute the visualisations were looking for. Even running a single computer around the clock, it took 9 days to render the first visualization.   The work-around was to remotely connect a dozen Reuters computers in Hong Kong, Singapore, Bengaluru and London into what became an ad hoc “render farm” using wifi to harness the computational power needed to make and edit the animation.   Finding the middle ground between realistic rendering and illustrative style was also difficult. Making the bold decision to alter any style or aesthetics of the final project was very challenging as we knew this would add time. Moving the camera angle a few degrees meant rerendering the entire animation and/or images which took many hours and sometimes days. "," An effective and powerful visualisation can be made from a very small set of numbers. There isn’t always a need for massively complicated data sets. This piece started with a single figure, how many bottles were sold in 2018. ",https://graphics.reuters.com/ENVIRONMENT-PLASTIC/0100B275155/index.html,,,,,,,"Simon Scarr, Marco Hernandez"," The Reuters graphics desk publishes visual stories and data visualisations to accompany the Reuters news file. We typically cover all areas of the news, with content ranging from climate to financial markets. The team conceptualises, researches, reports, and executes many of the visual stories published. ",,,
Finland,"Svenska Yle, Sveriges Radio, Yle",Big,Participant,Best data-driven reporting (small and large newsrooms),Time to emigrate? How would moving to another Nordic country affect your wallet,28/02/19,"Cross-border,Multiple-newsroom collaboration,Quiz/game,Infographics,Chart,Lifestyle,Immigration,Economy,Employment","Personalisation,Google Sheets,R,Python","We compared salaries, t for 98 occupations using the International Standard Classification of Occupation ISCO-08, calculated minum costs of living, income taxation and housing costs for Finland, Sweden, Norway and Denmark to decide how much money professional people would have to spare in the different countries. We used all the data to create a easy to use and fun calculator that calculated where you would have the most money left in you pocket after all necessery expences were payed. We also used the data t The project was a cooperation between Svenska Yle, SR in Sweden ans Yle, initiated by"," Within the frame of the project we published stories about financial politics within the nordic countries, salary levels, taxation and migration. The outlets were Svenska Yle, Sveriges radio and Yle. The stories were published online, on the radio and on TV. Our findings generated wider discussions about salary levels in Finland and differences between the labour market within the Nordic countries. Later our findings have been used by trade unions in Finland as a mean to improve vages. The project generated new knowledge about the actual economic impact emigration/immigration to another Nordic country would have for different professionals. The project reveled serious differencies in salaries for the same occupation and in a finnish perspective it was an eyeopener that the finns earn less in almost every occupation. Almost no one would benefit from moving to Finland from another nordic country for work.    The application was published in four different language versions and on three different publication platforms; In Swedish, finnish spoken in Sweden, finnish and swedish spoken in Finland. The plattforms were Svenska Yle, Yle and Sveriges Radio.  This was the first ever joint datajournlistic venture by two national public service companies (three organisations). We shared data, knowledge, analytic tools and publication plattforms.  "," We used R and Pandas to analyze the data, vue.js and Highcharts to visualise, Trello and Slack to coordinate. ", One big obstacle was to re-calculate all the differencies in the datasets so that they were comparable. In order to do that we had to do an in-depth analysis of the origin of the data so that we would know that we are comparing the samt thing (full time salaries without overtime compensation or other forms of compensations). We also had to make sure the costs of living and taxation analysis were comparable by braking the factors down and rebuild them in the same way.     Another struggle was to coordinate and project manage two so very different teams with different work culture and to see that our data generated as myúch impact as possible.     , That it is fruitful to compare regional data but the more the alike the countries are the better. This kind of comparison would be very hard to do on a european level.   This was a project where the data generated a lot of stories apart from the main publication.    Cooperation pays off but it demands a lot of work.  ,https://svenska.yle.fi/artikel/2019/02/28/i-danmark-far-finlandssvenska-lotta-hogre-lon-for-samma-jobb-kolla-hur-din,https://svenska.yle.fi/kategori/specialtema/arbeta-och-bo-i-norden,https://sverigesradio.se/sida/artikel.aspx?programid=185&artikel=7160785 https://yle.fi/uutiset/3-10662005,https://yle.fi/uutiset/3-10662005,https://svenska.yle.fi/artikel/2019/12/17/industrifacket-bryter-arm-med-arbetsgivaren-finlandska-metallarbetare-har-de,,,"Malin Ekholm, Yasmine el Rafie, Linus Lång, Petter West, Linda Söderlund, Maxim Hansén, Eric Weber. Miro Johansson, Camilla Tranberg"," Svenska Yle Data is a small datajournalistic team within the minority media organisation Svenska Yle. Svenska Yle is a seperate organisation within the finnish public service company Yle. Svenska Yle produces tv-, radio- and online journalism in swedish for the 300 000 head strong swedish speaking minority in Finland. Svenska Yle Data consists of a project manager/datajournalist (Malin Ekholm) a datajournalist (Linus Lång) and a developer (Petter West). We work independently with a focus on online datajournalism but closely together with other newsrooms within the organisation. <a href=""https://svenska.yle.fi/datajournalistik?page=0"">https://svenska.yle.fi/datajournalistik?page=0</a>   Digitala insatsstyrkan was Sveriges Radios datajournalistic team but sadly the project was cancelled in may 2019. It was lead by Yasmine el Rafie and focused on UX and webdesign with a journalistic focus.  ",,,
Greece,OBC Transeuropa for European Data Journalism Network,Small,Participant,Best data-driven reporting (small and large newsrooms),The problem with refugee camps in Greece - How the Greek policy on migration is changing,27/12/19,"Investigation,Long-form,Database,Illustration,Infographics,Chart,Map,Human rights","Scraping,Json,Adobe,CSV,Python"," Greece has a new center-right government since summer 2019. Its approach to migration differs sharply from its predecessor’s, as more controls and constraints are imposed on asylum seekers. OBC Transeuropa collected and analyzed data on the refugees and migrants who reside at refugee camps and detained centers in Greece. The data-driven investigation is divided into two parts.  "," The impact of the project is that because it was translated in many differenct languages (English, Greek, Italian and German) many people were informed about the refugee camps and the detained centers in Greece. Besides that, the Greek government has decided to re-open the recently closed Ministry of Migration. Till then the Ministry of Protection was responsible for the migrants and refugees. One more thing is that the research was used by many human rights organizations for advocacy and lobbying.  "," I used python, pandas, javascript and html and illustrator in order to create the datasets, to clean and analyze them and then visualize the data.  "," I have created the datasets from scratch and I had to verify the data that I have found. Besides that I have done the production of the research and the data analysis and visualization by myself, which is a difficult task but I feel very satisfied because I have worked on a story that was under-reported. The popular narrative is mostly focused on the refugee flows but nobody writes about the people who reside at the camps. The majority of the refugee camps in Greece are located in old factories or old military spaces many kilometers away from the city centers, while others are quite close to populated areas like towns and small villages. We calculated the kilometers and hours that it take to travel between each of Greece’s refugee camps and the cities, the towns or villages nearby them, focusing in particular on the distance between the camps and the closest city with a hospital. Furthermore, we created a dataset on the detained centers and its capacities. The main goal was to show how the Greek migration policy has changed and how EU Commission promotes and supports this strategy. Data can contribute to migration reporting and on defending human rights, however we need each time to verify the data that we use.  ", How is to live at a refugee camp and at a detention center in Greece   The problem of the distance between the refugee camps and the urban fabric.   How the Greek policy on migration is changing and how EU promotes and supports politically and financially this new direction.   The refugee crisis has not ended it's still ongoing.   <h1> </h1>,https://www.europeandatajournalism.eu/eng/News/Data-news/The-problem-with-refugee-camps-in-Greece,https://www.europeandatajournalism.eu/eng/News/Data-news/How-the-Greek-policy-on-migration-is-changing,https://www.europeandatajournalism.eu/eng/Tools-for-journalists/Quote-Finder/Quote-Finder-notes/The-new-Greek-migration-policy,https://www.europeandatajournalism.eu/eng/Tools-for-journalists/Quote-Finder/Quote-Finder-notes/Refugee-camps-in-Greece,,,,Eleni Stamatoukou," Eleni Stamatoukou: Data Journalist, fellow of Stavros Niarchos Foundation and Columbia Journalism School. Main Interest: Human Rights, Migration, Corruption, Far Right, Europe, Turkey and the Balkans.  ",,,
Spain,Outriders,Small,Participant,Best data-driven reporting (small and large newsrooms),Behind the Fires,23/12/19,"Explainer,Long-form,Cross-border,Multiple-newsroom collaboration,Illustration,Infographics,Map,Satellite images,Politics,Environment,Corruption,Agriculture,Human rights","Animation,Json,Adobe,Creative Suite,Google Sheets,CSV,Node.js"," NASA satellites registered a record in the size and number of fires in Brazilian Amazon in 2019. Behind the fires, there is a bigger and more silence problem - deforestation. In just one year, almost 10,000 square kilometres of forest vanished in Brazil.   Photojournalist Gabriel Uchida, based in Amazons, meet the Uru-eu-wau-wau people, a threatened indigenous group is struggling to survive and is trying to protect the forest.   The article includes visualizations with Google Earth in South America, Rondonia and Bolivia, a map of changes in global forests and a live map that shows fires in the last 7 days.     "," Amazon fires captured the world attention and interantional media in August, however media and audience forgot about it soon after. With this article written by a Brazilian photojournalist who spent the last years documenting the changes in Brazilian Amazon and clear visualizations of deforestation and fires we tried that the public go deeper.   The article was published in English and Polish and it include a mobile version through Insta stories in Instagram. "," This article combines the basics of the traditional journalism (going to a place, talking with people and reporting) together with the an innovative way of visualization: a timelapse of Google Earth and a Live Map of fires continuosly updated.   On the one hand, Brazilian photojournalist Gabriel Uchida traveled to one of the most dangerous indigenous reserves in the Brazilian Amazon, he drived and used boats and took intimate portraits. On the other hand, Kasia Polus got to show in a simple and attractive way the changes in Amazons using Google Earth and Modis to show a live map that it is always updated. "," This interactive story has been a team work with several challenges: firstly it was to find the accurate journalist who knew well Amazons and what it was happened. Secondly, it was to analyze and understand the data and choose the best way of telling to the audience why these fires were different. ", With Behind the Fires the reader can learn about an unique community that until the 80's did not have contact with other people than them. So firstly this article is a thrilling trip through Brazilian Amazon. Secondly the graphics allow the reader to understand the magnitude of deforestation in Amazons. ,https://outride.rs/en/behind-the-fires/,,,,,,,"Author: Gabriel Uchida, Data research: Lola García-Ajofrín, Design: Kasia Polus, Web development: Piotr Kliks."," Brazilian photojournalist  Gabriel Uchida  decided a few years ago to leave the comfort of the big city in Sao Paulo and settle in the Amazon, there he has been documenting the transformation of the forest, the loss of trees due to illegal activities and the strength of the indigenous communities that resist. He usually works in long term project. His project about football fans was celebrated internationally.    Lola García-Ajofrín  is a Spanish journalist based in Asia. She is the author of the book 'Gigantes de la educación’ (FundaciónTelefónica, 2017). She has published with many of the major news outlets which include El Pais, El Mundo, El Confidencial, La Información, Telecinco and Cuatro TV. from many of the capitals throughout Asia, Africa, Europe, and the Americas.    Kasia Polus  is a designer from Poland and owner at muffi.pl.    Piotr Kliks  is a chief programmer at Outriders. He does all the magic that is behind the design of the website and makes the website function as the user wants it to function. He studied marketing and management at Poznań University of Economics and Business, but he was always keen about programming and new technologies. He has been working as a Web Developer since 2006. ",,,
British Virgin Islands,The BVI Beacon,Small,Participant,Best data-driven reporting (small and large newsrooms),How many did Hurricane Irma really kill?,25/07/19,"Investigation,Long-form,Database,Open data,Infographics,Chart,Environment,Health,Human rights","Personalisation,Adobe,Creative Suite,Microsoft Excel"," Four people died on the day Hurricane Irma devastated the British Virgin Islands in 2017, and that number quickly became the government's official death toll. However, as the population struggled to recover in dangerous living conditions, other residents seemed to be dying at an alarming rate. By comparing mortality data from the previous decade to the post-storm period, this project was able to show that Irma's total toll was closer to 34. In a population of about 30,000, this difference was shocking, and it had previously gone unreported. "," At a time when climate change threatens more frequent and more intense hurricanes, the project showed that the loss of life from a major storm in the BVI was about eight times higher than the official death toll reported by the government and repeated by the media. This discovery, in turn, suggested that the human cost of climate-change disasters in the region and further abroad is likely to be much higher than previously thought.     Locally, the project also highlighted the BVI government's failure to consider post-disaster mortality in the official death toll, as well as highlighting other problems with the existing methods for recording causes of death. Additionally, it exposed the human cost of failures in the government response in areas including medical services, shelter, medicines, relief supplies and transportation. As the project showed, many post-storm deaths appeared to have been a direct result of such failures.   Many readers also reported that the project brought closure. They had suspected that a loved one's death was a direct result of Irma, and they wondered why it hadn't been officially designated as such. By reading others' stories and the article's analysis, they learned they were not alone.    Finally, the project, taken with others like it in Puerto Rico and other countries, helped highlight the urgent need for an international standard for measuring disaster mortality that takes into account indirect long-term deaths. A consistent methodology is essential if the world is to properly understand the real cost of disasters in the age of climate change — and to properly prepare for them.     "," The project started with shoe-leather reporting and then progressed to data analysis using various programs.   At the BVI Civil Registry, post-storm data had to be copied by hand out of the death register. Then it was entered into Microsoft Excel, which was used in conjunction with Tableau and other visualization programs to analyze and graph death numbers, causes, ages, and other factors in order to better understand the possible reasons for post-hurricane deaths.   The most important analysis, however, focused on historical comparisons: The post-storm numbers were compared to aggregated data from the previous decade. In this manner, we were able to estimate a number of people who had died as an indirect result of the storm. Public health experts confirmed that this method is a reasonable approach for estimating the total toll of a disaster.   Finally, the data visualization programmes were used to present the information in an easily understood format in a series of graphs that we published in our print and online editions. "," The hardest part was carrying out such a lengthy investigation with limited resources at a time when the islands were still struggling to recover from the devastation of Hurricane Irma. With the trauma so fresh in the community's mind, many residents were unwilling to talk about their loved one's deaths. Additionally, many were struggling on a day-to-day basis to meet their basic needs. Because communications were patchy, it was also difficult to track down family members for interviews.   There were personal challenges as well. Irma destroyed our newspaper's office as well as the homes of all our reporters, some of whom moved away. As a result, the newspaper has been short-staffed (with only two to three reporters during the period of the investigation) and struggling financially since the storm. Carrying out even day-to-day reporting has been difficult, let alone a lengthy investigation.   The lack of digital records at the Civil Registry also posed a challenge: The death registry is only on paper, and all of the numbers had to be copied out by hand on paper. Additionally, in the course of copying the data, errors were discovered in the register that the agency then had to investigate and correct. It seem that these errors were largely a result of the chaos that ensued after the hurricane.     "," The project has wide-ranging implications for assessing post-disaster mortality. Currently, there is no international standard for calculating a disaster's death toll, and post-disaster deaths, though common, often go uncounted. This issue has particular relevance in the Caribbean, which in the age of climate change is facing disasters including larger and more intense hurricanes, flooding and drought, as well as non-climate-related disasters like volcanoes, earthquakes and tsunamis.   The project, then, highlights the importance of establishing an international standard for assessing death tolls. Only then can the impact of disasters in different locations be properly understood, compared and responded to at the international level.    For policymakers, aid organisations, and journalists, there is also much to learn about the longer term impacts of disasters. Too often, media coverage focuses on a disaster only for a short period. But the impacts can linger for weeks, months and even years after coverage fades — in the case of Irma, even while the great majority of victims were still dying. To fully understand climate change, the world needs to have a much clearer understanding of the long-term impacts of the disasters it brings. ",https://www.bvibeacon.com/irma-likely-killed-more-than-30-the-bvi-beacon/,,,,,,,Freeman Rogers," Freeman Rogers is the editor of The BVI Beacon newspaper in the British Virgin Islands. His reporting on climate change and the Caribbean has appeared in The New York Times, the Associated Press and news outlets across the region. He and the Beacon recently collaborated on an award-winning cross-border climate change investigation with the Puerto Rico Center for Investigative Journalism. ",,,
Switzerland,SRF Data,Small,Participant,Open data,Open Journalism Data by SRF Data,01/01/19,Open data,R," Apart from covering news stories with data-driven explainers or uncovering misconduct or corruption through data investigations, SRF Data also publishes the large majority of code and data behind its stories. This often concerns governmental data that was either previously unpublished or that was in a inaccessible format. All code and data is published on an overview page, which leads to the respective stories and GitHub repositories. With this service, SRF Data is a major producer of open data in Switzerland – data that can be re-used and re-published by other journalists and the public. ","Unlike with many other news organizations, each published code repository is described in great detail. The consistent use of RMarkdown and the provision of all necessary raw data allows our analysis scripts to be fully reproduced and understood by laymen. In the last years, a lot of additional effort was put into ""full"" reproducibility, meaning that a script can be executed several years after the publication – as if it was frozen in time. To our understanding, this practice is unique in data journalism. Alone in 2019, 13 new repositories were published on srfdata.github.io. This makes it the most active year since starting the publishing practice in 2015. Two examples of reproducible scripts that were published last year are linked to. First, the data analysis behind a story for the Swiss Federal Elections in 2019: ""What worries the Swiss?"" (link 2). The visualization behind the project won the Kantar Information is Beautiful Silver Award in ""News & Current Affairs"".The published code on srfdata.github.io does not only show how the data was sourced and pre-processed, but also gives insights into the design process for the final product (with different exploratory analyses). Second, also during Federal Elections, we tapped into Facebook's Ad Library API to monitor ad activities of policital parties and candidates (link 3). This was the first time the Ad Library was made available in Switzerland and we wanted to have some sort of dashboard that would allow us – and other interested journalists – to have a continuous update on how much money is spent on Facebook for political ads. Therefore we created an R script that would query the API every night and download the newest ad data for Switzerland. After that, another script would preprocess these data, filter them for political parties, automatically generate visualizations and publish"," We mostly use R & RMarkdown, Git and GitHub Pages. Most of the R stuff on srfdata.github.io is <a href=""https://github.com/grssnbchr/rddj-template"">based on the reproducible R template</a> by Timo Grossenbacher, which is also freely available (link 4). There, it is also  explained how to easily construct a GitHub Page like we did.   That template allows us to publish code and data as smooth and efficient as possible, keeping the resource footprint of reproducibility low (which we think is often a show stopper, as it takes too much time for many news organizations). Secondly, the template helps in ""freezing"" the software packages used for the scripts, thus making the scripts reproducible even years after initial publication and in a different software environment (e.g. different OS). This practice is, to our knowledge, pretty unique in reproducible data journalism. See below for further explanation. A second innovative aspect of the template is that the RMarkdown code can be deployed and published on a Github page by merely running a small bash script.     "," While me made our reproducibility publication process as efficient as possible, maintaining and updating our template still took and takes quite some time. This is also one of the reasons we decided to make the template itself public, as we are a publicly financed broadcaster. The biggest challenge remains to ensure actual and full reproducibility, given that software (R packages, for example) and software environments (OSes, third-party libraries) changes rapidly. After a few years, we discovered that this would actually render our scripts useless (or worse: they would still run but produce different, even wrong, results). One such case is described in a blog post (link 5). Here we executed a two-year old script and only thanks to Git we realized that some of the output data were transformed differently. This was due to an updated R package. Therefore, we invested a lot of time in making the software environment itself reproducible (through the so-called ""checkpoint"" mechanism), which resulted in the aforementioned template system (link 4). "," Other news organizations and data journalists can learn from us how we define reproducibility and how we implement it so that it actually works (also in the future). The most important aspect to profit from, though, is the fact that we tried to streamline the whole process as much as possible. With our well documented and open source template system, news organizations can make publishing (R) code and data a breeze, thus allowing them to return to new stories as soon as possible without sacrifizing documentation and long-term reproducibility of published work. ",https://srfdata.github.io,https://srfdata.github.io/2019-06-worries/,https://srfdata.github.io/2019-08-fb-ad-library/,https://github.com/grssnbchr/rddj-template,https://timogrossenbacher.ch/2017/08/this-is-what-happens-when-you-use-different-package-versions-larry/,,,"Timo Grossenbacher, Angelo Zehr, Felix Michel, Julian Schmidli", Timo Grossenbacher (1987) is a data journalist at SRF Data.   Angelo Zehr (1990) is a data journalist and frontend developer at SRF Data.   Felix Michel (1985) is a data journalist and frontend developer at SRF Data.   Julian Schmidli (1985) is a data journalist at SRF Data. ,,,
Nigeria,BusinessDay Media Limited,Big,Participant,Best data-driven reporting (small and large newsrooms),Is Okomu Oil Palm set to halt undulating earnings trend?,28/10/19,"Explainer,Cross-border,Business,Agriculture,Economy","Microsoft Excel,Google Sheets","One of Nigeria's largest producers of oil palm, Okomu Oil Palm is showing signs of maintaining steady growth in earnings following the decision of the Nigerian government to close the country's land borders with neighbouring countries such as Benin and Niger in a bid to curtail the smuggling of food items and proliferation of firearms in the porously bordered West African country. Okomu earnings have come under pressure despite having a huge market share in Nigeria's oil palm making industry which is battling with illegal importation of cheaper oil palm products.","The closure of the Nigeria's land border was interpreted as having enormous consequences on the Nigerian economy which struggled for growth since it emerged from recession in 2017. While that may not be entirely incorrect, the projects is gradually changing narratives that even though there are some negative implications of the border closure on international trade and Nigeria's relationship with other West African countries, it is helping struggling companies in the manufacturing and agriculture industries -- which the Nigerian government regard as priority areas -- market their products to local customers, grow sales and bolster income. Okomu's likely return to the path of steady profitability is a testament to the Nigerian government plans to bolster local capacity of essential food items, particularly oil palm which Nigeria used to be one of the world's largest net exporters in the 1980s.","I used Microsoft Excel to analyse the data which show that Okomu Oil Palm has an historic challenge of growing earnings steadily. This is because of myriads of saboteurs taking advantage of federal government policies and infiltrating the country with cheaper imports. So really understand this challenge, I gathered the financial statement of Okomu for five years, history of dividend payments, and stock price data from the Nigerian Stock Exchange (NSE). All these were carefully inputted using Excel sheets and analysed critically. The company has a record of reacting positively or negatively to federal government's policies relating to international trade, the border closure is one of the few instances the oil-palm making industry will react positively of the government policy. While the company was unable to sustained positive trend in the past when policies that could help it grow were made, the story looked at the potentials for the company to meaningfully maximise the opportunity this time for sustained performance.","The major challenge is analysing the impact of a company using a company's financial results. While financial statements are beyond how the much a company realises as profit in a period, I went deeper to extract what changed since the Nigerian government decided to close its land borders. The basis for this is because the company is the biggest publicly-listed oil palm maker in Africa's most populous nation with a vast area of farmland in Benin-City, Nigeria. Also, most fast-moving consumer goods firms serving an estimated population of 200 million people rely largely on Okomu for the oil palm products, letting go of other illegally products in a move that would reduce pressure in Nigeria's foreign exchange market and help the nation's currency remain stable.","What others can learn is that there is a lot happening to big companies in our respective countries, many of which their aggregate production capacity/value takes a bulk of our countries' economy size we celebrate as Gross Domestic Product (GDP). To find out this, we need to understand how to dig into their financial statement, get facts, analyse and make meaning out of the facts for our audiences, this could help in shaping their perspectives about critical national issue.",https://businessday.ng/companies/article/is-okomu-oil-palm-set-to-halt-undulating-earnings-trend/,http://www.nse.com.ng/issuers/listed-securities/company-details?isin=NGOKOMUOIL00,https://punchng.com/reversing-500m-oil-palm-importation/,https://www.export.gov/article?id=Nigeria-Prohibited-and-Restricted-Imports,,,,Oluwasegun OLAKOYENIKAN,"Biography Oluwasegun OLAKOYENIKAN is a young and versatile Bloomberg-trained financial journalist with more than five years of experience in the media including nearly three years focusing on financial, business and economic journalism. He holds a Bachelor's degree in Industrial Mathematics with strong concentration of Economics from Adekunle Ajasin University and a Master's degree (Distinction) from the University of Benin. He worked in BusinessDay Media Limited as a Financial Journalist where he conducted research and wrote analyses on public-listed companies, financial markets and the economy before he recently joined AFP at the Lagos Bureau. Prior to that moment, Oluwasegun was a Research Analyst in Proshare Nigeria Limited and a Business Correspondent in Ripples Nigeria Limited. He interned with the International Centre for Journalists (ICFJ) as a citizen journalist for two years, reporting vital health, environmental and poverty related issues in Nigeria's Niger-Delta region. He also led a team of five Citizen Journalists in Nigeria to produce a compelling story to win the 2014 HALA Nigeria Story Contest. Being an individual with strong background in numerical discipline and great interest in business journalism, he was offered a scholarship for Bloomberg Media Initiative Africa Executive (BMIA) Training Course in Financial Journalism at Lagos Business School: Pan-Atlantic University in 2015 The BMIA Executive Training Course, which is being held in Kenya, Nigeria and South Africa and funded by Michael Bloomberg, is a pan-African programme to build media capacity, improve quality of financial journalism and access to information in order to advance transparency, accountability and governance in Africa's emerging markets. Oluwasegun is passionate about financial journalism and he is ready to use his skills in data and financial analyses to cause social changes in Nigerian and all over the world.",,,
Nigeria,Dubawa Nigeria,Small,Participant,Best visualization (small and large newsrooms),Fact-checking Claims On Bayelsa's Population And Finances,15/11/19,"Investigation,Explainer,Multiple-newsroom collaboration,Fact-checking,Illustration,Infographics,Chart,Politics,Economy","Microsoft Excel,Google Sheets"," The project focus on false and misleading claims on the population and allocation, as well as the potential effect of these on Bayelsa state in southsouthern Nigeria, to meet its financial obligations as it was set to elect a new governor.   While it's mostly true that Bayelsa received the fourth highest allocation in the country, at least for the most of 2019, available data do not support the claim that the state received N66.76 billion.   We justified this using relevant data from Nigeria's repository of data, the National Bureau of Statistics. "," Th story filled a significant information gap on Bayelsa state population and finances at a time several claims were circulating on social media in the build-up to the state's governorship election.   It set the record straight for an important issue having implications on national discourse because the claims referenced the fact that the state was not only the smallest in the country but supposedly smaller than Nigeria's most populous local government area of Alimisho. In spite of this, the claims suggest the Nigerian southsouthern state was among the top states with highest federal allocation.     "," I used Infogram as my visualisation tool. The tool enables users to create engaging and interactive infographics for proper conveyance of messages and better reading experience.    I simply collated data on federation allocation account committee extracted from Nigeria's National Bureau of Statistics on a monthly basis, cleaned and presented on a sheet on the Infogram interactive tool. This chart is then embedded in the story. "," The hardest part of the project was collating the data. The National Bureau of Statistics releases the federal allocation account committee data separately on monthly basis. This makes it difficult to monitor the trend for each state and make any reasonable conclusion over any period. I had to extract these rough revenue figures for each of Nigeria's 36 states in some the available months, compile them together in a sheet, i proceeded to apply some features of the Infogram tool for visualisation purpose. "," Others can learn how to make data more appealing to readers by using tools such as Infogram, Infographia, Flourish and many others. They have different features that can help to capture most of the messages you're trying to pass across in one chart. It becomes very interesting when the reader can inetract with the charts, it builds loyalty and makes stories more compelling to read. ",https://dubawa.org/fact-checking-claims-on-bayelsas-population-and-finances/,https://nigerianstat.gov.ng/,,,,,,Oluwasegun OLAKOYENIKAN," Oluwasegun OLAKOYENIKAN is a young and versatile Bloomberg-trained financial journalist with more than five years of experience in the media including nearly three years focusing on financial, business and economic journalism.     He holds a Bachelor’s degree in Industrial Mathematics with strong concentration of Economics from Adekunle Ajasin University and a Master’s degree (Distinction) from the University of Benin.   He worked in BusinessDay Media Limited as a Financial Journalist where he conducted research and wrote analyses on public-listed companies, financial markets and the economy before he recently joined AFP at the Lagos Bureau.    Prior to that moment, Oluwasegun was a Research Analyst in Proshare Nigeria Limited and a Business Correspondent in Ripples Nigeria Limited.   He interned with the International Centre for Journalists (ICFJ) as a citizen journalist for two years, reporting vital health, environmental and poverty related issues in Nigeria’s Niger-Delta region.   He also led a team of five Citizen Journalists in Nigeria to produce a compelling story to win the 2014 HALA Nigeria Story Contest.   Being an individual with strong background in numerical discipline and great interest in business journalism, he was offered a scholarship for Bloomberg Media Initiative Africa Executive (BMIA) Training Course in Financial Journalism at Lagos Business School: Pan-Atlantic University in 2015   The BMIA Executive Training Course, which is being held in Kenya, Nigeria and South Africa and funded by Michael Bloomberg, is a pan-African programme to build media capacity, improve quality of financial journalism and access to information in order to advance transparency, accountability and governance in Africa’s emerging markets.   Oluwasegun is passionate about financial journalism and he is ready to use his skills in data and financial analyses to cause social changes in Nigerian and all over the world. ",,,
Sweden,"Approximately 170 different media including Borås Tidning, Lokaltidningen, Västerås Tidning, Aftonbladet, Blekinge läns tidning, Sveriges radio",Small,Shortlist,Innovation (small and large newsrooms),Visualizing the local – efficient data journalism for small newsrooms,09/02/19,"Investigation,Solutions journalism,Long-form,Database,Open data,Fact-checking,Illustration,Chart,Map,Sports,Elections,Politics,Environment,Business,Culture,Women,Agriculture,Immigration,Health,Crime,Economy,Employment","Scraping,Adobe,Microsoft Excel,CSV,R,RStudio"," Nyhetsbyrån Siren uses open data, FOI requests and public documents to create data journalism concerning the most pressing issues in Swedish society. On a daily basis we collect, clean and share datasets with over 170 newsrooms all over the country, so that their journalists can write stories about issues affecting their local communities. In addition to the dataset, we write out the key findings along with an expert interview to provide extra context. For every dataset we create a series of locally customized graphics for publication. This is done using limited editorial resources, most often only one person. "," The project helps strengthen local and national news rooms, in times of lower budgets and staff reductions. Journalists all over the country can offer their readers/listeners/viewers several data driven stories each week with customized graphics. And they do. Siren’s stories are picked up and published in many different media outlets on a daily basis. It is not uncommon that our news stories dominate the  national media as well. During autumn 2019, Siren has put a light on topics such as the falling numbers of physicians with a specialist degree, different opportunities for children to learn music and culture depending on where they live, the regional impact on a much discussed tax reduction for those with the highest income, the most polluting industries throughout the country and the fact that reports of domestic violence are increasing during and after the Christmas holidays. To name just a few. "," Excel and R are used for collecting and sorting data gathered from open sources and public offices. The project has been ongoing for three years and up until september 2019 we used Datawrapper and Infogram for creating graphics. In september we started using R scripts instead for visualizing the data and creating locally customized graphics. By using R scripts we have much more control over the appearance, it is much easier to update data or text in all the graphics at the same time or to download and name all graphics in batches. Our own editorial system Sirenen, launched in spring 2019, is used for the distribution of all our news to other media. "," We are a small news agency with high journalistic ambitions, especially when it comes to producing quality stories for struggling local media. But how to do that, reaching the whole of our sparsely populated country, with just nine editorial staff? The answer is in part data journalism. Using reproducable R scripts to easily multiply and customize graphics saves us a lot of time and effort. Our data journalist Anna Norberg worked out how we could fit R into our editorial process and is now spreading the knowledge to the rest of the team, so that everyone at Siren easily can contribute with their own data journalism stories.   How Sweden is divided geographically, with 21 counties and 290 municipalities, is another problem. The municipalities are not evenly distributed, for example over 45 municipalities belong to Västra Götaland county, but Gotland county has only one municipality. We usually make one chart for each county, showing data for all the municipalities in that county. This works fine for most of the counties, but for others the charts have not been very useful, either showing only one value or too many. With Datawrapper it has never been an option to trying to make individual charts for each municipality. That would be too time consuming. But with R we can now start to address this problem and just as easily produce 290 charts as 21, and in no time. "," If you dare try new methods and open source tools, and actually invest the time it takes to master them properly, you can save time and accomplish amazing things. Nyhetsbyrån Siren have limited resources and can not involve outside developers in everything we do. By letting our datajournalist Anna Norberg spend some time to develop R skills, we can now visualize our data journalism stories more efficiently than we could ever have imagined a year ago. ",https://github.com/nyhetsbyran-siren,http://www.bt.se/boras/anlaggningarna-med-mest-klimatpaverkande-utslapp-tva-fran-sjuharad/,https://ystad.lokaltidningen.se/nyheter/2019-11-28/-Simrishamn-l%C3%A4gger-mest-i-Sk%C3%A5ne-p%C3%A5-Kulturskolan-6686087.html,https://www.vasterastidning.se/vastmanland/familjebraken-okar-under-julhelgerna/reptaf!FEJ1rOsZNJ7V4uX6ZRqg/,https://www.aftonbladet.se/nyheter/a/2Ggorv/lagre-dodlighet-i-prostatacancer-i-skane,http://www.blt.se/nyheter/blekinge-har-betalade-flest-varnskatt/,https://sverigesradio.se/sida/artikel.aspx?programid=109&artikel=7312344,"Anna Norberg, Gunnar Jacobsson, Fredrik Johansson, Jesper Eriksson, Robin Orling, Marie Ericsson, Per Larsson, Jack Carlsson, Ulf Fahlén"," Nyhetsbyrån Siren started in 2003 and is a small news agency specialized in covering Swedish authorities and public offices. Local media are our primary clients, but we reach national media and trade media as well. All our news stories are based on public documents or data.   Anna Norberg has been working for Siren for three years. She has earlier experience from the leading public service newsroom Sveriges Radio Ekot, TT Nyhetsbyrån news agency and the once flourishing free newspaper Metro. Anna has studied journalism at the University of Stockholm, JMK, and Goldsmiths College in London. ",,,
Chile,www.24Horas.cl,Small,Participant,Best data-driven reporting (small and large newsrooms),"Torture at the police station: Complaints against police officials for torture, illegitimate constraints, unnecessary violence and other similar crimes",09/04/19,"Investigation,Explainer,Database,Illustration,Infographics,Chart,Video,Crime,Human rights","Adobe,Microsoft Excel,Google Sheets","Tortura en la Comisaría is a report that reveals violent practices of the Chilean police against citizens in contexts of detention or demonstrations today. Between 2011 and 2019, behaviors similar to those used in the military dictatorship of 1973, directed by Augusto Pinochet Ugarte, and which were supposed to be extinguished are revealed. However, they were in a dark and denied world that was only known to victims and perpetrators. A month after the report was published, complaints about human rights violations became massive, as a result of a social crisis. The background of the behaviors were already on the"," The project had an important impact for the  documentation and updating of Human Rights  known by the citizens in the decade from 2000 to 2020;  until then the concepts of torture and institutional violence were related to the dictatorial period in Chile  between 1973 and 1989 directed by Augusto Pinochet; but  they were not thought of as something still happening today.    Without knowing what was simmering in the social environment,  the work was published a month before a social crisis in Chile,  in which there have been  massive violations of Human Rights  according to international reports by Human Right Watch, Inter-American Court of Human Rights, the Office of the United Nations High Commissioner for Human Rights and Amnesty International; mostly by Carabineros de Chile according to the complainants.   Through databases of legal actions between 2011 and 2019, testimonies of assaulted persons, interviews with experts and police documents, it was possible to reconstruct what was happening in prison cells and arrests just before the crisis, violent behavior against citizens that was also being more denounced in recent years and was unknown in public opinion.   The report undoubtedly shows that  violent practices were integrated among police officers long before the social crisis  and that they are not conditioned by stress levels in the institution, nor as isolated cases.  It was possible to  identify behaviors defined as methods of torture  by the  Istanbul Protocol,  national laws and global agreements. "," To  reconstruct torture  situations we worked with an  historical database of the National Institute of Human Rights  with complaints and legal actions. We also work with the  qualitative descriptions  of each of them.   After  clearing the data  that corresponded to complaints against police personnel, we  added the variables of types of torture  defined by the  Istanbul Protocol  and separated the complaints by years and types of torture. Thanks to this crossing of data, we identified  patterns of behavior in the police that were repeated  in situations so that they seemed learned and not isolated. All of this, we made it using Microsoft Excel.    To review the  trend of occurrence of violent events,  we developed tables and graphs with Infogram to have  analytical visualizations  that allowed us to decide where to follow the report.   By  identifying the types of torture  employed and the frequency of these, we select the most repeated and contrast with reality through sources that will direct us to the victims; people willing to talk despite fear.   Once having the testimonies, we  contrast the cases with the medical records  of each and th e evaluation applied to them  of the same Istanbul Protocol as a scientific tool that will verify the words of the victims.    "," The most difficult part of the project was to identify patterns of behavior through the qualitative database and categorize it to systematize into a quantitative input: above all, to be able to affirm that it was not isolated or different situations between them.   Beyond the human empathy and pain observed, it was also difficult to work the data by doubting them, despite the compassion that the cases generated for us.   Finally, the delicacy that we had to learn through texts and experts about Human Rights and vulnerabilities issues was difficult; from thinking the language of the interviews attached to a human rights perspective as to the presentation of the final product. "," I think that the most valuable part of this work was to systematically quantify the types of repeated behaviors and contrast them with testimonies of flesh and blood.   Also, i think one of the things this report does is going beyond the individual story and gather information to check if there is a pattern behind one or two complaints. Being able to see the ""full picture"" let us get ahead of the massive complaints of human rights violations that ocurred in the months after the release of this report. ",https://www.24horas.cl/data/tortura-en-la-comisaria/,https://translate.google.com/translate?sl=es&tl=en&u=https%3A%2F%2Fwww.24horas.cl%2Fdata%2Ftortura-en-la-comisaria%2F,,,,,,"María Paz Fernández, Guillermo Acosta."," Paz Fernández is a journalist from the University of Chile and attended the Diploma of Audiovisual Executive Production at the Pontifical Catholic University. She began her career as a practitioner in the newspaper La Tercera, later as a news contributor on the Radio Cooperativa website. In the same medium was hired for the area of ​​specials, where she had to be part of teams that covered phenomena such as the 33 miners trapped underground, the 2010 earthquake and the return of the right to the government. In 2012 he joined the team of the first news site belonging to a television channel in Chile in the management of social networks, and then headed a section of thematic specials; which became reports and led to the creation of #24Data.She highlighted in the Poor Prize the One that Doesn't Change the Look 2018 and the Journalism Award of Excellence for Best Digital Special with “Nightmare behind bars: Imprisoned by mistake”. He was also part of the team that won the 2017 Digital Excellence Award with the last days of Víctor Jara.   Natalia García is the main designer and developer working in 24horas.cl. She is licensed and titled in Design from the Universidad de Valparaíso. She has participated in different design projects from their conceptualizatio to their formal implementation, mainly in the web field. Along 8 year she was the person entitled to design and implement the web standards to different companies outside of Chile while working in a multidisciplinary team. Since 2018 she integrates to TVN as part of the digital team in the press department.   Guillermo Arturo Acosta Olmedo has worked as a screenwriter, editor and director of cinematographic pieces, making a photographic and technical developing contribution currently in 24 horas news area. ",,,
Chile,www.24Horas.cl,Small,Participant,Best data-driven reporting (small and large newsrooms),Drunk history: 43% of regional capitals exceed the limited alcohol permit,13/03/19,"Investigation,Explainer,Database,Infographics,Chart,Health,Crime,Economy,Employment","Adobe,Microsoft Excel,Google Sheets,CSV"," How far from your home is the nearest liquor store or minimarket with alcohol? In Chile, most probably just a few blocks away. In our country exists a law that limits the amount of shops allowed to sale alcohol in each area and focuses on reducing the high number of alcohol consumption. #24Data analized the alcohol hops in 26 of the biggest urban areas of Chile and revealed that most of them don't comply with the limited permits for their territory. The explanation points to local representatives trying to gain support from voters and a deep lack of fiscalization.  "," This report was cited in a project for law in the Chamber of Deputies of Chile, delivering information about the law and the current state of the alcohol permits around the country. The project, which aims to reduce alcohol consumption, proposes to forbid delivery of alcohol via apps. In its official proposition, the lawmakers cite our work regarding the amount of permits and the number of them allowed. Link: h<a href=""https://www.camara.cl/sala/verComunicacion.aspx?comuid=47335&formato=pdf"">ttps://www.camara.cl/sala/verComunicacion.aspx?comuid=47335&formato=pdf</a>   Also, when we posted this work in the social media channels of 24horas.cl, our users interacted with our content confirming that they perceive an increasing amount of liquors stores and markets with sale of alcohol on their neighbourhoods, proposing and acussing their local representatives of aprobbing those permits with the objective of catching voters.     "," For the realization of this project, the # 24Data team asked via Transparency Request the lists of current limited alcohol patents to each of the 16 regional capitales and the 10 most populated communes in the Metropolitan Region. We cleaned those reports (often in pdf or with an excessive amount of commercial patents that we didn't asked for) and mixed that database with the resolutions from regional authorities that detail the maximun number of patent permits for each of the communes and their jurisdictions. Those documents were also asked for via Transparency Request to each of the regional authorities.    We used information from the National Service for the Prevention of Drugs and Alcohol (Senda) published in their reports of alcohol consumption at school age and in the general population, that data was disagregated by area.    We started this project from a tip from an anonymous source (telling us that a lot of jurisdictions where earning money from illegal alcohol permits) and the main challenge was obtaining the information to check that, specially with the reticence from the areas where this was happening, wich send us reports with numerical problems or with all the permits of the city (thousands) instead of the alcohol ones.    Finally, we had a challenge in the delivery of information very numerical and unattractive for the general audience, obstacle we overcome using visualization tools such as Infogram and Google Graphic.     "," The hardest part of this project was -with a team of exclusively two journalists- gathering, cleaning and analising all the information regarding the aprobbed alcohol permits and the resolutions wich determined the exact number of alcohol permits allowed in each area. We requested those records individually to 26 jurisdictions (via two separated Transparency Requests: one for the permits and one for the resolutions). Once we had the answers to our questions, we had to find a simple and effective ways of delivering that data to the audience.   Another challenge was to identify a link between the number of alcohol permits and the alcohol consumption in each area, wich we solved speaking with specialists in alcohol consumption and checking the records of dangerous alcohol ingest in the areas we investigated.    Finally, we focused in analizing each level of the problem: the people that consume alcohol in excess, the economic and political effect of closing an alcohol shop wich employs people, why the local authorities keep allowing more alcohol permits and how the amount allowed is fixated. "," Others can learn that is not necessary (althought it can be easier and quicker) to have a big team to gather, clean and analise data. We are just two journalists that managed to work with a big amount of information and still make it work through creativeness, order and self-capacitation.   We also tried to be extensive on our treatment of the issue analizing different levels regarding to it (individual alcohol consumption, local government, political effects of the alcohol limited permits, national law). ",https://www.24horas.cl/data/patentes-limitadas-de-alcohol-43-de-las-capitales-regionales-tiene-mas-de-las-que-permite-la-ley-3102624,https://www.24horas.cl/data/pasados-de-copas-documentos-anexos-3143244,,,,,,"Francesca Cassinelli, María Paz Fernández, Natalia García","   Paz Fernández is a journalist from the University of Chile and attended the Diploma of Audiovisual Executive Production at the Pontifical Catholic University. She began her career as a practitioner in the newspaper La Tercera, later as a news contributor on the Radio Cooperativa website. In the same medium was hired for the area of ​​specials, where she had to be part of teams that covered phenomena such as the 33 miners trapped underground, the 2010 earthquake and the return of the right to the government. In 2012 he joined the team of the first news site belonging to a television channel in Chile in the management of social networks, and then headed a section of thematic specials; which became reports and led to the creation of #24Data.She highlighted in the Poor Prize the One that Doesn't Change the Look 2018 and the Journalism Award of Excellence for Best Digital Special with “Nightmare behind bars: Imprisoned by mistake”. He was also part of the team that won the 2017 Digital Excellence Award with the last days of Víctor Jara.   Francesca Cassinelli is a journalist from the Pontificia Universidad Católica de Chila and working as a multimedia journalist in 24Horas.cl, in the section #24Data. Although she took different part time and freelance jobs during heir studies, she started her full time work dedicated to journalistic chronicle in 2013 in 24Horas.cl. In 2016 she left that newsroom to work in the web Upsocl.com, dedicated to positive and emotional stories. She came back to 24Horas.cl in 2017 to work in the ""Specials"" section, wich elaborates deeper reports and brings context to the day to day news and eventually evolved in the data journalism section #24Data.   Natalia García is the main designer and developer working in 24horas.cl. She is licensed and titled in Design from the Universidad de Valparaíso. She has participated in different design projects from their conceptualizatio to their formal implementation, mainly in the web field. Along 8 year she was the person entitled to design and implement the web standards to different companies outside of Chile while working in a multidisciplinary team. Since 2018 she integrates to TVN as part of the digital team in the press department. ",,,
Chile,24Horas.cl,Small,Participant,Best data-driven reporting (small and large newsrooms),Imminent danger in camps: families will be the first to suffer the deadly effects of climate change,31/10/19,"Investigation,Explainer,Database,Infographics,Chart,Video,Politics,Environment,Lifestyle,Agriculture,Health,Employment","Drone,Microsoft Excel,CSV"," Each year the global warming is more tangible than the previous. Heat waves, floodings and landslides are becoming part of our daily income of news from around the world. As this gets more intense every year, people who live in irregular housing or in camps will be the first ones to suffer its deadly effects. Most of the risk factors in this kind of disasters are tighly tied with the vulnerability criteria Chile meets: dry and semi dry areas, low height coastal areas, tree areas, dry prone and desertification areas.  "," Global warming and its effects are part of the regular coverage of the media in Chile. So are the natural disasters that occasionally hit our territory. However, inequality and vulnerability of some sectors of our society are often left in the dark.  This report was able to mix both elements and project something that could be fatal in the future: how climate change will first impact those in situations of greater vulnerability and need, those who do not have drinking water or pipes and whose homes are not able to withstand waves of heat, floods or landslides. The focus of this report is on those people who do not have the capacity to prevent or adapt to the eventual future climate and how they already perceive these dangerous changes. "," We began this investigation by making a Transparency Request to the Ministry of Housing and Urban Development the database of the national census of irregular housing and camps. Having cleaned and analyzed that data, we were able to create an interactive map - using GoogleMaps - to show geographically where our country's camps are located and how many families live in each of them.   With the same database - which includes information on the natural risks to which each area is exposed - we decided to visit some areas on the ground and talk to the neighbors. For this we made contact with the organization Un Techo para Chile, an entity dedicated to the construction of permanent homes and the eradication of camps in our country.   The investigation crossed the camps and risks data (for example flooding or landslides) with the increase of natural disasters in the country, showing the increase in the occurrence of the latter and the change-increase-in temperature in the last years. The source for the meteorological information was the Chilean Meteorological Directorate.   To show our analysis we used a variety of elements: photos, interviews on camera, databases, visualizations. Some technical tools we used were Excel, CSV, Infogram and DataWrapper for visualizations. "," One of the challenges we had was to clean and analyze the enormous amount of information we obtained regarding the irregular housing census made by the Ministry of Housing and Urban Development. In a team work - we are two journalists we managed to clean the database, organize it and be able to obtain relevant information from it, which allowed us to draw up a map that clearly shows where the camps are and how many families live in each one of them.   Another challenge was to take this information to different levels and not only talk about the phenomenon at the national level, but through testimonies of people who live in these places show how climate change is already evident and how it has diminished their work. A villager, for example, told us that in the area many citizens work collecting sand from a river whose channel is almost completely dry.   Finally, we set out to have an attractive audiovisual presentation that was in charge of a designer and an audiovisual director and that managed to give unity to a subject that at times could be segmented in its contents. "," Others can learn that is not necessary (althought it can be easier and quicker) to have a big team to gather, clean and analise data. We are just two journalists that managed to work with a big amount of information and still make it work through creativeness, order and self-capacitation.   We also tried to cover different levels of the information and not focused exclusively in the larger picture (nor the individual one): to show the public policies that relate to this issue, how the camps were created and what needs have the people that live in them.  ",https://www.24horas.cl/data/cambio-climatico-peligro-inminente-en-campamentos-3644745,https://translate.google.com/translate?sl=es&tl=en&u=https%3A%2F%2Fwww.24horas.cl%2Fdata%2Fcambio-climatico-peligro-inminente-en-campamentos-3644745,,,,,,"María Paz Fernández, Francesca Cassinelli, Natalia García, Guillermo Acosta.","   Paz Fernández is a journalist from the University of Chile and attended the Diploma of Audiovisual Executive Production at the Pontifical Catholic University. She began her career as a practitioner in the newspaper La Tercera, later as a news contributor on the Radio Cooperativa website. In the same medium was hired for the area of ​​specials, where she had to be part of teams that covered phenomena such as the 33 miners trapped underground, the 2010 earthquake and the return of the right to the government. In 2012 he joined the team of the first news site belonging to a television channel in Chile in the management of social networks, and then headed a section of thematic specials; which became reports and led to the creation of #24Data.She highlighted in the Poor Prize the One that Doesn't Change the Look 2018 and the Journalism Award of Excellence for Best Digital Special with “Nightmare behind bars: Imprisoned by mistake”. He was also part of the team that won the 2017 Digital Excellence Award with the last days of Víctor Jara.   Francesca Cassinelli is a journalist from the Pontificia Universidad Católica de Chile and currently works as a multimedia journalist in 24Horas.cl, in the section #24Data. Although she took different part time and freelance jobs during heir studies, she started her full time work dedicated to journalistic chronicle in 2013 in 24Horas.cl. In 2016 she left that newsroom to work in the web Upsocl.com, dedicated to positive and emotional stories. She came back to 24Horas.cl in 2017 to work in the ""Specials"" section, wich elaborates deeper reports and brings context to the day to day news and eventually evolved in the data journalism section #24Data.   Natalia García is the main designer and developer working in 24horas.cl. She is licensed and titled in Design from the Universidad de Valparaíso. She has participated in different design projects from their conceptualizatio to their formal implementation, mainly in the web field. Along 8 year she was the person entitled to design and implement the web standards to different companies outside of Chile while working in a multidisciplinary team. Since 2018 she integrates to TVN as part of the digital team in the press department.   Guillermo Arturo Acosta Olmedo has worked as a screenwriter, editor and director of cinematographic pieces, making a photographic and technical developing contribution currently in 24 horas news area. ",,,
Sweden,Breakit,Small,Participant,Innovation (small and large newsrooms),How far does the clothes you return travel? – We tracked returns from e-commerce companies with gps,11/05/19,"Investigation,Breaking news,Podcast/radio,Map,Environment,Business,Economy",Sensor," How far does the clothes you return travel? – We tracked returns from e-commerce companies with gps.   The market for online shopping in Sweden is bigger than ever, but almost one third of what people are buying is being returned.    But how far does the returns travel? Our examination with secret gps-trackers shows that returns from Sweden’s biggest ecommerce companies are sent to Estonia and Poland – just for repacking – before they are sent back to the store in Sweden. One of the parcels we track went 2360 kilometers. ", Through our investigation we put a bigger focus on what impact returns have on the climate. We also stressed the problem with customers who  buys a lot of clothes online and then send them back.    The story highlights two big social phenomenons – the climate and the growing ecommerce.   It shows that the hard competition between ecommerce companies has led to generous terms like free shipping and free returns that has created a customer behavior that is untenable in the long run.     , We bought clothes from five big Swedish ecommerce companies. When we got the parcels we hid gps-trackers from the company Automile in them and sent them back.    Then we could track the parcels on their way from our computers and see how far they went. We could see that some of them went to Poland and Estonia just for repacking.     The biggest ecommerce company – Ellos – had refused to answer questions about how they handle returns. Now we could see that they sent their returns to Poland.     , It was to find out where to find the best GPS-trackers and to make sure they worked out.  , That you can use tech to get answers you don’t get other ways. Tracking things with GPS is an effective way of showing exactly how far a returned parcel travel. ,https://www.breakit.se/artikel/22407/vi-sparade-natjattarnas-returer-sa-langt-aker-kladerna-som-du-lamnar-tillbaka,https://www.breakit.se/artikel/22410/e-handelscheferna-darfor-skickar-vi-returerna-till-estland,https://www.breakit.se/artikel/22482/ellos-morkar-for-kunderna-vi-foljde-deras-paket-till-polen,,,,,"Caroline Englund, Erik Wisterberg"," Caroline Englund is a reporter at Breakit since January 2018. She covers the area ecommerce. Before she has worked for Sveriges Radio, Dagens Nyheter and Värmlands Folkblad.   Erik Wisterberg has been a reporter at Breakit since May 2016. (Since January 2020 he works as a reporter at Svenska Dagbladet) before he has worked for Medievärlden.     ",,,
United States,Northeastern University School of Journalism,Small,Participant,Best data-driven reporting (small and large newsrooms),The Storybench 2020 Election Coverage Tracker,13/02/19,"Explainer,Elections,Politics","Scraping,D3.js,CSV,R,RStudio,Python"," The Storybench 2020 Election Coverage Tracker from Northeastern University’s School of Journalism is an ongoing project that keeps tabs on the media’s coverage of – and public discussion surrounding – the 2020 U.S. presidential election. Using data sources like the Facebook Ad Library, Media Cloud and Twitter, and mixed methods analysis, the Tracker reveals trends and gaps in both political journalism and the rhetoric made by and about the candidates on social media. Our most popular articles have revealed the negative sentiment of the media toward female 2020 candidates and how the media sets the 2020 news agenda. "," The Storybench 2020 Election Coverage Tracker has been wildly popular with media critics, journalism educators and political journalists. Most popular has been our sentiment analysis of 2020 political candidates by gender, “<a href=""https://www.storybench.org/women-on-the-2020-campaign-trail-are-being-treated-more-negatively-by-the-media/"" style=""text-decoration:none;""> Women on the 2020 campaign trail are being treated more negatively by the media </a>,” which was picked up by CNN’s Reliable Sources and broadcast nationally. Following that, The New York Times, The Washington Post, MSNBC and several other news outlets covered the analysis. Our most recent analysis, ”<a href=""https://www.storybench.org/how-news-media-are-setting-the-2020-election-agenda-chasing-daily-controversies-often-burying-policy/"" style=""text-decoration:none;""> How news media are setting the 2020 election agenda: Chasing daily controversies, often burying policy </a>,” was shared by popular media critics like Jay Rosen and Brian Stelter, and provided empirical data for a long-standing debate about political journalism.        In addition to providing the public and researchers with data and open-source methods (and code) for important questions related to political journalism, the Storybench 2020 Election Coverage Tracker is a teaching tool, permitting several journalism undergraduate and graduate students at Northeastern University to learn these data and media analysis techniques and publish articles of their own.      "," The Storybench 2020 Election Coverage Tracker initially started with textual analysis techniques from natural language processing including text mining with TF-IDF and dictionary-based sentiment analysis, provided by R’s “tidytext” package and performed in RStudio. As the project matured, it began using R and Python wrappers for various APIs (including the <a href=""https://www.storybench.org/pete-buttigieg-is-flooding-iowa-and-new-hampshire-with-facebook-ads-about-his-climate-plan/"" style=""text-decoration:none;""> Facebook Ad Library </a> and Twitter) and built a public-facing R Shiny app “<a href=""https://www.storybench.org/illuminating-the-road-to-2020/"" style=""text-decoration:none;""> Illuminating the road to 2020 through media coverage, candidate tweets and Google searches </a>” to display findings and allow users to explore the datasets and their own questions.   The project has also used more complex techniques like structural topic modeling, as well as Python scrapers (and the newspaper3k library) to collect full news articles and D3 Javascript to <a href=""https://storybench.org/2020topics/"" style=""text-decoration:none;""> visualize their distribution </a>. And to illustrate <a href=""https://www.storybench.org/facebook-political-ads-reveal-an-early-but-limited-look-at-the-democratic-playbook/"" style=""text-decoration:none;""> geographic distribution </a> of Facebook political ads, the project used GIS packages in R.   The project, like any in media criticism, relied on more analog, qualitative techniques. We measured news attention by eye, logging <a href=""https://www.storybench.org/fox-news-obsession-with-aoc-crowded-out-2020-candidate-announcements/"" style=""text-decoration:none;""> Fox News’ obsession </a> with Alexandria Ocasio-Cortez, and tallied up by hand the <a href=""https://www.storybench.org/men-are-writing-two-thirds-of-national-stories-about-the-2020-presidential-race/"" style=""text-decoration:none;""> gender of political reporters </a> covering the 2020 election.   Finally, for techniques we were not equipped to deploy ourselves, we collaborated with state-of-the-art companies like MarvelousAI to examine the volume and nature of <a href=""https://www.storybench.org/quantifying-the-twitter-attacks-on-kamala-harris-during-and-following-the-democratic-debates/"" style=""text-decoration:none;""> attacks on and support for 2020 candidates </a> on Twitter, providing analysis and visualization.   "," Lining up the right data set with the right research question and the right analytical tool was the most difficult part of each project. Doing that over and over again for almost 20 posts was a steep challenge. There were a lot of dead ends and wasted time and code. It’s also really hard to break through on a topic where (almost) everyone has an opinion! That’s why we hoped bringing data and open-source methods and code to bear on this project might help us get our work out farther – and add an iota to the discussion around 2020 election. Each challenge, of course, was a learning opportunity for our faculty and students and we’re fortunate to have managed to raise a few eyebrows outside of Northeastern along the way. "," The Storybench 2020 Election Coverage Tracker has been a fantastic opportunity to do data journalism from within an institution of higher education. The School of Journalism faculty and students without the deadlines and strictures of a more traditional newsroom, have had the time and resources to pursue research questions in the public interest and the space, Storybench.org, to disseminate our results (and, in many cases, code) widely. We hope that others in the data journalism community may see the value in collaborating with data journalism students and faculty who may have the time, skills and resources to collaborate on data collection, analysis and visualization for projects as ambitious as understanding the media coverage of U.S. politics.  ",https://www.storybench.org/category/election-tracker/,https://www.storybench.org/how-news-media-are-setting-the-2020-election-agenda-chasing-daily-controversies-often-burying-policy/,https://www.storybench.org/men-are-writing-two-thirds-of-national-stories-about-the-2020-presidential-race/,https://www.storybench.org/history-repeats-itself-as-women-on-the-2020-campaign-trail-receive-more-negative-coverage-than-their-male-peers/,https://www.cnn.com/videos/business/2019/04/07/women-candidates-facing-harsher-news-coverage.cnn/video/playlists/business-reliable-sources/,,,"Aleszu Bajak, John Wihbey, Dan Kennedy, Meg Heckman, Alexander Frandsen, Alexa Gagosz", The Storybench 2020 Election Coverage Tracker is run by a team of faculty and students at Northeastern's School of Journalism. ,,,
United States,"Better Government Association, DataMade",Small,Participant,Best news application,Illinois Public Pension Database,18/11/19,"Database,News application,Infographics,Politics","Google Sheets,PostgreSQL,Python"," The Better Government Association’s Illinois Public Pensions Database is the most comprehensive source of information about public pensions in Illinois, featuring more than seven years and 3 million lines of data.   A Chicago nonprofit newsroom, the BGA produces the database as part of its mission to make government throughout Illinois more transparent and accountable.  Shortfalls with funding Illinois’ public pension systems are perhaps the biggest emergency facing the state today. The BGA believes that by collecting this data and centralizing it into one place that is easy for citizens to access our organization brings greater clarity to this important topic. "," Illinois has become a state divided over the issue of public-employee pensions. Conservatives and some business leaders see them as a drain on resources that have brought Illinois to its fiscal knees while unions — threatened by such sentiments — have become fiercely protective of pensions as they defend their members and their own political power.   The Illinois Public Pensions Database aims to push aside such political banter and provide residents with unambiguous facts that will give them clear ideas about what is really behind the biggest issue affecting the state.   The project unearths details buried in complex fiscal reports and puts that information into an easy-to-use, first-of-its-kind tool that informs policy makers, media members and citizens whose lives are affected daily by this complex and intimidating issue.   The database provides an amount of detail on the critical topic of municipal and state finances that had never before been collected in one place. It added nuance to a discussion that could often devolve into empty talking points.   Though the project was unveiled only a few months ago, it drew early interest that has remained steady. Illinois policy experts and news organizations have regularly cited the database in articles, research projects and policy positions.   The database allows users to view basic information about the major pension funds including number and type of pension, annual payment, cost of debt and distribution of pension payments. It can be searched by fund and by year. Funds can be grouped by geography. It also revealed the cost of pension underfunding in a way that added great clarity to the issue. "," All records were obtained in a text-delimited format from Illinois’ 17 largest pension funds through public-records requests. The status of the request was monitored on a Google Sheet that tracked dates and responses by agency.    Data cleaning and organization for this project was performed in MySQL. The database itself is managed with Django. PDF reports were obtained from a state agency that provides data to state legislators about the fiscal health of the pension funds.    Agencies were categorized and uploaded using Python. Pension funding data was pulled from reports, entered into a CSV file, and entered into Django.    The figure for back debt paid annually on pensions had to be calculated by using the available data from each report. The total normal cost — or the money spent by the fund to pay for annual pension benefits — was matched with the employer normal cost, or amount paid by the agency to make up the difference between the pension cost and the employee contribution, and operational costs, revealing the amount used to pay back debt created by years of underfunding pensions.   "," Beyond normalizing the data reports from multiple agencies, finding the exact figures needed for the project from the state reports and making sure they accurately reflected the pension conditions was paramount to making the project work. While there was no accompanying story, this application was a journalistic project, meaning it was subject to the same standards of accuracy that an investigation would be.    Making sure that all of the information presented was useful both to pension experts and everyday users was also important.    There is also difficulty in sending 15 public-records requests. Agencies must be contacted regularly if they do not comply with the request. It took about six weeks to obtain all of the records for 2019, and data cleaning took about three weeks. The process will be repeated each year going forward.    "," The project shows how packaging publicly available information can serve multiple audiences. The tool can serve pension experts as well as users looking to learn more about the topic.    While the topic area might not be interesting for all audiences, this shows a way to make a complicated topic easy to understand for all users. It also includes enough data to make it useful for experts on the topic.   Government financial data is a cornerstone of the BGA approach. Projects like the Public Pension database show that a focus on these topics pay dividends to organizations willing to put the time and effort into building and maintaining these things.   Data like this are available in many U.S. states as well as other countries. Other organizations could use this model to build something that would be of interest to their local users. Pension problems are different in other places, and the Illinois Public Pension Database might be a starting place for other organizations to start to create something of interest to their audience.    This project also serves as a good reference for the BGA team, as well as reporters at newsrooms throughout the state of Illinois. Anytime they are reporting on a figure collecting a pension, they can pull up years of data instantly to add context and character to their reporting project. ",https://pensions.bettergov.org,https://pensions.bettergov.org/user-guide/,,,,,,"Jared Rutecki, Hannah Cushman Garland, John Chase, Bob Secter"," Jared Rutecki is an investigator and data coordinator with the BGA. He manages the BGA databases, and creates stories on state and local government matters, including topics such as pensions, payroll and crime. He also conducts analysis for other investigators at the BGA.   Hannah Cushman Garland is a wayward journalist turned software developer. She cut her teeth on public life in mid-Missouri, covering municipal economic development (ask her about <a href=""https://www.columbiamissourian.com/news/columbia-s-blight-debate-a-question-of-jobs-vs-neighborhood/article_3a96e676-20a0-5bc0-93ba-7f153197ff12.html"">enterprise</a> <a href=""https://www.columbiamissourian.com/news/columbia-s-blight-debate-eezs-value-as-job-creation-tool/article_caf34108-c48e-5d75-ac9b-506f74b19332.html"">zones</a>) and elections. An alumna of the Missouri School of Journalism and a veteran of the Associated Press, Hannah remains deeply interested in how information is consumed, shared, and acted upon. At DataMade, she loves projects that derive meaning from data, both narratively like <a href=""https://justicedivided.com/"">Justice Divided</a>, and practically, like <a href=""https://dedupe.io/"">Dedupe.io</a>. She's also devoted to documenting of <a href=""https://github.com/datamade/testing-guidelines"">common</a> <a href=""https://github.com/datamade/data-analysis-guidelines"">patterns</a> and commenting clever functions. In her spare time, Hannah enjoys pondering particle physics, eating just about anything, and simply existing in the Lincoln Square apartment she shares with her husband, plants, and cats.   John Chase is the director of investigations at the Better Government Association. He was named to the post after 18 years as a general assignment, political and investigative reporter for the Chicago Tribune.   Bob Secter was senior editor for the BGA. He has previously served as a political reporter and Political Editor of The Chicago Tribune, a national, Southeast Asian and congressional correspondent for the Los Angeles Times, and the Springfield bureau chief for the now defunct Chicago Daily News. ",,,
United Kingdom,Carbon Brief,Small,Participant,Best visualization (small and large newsrooms),Interactive: How the UK transformed its electricity supply in just a decade,06/12/19,"Explainer,Long-form,Database,Infographics,Chart,Map,Environment","Scraping,D3.js,JQuery,Json,Creative Suite,Google Sheets,Python"," In June 2019, just as the UK became the first nation in the world to legally commit itself to a ""net-zero"" carbon emissions target by 2050, Carbon Brief published an <a href=""http://click.revue.email/mpss/c/JgE/cM9xAA/t.2sg/BSvvKIa5QiCcoZnHZ2ORfQ/h2/p-2BJCe4G4VLTYm5MjgYIuSOvwV8jVBl037qFRfUcKIAf186AVM3-2BOhtja9qJl9peEjwHmiKCwzFxd8LKiDkrjeC3NtwLwWl9YQNMV0G9A2OAcGe99CNhMOkHA3rDNcy6oQV9aJN2OVwu728j2GchxR4q2Pz1soKaCWdzxnUc2g4xQTqg6J3zwKDl4t-2FRhDjW8UXpB3OLbuCGZL9Hx7f18Ofvk6r1uwKasismGeTMIzfA-3D"" target=""_blank"">interactive feature</a> and accompanying map showing exactly how the UK had radically transformed its electricity supply in just a decade. The ""scrolling narrative"" article took more than a year to research, write, design and code, and brought together six datasets and 800,000 data points. It was the first time this dramatic transformation to the UK grid had ever been visualised in such detail before. "," The feature was a huge hit with Carbon Brief's readers straight away and has since become one of the website's most-read and most-shared articles since first launching in 2011. Civil servants even told Carbon Brief that the UK's climate minister, Claire Perry, had shown the article in her own presentations and had ""pinned a copy on her wall"".   Rachel Kye, the special representative of the UN Secretary-General for sustainable energy, <a href=""https://twitter.com/rkyte365/status/1139261280575991810"">tweeted</a> a link to her followers, adding: ""We talk about energy transitions and it is wonderful to be able to see one. Dr Simon Evans et al at Carbon Brief produced this. Fascinating. Superb. Important.""     "," It took several months to identify the various databases holding the information required and then establish whether the data could be scraped, verified and ""wrangled"" into a single dataset. The 800,000 data points were carefully melded together using Python and then visualised into the scrolling narrative and map using D3 and Mapbox GL JS. These tools allow the map within the article to be interactive and, for example, let the user ""journey through time"" from the handful of coal-fired giants that dominated UK generation a decade ago up to the reality today which sees a much more decentralised grid with hundreds of windfarms and thousands of solar rooftops.   This data had never been amalgamated and presented in this way before. Therefore, great care was taken to check the underlying geo-tagged data by doing random samples of generating sources and comparing their location on Google Maps. During this long process, Carbon Brief found several errors in the primary datasets and notified their owners. The article contains a lengthy “Methodology” section to ensure full transparency and allow the reader to see exactly how the data was handled and then visualised. "," In 2015, Carbon Brief had tried to ""<a href=""https://www.carbonbrief.org/mapped-how-the-uk-generates-its-electricity"">map</a>"" the UK's power generation using Carto. While that provided a fascinating snapshot of the UK's grid for that year, it could not show the rapid changes the grid had seen since the end of the previous decade. So it was important for the 2019 iteration to provide the reader with an interactive narrative - part in-depth feature, part data visualisation.   A lot of time was spent by the writer Simon Evans and multimedia journalist Rosamund Pearce working out how to make key moments and insights in the article ""spark"" a corresponding visualisation in the map as the reader scrolled down.   They also found that, due to the article's length - almost 7,000 words - it was important for the reader to always know which year they were looking at as the map changed visually before their eyes. So a legend was introduced on the right-hand side of the page to highlight the year-by-year transition as the reader descended down through the article. Similarly, a stacked bar chart showing the year-on-year change in generation sources was also introduced on the left-hand side.   The article then concludes with a full-width interactive map of all the data to allow the reader the chance to explore it themselves in detail using the dashboard. This was added at quite a late stage when testers said they wanted to ""play with the data"" by themselves as well as be guided through it by Carbon Brief.   A sense of the thought process that Carbon Brief went through when designing this feature can be seen in this early rough sketch of how it might work...    <a href=""https://twitter.com/_rospearce/status/1140627284610682880"">https://twitter.com/_rospearce/status/1140627284610682880</a> "," Consulting and testing are two key lessons that Carbon Brief took away from this project. The two journalists who researched, wrote, designed and coded this feature consulted at each stage with the whole team (8 staff) to get as much feedback as possible, particularly to ensure ""usability"" and a ""sense of journey"". Carbon Brief also asked a small handful of trusted contacts to ""road-test"" the interactive ahead of publication to further refine it.   It took a lot of time and resource to complete the project - Simon Evans had to teach himself Python along the way! - but it was a perfect example of how such an investment can pay off. An ambitious (yet daunting) vision was combined with the creativity and skills of the two journalists to produce one of Carbon Brief's most influential showcase articles. ",https://interactive.carbonbrief.org/how-uk-transformed-electricity-supply-decade/,https://twitter.com/_rospearce/status/1140627284610682880,,,,,,"Simon Evans, Rosamund Pearce"," Dr Simon Evans is Carbon Brief's deputy editor and policy editor. Simon covers climate and energy policy. He holds a PhD in biochemistry from Bristol University and previously studied chemistry at Oxford University. He worked for environment journal The ENDS Report for six years, covering topics including climate science and air pollution. He joined Carbon Brief in 2014.   Rosamund Pearce was Carbon Brief's multimedia journalist, focusing on graphics, interactives, videos and other multimedia, from September 2014 to June 2019. Rosamund now works as a visual data journalist at the Economist. ",,,
Malaysia,Malaysiakini,Big,Participant,Best visualization (small and large newsrooms),On the trail of parliamentary defectors after Malaysia's 14th general election,15/02/19,"Explainer,Chart,Elections,Politics","D3.js,Microsoft Excel,CSV"," The balance of power in the parliament of Malaysia is constantly changing due to a string of floor crossings after a historic general election in 2018 which saw the country experienced its first-ever regime change.   Our project focuses on changes in party allegiances by members of parliament (MPs) in the lower house of the Malaysian parliament. The special tracker page we created is to record all MPs defections since the general election.   It aims to help Malaysians and journalists keep track of changing allegiances, the current balance of power, and changes in the number of lawmakers for each political party.    "," The story has accumulated 126,000 page views within five weeks since its publication, from Feb 15 to March 22, 2019. It has over 115,000 unique page views.   There are always spikes in traffic to the page when a defection by a Member of Parliament occur, indicating readers are using the page as a reference to understand political developments.   The project was published in three languages - English, Mandarin, and Malay - in order to reach as many Malaysians as possible. The three pages have been shared a total of 4,500 times on Facebook. "," The page was created with HTML, CSS and Uikit front-end framework.   The changes in the political allegiances of lawmakers were recorded in a CSV file. They were based on the announcements made by the lawmakers when they quit their party to join another, or become independent.   The data in the CSV file include their original political party, their new allegiances, and the dates of them switching camp. The timeline and waffle charts were drawn based on the CSV file with JavaScript and D3.js library.   There is another CSV file that records the changes in the number of seats in the lower house of Parliament held by each political party. The data is presented in a table on the published page.   The timeline, waffle charts and table can be updated easily by changing the data of the CSV files. "," The hardest part of this project is to find the best way to show the chronology of the changes of party allegiances by each of the members of parliament (MP). We spend quite some time debating on how the data should be presented.   Initially, we only thought of creating just a waffle chart, which has been used by many newsrooms to present the balance of power of the parliament/congress of different countries, to show the latest status of the Malaysian MPs. However, since the page is about tracking the movements of the defected MPs, waffle chart is not the best chart to identify those MPs and to reflect the changes of their party allegiances over time.   As such, we decided to use a timeline to present the changes and two waffle charts to display the latest balance of power in the lower house of the parliament. One shows how many seats are needed for the ruling coalition to gain a two-thirds majority in the lower house. Another waffle chart shows the changing composition of the opposition.   We think we should be selected because there are not many similar journalism projects in tracking political defections. Our project does not just track when a lawmaker quits a party, but also which party he or she joins next.   We also provide an option for readers to compare the composition of the government and opposition based on the general election results, versus the latest scenario. Given that the loyalties of Malaysian lawmakers are constantly changing, the information, timeline and chart on the tracker page can be updated accordingly.   Sufficient text narrative is provided to help readers understand the context behind the series of events. It also shows the bigger picture in the consolidation of power over time. "," We think that using a timeline to track the changes of allegiances of the lawmakers over time could inspire other newsrooms as there are not many similar projects. It is a good way to identify individual politicians who cross the floor.   Another import aspect is the way of using of waffle chart to show the balance of power of a parliament/congress. Oftentimes, we see newsrooms use a single waffle chart to show an election result.   However, we see there is room for improvement as a single waffle chart may cause confusion for the readers. It is because a waffle chart could be viewed from either side and the way the readers interpret the chart might not be the same with what the newsrooms intended the readers to.   As such, we decided to present the data with two waffle charts to show the number of seats the ruling parties and the opposition have respectively. As to guide our readers to interpret the charts as we intend to, we only partially coloured the parts of the charts to make them more focused and be understood easily. ",https://pages.malaysiakini.com/defectors/en/,https://pages.malaysiakini.com/defectors/zh/,https://pages.malaysiakini.com/defectors/my/,,,,,"Lee Long Hui, Sean Ho, Syariman Badrulzaman, Andrew Ong"," Lee Long Hui is the writer, programmer and chart creator for this project. Long Hui has been a journalist for 10 years. Since 2018, he has led the Kini News Lab, a unit within the newsroom which aims to use innovation and technology to do better journalism. Under his leadership since 2018, the Kini News Lab has produced two dozen projects and set a benchmark for the Malaysian journalism industry.   Sean Ho Senior Products Manager and web designer for the project. Sean primarily works in web design and product management at Malaysiakini, and had previously volunteered to work on various editorial projects. As a result of several successful editorial projects, he is now seconded to the newsroom support the Kini News Lab.   Syariman Badrulzaman is the graphic designer of Malaysiakini and also one of the members of the Kini News Lab.   Andrew Ong is the news editor of Malaysiakini, he did research and data mining for this project. ",,,
United Kingdom,BBC Shared Data Unit,Big,Shortlist,Open data,BBC Shared Data Unit,01/10/19,"Investigation,Solutions journalism,Multiple-newsroom collaboration,Open data,Fact-checking,Map","Personalisation,Scraping,Json,Microsoft Excel,Google Sheets,CSV,R,RStudio,OpenStreetMap"," A commitment to transparency and the open data movement is at the heart of our team and our output.  The Shared Data Unit’s remit is twofold - to find and clean unexamined data sources and put them into the public domain for journalists to use, and to train up the next generation of regional data journalists to use open data for their own stories.  We demonstrate our commitment to transparency through sharing source data, methods and code for every project upon which we embark. Our innovative industry partnership has had tangible positive impact upon the regional news marketplace. "," Our world-class training programme has impacted on the scale and quality of regional data journalism across the industry.  We strengthen local news output by giving journalists the skills to interrogate data and tell stories of importance to their communities.  Journalists benefit from dozens of training sessions over a three-month period, including sessions on Excel, Open Refine, Flourish, Datawrapper, Freedom of Information laws, how to use APIs and the programming language R.  In March 2019, JPI Media launched its first dedicated data journalism team. Five of its 11 members were our former secondees. Newsquest also launched its own unit in June 2019 staffed by three SDU alumni.   Secondly, our original journalism sourced from open data has generated hundreds of stories across the regional news marketplace.  In 2019, the data we published,  generated around 300 stories for local news partners, bringing the total to around 850 stories since the project began.  Our journalists focus on stories that matter to local audiences - the state of their roads, the cost of garden waste recycling and the ability to get a GP appointment outside working hours.  Our reporting was picked up for 15 reports in national newspapers and 11 times on flagship BBC TV and radio programmes including the 1pm national TV news, Today on BBC Radio 4, BBC Radio 5 Live, BBC Reporting Scotland and Wales Today. It informed 68 local radio reports.   Finally, our journalism regularly provokes public debate.  For example, on the day we reported one in two people who appealed in court against a decision to deny them disability benefits was successful, our research was raised at First Minister's Questions in Scotland and the Secretary of State at the Department for Work and Pensions (DWP) faced questions about the department’s performance on the campaign trail ahead of the General Election. "," Our commitment is to use technical skills to bring untapped, but open data sources into the public domain and make them accessible to journalists.  Our data, code and methodologies are published on our Github repository, posted as inline links to Google in our stories, and shared with more than 900 media titles across the UK.   For example, we wanted to explore whether the electric car charging point infrastructure was being developed at a sufficient speed to meet the anticipated rise in demand.  We interrogated an open-source API to report the locations of UK electric vehicle charging points.  We used R and the Haversine formula to perform 49 million calculations for each distance between each of the 7,000 UK charging points, using their latitude and longitude coordinates and then storing the shortest distance.. Six months later the government published charging points locations data for the first time while encouraging local councils to improve charging infrastructure.   In another project, we used R to merge together five years’ of police statistics for an analysis revealing community resolution orders – informal punishments which do not appear on criminal records - were still being used by police against suspects of violent crimes, despite guidance restricting the orders’ use to low-level offences.   Another project saw us produce the largest UK-wide report on the re-sale of homes bought under the Right to Buy policy, which allowed council tenants to purchase their former council homes at a discount. When Northern Ireland Housing Executive sent its FOIA response as PDFs including 83 pages of scans of a paper ledger, we manually entered data into a public-facing spreadsheet to enrich the data commons.  "," We have set out on an ambitious project to ensure the vast amount of open data published in the UK is reported on by local newsrooms.  We want the data we find and analyse to be used as widely as possible, and that involves breaking down journalists’ aversion to using data for their stories.  In order to break down barriers, we use a number of techniques, which include answering queries through a Slack Channel and publishing ‘how to’ guides to accompany the data we distribute.  In addition, we have held hack days and conferences where journalists are invited to learn more about handling and processing data for news stories.  And the journalists who have completed our secondments regularly hold data training sessions when they return to their substantive posts, ensuring the skills they have picked up cascade down to their colleagues.    Another challenge is posed by organisations who have not previously been exposed to the level of scrutiny.  During the Right to Buy investigation we had around 150,000 rows of data – each row representing a property title in Great Britain formerly sold under the policy. Both HM Land Registry and Registers of Scotland underline they cannot guarantee their datasets are error-free.   We found some 60,080 rows did not have comparable sales prices. When we highlighted this, Land Registry re-ran its script and found 2,582 more price entries so we updated our calculations. Similarly, we encountered 734 dates anomalies in Scottish data.  It’s important to treat data as a source just like any interviewee and ask it questions and not treat its first answers as objective truth. "," Our project was established to tap into public interest open data, which was hiding in plain sight, and to train up journalists with data journalism skills.  We have demonstrated how adopting open data principles enhances the trust in our reporting, builds a personal relationship with audiences and helps to engage them in the process of journalism.   We have demonstrated that the openness of data in itself is not enough, it is necessary to engage with journalists and help them find what they’re looking for and understand its implications for their audiences.  Through training key individuals, the levels of data journalism can be enhanced across a media landscape.  We give the journalists who undertake a secondment with us the skills and confidence to deliver their new-found knowledge to colleagues when they return to their newsrooms.  Our two-day Train the Trainer course equips participants with the skills and techniques to prepare, plan and structure a training session.  Secondees from Newsquest and JPI, for example, have carried out training days for colleagues upon completing their secondments. ",https://www.bbc.com/lnp/sdu,https://github.com/BBC-Data-Unit/shared-data-unit,https://www.bbc.co.uk/news/uk-47696839,https://www.bbc.co.uk/news/uk-47443183,https://www.bbc.co.uk/news/uk-49891159,https://www.bbc.co.uk/news/uk-49085346,https://www.bbc.co.uk/news/uk-47697778,"Peter Sherlock, Alex Homer, Eileen Murphy, Matthew Barraclough, Anna Khoo, Paul Lynch","  Pete Sherlock  is the Assistant Editor of the Shared Data Unit. In 2015 founded a team which produced data-driven stories for the BBC website before moving into his current role. He passionately believes data journalism is informing citizens and holding those in power to account. He was formerly a reporter for Newsquest’s Enfield Independent, and went on to work for Archant’s Hackney Gazette and the East London Advertiser before joining the BBC in 2010.    Alex Homer  is a Senior Journalist on the Shared Data Unit. Before joining the BBC in 2014, Alex was a senior reporter for the Express & Star daily newspaper based in Wolverhampton. He is a specialist in the Freedom of Information Act and loves public interest reporting. His first foray into data journalism was investigating the accounts of the 92 clubs in the Premier League and English Football League.    Eileen Murphy  is the Executive Editor of Digital England for the BBC. She was involved in the earliest incarnation of the Local News Partnership and worked with regional news colleagues to build the Shared Data Unit proposition within the strategy. She now oversees the unit and BBC digital output across England.    Matthew Barraclough  is Head of the BBC Local News Partnerships scheme, which he helped design over years’ of collaboration with the local news sector starting from the Revival of Local Journalism conference he organised in 2014. Matthew has been a BBC journalist since 1995, mainly in the North East of England in roles including reporter, programme producer and eventually Editor of BBC Tees in Middlesbrough.    Paul Lynch  is a journalist on the Shared Data Unit. He began his career with Johnston Press newspapers. While he was chief reporter for the Northampton Chronicle and Echo, he was awarded Johnston Press’s journalist of the year award twice – once for exposing the corruption at the heart of the town’s football club and a second time revealing the extent of historical abuse in a locally-formed religious sect. From 2016 he was also a member of The i newspaper’s investigations unit, which won a Mind Media Award for its work tackling the subject of suicide among military veterans.    Anna Khoo  is a journalist on the Shared Data Unit. Anna is a proud coding geek and specialist user of R. She was formerly a senior reporter on the Chichester Observer and its sister titles in Sussex, where she worked for three ",,,
Kenya,Africa Check,Big,Participant,Best data-driven reporting (small and large newsrooms),"Yes, Ethiopia ‘the fastest growing economy globally' – but it's all in the details",17/12/19,"Explainer,Solutions journalism,Database,Open data,Fact-checking,Illustration,Business,Economy,Employment","Microsoft Excel,Google Sheets"," The project sought to establish the truth behind the claim made by a renowned analyst and president of African Policy Institute, a Nairobi-based geopolitical think tank, Dr. Peter Kagwanja, that Ethiopia found in the Horn of Africa was the fastest growing economy in the world. Dr. Kagwanja made the claim during a morning talk show on world economies on one of Kenya's major television channel. I sought to pursue the claim as he stated that, ""Ethiopia is the fastest growing economy globally"" by doing depth research and interviewing economists from reputable organizations for their views on the claim.   ","  The in depth fact-checking project helped to get rid of the source misinforming the public from the claim after establishing that indeed Ethiopia was the fastest growing economy in the world. We live in the era of fake news and fact-checking claims made by people holding leadership positions is important not only to hold them accountable but also to keep the public well-informed. The project put Ethiopia, a country that has been bediveled by disasters such as drought and starvation in recent past, on the global arena among world economic giants like China, India, US, among others, that always realize rapid economic growth. This enables prospective investors around the world to look at Ethiopia as a possible investment hub in Africa in addition to countries in the continent looking at it for benchmarking on how their economy can move upward. The expert views by economists and scholars from reputable organizations in the world such as IMF and World Bank enabled Ethiopia's leadership to put into consideration issues raised particularly sectors that need to be opened up for the country's economic growth to rise further. The project saw Africa Check gain more followers both on its site and social media handles as a trusted non-profit media organization that separates the truth from falsehoods. ", In depth researching through fact-checking the validity of the claim was the main techinque used. The lengthy process of fact-checking involved digging between piles of global economic reports by reputable organizations such as the International Monetray Fund and World Bank for the last ten years of Ethiopia's economic growth to arrive at a more accurate conclusion. Interviewing economists and scholars from reputable institutions with vast knowledge on world economies was another technique used in fact-checking. Interviewing involved phone calls and emails. I used my laptop and internet for researching  and included hyperlinks to numerous reports from organizations and quoted in the story. I also used photos (courtesy) in the story showing Ethiopians going about their economic activities. I further used the explainer technique to make clear certain concepts that my readers might not understand such as why many Ethiopians are still struggling to make ends meet despite their country realizing an outstanding economic growth globally.       , The hardest part was looking at data on Ethiopia's economic growth over a period of 10 years in fact-checking to come up with a more accurate verdict on the claim. It was so demanding a process going through numerous reports and comparing Ethiopia's figures with all world countries over the 10-year period. Finding economists with fast knowledge on economies of African coiuntries was also the hardest part and this made the project to take months before its completion for publication. The jury need to apprise themselves on economies of African countries and the world since Ethiopia isn't the biggest economy even among regional nations despite being the fastest growing economy. The project should be considered for selection because it sought to give the public reliable (fact-checked) information which can also be used as an authority by researchers in this era of fake news.  ," This fact-checked information is useful for others to differentiated between the truth and falsehoods propagated by various people in leadership positions. This information helps other people learn how researching and fact-checking in important in establishing the truth from claims made to hold leaders accountable and the public to remain well-informed. The project helps the world to know the status of economic growth of various countries whether in developing or developed nations. For instance, the remarkable economic growth by Ethiopia globally can easily attract investors around the globe to invest in various sectors thus spurring its growth further. The project enables journalists and researchers to appreciate the importance of fact-checking claims to rid fake news consumed by many people. The misinformation makes the public to make wrong decisions and end up regretting. The project enables other people understand how data-driven journalism is important in arriving at the truth from the claim made.       ",https://africacheck.org/reports/yes-ethiopia-the-fastest-growing-economy-globally-but-its-all-in-the-details/,https://africacheck.org/reports/kenya-logging-ban-do-senators-claims-about-gdp-and-demand-add-up/,https://africacheck.org/reports/are-nearly-80-of-kenyas-farmers-smallholders-no-data-shows-this/,https://africacheck.org/reports/top-kenyan-lawyer-gets-parliament-and-judiciary-funding-numbers-wrong/,,,,"Mr. Samuel Kisika, Mr. Alphonce Shiundu"," Mr. Samuel Kisika   I'm a multimedia journalist with eight years of experience and passionate about reporting on politics and governance, and diplomacy. Since 2012, I have reported for Kenyan mainstream media  and international news outlets including the Turkish State media, Anadolu Agency; Demotix news agency, United Kingdom; News24, South Africa; and DW's Reporting on Good Governance Kenya, Germany . I joined Africa Check, Africa's leading independent fact-checking organization, as a writer and researcher in March 2019. I'm also a creative writer for short stories and novel (waiting publication) which defines my excellence in written and verbal communication skills. I'm passionate about responsible journalism not only in Africa but the world at large in keeping the public informed. I hold a BA in Journalism and Media Studies from The University of Nairobi, Kenya.          Mr. Alphonse Shiundu       He is a multimedia journalist and Africa Check's Kenya pioneer Country Editor with 13 years of experience in the media industry having specialized in political, legislative and development journalism. He has strong global professional network being a 2017 Chevening Scholar at the University of Westminister, London; a 2015 alumnus of the Young African Leadership Initiative Regional Learning  Centre in Nairobi; a 2013 Media Fellow of the Professional Fellows Exchange Programme, Washington DC; and a 2012 fellow of the School of Authentic Journalism, Mexico City. He has excellent verbal and written communication skills, expertise in media training currently on fact-checking and is passionate about journalism in East Africa and the continent at large. He worked as an independent journalist and content developer before becoming Africa Check's Editor in January 2017. Before used to work as a Parliamentary Editor for The Standard Newspaper, Kenya's second leading newspaper, after moving from The Daily Nation, Kenya's top newspaper, as a senior reporter. Mr. Shiundu holds an MA in Media and Development from the University of Westminister, London; and a Bsc Information Sciences, Publishing and  Media Studies from Moi University, Kenya.    ",,,
Russia,OVD-Info,Small,Participant,Open data,How Article 20.2 is used by courts: statistics,30/06/19,"Database,Open data,Politics,Human rights","Scraping,D3.js,Json,CSV,Python"," Our project reveals the problem of prosecution of rally participants in Russia. The most widely used legal tool for that is Article 20.2 of the Code of Administrative Offences. We parsed and published the court statistics on this topic for 15 years and all Russian regions, providing built-in analytical tools, interactive maps and charts, and text descriptions with it.  We describe how the punishment for rally participants was changing, becoming more severe. We also describe the main numbers and characteristics of the prosecution of rally participants in the FAQ section and automatically generate a description for each region. ","   All gathered data is open and can be downloaded and used by journalists and researchers. Besides the publication of the data, we provide built-in analytical tools and interactive visualizations so that the users could draw their conclusions from the data provided.  Our data was widely used by other media especially when the attention of Russian society was attracted by massive demonstrations that took place in Moscow this summer. Since our project was published we counted 21 publications in other media where our data was used. We expect that journalists will keep using our project as we plan to parse new data as soon as it’s available on the Judicial Management Department’s website (at the end of March 2020). ", We wrote Python scripts to scrape the data from the websites of the Judicial Management Department and the Russian Federal State Statistics Service and prepare it for the publication. The project's page was developed as a one-page-application using the Vue JavaScript framework using D3.js for data visualization. ," The summarised court statistics is collected by the Russian Supreme Court. However, the data on how often people are accused and convicted under article 20.2 on the regional level is not published openly. To get access to this official data one has to write a request to the Judicial Management Department and wait several weeks.  We managed to get court statistics for 15 years (2004-2018) and not only for the whole country but also for every Russian region. We parsed the data and republished it in a machine-readable format so that it became possible to compare this data and analyze it more deeply. We also enriched this data with data on the population of Russian regions. "," The project clearly shows that rally participants in Russian are getting heavier punishment in recent years: the fines for demonstrators are getting bigger and the new forms of persecution such as community works are being used.  However, the provided data can help to answer more specific questions regarding the dynamics of individual metrics such as the number of cases processed by courts under article 20.2, the number of cases with specific outcomes (conviction, dismissal or other outcomes), cases with different types of punishments, such as fines, arrests, community works, and other metrics.  All these metrics can be retrieved for the whole country as well as for every region in Russia. ",https://data.ovdinfo.org/20_2/,,,,,,,"Daniel Beilinson, Ekaterina Borovikova, Natalia Smirnova"," OVD-Info is an online monitoring project that keeps track of arrests and detainments launched by a group of volunteers in an initial response to mass arrests in 2011. In the years following, the project has expanded to look at all issues of freedom of assembly and political oppression, collecting information, personal stories and data on people who have been detained, generating public support for their rights to free expression.   Daniel Beilinson - a programmer,  his area of interest is the development of databases and other solutions in the field of data journalism.   Ekaterina Borovikova - analyst, in OVD-Info works on data processing. Previously worked as a science journalist.   Natalia Smirnova - works on data gathering and analysis. Previously worked as a journalist for Lenta.ru ",,,
Sweden,Newsworthy,Small,Participant,Best data-driven reporting (small and large newsrooms),How many white Christmases will there be in 50 years?,24/12/19,"Investigation,Open data,Chart,Map,Environment","QGIS,CSV,R,RStudio,Python,Node.js"," How many white Christmases will there be in 50 years? With climate change and fewer days with sub-zero temperatures, large parts of Sweden look set to have barely any snow at all. Using observed and modelled climate data we were able to show readers what winters look like where they live - today and in the future.   Take Stockholm, for instance: The Swedish capital currently has about one and a half months of snow every winter. In 50 years, scientists predict it will have only between four and 15 days, depending on how quickly the world acts on climate change. "," Climate change is the issue of our times, but it can be difficult to report on it in a way that engages audiences. That’s why we wanted to focus not on abstract temperature anomalies, but instead the very tangible effects these are having on people’s everyday lives. Using granular climate data we were able to show our audience how things look in their local area, describing a change in snow days that they can already see starting to happen - and revealing future changes, which in many cases are quite dramatic.    Everything was done using publicly available data on current and future snow depth, from the Swedish Meteorological and Hydrological Institute (SMHI), which we joined together to produce new insights.    This was published as a national version by Aftonbladet (the largest Swedish tabloid) and on our site. The story was also distributed to and published by local newsrooms across the country through our Newsworthy subscription service in 290 local versions (one for each municipality). The local newsrooms were free to use the findings and visualisations.   For us this is one of the most important impacts of this project: producing original data journalism in a small team, and then sharing it with local newsrooms around the country, enabling them to make use of data in a way they otherwise could not have done. "," The bulk of the data analysis and visualisation was done using R. The actual climate projections and forecasts about future chances of snow we were able to pick up from the meteorological institute.    We depended on QGIS for some geospatial analysis, such as mapping towns to their nearest weather station with snow depth data. It was a challenge to combine historical data about snow depths tied to specific weather stations with climate projections that are much more granular. We had to determine what specific coordinates the best represent a certain municipality (which is not always obvious in a spacious country like Sweden).     To produce local versions of the story we used an homemade “robot writer” that generates text from data. This is a Node.js based interface that allows us to write dynamic articles with programmatic logic. "," One of the hardest parts was not necessarily data-related, but a broader editorial point: Thinking about how to communicate climate change in an engaging way as well as how to communicate the uncertainty in projected data and the definition of future pathways to readers. Should we emphasize the worst case scenarios or the more modest ones?    Climate forecasts are often presented in a rather abstract manner. We wanted to make them as tangible as possible: Will there be snow in my home town in the future?    Another challenge was the fact that Newsworthy is no larger than a two-man band. We don’t have the editorial muscles of a large, established newsroom. All research, analysis, text and chart generation and distribution was done by Clara Guibourg and Jens Finnäs. "," The project serves as an excellent case of how a small team of data journalism experts can reach and empower local reporters at scale. All the data used for the story was public, but not necessarily accessible for ordinary reporters. We had to scrape APIs and parse geospatial data files to get the numbers that, in the end, were rather straightforward to understand.    It also serves as a starting point for a broader conversation about how we can produce data journalism on climate change which actually engages people.    From a more technical standpoint, we could also use this project to introduce topics like mapping in R, or doing geospatial analysis in QGIS, including things like matrices and points-in-polygon analysis. ",https://www.newsworthy.se/sv/vitjul/,https://us10.campaign-archive.com/?u=f5c9b898477bcd7a7e64e37d9&id=f2ce185eef,,,,,,"Jens Finnäs, Clara Guibourg"," Newsworthy is a Swedish news service that automates the quest for newsleads in statistical data, and helps local newsrooms turn data into journalism.   Jens Finnäs is a data journalist and founder of Newsworthy and J++ Stockholm.   Clara Guibourg is a data journalist at Newsworthy. Before this she was most recently at BBC News and Google News Labs.  ",,,
India,India in Pixels,Small,Participant,Innovation (small and large newsrooms),India in Pixels,12/08/19,"Explainer,Open data,Infographics,Chart,Video","D3.js,Canvas,Json,Google Sheets,Python"," India in Pixels is a   data visualization based YouTube channel   that explores India through the lens of data. Each video covers a particular field such as   politics, economy, sports or demography   and illustrates the trend that happened in that field for over 50-100 years and summarizes it in 4-5 minutes as animated racing bar graphs - entirely generated through code. The data is publically available but is not consumable in its raw form. India in Pixels is an attempt to convey the essence of the data without artificial distortions that introduce bias and yet make it interesting. "," India in Pixels was launched in March 2019. As of January 2020, it has garnered  16 Million Net Views  and  73,000 subscribers . This is a testament to the huge appetite people have for quality data visualization. The comment section has created a national platform for discussions and debates around social trends backed by data. People have been using arguments like ""If you notice in the 90s, Karnataka took a huge leap in its GDP while Uttar Pradesh plunged down to 4 places - most likely the impact of the IT Revolution in India"" - this is a huge leap in making data driven conversations a living reality.    India in Pixels is helping educate millions of Indians about data visualization and is creating a new kind of audience from developing nations who are not just aware of data viz but are actively involved in it. Other than videos, I am also creating static images of maps, charts, and data-art that have also resonated with a lot of people. The visualization ""Second most spoken language in every state of India"" turned viral garnering over 700 retweets, including one from the Bollywood actress Sonam Kapoor. I have actively worked towards creating a community of involved data viz lovers interested in seeing India through multiple perspectives. It has inspired them to create their own creations and posts and getting started in the world of data viz.   As I see it, India in Pixels is news for the millenials - the young Indians who need a heavy visual component to engage with their content. But unlike mainstream millenial news, India in Pixels does not patronize its audience through clickbaits and opinions - every piece released is linked with hard verifiable publically accessible data. "," I use  Python  (in the Jupyter lab environment) for fetching data from public sources. I use the  BeautifulSoup  package to extract content from the fetched data. Usually, if the dataset requires additional cleaning, I use  pandas  to help normalize values and extrapolate missing values.   Then I use JavaScript, in particular,  d3js  and  p5js  to create visualizations from the data. Often, I layer the output I create with audio and supporting video content which I do use  Adobe Premiere Pro . I regularly use  Adobe Photoshop  to create my static visualizations. ","India is a mobile-first nation: 90% of India's internet users use mobile phones. Quality data viz relies on interaction, computation and manipulation of data. Having worked in India's leading newspaper Hindustan Times as a data journalist, I know the kind of constraints that come when making visualizations for mobile. Not a lot of computation can be done on the frontend and responsiveness limits a lot of fancy animation that can be done. An additional challenge that comes with India is that we have just started to get familiar with the web as a medium of expression, so a huge chunk of Indian demography isn't familiar with complex gestures and interactions that can be done on the Web. Thus, keeping these aspects in mind, I have consciously chosen to use YouTube as the final output of my work. Not only is it familiar but the platform also takes care of making the visualization responsive for all devices. What I lose on interactivity, I gain with an active comment section where people can engage, participate and truly make the visualization their own. India in Pixels is a proof of concept to all the data visualizers around the globe to try a ""Video First"" approach in their data stories to reach a wider audience who gets left out because of the medium, either because of device compatibility issues or the lack of technical know-how to interact with a data viz. India in Pixels deserves to win the Sigma Award because it is true to the core of what good journalism should be - inclusive and unbiased. It is making active attempts to include and engage a wide section of the globe that has never come across visual journalism yet. Awarding India in Pixels the Sigma Award 2020 will recognize the spirit of journalism"," I am not a journalist by profession. I am a software engineer. The first thing anyone can learn is to not assume that ""journalism is not for me!"". Journalism is an extremely dynamic field and India in Pixels is a testament to this fact. There is ample space for technology to create new pathways for journalism and this project should be seen as a proof of concept for that. It is possible to engage 16 Million eyes using just one laptop and a curious mind.   Second, do not let your medium be an obstacle. Think how will people consume your content? Are they familiar with the terminologies you are making? Do they know how to use the gestures you want them to use? Is it too intensive for their phones that not have the best computing and internet abilities? India in Pixels was mindful of all these constraints and arrived at a video-first approach to data journalism. I think creators will benefit from thinking about their target audience before they make their creations.   Third, there are more people beyond the First world nations! As an Indian, I feel let down by how little space 70% of the world gets in hard cutting edge data viz. There are lot of people hungry to consume content that is not just about the US or Canada or Europe. People in Mexico want to know what GDP of Guadalajara is, people in Pakistan want to know how did their literacy rate change over time. These topics are going to be of key importance in the near future when most young internet users are going to be in developing nations and I think creators will gain tremendously by focusing on them. ",https://www.youtube.com/indiainpixels,https://www.youtube.com/watch?v=_1bekqNa1ic,https://www.youtube.com/watch?v=QecuDX55bCM,https://www.youtube.com/watch?v=nbGad2c3_ik,https://www.instagram.com/india.in.pixels/,https://www.news18.com/news/buzz/this-video-of-sachin-tendulkars-meteoric-rise-in-odi-proves-why-hes-the-goat-2083697.html,,Ashris Choudhury," Ashris is a software developer based in India with a background in design, architecture and cognitive science. He loves nerding about India and exploring different facets of the country through code. He has worked as a data journalist at the Hindustan Times and as a game developer at the Fluid Interfaces Lab at the Massachusetts Institute of Technology, Boston. He loves to fuse code and design to create engaging web-based experiences in journalism, art and algorithmic design. You can have a look at his works at <a href=""https://https://"">https://iashris.com</a> ",,,
Malaysia,Malaysiakini,Big,Participant,Best data-driven reporting (small and large newsrooms),"May 13, Never Again: The riots that changed Malaysia",13/05/19,"Investigation,Explainer,Long-form,Fact-checking,Illustration,Infographics,Chart,Video,Map,Politics","Adobe,Google Sheets,CSV,OpenStreetMap"," Part of a package to commemorate the 50th anniversary of the May 13 deadly riots, this multimedia project features a blow-by-blow account of what happened on the first day of communal riots of 1969 which changed the direction of Malaysia. It tackles this taboo topic by locating the incidents of the riot onto an interactive contemporary map of Kuala Lumpur, giving the reader a visceral experience anchored on geographical and archival data, plus corroborated eye witness accounts. It also sets the context and consequence to explain how this riot is pivotal in Malaysian history. "," This project was part of a series of articles on the May 13 riots. The package received million page pageviews, while the microsite (submitted here) received 307,831 pageviews and about 43,000 social media shares in the first fornight. This is at least three times more than other special reports we produce. It generated a record number of new subscribers.   The project forced a national conversation, especially among the younger generations who live with the taboo of May 13, but were barred discussing it. Young people told us it was first time they were learning about the riots in detail because it is not taught in school due to 'sensitivities'.   Three US-based researchers contacted us to include our work in their research. We were invited to speak about it at journalist events and on television.     About 100 people - mostly under 35 - attended our intergenerational discussion between eye witnesses (including one who was a police officer at the time) and young Malaysians. One of the eye witnesses died soon after, a poignant reminder of how important it was that his recollections were recorded. The forum was guarded by police as some conservative groups threatened to disrupt it. This, however, did not materialise.   Because the narrative of May 13 is so contested, our journalists received online attacks and had to lodged a police report after safety threats.   The page also featured a call-out to witnesses to share their May 13 stories. They submitted stories through Google forms, email, social media and through phone calls to the newsroom. We closed submissions on June 13, 2019. A total 42, which we could corroborate, were published on a linked page (https://pages.malaysiakini.com/may13/stories/), as a popular archive of the event. The National Archives told us it was also including the project into its collection. ","  Google Sheets  - We used this to collate the information from various sources into a timeline which could be used within the team. Team members would include short description of events of the riot which they discovered through their research into the spreadsheet, along with source, date, time and other details. This was distilled to a key number of events. We imposed character limits to ensure writers comply with the design. We also used sheets to organise with the order of the page and the corresponding media and text, and used separate tabs for translations. It helped communication between programmers, writers, translators, editors and designers.     Google Drive  - We organised all the material (photos, videos, books, other documents) into folders which could be accessed by all the team. The locations were linked to in the master Google sheet.     WhatsApp  - For team management and communication. We opened a separate WhatsApp group to accommodate discussions and sharing of discoveries, when we could not meet in person due to other commitments. This is key because we were working across seven departments in Malaysiakini. All communications were kept within this group.     HTML, CSS and Javascript  - For web design, including the interactive map.    Leaflet.JS and Waypoints  - To build the interactive map and anchor the narration to the locations on the map.     Adobe suite  - For photo editing and video editing.      "," The most challenging aspect of the project was keeping as balanced and as objective as possible. Despite a white paper published months after the riot, the event has been the site of urban myth and legend, with ethno-political narratives of the riot used to prop up power. We spent a lot of time in the research, writing and design to ensure the page had a neutral tone and was anchored by data. Corroboration was a key challenge, including locating specific points to the map because Kuala Lumpur has changed drastically since 1969 and the documentation of the riots in 1969 were usually tied to landmarks or roads which no longer existed.   However, despite our careful considerations, we made mistakes in some phrasings which caused outrage within some communities and resulted in threats of safety against our journalists. This also faced difficulties with police, who threatened to probe us for sedition.    The project also involved multiple long-form accounts of witnesses, written in first-person format. We cross-checked everything we published against multiple sources to ensure they were corroborated, but some person details (like where someone was at a particular time of their account) could not be independently verified, although for every account, they were all plausible and historically accurate. To be transparent, we included a disclaimer about inability to independently verifying those aspects - but this was used by critics to the entire project as disinformation.    Unfortunately, the format we chose (first person narrative) also encouraged others to published made up accounts - which we knew were untrue because it contained crucial historical inaccuracies. This was particular demoralising and difficult, especially when these accounts were published within the first week of our publication thereby challenging the integrity of our project. (We published the articles in a daily series which lates one week) "," We hope the project inspires others to use technology and data to revisit and investigate important but unresolved events in their national histories.   A key learning for us was when faced with a controversial topic, it was crucial to be transparent to the reader and to link to or cite as many sources as possible to protect the product's integrity and build community trust. As we increasingly learn in our field, journalists can no longer be the final arbiter of 'truth' and must let readers into how this 'truth' was derived so they can judge themselves.   We ensured we had a diverse team of journalists and editors, given the ethnic-sensitivities of the topic. We also ensured young journalists worked on the project because our target audience was young Malaysians - the post-riot generation. This was to mitigate our own investible ethnic and age biases. That said, in retrospect, we could have recruited more beta readers outside the newsroom, from a wider range of backgrounds.   For distribution, the fact that we published in a series and directed readers to the microsite in each of the articles within the series and helped keep the topic in the headlines and drive traffic. The combination of paywalled and free article helped attract subscribers. The number of new subscribers was that month was the highest in Malaysiakini history.   Another learning was in analytics collection - because we targeted young people, we could have done better to try to measure if this was successful beyond passively waiting for feedback from young readers and seeking opinions at the forum.     ",https://pages.malaysiakini.com/may13/en/,https://www.malaysiakini.com/news/475911,https://www.malaysiakini.com/news/475915,https://docs.google.com/spreadsheets/d/1A7cAdnkkp4F7E6MhJVDrNqr7_PgBLQ8IYxjvRBtn-ME/edit?usp=sharing,,,,"Aidila Razak, Lee Long Hui, Sean Ho, Thiaga Raj, Tham Seen Hau, Wong Kai Hui, M Fakhrull Halim, Annabelle Lee","Aidila Razak Special Reports Editor and project lead. Aidila has 10 years experience as a journalist, covering Malaysian current affairs. She now works on investigative, in-depth and data features. She was a shortlisted Society of Publishers Asia award and was named Online Journalist of the Year by the peer-reviewed National Press Club award. Lee Long Hui Assistant Editor and programmer for the interactive map. Long Hui has been a journalist for 10 years. Since 2018, he has led the Kini News Lab, a unit within the newsroom which aims to use innovation and technology to do better journalism. Under his leadership since 2018, the Kini News Lab has produced two dozen projects and set a benchmark for the Malaysian journalism industry. Sean Ho Senior Products Manager and web designer for the project. Sean primarily works in web design and product management at Malaysiakini, and had previously volunteered to work on various editorial project. As a result of several successful editorial projects, he is now seconded to the newsroom support the Kini News Lab. Thiaga Raj Web Developer. Raj contributed to the Our Stories page and volunteered to translate all the articles into Tamil for the minority Indian community. He ha Wong Kah Hui Journalist - Chinese desk. Kah Hui conducted research, wrote and translated the project into the Chinese language. She covers current affairs and has been a journalist for three years, during which she produced tremendous investigative works, particularly in the area of environment. M Fakhrull Halim Journalist - Bahasa Malaysia desk. Fakhrull has been a journalist for three years. He covers current affairs and writes in Bahasa Malaysia. For the project he conducted research, wrote and translated the project into Bahasa Malaysia. He is considered an all-rounder on his team, and so is often deployed to cover a wide range of assignments from politics to arts and culture. Annabelle Lee Journalist - English desk. Annabelle has been a journalist for three years. She covers current affairs and has produced high-quality news features on a wide range of topics, mostly on social justice. She has a special interest in human rights. Tham Seen Hau Video producer and journalist, and head of KiniTV. Seen Hau leads KiniTV, the video unit of Malaysiakini. In this project she helped conduct interviews with various eye witnesses and produced multimedia elements for the page. She is multidisciplinary and works in English and the Chinese",,,
United Kingdom,BBC England Online,Big,Participant,Best data-driven reporting (small and large newsrooms),Rivers used as 'open sewers',22/08/19,"Investigation,Multiple-newsroom collaboration,Open data,Illustration,Infographics,Chart,Video,Map,Environment","Microsoft Excel,Google Sheets,CSV,R,RStudio"," The pollution blighting rivers across the UK was revealed in an exclusive data-driven investigation that led the Environment Agency to reveal it was reassessing long-term targets.  Our analysis found around eight in ten waterways were not in good health, prompting the WWF charity to say they were being treated as “open sewers”, and failed to meet environmental standards. And we showed that despite this challenge, the government was still predicting that three quarters would be in good health by 2027.  Commenting on the findings, the Green Party co-leader Jonathan Bartley said the state of our rivers was “totally unacceptable”. "," The Environment Agency said it would now review the targets ""based on what can realistically be achieved"".   The research led to coverage across BBC News platforms, with environmental charities, campaigners and academics highlighting the issue of river pollution. It showed the UK was a long way from meeting targets it had set itself as part of Europe-wide efforts to improve the health of rivers.   WWF media manager Lis Speight said: ""Without such investigative journalism, important stories that lie buried in dense data would simply not be told. The team behind this piece were meticulous, really diving down into the numbers and spending time to really understand this issue and to get their facts straight.""   The BBC's Head of UK News Richard Burgess said: ""This was an important piece of public interest reporting on a topic that needed investigation and discussion. Through the data journalism that went into analysing, visualising and presenting the figures our programmes were able to tell compelling stories about the challenges facing England’s waterways that would otherwise have remained hidden in plain sight.""   We invited questions from readers using an online form and answered those questions, inserting responses into the piece.   The questions showed we had got people thinking about the waters they swim in and what they could do to make a difference, whether that was reporting a pollution incident or joining a group or charity that would work to improve the ecological health of waters.   Our dataset was shared across the BBC with journalists on TV and radio so that they could identify stretches of river that were in poor health and use that for filming, location two-ways etc. "," We worked with open data to show the ecological health of rivers at the last published check and the predicted health status for 2027, showing that for many there was a very long way to go to reach that goal and that it was not likely to be met.   We then broke the figures down by overall waterbody for rivers.   This meant the local data could be used to help regional broadcast colleagues find potential sites for filming.   Each river basin district's data were stored in almost a dozen different zip files on the Environment Agency website. Paul Bradshaw used R (in R-Studio) to merge these together and filter them so that we were focussing specifically on rivers and their status at previous checks and their predicted status in 2027.   Daniel Wainwright visualised these using bbplot2, an R package based on ggplot2 and developed by the BBC's Visual Journalism team to create bespoke charts. He also created a map in R showing the spread of healthy and unhealthy rivers, using an unusal shapefile for England that showed the river basin districts that crossed borders into Scotland and Wales.   We also shared our data with colleagues in simple spreadsheets, writing formulae that would take the numbers and turn them into sentences explaining what they meant. These were used to assist broadcast colleagues in finding potential sites for filming.   We invited questions from readers using Hearken, a tool that provides readers a chance to ask anything they they want to know about the story.  "," We took Environment Agency data and broke it down as locally as it was possible to do and show the scale of the challenge that England faces in restoring its natural river waters to a state where they are not suffering from the effects of sewage, industry and farming.   There was an enormous amount of data available about the health of water across England but it was not in a format that the general audience would necessarily understand or find accessible.   Making the data understandable and relatable to the audience was the biggest hurdle.   Firstly, we were using geographies that many people simply wouldn't recognise - river basin districts. These simply did not follow more recognisable geographies such as local authorities or government regions.   We had to ensure we showed people on a map where their area was and how it compared to others for river water quality.   It also meant writing into the story recognisable place names to explain where we referring to.   The same had to be done for assisting our own colleagues in finding the areas that were relevant to them.   We therefore used the data to highlight examples of rivers that had consistently been found not to be in ""good"" health and wrote simple, narrative explanations using simple, formula-generated sentences in an Excel spreadsheet, to explain where each river was and in how many of the previous four years it failed to achieve a good rating.     "," Others can learn technically from the project's processes for finding, combining, analysing and communicating data — these are shared in RMarkdown notebooks in the project's GitHub repository (<a href=""https://github.com/BBC-Data-Unit/river-quality"">https://github.com/BBC-Data-Unit/river-quality</a>).   This also makes it easier for other journalists to visualise environmental information, with the river basin shapefiles.   The story also shows how data can be used in broadcast journalism to identify sites for filming, and organisations to approach for interview, while more broadly it demonstrates how a data project can lead to wide range of stories across multiple platforms and areas, engaging multiple audiences in an issue that affects them all.   The use of simple automation internally (personalised formula-generated sentences for colleagues to understand how the data relates to their areas) is an area of innovation we have not seen elsewhere. ",https://www.bbc.co.uk/news/uk-england-49131405,https://github.com/BBC-Data-Unit/river-quality,https://github.com/BBC-Data-Unit/river-quality/blob/master/Rivers%20data%20-%20BBC%20England%20data%20unit.xlsx,,,,,Daniel Wainwright; Paul Bradshaw,"  Paul Bradshaw  works as a consulting data journalist with the <a href=""https://github.com/bbc-data-unit"">BBC England Data Unit</a>, and also runs the <a href=""http://www.bcu.ac.uk/media/courses/data-journalism-ma-2018-19"">MA in Data Journalism</a> and the <a href=""http://www.bcu.ac.uk/courses/multiplatform-and-mobile-journalism-ma-2018-19"">MA Multiplatform and Mobile Journalism</a> at Birmingham City University. A journalist, writer and trainer, he has worked with news organisations including The Guardian, Telegraph, Mirror, Der Tagesspiegel and The Bureau of Investigative Journalism, and his awards include the CNN MultiChoice Award for an investigation into people trafficking in football. He publishes the <a href=""https://onlinejournalismblog.com/"">Online Journalism Blog</a>, is the co-founder of the award-winning investigative journalism network HelpMeInvestigate.com, and has been listed on both <a href=""http://Journalism.co.uk"">Journalism.co.uk</a>'s list of leading innovators in media, and the US Poynter Institute's list of the 35 most influential people in social media.   His books include <a href=""https://leanpub.com/scrapingforjournalists"">Scraping for Journalists</a>, <a href=""https://leanpub.com/spreadsheetstories/"">Finding Stories in Spreadsheets</a>, the <a href=""https://leanpub.com/DataJournalismHeist/"">Data Journalism Heist</a>, <a href=""http://leanpub.com/snapchatforjournalists/"">Snapchat for Journalists</a>, the <a href=""https://www.routledge.com/The-Online-Journalism-Handbook-Skills-to-survive-and-thrive-in-the-digital/Bradshaw/p/book/9781138791565"">Online Journalism Handbook</a> and most recently <a href=""https://www.routledge.com/Mobile-First-Journalism-Producing-News-for-Social-and-Interactive-Media/Hill-Bradshaw/p/book/9781138289314"">Mobile-First Journalism</a> with Steve Hill. He can be found on Twitter <a href=""https://twitter.com/paulbradshaw"">@paulbradshaw</a>.    Daniel Wainwright  leads the BBC England Data Unit, having been a founder member of the team in 2015. He specialises in original data-led projects that are often shared across BBC regional and network news. He began his career at the Express & Star, which at the time was Britain's biggest selling regional newspaper, rising to the role of political editor and was named 2014 columnist of the year at the Midlands Media Awards. His work at the BBC has seen him host podcasts on data journalism for the BBC Academy and give a keynote speech for the European Forum for Geography and Statistics Conference in 2019.   He is on Twitter <a href=""https://twitter.com/danwainwright"">@danwainwright</a>.         ",,,
Kenya,Mediamax Network Ltd,Small,Participant,Best visualization (small and large newsrooms),The rising health burden of undiagnosed diabetes,18/11/19,"Open data,Illustration,Infographics,Map,Health,Human rights","Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV"," Undiagnosed diabetes is common in Kenya, and often usually leads to serious health complications. Untreated type 2 diabetes can go unnoticed for a long time considering that mild to moderate blood sugar levels often don’t have any symptoms.In the report released by the International Diabetes Federation (IDF) on World Diabetes Day on Nov 14, 38 million more adults are living with diabetes, compared to 2017 and global prevalence has reached 9.3 per cent, with more than half of adults undiagnosed. "," As one of the top causes of morbidity globally, highlighting on this condition that accounts for 8,000 deaths in Kenya as at 2019, causing concerns over increasing number of untreated patients and related medical costs is timely for the readers ", Adobe   Creative Suite   Microsoft Excel   Google Sheets   CSV   Maps , Embracing data visualisation is a not an easy thing especially to editors. But with parsistence and practical know-how has made it possible.     , Data Journalism is evolving and newsroom have no choice but to embrace it. Data visualisation aid render stories for easy read. ,https://www.pd.co.ke/health/the-rising-health-burden-of-undiagnosed-diabetes-13563/,,,,,,,"Nduta Waweru, Virginia Wambui and Sandra Wekesa"," Michael Mosota is an independent, objective and visual journalist working at Mediamax Network Ltd after being at Nation Media Group for over 12 years. I strive to engage digitally in a creative, visually compelling and audience driven manner. My keen eye to detail – ah maybe could be a unique blend in handling my duties keenly giving companies I work with a competitive edge.   As a Graphic Designer, I have experience in conceptualizing a wide range of data-driven, long and short-term stories. I aid in newsgathering operations, web content management and developing concepts on top stories in liaison with editorial teams and fellow designers. My role has earned me a number of awards as demonstrated in my resume.   I have studied design and communication at the University of Nairobi (Kenya) with experience in print, digital and electronic media. I am also a trained data journalist with Internews after making a leap through winning a 4 months Health Fellowship programme. My professional experience range from design, communication, data visualization and advanced skills in multimedia graphics.   My objective is to work in teams, build capacity and drive digital content in tandem with media global trends. To this end I aim to utilize available opportunities in sharing, learning and apply new concepts in the media industry.   I foster professional growth, value diversity, committed to equal opportunities and versatile in adopting positive change that values creation, humanity and nations at large. ",,,
United Kingdom,The Times,Big,Participant,Best visualization (small and large newsrooms),What does Boris Johnson really think?,25/10/19,"Investigation,Explainer,Quiz/game,Illustration,Infographics,Chart,Politics","Scraping,D3.js,Json,R,Node.js"," Analysis of more than 350,000 words written over 15-years of Boris Johnson’s Telegraph columns helped us identify the topics and people he discusses the most.    We also used a computing technique called “natural language processing” — a form of artificial intelligence that can interpret the meaning of sentences — to reveal the sentiment behind his words.    This was a complicated analysis of a relatively light hearted subject, and so we set out to display the findings in a clear yet fun way. This led to an interactive extravaganza, topped off with a never ending Boris quiz. "," The overall reaction to the project was extremely positive. I received good feedback in the comments section, from others in the newsroom and on twitter where I shared the process behind the piece.   The article was picked up by several outlets and featured in Politico’s London Playbook newsletter. I was also contacted by Factiva - a business information and research tool owned by Dow Jones - about how we could use their data to collaborate on other future projects.   When we first discussed doing a project like this in the Spring of 2019, we planned to focus on the then prime minister Theresa May. However, the only source we had for was her parliament speeches, which were very scripted, formulated and similar.   So when Boris Johnson became prime minister I had the idea to use his Telegraph articles as a basis for the analysis. This worked out perfectly - never before has a prime minister provided such a personal insight into their mind. The overall project benefited from having this volume of personal musings available and out there to analyse.     "," The process can be broken down into three sections: the collection of the data, the analysis of the data and the front-end interactives.    The data was collected using a script written in R. The Telegraph has a page for each of its writers that has links to all of the stories they have written.   The analysis of the data was done using JavaScript. To work out the people he discusses, I wrote a script to search for names (e.g. Theresa May) and counted how many times he mentioned them each article.    I used IBM Watson’s Natural Language Understanding API (<a href=""https://www.ibm.com/uk-en/cloud/watson-natural-language-understanding"">https://www.ibm.com/uk-en/cloud/watson-natural-language-understanding</a>) to see which topics he discusses in each article. I limited it to the top ten for each and then grouped this data by year to see how the trends have changed over time.    Finally, I used the API to calculate the sentiment and emotion behind how he uses certain terms and phrases. Our findings show that he discusses the Queen with the most joy, Labour with the most sadness, the police with the most fear, Liberal Democrats with the most disgust and democracy with the most anger.   For the Boris bot game, I trained a text generating neural network (textgernn) with all 350 articles. I then split the output into sentences for the purpose of the game. The “real” sentences were chosen at random.   The interactives were built in React js as components that sit within pages on our site and in the app. For the first two interactives, I used D3.js to create the graphical elements. The static graphic on the page was created using Illustrator. "," This project should be selected due to the array of interesting data visualisations I produced to help explain the complicated analysis in a simple way.   I showcased a variety of skills: creating a static graphic in illustrator, building the interactive elements using D3.js and developing the Boris vs Boris Bot game in React js. I had to use a multitude of skills to get to the finished product. What was produced was a fun, informative and interactive article that presented our complicated findings in an engaging and interesting way.   The analytics from the piece were strong as well: readers spent an average of 3 and a half minutes on the page, considerably higher than average and most likely down to the interactive elements. The story also drew in almost 1,000 new registrations and 87 subscriptions.   The hardest part of the project was working on all of the different technical elements, from the scraping to the analysis and building the finished interactive elements. It was a daunting process to begin with, but one I dealt with by splitting up the different parts into more manageable chunks. "," Good data visualisation can help bring a story to life and explain findings in an interesting and simple way.   Using computing techniques, such as machine learning and natural language understanding, can be a difficult process to explain to editors in the newsroom.   It’s important to focus on the conclusions you can draw from the data and to use those as a basis for a story when pitching it.   I think the way we presented the data in fun and interactive visuals definitely helped the story. ",https://www.thetimes.co.uk/article/reading-boris-johnsons-mind-unique-analysis-reveals-what-the-pm-really-thinks-00jqvn9lq,,,,,,,"Sam Joiner, Matt Chorley"," Daniel Clark is an interactive journalist for The Times and The Sunday Times. He specialises in creating bespoke charts, maps and graphics to tell data-led stories.   Sam Joiner is the interactives and new formats editor for The Times and The Sunday Times. From charts and maps for daily news stories to large-scale projects and interactive games, his team focuses on data-led visual storytelling, bringing ideas to life for readers online.   Matt Chorley is editor of Red Box, The Times’ political email and podcast, giving an insider’s guide to life in Westminster with additional comment, polling, analysis - and a sense of humour. He joined The Times in 2016 after a decade covering politics at MailOnline, Independent on Sunday, Western Morning News and Press Association. ",,,
Germany,Deutsche Welle,Big,Participant,Best data-driven reporting (small and large newsrooms),What Hollywood movies do to perpetuate racial stereotypes,21/02/19,"Explainer,Database,Infographics,Chart,Culture","Scraping,Adobe,Creative Suite,Microsoft Excel,CSV,R,RStudio"," Although there has been great reporting on the under-representation of ethnic minority characters in Hollywood movies, a data-driven look at how they are presented has been missing from the conversation. So, to illustrate how stereotypes have developed in Hollywood, we analyzed the tropes about different ethnic groups present in more than 6,000 Oscar-eligible movies since 1928.  Our analysis and the accompanying reporting show that, even though some of the more obviously racist tropes have faded from cinema in the past decades, many stereotypical narratives have shifted rather than disappeared. This analysis was published in English, German, Turkish and Chinese. "," This article has enjoyed an exceptionally 'long tail' of visits: it still receives at least as many weekly visits as when it was first published. It also has an average dwell time of more than five minutes. Users generally spend more time with our data-driven stories than they do with other DW content, but this one stood out even among data-driven articles.  The story was also discussed intensively on the DW News Facebook channel. The Turkish and Chinese language teams of DW also adapted the article, prompting discussion on social media in these communities as well. "," You can find a full account of our methodology, as well as the data and code behind the analysis, on our GitHub page (see additional links).  The main tool used for this story was the statistical programming language R for scraping, analysis and preliminary visualization.  From the annual Academy Awards """"Reminder List"""", as well as the official Academy database of Oscar nominees and winners, we compiled a list of around 28,200 relevant Hollywood movies. We also merged these datasets with IMDB data where possible for further metadata and fact-checking.  For the stereotypes present in those movies, we systematically scraped results from the user-generated wiki TVTropes. As with all user-generated data, there is bound to be some margin of error. Still, TVTropes is the best option for getting detailed and large-scale data on something as complex and subjective as movie tropes. As one of our precautions, we excluded entries that had been only edited by one user.  To match TVTropes entries to the correct movies and avoid false matches, we used a mix of pattern recognition rules and manual work in R and OpenRefine. When in doubt, we erred on the side of caution, and in the end, we had a sample of 6,637 matched eligible movies and 21,789 unique encountered tropes to work with.  In addition to the methodology page in our GitHub repository, we also created an interactive table that allowed users to search through our trope database themselves (see additional links).  The data cleaning software Open Refine was used for pattern-based matching between metadata and TVTropes entries. We also used Adobe Illustrator to adapt the visuals for publication. "," Analyses of how minority roles play out on screen has remained largely anecdotal in the past years. That's because quantitative content analyses are usually expensive and time-consuming, especially at the scale we aimed for. Our approach worked around that by utilizing the best aspects of user-generated data, and avoiding its drawbacks as best possible. Still, the main challenge was building a robust sample, maintaining correctness while including as many movies as possible. In scraping, matching and analyzing the data, we used a mix of automation, pattern recognition and manual work. We took great care in the development of our methodology, and made our process as transparent as possible. Our methodology, as well as the data and code themselves, are published on our GitHub page. This way, we keep ourselves accountable to our audience and help any other interested party benefit from our work. With a final sample of 6,637 matched eligible movies and 21,789 unique encountered tropes, this project offers a scale of analysis that previous projects, both journalistic and scientific, did not reach.  With this project, we attempted to add a new perspective to the discussion around the Hollywood representation of people of minority ethnicities.  Our data-driven approach adds to the qualitative analyses that media experts, and people of color in general, have been publishing for decades. The final article aims to combine the power of data-driven reporting with traditional journalistic work that explains the why, the how, and the real-world implications of stereotypical depictions in Hollywood. "," Since all of our work is available via our GitHub respository, everyone is welcome to use our learnings for their own analyses. Specifically, our approach of using large-scale user-generated data to quantify complex concepts like stereotypes might be helpful to others. We welcome people using and refining the system of checks and balances we devised to check the validity of this type of data as well. Utilizing these methods can open the door to new types of data-driven stories, and provide new perspectives to ongoing important discussions in media and society. ",https://p.dw.com/p/3DYxY,https://github.com/dw-data/movie-tropes,https://dw-data.github.io/movie-tropes/,https://p.dw.com/p/3DYwa,https://p.dw.com/p/3E32A,https://p.dw.com/p/3DtIJ,,Kira Schacht," Kira Schacht studied data-driven journalism in Dortmund, Germany. During her studies, she co-founded the data journalism initiative Journocode, which provides resources and trainings on data-driven journalism. After working with Rundfunk Berlin-Brandenburg, the Guardian, the Times in London and others, she joined the DW Data team in October 2018. In 2019, she was named one of Germany's top 30 journalists under 30. ",,,
Germany,Deutsche Welle,Big,Participant,Best data-driven reporting (small and large newsrooms),Border checks in EU countries challenge Schengen Agreement,11/12/19,"Investigation,Open data,Fact-checking,Infographics,Chart,Politics,Immigration,Human rights","Microsoft Excel,R,RStudio"," The Schengen area is supposed to be a border-free zone within Europe. But as large numbers of displaced people arrived in 2015, some Schengen countries reintroduced border controls. And they're still in place, since the countries have kept extending them for the past four years. Our data analysis shows how just how extraordinary that situation is, and interviews with MEPs and migration policy experts conclude that the continued extension of the controls may be illegal under EU law. This story was translated into Arabic, German, Romanian, Russian, Spanish and Urdu. "," This article was the most viewed article on DW.com for several days following its publication. It also got almost seven times our average views at DW Data, making it one of the most successful of our data-driven pieces to date. European news portal InfoMigrants picked up the story as well. It was also discussed widely on the social media channels of DW. "," The European Commission publishes a list of all reintroductions of border controls since 2006 as a PDF on their website. We used Tabula and manual data cleaning to convert this list into a machine-readable Excel table. After some background research, we classified the reasons given for the reintroductions into the categories ""migration"", ""terrorism"" and ""foreseeable event"". We analyzed and visualized the data with the statistical programming language R, and finished and translated the graphics with Illustrator. We reached out to MEPs and migration policy experts for background research. We also used data from Frontex and German crime statistics to provide context for the reasoning put forward by the member states. "," With this story, explaining the legislative processes and the power structures at play was just as important as the data work. The ongoing border controls have been covered by DW and other media occasionally, but mostly in the form of short news statements. A look at the broader situation was missing from the discussion. With this piece, we aimed to add a new, fact-based perspective to an ongoing emotional discussion. This, we believe, is one of the valuable functions data-driven journalism can and should fulfill in the newsroom. "," We publish the data, methodology and code behind all of our analyses to enable audiences to check and reproduce our work. With this project, we went one step further and published a markdown document containing meta information about the story as well as a line-by-line walkthrough of our programming code with explanations. ",https://p.dw.com/p/3S8Ad,https://github.com/dw-data/border-controls,https://www.infomigrants.net/en/post/20754/border-checks-in-eu-countries-challenge-schengen-agreement,https://p.dw.com/p/3S8Ag,https://p.dw.com/p/3StY3,https://p.dw.com/p/3Sv3V,https://p.dw.com/p/3StTq,Kira Schacht," Kira Schacht studied data-driven journalism in Dortmund, Germany. During her studies, she co-founded the data journalism initiative Journocode, which provides resources and trainings on data-driven journalism. After working with Rundfunk Berlin-Brandenburg, the Guardian, the Times in London and others, she joined the DW Data team in October 2018. In 2019, she was named one of Germany's top 30 journalists under 30. ",,,
Germany,Deutsche Welle,Big,Participant,Best visualization (small and large newsrooms),African megacities adapt to a climate crisis,12/06/19,"Explainer,Solutions journalism,Long-form,Cross-border,Infographics,Chart,Video,Map,Environment","Drone,D3.js,JQuery,Json,Microsoft Excel,Google Sheets,CSV,Python,Node.js"," Africa's cities are booming. But they will be hit hard by climate change. In interviews with 30 urban Africans, from people scraping a living by picking through waste to UN climate scientists, DW looks at how four big and fast-growing cities are adapting to the problems each faces: Lagos to scorching heatwaves, Kampala to rising waste, Cairo to potentially looming drought and Dar es Salaam to choking traffic. "," One important metric for measuring the success of our articles is to look at the dwell time. In the two weeks after launch, visitors stayed on the page for an average of 24 minutes. The article also did well in terms of overall visits and page impressions. Another qualitative metric that is important to us is who shares the piece on social media. In line with other data-driven pieces, the megacities project was well-received among multipliers and stakeholders, such as the deputy executive director of the UN Environment Programme; the director of the International Institute for Environment and Development; Oxfam's Pan Africa director; ActionAid's acting secretary-general as well as multiple policy makers and researchers familiar with the topic. "," We used different tools, techniques and technologies at different stages of the project. For data analysis and preliminary visualization we used Python, Excel, Google Sheets and d3.js. Mapping was done using OpenStreetMap and QGIS. The styling of the charts was adjusted using Adobe Illustrator, which was also used to create the more illustrative infographic on waste production. The UI-Design was created using the software Sketch and shared between designers and developers using the platform Zeplin. Quality control of the UI design relied heavily on the web developer/inspector tools of the web browsers Firefox and Chrome. In the production phase, building all the modules together into one webpage, our developer team relied on multiple web technologies to ensure seamless parallax-effects with both the chart elements as well as the looped video elements with overlaying text boxes, alternating with linear photo and video elements as well as text. Among the technologies used were d3.js and scrollama.js to implement the parallax effect and display images and charts. Lazy loading and the intersection observer worked hand in hand to optimize loading behaviour of the page in order to make it as efficient as possible. All images were compressed using squoosh.app to further optimize the data volume of the page. Node.js was used for assembling modularized scss files to the main.css. Jquery.js was used for the scroll-to-top button as well as for displaying images. The performance of the assembled website was assessed using Google's Lighthouse Report web developer tool. "," The core idea is to guide the user through the story step-by-step, for example with graphics that build up gradually. The user need not click on anything; his or her scrolling is the only form of interaction.   Coding the entire story from scratch granted us more creative freedom than your average content management system or any software tool does our opportunities were only limited by our imagination, skills, time and budget. Such freedom can be not only an advantage but also a challenge, as you have to create everything by yourself. While we built upon open-source packages, most of the modules were implemented for the first time at DW.   A second aspect we'd like to highlight is that we were not allowed to share some of the data freely, which is why we decided to go with static versions of the graphics rather than do interactives as that would have allowed for tech-savvy users to reverse-engineer the original data.   A third aspect that is special about DW is that we are catering to an audience mostly in the global south, so we have to balance data-heavy multimedia content with bandwidth availability in our target regions. ", This project was the cumulation of many lessons we learned over several months building a visual journalism team. We have documented our learning experience in three blog posts (see additional project links) to make it easier for other media outlets to learn from our experiences. The most crucial lesson we would like to pass on would be to plan your new features in an A/B way: dream wild and envision the ideal version of a certain feature - but also think of an easier solution as a fallback option and plan in buffer time to implement those fall-back options in case you need them.  ,https://dw.com/african-megacities,https://github.com/dw-data/megacities,http://blogs.dw.com/innovation/better-templates-for-visual-journalism-scrollytelling-7-protips/,https://blogs.dw.com/innovation/interdisciplinary-storytelling-five-tips-for-better-results/,https://blogs.dw.com/innovation/interview-how-to-build-a-visual-journalism-team/,https://visualstories.dw.com/afrika-megacities-umwelt-klimawandel/,https://visualstories.dw.com/metropoles-africa-meio-ambiente-adaptacao/,"A. Niranjan, T. Walker, A. Kimmig, N. Isenson, J. Collins, M. Lesser, K. Schacht, T. Wills, S. Wüst, G. Cutanoski, K. Funck, K. Esterluß, L. Jandel, G. Grün, S. Hüls, B. Stöß, G. Fallisch, T. Giemza, O. Urusova-Maisels, S. Plank, O. Pock, V. Fischer"," This story was created by DW's Visual Journalism Team. DW founded this team early 2019 and tasked it with finding ways to intertwine different multimedia elements into a coherent narrative, with a strong focus on visual elements. To facilitate an immersive user experience, the team is interdisciplinary: developers, designers, data journalists and reporters and editors work hand-in-hand on bringing the story to life. For each project, a different team of storytelling experts joins forces. The team for this story formed around Ajit Niranjan from DW's environment desk who had the idea of highlighting how African megacities adapt to environmental breakdown and climate challenges. ",,,
United States,HBO Real Sports,Small,Participant,Innovation (small and large newsrooms),Game Change,29/01/19,"Investigation,Long-form,Database,Open data,Sports,Health,Human rights","Animation,Microsoft Excel,Google Sheets,CSV"," “Game Change” explores the changing face of America’s most popular and violent sport – football – a decade into the game’s concussion crisis.    Real Sports  spent over a year filing public records requests to school districts around the country, seeking demographic data on boys playing the game. We found that over the last five years there has been a marked shift in exactly who is participating, with decreases among middle- and upper-class boys but  increases  — both as a percentage and a raw number — in participation by the poor. ","This was the first report of any kind – in journalism or academia – to show with actual data the shift in participation in American football. The story not only lays out the incontrovertible numbers that document this shift, but also considers the implications – not only for the game of football, but more importantly, for the disadvantaged young boys who are increasingly filling football rosters, and facing the risks that come with it. The segment further explores how the leaders of football in America, including the NCAA and the NFL, have responded to this trend by increasingly courting children and parents from the poorest and least educated communities in America. For example, the IRS form 990s of the NFL's 32 teams show millions of dollars of contributions to tackle football leagues in in some of the poorest and least-educated communities in the country. It's a finding that troubled public health experts, as studies show that parents in these communities have the least information and education about the risks of the game. The report had a significant impact on the public discourse. Journalists, policy makers, and the leaders of think tanks extensively referenced our research and reporting, noting the first-of-its-kind import. The Atlantic's Patrick Hruby wrote: ""Last night Real Sports ran a must-watch segment on how high school football may be following the path of boxing: wealthy families and kids are abandoning the sport, while poorer families and kids are not. It fleshes out – with data! – what many people who cover and think about football and brain injuries have suspected for a while: richer families are pulling their kids, poorer family aren't. Highly recommend watching it."" The Aspen Institute's Jon Solomon, the editorial director of the Sports and Society program, wrote: ""Must watch… Terrific reporting through use of","     For nearly a decade, academics, policy makers, and journalists have been trying to answer a relatively simple question: As society learns more about the dangers of American football, will parents and children of financial means turn away from the game, leaving only disadvantaged families to assume its risks? We too have wondered about the answer to this question. After years of searching for useful data and finding that there wasn’t any, we decided to try to get the data ourselves.   The first step was to figure out what type of data might be available – on file with government institutions and subject to FOIA requests – that could shed light on the financial means of the families of the boys who play youth football. After a few false starts, we learned that most schools’ databases give each student an anonymized identifying number—a number that can then be cross-referenced with both their participation in the school’s free or reduced-price lunch program and their participation in extra-curricular activities. What’s more, most schools were required to keep this data for at least five years.   The second step was figuring out how to get a statistically-significant data sample. We decided we’d choose a state, and then send FOIA requests to each and every school district in that state. After consulting with experts, we settled on the state of Illinois --  a state with a mix of urban, suburban, and rural areas; a blend of white, black, and Latino residents; and a long tradition of football.   As a check on our work—and to make sure Illinois wasn’t anomalous--we filed hundreds of additional FOIA requests, many to the largest school districts in the country. We also filed FOIA requests to the top performing school districts in nearly every state. "," This story took over a year to report. The difficulty stemmed from the fact that we needed to acquire participation data through public records requests, and we needed to make sure the data we acquired was statistically significant.   After feeling like we had control of the available data at the high school level (see above) we turned our attention to the college game. We submitted FOIA requests to the schools in the country’s two most lucrative conferences, the SEC and the Big Ten. There we found a 25% growth in the number of low-income scholarship football players since the concussion crisis began (as measured by the percent of those players who were eligible for Pell grants). In addition, we analyzed the NCAA’s data on race and participation in football, and found that in the highly educated Ivy League the number of black football players is in fact up 50% since 2010, while the number of white football players has fallen significantly.   The final stage of our reporting process involved trying to tease out how the NFL tries to keep participation in the sport robust. By combing through thousands of pages of IRS Form 990s, we found millions of dollars of contributions by the NFL and its teams to tackle football leagues in some of the poorest and least-educated communities in the country. It’s a finding that troubled public health experts, as studies show that parents in these communities have the least information and education about the risks of the game. "," After receiving responses from the majority of districts in the state, we sent our data to two experts with strong statistical backgrounds. The first, Dr. Emily Kroshus, who was ultimately featured in the HBO report, studies participation patterns in youth sport. The second, Dr. Stephanie Cook, is a biostatistician with extensive experience studying data in low-income communities. Both experts confirmed the statistical significance of our findings.   Throughout the process we looked to buttress our reporting with any available studies on related topics. One such study found that the availability of flag football in a community – as a theoretically safer alternative to tackle football – is tied to the education levels of the parents who live there. Another found low-income families to be far more likely to think about the possibility of a college scholarship when choosing which sport to enroll their sons in to play, even at a young age. ",https://vimeo.com/387350814/46cdacdac4,,,,,,,"Josh Fine, Nisreen Habbal"," Josh Fine is a Senior Segment Producer for HBO’s ""Real Sports with Bryant Gumbel."" His report on Qatar’s questionable methods and abusive practices for achieving prominence in sports received the Alfred I. duPont-Columbia University Award, the broadcast equivalent of the Pulitzer Prize; the piece was also the first sports story to ever receive an Overseas Press Club Award, which the OPC called ""enterprise reporting at its best.""   Josh was the Senior Reporter and a producer on Real Sports’ 90-minute International Olympic Committee segment ""The Lords of the Rings."" The piece won an Alfred I. duPont-Columbia University Award, the Overseas Press Club's Peter Jennings Award for best report about international affairs one hour or longer, the Investigative Reporters and Editors (IRE) Award for best television piece of the year, and an Emmy. The New York Times’ Pulitzer Prize-winning reporter John Branch called the piece “required viewing before you turn on NBC (to watch the Olympics).""   In 2012, three of Josh's investigative reports helped Real Sports win a Peabody Award for television excellence and meritorious public service.   Josh has been nominated for nine Emmy Awards, four for his series on the treatment of NCAA student-athletes and two for his reports on Vladimir Putin's use of the Russian treasury to fund his sporting ambitions.   Prior to HBO, Josh was an Associate Producer for Mike Wallace at CBS News’ 60 Minutes where he helped produce interviews with Yasser Arafat, Boris Yeltsin, and Chinese President Jiang Zemin. From 1997-2000 Josh was an off-air reporter for the Investigative Unit of ABC's ""World News Tonight with Peter Jennings."" Josh has also covered two presidential campaigns.   Josh is a board member of the Overseas Press Club and was a term member at the Council on Foreign Relations. He has guest lectured at American University's Washington College of Law and taught research master classes to graduate students at NYU's Journalism Institute. ",,,
Egypt,masrawy.com,Big,Participant,Best data-driven reporting (small and large newsrooms),Official Request ..How does social media provide world's governments with your data?,15/04/19,"Investigation,Explainer,Long-form,Fact-checking,Illustration,Infographics,Chart,Map,Politics","JQuery,Microsoft Excel,Google Sheets"," An international investigation with an international dimension that reveals, through international data and communications to the most important telecommunications companies in the world, how social media sites deliver their users ’data to the world's governments, how the world codified these practices, and what are internationalist moves against them. "," Primarily, the awareness of more than 3.196 billion people with an account - at least - on a social networking site, which prompted daily new discoveries about the leak of user data.   In addition to the major communications companies apologies to its users. And their officials stand against international courts before the world. "," The data was the main pillar of the investigation .. As it was through the reports it obtained, it was able to draw a complete picture of how telecommunications companies and social media collaborate with governments to deliver user data and its mechanism.   At first, I had to clean that complex data, via the open refine program, then format it and analyze it on Microsoft Excel, and keep copies on Google Spread Sheet.   As for visual representation, it was its main pillar, interactive maps of the world, to show the variation in the numbers of requests, the numbers of activists' accounts, and the percentage of response to them, through the Tableau Public program.   In addition to graphical analysis via popular programming languages, CSS, javascript, 5 HTML, etc., the TimeLine drawing for analysis is also included.   We designed the investigation in the form of the most popular social media pages and designed the data in formats that fit like gifs.   And we added interactive games to break the deadlock of that data-busted investigation.   Our types of data display between graphs, images of various kinds, and video graphics, and others.   We designed the homepage in the form of cartoons expressing the story.     "," The hardest part at first was finding hard evidence to build the investigation, and the data listed in places unknown to social media itself were the best evidence.   But they put it in a complicated form, not at all irregular, I was not adept at the time cleaning the data, it took me months, until I discovered the specialized programs in this.   Hence, I started building the investigation, through research, and attempts to reach officials within those companies.   The hardest part was the official responses, which I was able to get after months.   Then came the design, which was repeated more than once to reach the current look. "," Open source is full of treasures that once we dusted off them we found interesting stories.   How to tell a complex story in an interesting way as well, and that data can be a source that answers questions that no living source can answer.   Then you put these facts in front of officials and receive the other side of the truth, because at that time there is no place for diplomatic answers.   It is the language of facts! ",https://www.masrawy.com/crossmedia/internet/index.html,https://drive.google.com/file/d/1WTkdIoLS4K5OBohI1q1IMgk-SDE9Z3NR/view?usp=sharing,,,,,,Investigative: Maha Salaheldin draw: Sahar Issa Graphic: Ahmed Yassin Video: Ahmed Abdel-Ghani Infographic: Michael Adel Video: Nassim Abdel-Fattah Editing: Ahmed Abdel Shafie Programming: Mohamed Ezzat General supervision: Alaa Al-Ghatrifi," An investigative journalist at Masrawy, specializing in the production of digital stories, data-driven stories, satellite imagery, and cross-media and humanitarian investigations. I work on the development of digital stories using the cross-media technology at Ona Press and Media Foundation (Masrawy, Yalla Kora, Consolto, Ona News).    Hold a number of Arab and international awards. ",,,
United Kingdom,Visualize News,Small,Participant,Best data-driven reporting (small and large newsrooms),Lok Sabha 2019 - India goes to polls,03/01/19,"Investigation,Explainer,Long-form,Chart,Map,Elections,Politics","Scraping,D3.js,Node.js", A visual essay of the Indian," The project has been featured in several pages and social networks, however the most important impact was to empower the readers to understand how the different news outlets in India were representing the elections.   We were aware of the fact the thousands of trolls and bots were giving partisan (or false) views of India during the campaign, we hoped that giving a not-biased, and easy to access story of elections updated every day would have helped the people. We believe that, even if on a small scale, we have been able to reach the people and help them understand what was happening in their country during the campaign. ","   On the frontend we used the latest web technologies as react and d3, to provide a mobile first smooth experience and hopefully :D easy and involving consumption of the data.   More interstingly on the backend, data is collected and analyzed by a set of web-bots"," News in India follow their own patterns and way to say. We had to crack the language, acronyms, topics and name of politicians to understand the data better in order to decide how to visualize the data. We wanted to keep a fun and engaging visual approach and guide the user from a high level view down to a more details set of analysis.   In order to understand what we were working on we had to get in touch with journalists and friends from India to understand better some of the aspect of the Indian Elections, from parties, head of parties and their position (left-right or conservative-progressive).   Ultimately collecting the data and instruct the NLP engine to categorize tweets and news was the final part of this learning process. ","   Immerse yourself into the reality that you are trying to visualize, first of all to visualize the topics in a meaningful way and secondly to support your findings   Try to keep an unbiased point of view when you are trying to give an overall view of a political campaign   ",https://india.visualize.news/,,,,,,,"Simone Lippolis, Carlo Zapponi"," Carlo Zapponi  Carlo creates Data Visualizations in the AI Analytics team at Cisco. Formerly in the Visual Team at The Guardian, Microsoft/Nokia and frog design. In his free time he co-authors Visualize News and Fútbolismo. He's passionate about everything that mixes creativity + data + people.   Simone Lippolis  Simone creates Data Visualizations in the AI Analytics team at Cisco. He spent almost ten year in the technology department of frog where he worked mainly on healthcare, telco, and automotive projects. Before that, he worked in advertising, marketing and web agencies. In his free time he co-authors Visualize News and Not a Magazine. ",,,
Spain,Mundo Deportivo,Small,Shortlist,Best visualization (small and large newsrooms),Rafa Nadal's career in detail,11/05/19,"Database,Infographics,Chart,Sports","Adobe,Creative Suite"," In the paper edition of the newspaper we made this infographic to pay tribute to tennis player Rafa Nadal for once again achieving number 1 on the ATP list.  We make a pie chart dividing it by years, in each year you can see the achievements and their classification. ", It had a lot of repercussion when gathering the information in a (circular) way of a career as long as that of Rafa Nadal in an unconventional way.  In social networks he had a lot of repercussion and tennis specialists named him on his channels. ," We collect data in Excel, use RAW for a first circular version of the data and finally use Illustrator to design the elements. "," The realization of a round visualization was a challenge, in many occasions I saw attempts to show the data in this way and they did not work very well, in my modest opinion I think that the visualization fulfills the challenge that marks me in said work. "," One of the satisfactions that I highlight of this project is that, the tennis specialist in the writing told me that thanks to the graphic, he understood Rafa Nadal's career in detail much better, just for this phrase it is well worth the realization of these visualizations.    ",https://drive.google.com/file/d/1xZf0jfYlQqVI0J-zC5IE9yWpD1CSsn9U/view?usp=sharing,https://www.mundodeportivo.com/tenis/20191104/471366094511/rafa-nadal-inicia-hoy-con-suspense-su-octava-etapa-como-n-1-mundial.html,,,,,,"Ferran Morales (Data visualization), Angel Rigueira"," I am an infographist in Mundo Deportivo, a sports newspaper from Barcelona (Spain), I am passionate about new digital narratives, I have won different international awards (SND, Information is Beautiful and ÑH) in recent years I am investigating new ways to show data in augmented and virtual reality. ",,,
United Kingdom,The Telegraph,Big,Participant,Best data-driven reporting (small and large newsrooms),Crime series. Just one in 100 thieves being charged in parts of UK - how good is your police force?,24/10/19,"Investigation,Breaking news,Open data,Infographics,Chart,Crime","Personalisation,D3.js,Microsoft Excel,Google Sheets,CSV,R,RStudio"," I have delivered over five Telegraph front pages on rising crimes and declining charge rates in the UK through the use of open data and freedom of information. The UK's Home Office publishes quarterly and annual data on the number of crimes and the police outcomes of each of them - broken down by police force area and crime type. This is a wealth of data which I've used to write multiple public interest stories on the increasing crime problems in the country. They are topics that interest our readers, and so we use data journalism to surface these stories. "," My data-led stories and interactives on crime have highlighted how Government decisions have directly impacted the likelihood of catching criminals. This is something that Telegraph readers, and wider society, care a lot about.   We created two interactives this year on the topic - one of which is a tool in which our subscribers can type in their postcode and find out how the story impacts themselves. The reader can find out how many crimes take place in their area, how many end up in the criminal being charged and how this rate compares to the national average.   This is what we try and do at The Telegraph’s Data Journalism team. We aim to surface exclusive news stories that our readers will devour on the front-page, but also combine that with interactives that have genuine utility and connection to our readers’ lives. The impact of this is clear: we can surface important public interest stories, and show personalised data to our reader. This gives them the tools to hold their local authorities to account. "," Most of my data analysis is done in either R or Microsoft Excel, depending on the size of the dataset and how big the story seems. Dplyr, tidyverse and ggplot are my go-to tools in R. Static visualisations are created with ggplot and Adobe Illustrator, while interactives are built with D3. ​ "," Collaboration with other journalists in the newsroom who aren’t data specialists can be a challenge. In all of these crime stories, I’ve worked with a journalist who specialises in a crime or home affairs patch. This enables us to improve the story by having all skillsets covered.   Previously, when I started out as a data journalist, there were plenty of times when people didn't understand what I did. The usual response was usually one of two: assuming I was a number cruncher to help research reporters' stories, or assuming that I was an extension of the graphics team - again, for other reporters' stories.    It took me a while to establish that I as a data journalist was indeed a journalist in my own right. That my primary function was to source stories in exactly the same way as any other reporter, and that the only real difference was what my primary source of my stories were.   The way myself and, then, the wider Data Journalism team tackled this was simply to get our heads down and produce good stories. The more we delivered exclusives for the newspaper, and the more we produced innovative visuals for the website, the more people understood what we did and how we can collaborate. "," Open data is a treasure trove of stories. Most journalists overlook this source, as it’s often hidden behind multiple tabs in a spreadsheet or simply because the skills to be able to dig into a wall of numbers to find a story still aren’t that widespread in a newsroom. But often a simple pivot table in Microsoft Excel can be the key to a front-page story. If we go a step further and use R to dig into even larger open datasets, we are able to find even more stories and visualise them in innovative ways.  ",https://www.telegraph.co.uk/news/2019/10/17/number-crimes-solved-halves-four-years-home-office-figures-reveal/,https://www.telegraph.co.uk/politics/2019/04/23/police-chief-admits-60-per-cent-crime-not-fully-investigated/,https://www.telegraph.co.uk/news/2019/10/24/repeat-knife-crime-offenders-escape-jail-third-cases-despite/,https://www.telegraph.co.uk/news/2019/10/17/knife-crime-hits-new-record-high-england-wales-7-last-year/,https://www.telegraph.co.uk/politics/2019/10/08/violent-sex-offenders-escape-prosecution-confessing-serious/,https://www.telegraph.co.uk/news/2019/04/25/knife-crime-rises-record-levels-england-wales-homicides-hit/,https://www.telegraph.co.uk/news/2019/02/23/victims-violent-crime-denied-justice-figures-show-huge-rise/,"Ashley Kirk, Charles Hymas, Ellie Kempster", Ashley Kirk is a senior data journalist at The Telegraph and Charles Hymas is the home affairs editor at The Telegraph. Ellie Kempster is the developer who helped build the tools that are in the crime stories. ,,,
United Kingdom,"About 150 JPIMedia titles including i, The Yorkshire Post, The Scotsman, Belfast News Letter, Sheffield Star, Portsmouth News, Lancashire Post, Sunderland Echo and Edinburgh Evening News.",Big,Participant,Best data-driven reporting (small and large newsrooms),The Lost Billions,15/10/19,"Investigation,Long-form,Database,Open data,Infographics,Chart,Video,Politics,Business,Health,Economy","Microsoft Excel,Google Sheets,CSV"," In The Lost Billions, JPIMedia Investigations uncovered the alarming rise in cost of the UK’s Private Finance Initiative – a controversial method of funding public sector infrastructure projects through long-term deals with the private sector.    We revealed it is now set to cost the UK taxpayer £5 billion more than expected, while some buildings costing millions each year are simply lying empty.   This collaborative investigation ran across some 100 JPIMedia titles, including the national newspaper  i , as well as major regional and local newspapers including The Scotsman and The Yorkshire Post. "," The Private Finance Initiative sees private companies build and run hospitals, schools, roads or other projects for a set term (usually 25 years but sometimes longer), with public sector bodies paying an annual fee. At the end of the deal, ownership of the building usually passes to the public authority.   While PFI has now been scrapped for new infrastructure schemes, some 700 existing contracts remain in place, with the last repayments set to end in 2050.   Our investigation revealed:  <ul>    The total lifetime cost of these 700 PFI contracts is now set to be £5bn higher than had been expected when the agreements were first signed, with the taxpayer picking up the bill;       Some PFI-financed buildings have been mothballed while expensive payments continue;       Private firms are charging the public sector a high price to make changes to PFI buildings, such as a school billed more than £25,000 for three parasols.    </ul>  The investigation made the front page of the  i  newspaper, with its extensive coverage running across three days. Regional and local versions of the investigation, highlighting rip-off contracts in different parts of the UK, ran in more than 100 local and regional titles across the JPIMedia stable – many making their front pages.   The Lost Billions put the spending of hundreds of health bodies, police forces, councils and Government departments under the spotlight, asking difficult questions of today’s decision-makers as well as those who originally signed up to the contracts.   The investigation prompted a public discussion of how the UK can rid itself of the expensive legacy of old PFI contracts, with Shadow Chancellor John McDonnell ending up in a Twitter spat with the Conservatives over who was to blame for the situation and how best to rectify it. "," We lodged hundreds of Freedom of Information requests, one to each public body holding a PFI contract.   We combined the responses with open data published on the deals to create a huge database ready for interrogation.   We took into account issues such as inflation to make sure we were making fair comparisons between the expected costs back when the deals were signed and today’s expected costs.   When we came to present our story, we used a ‘scrollytelling’ digital tool developed in-house by JPIMedia called Mooding to showcase our long-read version of the investigation. This incorporates interactive elements such as click-through timelines and polls to draw the reader in.   This long-read ran across all JPIMedia sites (more than 100 in total), to supplement our coverage of the investigation. "," JPIMedia Investigations is a team of about 10 reporters based in JPIMedia newsrooms across the UK, who team up on a regular basis to put together in-depth investigations on matters of public interest. Working in different offices all across the UK was not easy, so communication between team members (and editors) was hugely important. This was conducted mainly through group messaging systems and conference calls.   The sheer size of the batch Freedom of Information request posed significant challenges and represented a major investment of time. Each authority responded to the request in different ways (despite our best efforts to encourage responses in a standardised format) so standardising the results ahead of analysis was also time-consuming and difficult.   To tell this story well, we also had to explain complex systems of public-private finance in simple terms, which was a challenge.  "," At a time of ever-tighter budgets, this project shows the benefit of newsrooms working together and pooling resources to run in-depth investigations.    It demonstrates that systematic use of Freedom of Information laws to submit batch requests can result in powerful journalism. It also shows the importance of using FoI laws in a responsible manner, by using any and all open data that is available before turning to them.  ",https://www.yorkshirepost.co.uk/interactive/lost-billions-private-finance-initiative-scandal-NHS-schools,https://inews.co.uk/news/legacy-pfi-contracts-wasteful-shocking-exclusive-investigation-815305,https://www.sunderlandecho.com/news/traffic-and-travel/pfi-anger-cost-maintaining-a19-north-east-rises-almost-ps280m-814975,https://inews.co.uk/news/uk/private-finance-contracts-exclusive-investigation-pressure-mounts-solution-815827,https://www.fifetoday.co.uk/news/politics/fife-schools-built-under-pfi-costing-14m-more-than-originally-estimated-1-5026186,https://inews.co.uk/opinion/editor/pfi-deals-opinion-political-fraud-curses-future-generations-public-services-815887,https://www.chichester.co.uk/news/politics/sussex-police-under-fire-for-not-using-custody-suite-despite-continuing-to-pay-for-it-1-9109541,"John Blow, Philip Bradfield, Isabella Cipirska, Tom Cotterill, Michael Holmes, Anna Khoo, Dean Kirby, Joel Lamy, Paul Lynch, Chris McCall, Claire Wilde"," The JPIMedia Investigations team is made up of about 10 reporters, each one based in a different JPIMedia newspaper office and working for the team as a side-project to their main roles.   The team is overseen by JPIMedia News Editor (Data and Investigations) Claire Wilde. It produces about three investigations each year, which are available for all JPIMedia titles to carry.     ",,,
United Kingdom,BBC Shared Data Unit,Big,Participant,Best data-driven reporting (small and large newsrooms),Right to Buy - BBC Shared Data Unit,14/03/19,"Investigation,Cross-border,Database,Open data,Chart,Map,Politics,Business,Economy","Personalisation,Adobe,Microsoft Excel,Google Sheets,CSV"," Alex Homer from the BBC’s Shared Data Unit carried out the most in-depth analysis to date of the controversial Right to Buy policy, which allows council tenants to purchase their council homes at a discount.  He used a variety of investigative and data journalism techniques to establish whether there was any evidence that the purpose of the policy was being undermined by former tenants treating it as a get-rich-quick scheme.  The story involved sourcing multiple datasets and using data analysis to paint a comprehensive UK-wide picture of a policy that has come under fierce scrutiny. "," This report and research sparked widespread discussion. It was carried by three national newspapers, it had 1.6m readers on the BBC website and it was broadcast on television by the BBC News Channel. It was debated on TV, radio and across social media. A further 65 regional media outlets used Alex's research in reports of their own, as did nine BBC local radio stations and a further local BBC TV news programme.    It was published against a background of Scotland and Wales having brought the policy to an end, and while Northern Ireland consulted on the future of its equivalent scheme. An extension of the policy to include housing association tenants was however being trailed in England.   Supporters said this policy had given millions of people the chance to get on the housing ladder and secure their families' financial future. Opponents blamed the policy for distorting the housing market and for a huge reduction in the amount of social housing stock. Our analysis - the first on this UK-wide scale, covering 18 years' of data - revealed around 92,000 former council homes which were later resold, made a collective £6.4bn profit in real terms.   Alex's findings led Labour's London Assembly housing spokesman to renew his call for the policy to end <a href=""https://twitter.com/tomcopley/status/1106173993881649152"">https://twitter.com/tomcopley/status/1106173993881649152</a> and the Mayor of Newham to say the same <a href=""https://www.newhamrecorder.co.uk/news/newham-council-right-to-buy-resales-1-5956677"">https://www.newhamrecorder.co.uk/news/newham-council-right-to-buy-resales-1-5956677</a>.   Many more of Alex's interviewees called for the existing scheme to be halted. His Interviews held the politicians of the day to account over the trial extension of the policy and the author put further questions about its impact to one architect of the scheme nearly 40 years ago, Lord Heseltine, although he declined to answer.   This report was featured in the top 10 round-up of data journalism projects by the Global Investigative Journalism Network: <a href=""https://gijn.org/2019/03/21/gijns-data-journalism-top-10-frances-shocking-yellow-vest-injuries-yield-curve-music-migration-maps/"">https://gijn.org/2019/03/21/gijns-data-journalism-top-10-frances-shocking-yellow-vest-injuries-yield-curve-music-migration-maps/</a>. "," A variety of techniques were used to unearth these previously-unpublished data.   Alex submitted requests for information under the Freedom of Information Act (FOI) to HM Land Registry for data covering England and Wales. He submitted similar requests to the Northern Ireland Housing Executive and requested a bespoke dataset from the Registers of Scotland. Northern Ireland’s FOI response was 83 pages of scans of a paper ledger in PDF files. It was not possible to digitise with OCR software so Alex spent days manually inputting it into a public-facing Googlesheet to enrich the Commons.   In his analysis, Alex factored in inflation to establish real-terms price differences by sourcing CPI data from the Office for National Statistics to match to dates of sales. He normalised the findings on profits by establishing the profits in Pound sterling generated per day of ownership by former council tenants before they resold their homes. He also matched homes sold to government regions. He visualised the report with charts and an interactive Carto map, illustrating that vendors in the south east of England made more money in real terms from the year 2000-2018.   Interviews with people with direct experience of this policy or impacted by the shortage of social housing, made this report engaging, meaningful and relatable to a wide audience. The data were published on the BBC Shared Data Unit's public-facing Github page and are available for reuse and further interrogation. "," A challenge that came up during the data wrangling for this story was because these were previously-unpublished data, they had not been previously exposed to the same level of scrutiny as datasets which are routinally and habitually used in news reports. During Alex's Right to Buy investigation, he was presented with around 150,000 rows of data – each row representing a property title in Great Britain formerly sold under the policy. Both HM Land Registry and Registers of Scotland underline they cannot guarantee their datasets are error-free. Alex found some 60,080 rows did not have comparable sales prices. When he highlighted this, HM Land Registry re-ran its script and found 2,582 more price entries so he updated his calculations. Similarly, he encountered 734 dates anomalies in Scottish data. This underlined the importance for any journalist not to treat a dataset as a source of objective truth but, just like any interviewee, to ask it questions and not simply accept its first answers.   Another challenge was posed by the lack of digital records of property sales in Northern Ireland. When Northern Ireland Housing Executive sent its response to Alex's request for data under the Freedom of Information Act (FOI) it sent 83 pages of scans of a paper ledger in PDF files. First, Alex sought to establish if these data could be digitised with OCR software, but when that could not be achieved, Alex decided it would still be of public value to make these data open themselves so he spent days manually inputting it into a public-facing Googlesheet to enrich the Commons. The data were published on the BBC Shared Data Unit's public-facing Github page, in an inline link from the BBC online report and are available for reuse and further interrogation because of his labours. ","A significant takeaway from this project was carrying out this data analysis, and having the opportunity to have an open dialogue with the data holders about data quality, led to improved data quality and presentation so those data were easier for everyone to interpret and they had greater public value as open data, when they were brought into the public domain. The different approaches of the devolved governments across the UK might also have provided a reason to restrict the scope of this study, which Alex refused to do and it led to a more impactful investigation. Comparing sales across the UK was complex due to differences in policy: Scotland had brought the policy to end in 2016, Wales had done the same in January 2019 and Northern Ireland did not hold data on the prices paid for properties nor the duration of their ownership. Despite the difficulties, Alex realised these nations' data would provide valuable insights into the consequences of the policy there. Together with patterns found in England, the research was able to inform questions that needed to be put to government ministers in England, as they intended to further extend the policy in England to include housing association tenants in addition to council tenants. Another key takeaway was Alex realised the limitations of the dataset and was clear on them in his reporting. Each row in the data represented a property title: sometimes they were merged or split as different owners bought/sold/developed on their land. Due to rare enormous profit margins in the data, Alex used median averages for fairness as he could not investigate the idiosyncracies of every individual property title changing over time, as there were tens of thousands of them. The data also did not include the levels of discount repaid by ex-council tenants",https://www.bbc.co.uk/news/uk-47443183,https://github.com/BBC-Data-Unit/right-to-buy,,,,,,"Alex Homer, Peter Sherlock","  Alex Homer  is a Senior Journalist on the Shared Data Unit. He is a self-confessed news nerd and loves reporting in the public interest and holding the powerful to account. He is a specialist in using the Freedom of Information Act with a particular interest in home affairs, local government and the business of sport. Before joining the BBC in 2014, Alex worked for the Wolverhampton-based daily newspaper The Express & Star, rising through the ranks to become a senior reporter and education correspondent. His ability to manipulate data was first honed during a three-month BBC attachment in 2016 investigating the accounts of the 92 clubs in the Premier League and English Football League, during which he manually entered 18,785 individual pieces of information to build a giant database.    Pete Sherlock  is the Assistant Editor of the Shared Data Unit. He started his career as a reporter for Newsquest on the Enfield Independent, and went on to work for Archant’s Hackney Gazette and the East London Advertiser. He joined the BBC in 2010 and worked as an online reporter in the South East, before joining the English Regions HQ team two years later where he worked as a news and sub-editor on breaking stories. In 2015, he was tasked with setting up a unit to provide innovative, data-driven content for the website. The unit produced a series of original news stories and features that resonated with the BBC’s online audiences Pete moved into this current role in 2017. He passionately believes that data journalism is breathing new life into journalism, playing a key role in informing citizens and holding those in power to account. It is also very fun.  ",,,
Spain,El Confidencial,Big,Participant,Best data-driven reporting (small and large newsrooms),Non-accessible Underground,29/07/19,"Investigation,Long-form,Infographics","Scraping,D3.js,QGIS,Microsoft Excel,CSV,R,RStudio"," During one month and a half, El Confidencial Data Team has scraped every 30 minutes the real-time Madrid subway incidents web to find how many times escalators and elevators have not been working. The data collection was captured in a database with 1,549 records for each station. With this data, we produced two articles: the first one, published on July 29th, shows the stations with more unavailable escalators and elevators; the second one, published on September 6th, is focused on the difficulties for parents to move children's carriages on the subway. "," It was the first time that anyone could measure how many times escalators and elevators of Madrid subway were broken. After that, we made a second report about newly born families that have a lot of problems to access to non-accesible stations. The two reports has a huge impact in terms of pageviews, with more than 60,000 visits. Further, Madrid subway company reacted with some internal criticism towards our reports. "," For scraping the real-time Madrid subway incidents web we used a Javascript code to scrape the incidents of 300 stations every 30 minutes. After one month and a half of compilation, we built a database with every incident or 'no problem' notice recorded every 30 minutes. In total, we had 1,549 records for every station. Due to the size of the database, we managed it with R in the very first moment to extract the incidents. Then, we analyzed the data with Microsoft Excel. The infographics are made with Datawrapper and Javascript, and the maps with QGIS. "," The hardest part of the project was to find an affordable way to scrape the real-time Madrid subway incidents web. We wouldn't be able to scrape it in real time because we would down the server, we found a way to scrape it every 30 minutes. With this formula, we managed a sample of 1,549 records for every station, "," The best lesson of this project is the original and imaginative way we found to scrape the real-time Madrid subway incidents web. The ideal would be to have the data in real-time, but it was unable, so we had a huge sample of observations for every station to find how many times elevators and escalators were broken. ",https://www.elconfidencial.com/espana/madrid/2019-09-06/metro-accesible-paradas-ascensor-escaleras_2209007/,https://www.elconfidencial.com/espana/madrid/2019-07-29/metro-madrid-escaleras-mecanicas-averia_2147999/,,,,,,"Cristina Suárez, Michael McLoughlin, Jesús Escudero, Antonio Hernández, Pablo López Learte, Laura Marín, Pablo Narváez, Luis Rodríguez, Carmen Castellón"," This project is the result of the team work of the Data Unit and Storytelling team along with Michael McLoughlin, a reporter from the Technology section, and Cristina Suárez, a data reporter. Besides, Jesús Escudero and Antonio Hernández led the data compilation and analysis; Luis Rodríguez and Pablo Narváez developed the code behind the special format and the interactive infographics; Pablo López Learte were the designers and produced the graphic line of the project, and Carmen Castellón made the phographs for the special report. ",,,
Germany,Der Spiegel,Big,Participant,Best visualization (small and large newsrooms),Deutschland schwarz-weiß / Germany in black & white,21/06/19,"Explainer,Long-form,Open data,Infographics,Map,Environment,Culture","QGIS,Canvas,OpenStreetMap"," The urban fabrics of cities tell a lot of stories. From roman roots to monarchs and dictators who shaped cities according to their will to the consequences of WW2 bombings to all the different approaches to building and structuring a city that emerged since the 19<sup>th</sup>century. Being a trained urban planner, I know a lot of those stories by heart and have explained them, pointing at a city plan to friends and family many times. For this story, my colleague Anna-Lena Kornfeld and I took all those experiences and condensed them into a map/visualization-heavy storytelling for a broader public. "," We've luckily received a lot of positive feedback from our readers. Ranging from ordinary readers to experts (geographers, historians and urban designers). One expert is now using our story as teaching material in his university urban planning history course.     "," This story wouldn’t have been possible without the fantastic work of the OpenStreetMap community in Germany. There is an official data set of all buildings in Germany, but it’s unaffordable (roughly 100.000 $). Luckily OpenStreetMap has great coverage and is under an open license.   For data processing we've used the national OSM raw data extract and filtered out all buildings with GDAL. Some further processing was done with QGIS, beofre the data was uploaded to Mapbox (via Tippocanoe) and rendered as static and interactive maps in MapboxGL.js.   The additional graphics were produced with AI2HTML. "," Getting our hands on the data (see above) and researching the urban design history of the manifold examples. It took as a lot of time to find meaningful, and at the same time visually appealing examples for each epoche.   Finding a style for our story wasn't hard. Figure ground diagrams were predestinated as key visualization elements. Just the way they help urban planners reading the urban fabric we used them to reduce what’s being displayed to a bare minimum. The alignment of buildings (with some very lightly shaded roads and waterways) is all it needs to give readers orientation and tell the story of the built environment. But this also meant that we had to find archetypical examples that emphasize the characteristics of each epoche. This was easier said then done, since most German cities are rather a dense mix of different stages of development then a homogenous urban landscape. "," If you can't get your hands on public data, give it a try with open data. There are many fileds where OpenStreetMap, Wikipedia and the like offer great quality and are a suitable ground for reporting.   Also, try to develop a strojng design idea for your story and stick to it. This helps a lot to keep readers emerged in stories with longer reading time.   Finally, keep an eye out for what others are building. In our story we gave credit to “A map of every building in America” (<a href=""https://www.nytimes.com/interactive/2018/10/12/us/map-of-every-building-in-the-united-states.html"">https://www.nytimes.com/interactive/2018/10/12/us/map-of-every-building-in-the-united-states.html</a>) by Tim Wallace, Derek Watkins und John Schwartz, published in the NYT in 2018. Visually the story was an inspiration to us. Even though, we have to admit, that in our eyes it fell a little short in explaining patterns and circumstances. Something we ended up spending a lot of research and editing time on. ",https://interactive.spiegel.de/int/pub/nextgen_migration/wissenschaft/2019/schwarzplan/v0/pub/index.html,,,,,,,"Patrick Stotz, Anna-Lena Kornfeld"," Patrick Stotz is a data journalist at Der Spiegel. He's got a B.Sc. and M.Sc. in urban planning, worked as a research fellow in the field and found his way into data journalism through his interest in interactive maps.   Anna-Lena Kornfeld worked as a research fellow in geoinformatics before joing the graphics department at Der Spiegel, where, amongst other things, she's responsible for making maps. ",,,
Spain,El Confidencial,Big,Participant,Open data,Rentals of the Spanish football stadiums,12/07/19,"Explainer,Open data,Map,Sports","CSV,OpenStreetMap"," Thanks to the Spanish Transparency Law, we would be able to obtain 29 agreements between city councils and football team from the First and Second Division of LaLiga for the rental of the local stadiums where the football teams play. It was the first time that this information was compilated and published in Spain and shows the agreements between the councils and the teams for the use of each stadium. "," It was the first time that the agreements between city councils and football professional teams for renting public stadiums was published in Spain. All the agreements and documents are published in El Confidencial's Github repository (<a href=""https://github.com/ECLaboratorio/unidad-de-datos/tree/master/proyectos/convenios_estadios"">https://github.com/ECLaboratorio/unidad-de-datos/tree/master/proyectos/convenios_estadios</a>). In terms of pageviews, the article had more than 60,000 views, much above the average and median for a report published on El Confidencial. "," We used the Spanish Transparency Law to obtain 29 (of 42) agreements between city councils and football teams. We asked for them since September 2019; public administration has one month, plus other possible month, to give the information asked by the citizens. For the infographis, we used both Leaflet and Datawrapper, plus the repository in our GitHub account. "," The hardest part of the project, obviously, was to obtain the rental agreements. We would wait for one or two months for them; after we received, we started producing the report and the infographics. "," One thing that others can learn from this project is that anything related with the public sphere, including agreements for the use of public infrastructures, may be subject to FOIA requests. ",https://www.elconfidencial.com/deportes/futbol/2019-12-07/estadios-liga-alquiler-primera-division-segunda-precios_2355456/,,,,,,,"Diego Cayota, Darío Ojeda, Antonio Hernández"," Darío Ojeda, reporter for the Sports section, and Diego Cayota, intern of the Data Unit, sent the FOIAs to every council city and, then, write the report. Antonio Hernández, Data Unit's developer, made the infographics and linked the rental documents published in El Confidencial GitHub repository with the table on the bottom of the article. ",,,
Mexico,Animal Político,Small,Shortlist,Open data,Sinapsis (Synapses): Fighting corruption with open data to discover connections between illegal companies that received public resources in Latin America,12/09/19,"Investigation,Solutions journalism,Database,Open data,Infographics,Chart,Map,Politics,Corruption,Business","Scraping,D3.js,Canvas,Json,Adobe,Creative Suite,Microsoft Excel,Google Sheets,CSV,OpenStreetMap,Anime.js,Node.js"," Mexico is extremely corrupt, politicians steal money without sanctions.   The modus operandi all across Latin America is very similar: government officials create ghost companies, the government assigns projects to these fake businesses and the money disappears.   How could we detect these companies? That’s how we started this project. Sinapsis is a tool that visualizes relationships between businesses and government entities, as well as how companies and their personnel are interconnected. As a free, open-source project, it’s designed to help detect irregularities and suspicious activity in business interactions and practices. "," Animal Político has already published two very important investigations done with Sinapsis on corruption in Mexico: The Ghost Companies of Veracruz and The Master Scam. The latter revealed that the Mexican Federal Government disappeared 7,670 million pesos (around 400 million dollars) through 186 companies. These investigations won the National Journalism Award. Even more important, they have been the basis for the prosecution of several former and current public officials in Mexico.    It is a tool that has been proved and used. Only a couple months into its publication, there are already 180 different projects using it and at least 5 people forking the code on GitHub to adapt it to their needs. The tool also includes the investigation methodology; tips on useful public databases; promotes the use of open data pre charging relevant databases on the tool to facilitate analysis; builds community through a Telegram channel open to doubts and suggestions; and is working on the process of systematizing information on corrupt companies across Latin America.   To date, we have allies collaborating from Argentina, Bolivia, Brasil, Chile, Colombia, Costa Rica, Ecuador, Guatemala, Honduras, Panama, Paraguay, Peru, Dominican Republic and Venezuela.   During this first phase, we want to train local journalists in Mexico. So far, we have given workshops in Mexico City, Hidalgo, Guadalajara, and have already scheduled Puebla, Tijuana and Mexicali. We would like, further down the line, with the right allies and resources to be able to reach different countries in Latin America.   The project has had international recognition, the United Nations Office on Drugs and Crime has invited us to their regional convention in Quito, Ecuador and are presenting in TicTec in march in Iceland. "," Beyond the platform, Sinapsis is also an investigation methodology. The way in which we processed data had to be easy for journalists (that generally don’t have a lot of technical knowledge) and functional for a developer (machine readable).   Part of this process was generating a CSV template that could be easily opened in Excel for journalists to work on and understand, and that could be uploaded to Sinapsis at the same time. Gathering the information from different sources was done manually, however, being able to categorize and standardize the information has proven very valuable for the investigation of government-companies corruption schemes in the region. This process was possible through the interdisciplinary work of journalists, information designers and developers.   Technically, the platform is built with React (ES6), making it modular, flexible and scalable. It uses as its main library D3.js and the visualizations are SVG files to allow designers and journalists to export it in different formats according to their needs, providing small news outlets and freelance journalists without resources with graphic material ready to publish.   Sinapsis doesn’t only show the visualization of connections; it also gives statistics on sums and number of coincidences; a geographic interactive map that georeferences addresses providing strategic insights for field research; an exportable CSV with the lists of coincidences; and it is possible to export the data in JSON formats (for developers and data analysts), CSV, as well as a .sinapsis file that has the full project to facilitate collaboration between investigations.     "," To explain this, we’ll provide some context.    Animal Político is a digital and independent newspaper in Mexico, specialized in topics such as human rights, migration, drug trafficking, security, politics and inequality.   During the Ghost Companies of Veracruz and The Master Scam investigations we worked through: How could we detect these companies? Having limited time, how could we systematize and automatize this process? With the volume of information we had, could technology give us additional insights to our manual analysis?   The hardest part was conceptualizing the project. It started with hundreds of public documents, a hypothesis and a lot of information to process. We developed the tool as we were structuring and advancing the investigation with an interdisciplinary team. It is very rare for a small independent news media to build technology.   At the beginning, Sinapsis was going to be a dashboard showing coincidences between companies, and as time passed on we kept adding features. First a node map, later a georreferenced map, data analysis and statistics on the databases. The project slowly grew over time.    Additionally, we faced challenges with the interdisciplinary nature of the team: developers, information designers and journalists work in very different ways. Being able to communicate effectivly and build this product took 3 years!   Sinapsis represented several challenges in a technical level: it is a tool that had the user’s privacy as a top priority, which meant that the data processing had to be done through the browser without using a backend.   We know Sinapsis is not self-explanatory yet, which is why we added a “guide” button so that the users get to know some parts of the tool. However we know that it is not enough, so we started giving workshops and would like to produce tutorial videos / material in the next iteration. "," People will be able to see how with a very small team, from an independent news media with very little resources, you can develop tech and tools that have an impact in society. They will also be able to see that the investigation journalism world can also think about technological outputs, since developing software in media almost never happens.   Newsrooms can learn a lot from the interdisciplinary work thar makes these types of projects possible, and this might sound obvious in other countries or contexts, but in Mexico it is very rare that a media outlet hires a developer to work full time on journalistic projects. We also are not aware of any other outlets with an information designer for content production.   Showing work flows and methodologies would be very valuable for the audiences. ",https://sinapsis.lat/herramienta/estafa-maestra,https://sinapsis.lat,https://sinapsis.lat/herramienta,,,,,"Yosune Chamizo Alberro, Gilberto León, Mauricio Martínez Robles, Jesús Santamaría, Elizabeth Cruz, Ethan Murillo, Jorge Ramis, Nayeli Roldán, Manuel Ureste, Miriam Castillo, Arturo Ángel, Tania L. Montalvo, Francisco Sandoval, Daniel Moreno"," Yosune Chamizo Alberro is the Information Designer at Animal Político, an independent and digital newspaper in Mexico City where she has been part of projects such as:  To Murder in Mexico: Impunity Guaranteed ;  The Master Scam: graduated in disappearing public money ,  The Ghost Companies of Veracruz ,  NarcoData: In-depth digital analysis of the organized crime in Mexico , among others.   She is the regional representative in Mexico for the International Institute for Information Design (IIID) and her work has been recognized in countries such as Germany, England, Austria, Italy, Spain, Latvia and Brazil. In March of 2019 she was part of the jury for the 27th edition of the Malofiej World Infographics Awards organized by The Society for News Design.   She has given conferences and workshops to more than 30 civil society organizations and holds a master’s degree in Design, Information and Communication from the Metropolitan Autonomous University Cuajimalpa Unit (Mexico) and a master’s degree in Graphic Design from Elisava (Spain). ",,,
United States,"WKMG-TV, News 6 Orlando",Big,Participant,Best data-driven reporting (small and large newsrooms),Gun violence: A growing threat at American schools,02/11/19,"Investigation,Long-form,Database,Fact-checking,Infographics,Chart,Map,Health,Crime,Gun violence","Animation,Microsoft Excel"," As part of our <a href=""https://www.clickorlando.com/news/gun-violence/news-6-hosts-special-day-of-coverage-dedicated-to-gun-related-issues"" target=""_blank"">""Generation Under Fire: Guns, Right & Safety"" special</a>, News 6, WKMG-TV embarked on a research project to study the history of fatal shootings on America’s school campuses. After an exhaustive search, we discovered no comprehensive and visual representation of this issue. All but four states in the country have had a fatal school shooting since 1960. In analyzing thousands of incidents during that 59-year period, the News 6 Investigators identified more than 600 fatal school shootings that killed over 900 people and wounded another 700+. "," For our audience, this story was important as it coincided with the one-year remembrance of the shootings at Marjory Stoneman Douglas High School in Parkland, FL, a shooting that killed 17 teachers and students. WKMG did not take a stand for or against or delve into the debate over Second Amendment rights.  We instead focused on the number of fatal shootings and the specific instances of concern derived from the data.   None of us expected to find out almost a thousand people have died in school shootings in the U.S. since 1960, but it wasn’t a surprise to discover that most of the deaths occurred in high schools. In fact in the 1960s, there were seven fatal shootings in high schools; in the 1990s, that number peaked at 111. University fatal shootings are on the rise as well coming dangerously close to the number of fatal shootings in high schools in the 2010s. States with the most fatal school shootings held surprises as well. The top three states: California, Texas, and Florida. New York state was number 12 on the list and was tied with Oregon, South Carolina, and Washington D.C In a significant representation of this project’s impact after the story was published, the Department of Homeland Security recognized our project on one of their social media accounts and thanked us for the inclusion of their own studies.   For 2020, WKMG along with five other sister-stations in the Graham Media Group, is exploring the idea of adding 2019 statistics to this study and mapping each individual school shooting location. One other possibility is to create 47 other infographics to delve deeper into statistics and individual stories for each state and the District of Columbia that has had a fatal school shooting across what would then be a 60-year period. "," The project was designed as an infographic and utilized the online program, Infogram. Using different Infogram tools, we were able to break down our data into multiple subsets easily digestible by the reader. Depending on the level of detail sought by the user, different charts, maps, or graphs can be filtered enabling the reader to drill down to a specific set of visualized data.   While the program worked well for data visualization, it was useless without the data. The data is the backbone of this project as it took months to assemble the raw information and needed multiple Excel spreadsheets to record and categorize each shooting and organize for input into Infogram.   Excel sheets/tabs had to be made for each individual visualization in the infographic (i.e. 20-year trend since Columbine, worst states, types of schools, safest places in a school, etc.). Four sheets were needed for the data with a total of 74 different tabs. One tab included information spread out over 1,189 rows; another across 73 columns. Because Infogram only supports five additional columns for text in its maps, one spreadsheet was created to collate multiple cells into one. Through trial and error, we also discovered that we could get around the five-column limit in Infogram by importing an Excel spreadsheet with up to (at least for us) 73 columns.   Once the infographic was complete, a simple embed code was used to place the final product in a web story. "," The data. Definitely, the data. As previously stated, when we first embarked on this project, we thought it would be a relatively straightforward and simple process to gather a list of fatal school shootings that had occurred in the U.S. What we found instead was a mish-mash of incomplete lists of many shootings with no one list as comprehensive and authentic as the one we ended up creating.   For every fatal incident, WKMG used multiple accounts of the shooting to extract details for the data set. Aside from the 600+ shootings we included, we eliminated 91 other entries classified by various resources as fatal school shootings that did not meet our criteria. Nine more fatal school shootings could not be verified with an independent second source.   Finding the time to work on this project was a challenge as well. No one in our market (print or broadcast) has attempted to tackle this topic, and like many newsrooms, the resources for multiple individuals (or just even one) to take on a long-term project such as this is limited. One hundred percent of this project (research, design, and data visualization) was done by one individual. Because of the leadership in our organization, and the commitment to data journalism both in our newsroom and from a corporate level, the Graham Media Group has directed newsroom leaders to tackle more lomg-term data driven projects. "," The best way to sum up this answer is with bullet points:   -Don’t be afraid to take on long-form projects that don’t have an immediate return. Though you won’t always get an immediate rise in ratings or increase in clicks, your audience will appreciate the effort that your organization is not always in it for the low-hanging fruit.   -Something like this can be done with just one person, but when taking on a project like this, teamwork will make things easier. Within that teamwork, designate individual tasks, but provide enough flexibility for people to cross in and out of different responsibilities.   -Always start with the data. More importantly, don’t be afraid to add to the data. Multiple times through this project, more details were added as we went along with our research, forcing us to go backwards in some instances to make sure we were being consistent.   -Get a buy-in from the boss. This project solely saw the light of day because our management made a conscious decision to free up time for an individual to do a very deep dive into a complicated topic. Because of projects like this, parent company Graham Media Group has now designated two individuals per newsroom as dedicated data journalists who have monthly conference calls and collaborate on corporate wide projects.     ",https://www.clickorlando.com/news/2019/02/11/gun-violence-a-growing-threat-at-american-schools/,https://infogram.com/gun-violence-a-growing-threat-at-american-schools-1h8n6m58ojrm2xo?live,https://drive.google.com/file/d/1ialXpxCBfcFbgjC7ESfQPseUP7HpaR_H/view?usp=sharing,,,,,"Donovan H. Myrie, Ph.D."," Donovan Myrie is an Investigative & Special Projects Producer at WKMG-TV News 6 (the CBS affiliate in Orlando, Florida) and one of two News 6 Data Visualization Journalists. Dr. Myrie, who has a Ph.D. from Union Institute & University, has been with News 6 since 2016. Besides his terminal degree, Donovan studied at Ithaca College as an undergraduate and received a graduate degree from Columbia University. On a daily basis he may be working on an investigative or special project piece or be assigned to daily news coverage. Dr. Myrie is also one part of the Graham Media Group’s data journalism team.    ",,,
Hungary,index.hu,Big,Participant,Best visualization (small and large newsrooms),"The housing market boom in Hungary, mapped",12/11/19,"Explainer,Map","QGIS,Adobe,CSV,OpenStreetMap"," The Hungarian housing market recently went through a price boom mostly affecting larger cities and the capital, partly as a result of the stabilisation following the economic world crisis and low interests. Budapest prices increased extraordinarily even in worldwide comparison, as over the past decade, prices quadrupled in numerous streets. We mapped this process, highlighting neighbourhoods and construction methods that were considered cheaper and less popular earlier, but were greatly affected by the changes. We attempted to show how housing became a severe problem for some in Budapest where others might have seen a profitable investment opportunity. "," The article achieved its goal, our readers received an easy-to-grasp overview of the processes on the housing market that made home purchases significantly more difficult for some while creating a lucrative investment opportunity for others. A fifth of Index's daily visitors have read the original Hungarian article, a tenth of whom went on to share it on social media. "," We used the database of Open Street Map in order to show the streets of different cities, and we have filtered the data to show only residential areas. We used the cartography software Qgis to pair streets to zip codes and filter streets with duplicate names. From Qgis, we exported the map files into Tableau. We used Tableau to categorize and express values. After that, we transferred these files to Adobe Illustrator in an SVG format, where we finalised the design and created the different formats to suit ideal viewing on various devices (desktop, mobile, tablet). To display the final code and graphics, we used the Ai2html script, developed by the team at the New York Times. "," The most difficult task was to marry the incomplete database with the similarly incomplete cartographic data. The most complicated question during the visualization process was how to express the increasing prices in a way that is obvious, simple, and easy to consume. Designing how to display the graphics conveniently on different devices also required considerable effort. "," That a seemingly simple, but unique data visualization requires many technological layers and work processes. ",https://index.hu/english/2019/12/19/hungary_large_towns_budapest_real_estate_market_prices_demand/,https://index.hu/gazdasag/2019/12/11/lakasok_dragulasa_2006-2018_ingatlanpiac_budapest_es_videki_nagyvarosok/,,,,,,Tamas Szemann," Tamas Szemann is a visualization expert in Budapest. Currently working as visualization lead for Index.hu an independent online news wire. He has 3 years of experience in the field, coming from a visual background, was a photo editor and photographer before jumping into data viz. You can reach him at <a href=""mailto:tszemann@mail.index.hu"">tszemann@mail.index.hu</a> or find him on twitter @szeemi ",,,
United Kingdom,The Times,Big,Participant,Best data-driven reporting (small and large newsrooms),GP shortages,20/12/19,"Investigation,Solutions journalism,Database,Infographics,Chart,Map,Women,Health","Personalisation,Scraping,D3.js,Json,Google Sheets,CSV,R,RStudio"," We revealed that a national shortage of GPs has left some surgeries with one permanent doctor caring for as many as 11,000 patients.    The local scandals were uncovered using detailed analysis of every general practice workforce report released by NHS England between 2013 and 2019. From here we began our reporting, visiting the worst-hit (as well the best performing) surgeries to verify what the data had revealed and humanaise our findings.    Looking into the demographics, we were also able to reveal that 1.3 million women in England do not have regular access to a female GP. "," Our investigation into the scale of the crisis in general practice services had a strong impact on our readership, as well as in the public debate. It was picked up by other national media such as <a href=""https://www.bbc.co.uk/news/blogs-the-papers-50875127"">BBC</a>, <a href=""https://www.dailymail.co.uk/news/article-7815871/A-single-GP-looks-11-000-patients-one-practice-NHS-figures-reveal.html"">Daily Mail</a>, <a href=""https://www.thesun.co.uk/news/10599480/national-shortage-of-gp-patients-crisis/"">Sun</a>, <a href=""https://www.telegraph.co.uk/news/2019/12/22/britains-gps-among-best-paid-western-world-study-finds/"">Telegraph</a>, <a href=""https://metro.co.uk/2019/12/21/gp-crisis-leaving-surgeries-one-doctor-caring-11000-patients-11944065/"">Metro</a>.     The project was published soon after the December general election, contributing to setting the new political agenda. By highlighting the most extreme cases where patients were suffering the most, this story renewed pressure on the Prime Minister to fulfill his electoral pledge to recruit 6,000 GPs by 2024-25.     After the story was published, the Health Secretary Matt Hancock spoke out to reassure the public that the Tory pledge to tackle the NHS staffing crisis was underway.   The story received over 1,500 comments from online readers, showing a strong interest in the topic and high engagement. Since the findings were published, The Times has been contacted by hundreds of people sharing their personal experiences with their GP practices. Their stories have been gathered for a piece of community journalism, <a href=""https://www.thetimes.co.uk/article/i-ended-up-with-pneumonia-patients-share-their-experiences-of-gp-appointment-waiting-times-ggxtw3p6z"">published</a> a month later.    The lookup interactive tool we created to allow readers to check how services have changed in their area, has been viewed over 23,000 times, with more than 3,000 searches. "," We analysed every general practice workforce quarterly report released by NHS England between 2013 and 2019. Data in the form of  CSV  files were imported and analysed in  RStudio . For each surgery we filtered specific categories: number of patients, GP headcounts, full time equivalent GPs (by gender), registrars and locums. This allowed us to determine the number of patients per fully qualified GP in each surgery over time. From this, we listed the worst and best performing practices that needed further investigation. For each of them we cross-referenced NHS records and Care Quality Commission reports, which provided a better picture of the status of their care services.   We shared findings using  Google Sheets,  which gave us the opportunity to work on the data simultaneously.   Figures of appointment times, closed and dormant practices provided by the NHS for each Clinical Commissioning Group were also analysed in Google Sheets and then used to create interactive maps and charts to dress up the story online. For this, we used the web tool  Datawrapper .   To personalise our story, we built a searchable tool that allowed readers to enter their postcode and check how GP services changed in their areas over time.    We used the postcodies.io  geolocation api  to match up readers postcodes to their clinical commissioning group (CCG). The data for each CCG (including the latest figures for registered patients, doctors’ surgeries, GPs, closed and dormant practices and the number of patients per GP over time compared to the national average) was stored in separate JSON files. This meant we only had to load in the data for the relevant CCG and the interactive was a lot faster as a result. The interactive itself was built using  React JS . The graphical element of the interactive was created using  D3 . "," One of the most difficult parts of this project was to handle inconsistencies in NHS workforce data: this was due to a lack of clear guidelines on how surgeries should declare staff figures, but also to duplications in the case of practice mergers and closures.    In order to be as complete and accurate as possible, we had to filter our results in RStudio, identify the best and worst practices manually and address them directly to confirm our figures.    Once happy with the numbers, we spent quite some time discussing the best way to visualise them, so that readers could get key information for them. To overcome the issue of inconsistent figures for merged practices, we finally decided to use data grouped by clinical commissioning groups.    Also, some last minute updates (on the day of publication) on the NHS website were easily handled by a simple re-run of our R script.    It’s important to note that, for this project, we only relied on publicly available data that no one else had analysed in such great detail before: to figure out trends over time, to point out the best and worst performing practices, to look into demographics and identify a gender gap in surgeries, it took both meticulous digital analysis and in-depth reporting on the ground. "," We can identify three key points we have learned from this project:       Using a programming language such as R to analyse data allows you to work with a clear script that can be easily tweaked and re-run for last minute changes in the data.        Data led investigations are very useful to identify problems, but a combination with on the ground reporting remains very important.        Data is a powerful tool to pinpoint strong news lines, as well as constructive angles for your story.      ",https://www.thetimes.co.uk/article/gp-crisis-nhs-shortages-mean-one-doctor-has-to-care-for-11-000-patients-v25nt2cdl,https://www.thetimes.co.uk/article/the-best-and-worst-hit-gp-surgeries-patients-queue-in-cold-to-see-a-doctor-td9fxmvmz,https://www.thetimes.co.uk/article/gp-crisis-third-world-conditions-at-surgeries-with-no-female-doctor-jgkqb8wcp,,,,,"Anna Lombardi, Paul Morgan-Bentley, Daniel Clark","  Anna Lombardi  is a digital interactive journalist for The Times and The Sunday Times. After obtaining a PhD in experimental physics and a masters degree in science communication, she has moved into the field of data journalism bringing her scientific approach into the newsroom. She specialises in visual journalism and data-led investigations.    Paul Morgan-Bentley  is the head of investigations at The Times and specialises in in-depth reporting and undercover work. He has worked as a journalist in London, New York, Birmingham and Manchester and is a previous winner of the Cudlipp Award for campaigning journalism at the UK Press Awards.    Daniel Clark  is an interactive journalist for The Times and The Sunday Times. He specialises in creating bespoke charts, maps and graphics to tell data-led stories. ",,,
United States,The New York Times,Big,Winner,Best visualization (small and large newsrooms),See How the World's Most Polluted Air Compares With Your City's,12/02/19,"Explainer,Mobile App,Map,Environment,Health","AR,D3.js,Adobe,R,Python"," Outdoor particulate pollution known as PM2.5 is responsible for millions of deaths around the world each year and many more illnesses. We created a special project that visualizes this damaging but often invisible pollution. The interactive article allows readers to (safely) experience what it’s like to breathe some of the worst air in the world in comparison to the air in their own city or town, giving them a more personal understanding of the scale of this public health hazard. "," This air pollution visualization project was one of The Times' most-viewed stories of the year, garnering well over a million page views in a single day. It also had some of the highest reader engagement. Readers took to social media, unprompted, to share the air pollution averages for their own city as well as screenshots of the project’s visualizations, and to express concern over recent upticks in air pollution.   Making air pollution more tangible to the general public is especially important today, as air quality in the United States has worsened after decades of gains, while much of the world’s population continues to breathe high levels of pollution. At the same time, it is becoming more clear that air pollution affects human health at ever more granular scales.   Experts from the public health community, including the United Nations and WHO, have reached out about using the project for educational purposes. ",  Particle visualization and charts:  The data analysis was done using Python. Visuals in the story were created using WebGL and D3.     Augmented reality version:  The AR experience was created using Xcode and Apple SceneKit. (The AR scene being responsive to data was created using Swift in Xcode.) Please not that the AR version is only available on the New York Times app and on iPhones due to technological constraints of the Android operating system.    Map:  The map was rendered by converting netCDF files using R and gdal. The animation was done using Adobe's After Effects and Illustrator. ," We wanted the project to build empathy through data by connecting people's own experience (what average air pollution is like in their own city) to various case studies of polluted air that have recently made news.   To achieve that, we strove to make sure the visualization had the right feeling of movement in space to evoke polluted air, while still reflecting that it a  data visualization  rather that an accurate reflection of what pollution might look like at a specific place and time.   We went through many ideas for how to represent this pollution – as particles, as haze, etc. – and many ways to show it to our audience. The end goal: Walking the line between what is scientifically accurate while also allowing people to feel a natural connection between the viz and the subject being visualized (pollution).    To ensure scientific accuracy, we ran our visualization ideas past half a dozen experts who study particulate matter pollution in order to best decide how to show these damaging particles.    In the end we settled on a deceptively simple presentation: Filling up your single screen (or room in AR) with particles as you scroll (or tap) in order to create a sense of ""filling"" your lungs with this sort of air. Our readers' reactions to the piece suggest that we got the balance right. "," One lesson we hope people will take away is that it is possible to create emotional connections to data through visualization. We built the story introduction so that readers become the central character, allowing them to use their own experience of polluted air as a benchmark by which to judge and understand the scale of pollution elsewhere. That builds a deeper understanding of the issue at stake than just showing data for far-away places they may have never visited.   On the more technical side, many people commented on the project's innovative use of augmented reality.  Theproject leveraged AR to make something that is all around us but often invisible actually visible in 3D space. Previously, experiments with AR at the Times and in other newsroom mostly consisted of placing objects into space (such as the Times' Lunar landing project) or creating a novel 3D space for exploration (such as the Guardian's augmented reality project that allowed users to experience what being in solitary confinement is like).    Selected praise for the AR experience:  <ul>  “This is easily the most compelling use of augmented reality I've ever seen in a news context.” – <a href=""https://twitter.com/_cingraham/status/1201879522225934338"">Chris Ingraham, Washington Post</a>   “I've been always (and I still am largely) skeptical about the application of #AR and #VR especially in #dataviz but this made me change my mind: it's all about the way it relates to our perception and experience of the world around us.” – <a href=""https://twitter.com/pciuccarelli/status/1201891442865520640"">Paolo Ciuccarelli, prof at NortheasternCAMD</a>  </ul>",https://www.nytimes.com/interactive/2019/12/02/climate/air-pollution-compare-ar-ul.html,,,,,,,"Nadja Popovich, Blacki Migliozzi, Karthik Patanjali, Anjali Singhvi and Jon Huang","  Nadja Popovich  is a reporter and graphics editor on The New York Times’ Climate Team. She covers climate science, energy policy and the real-world impacts of our warming world. She is particularly interested in using interactivity and personalization to help readers more viscerally relate to the effects of climate change through data.    Blacki Migliozzi  is an award winning data journalist and storyteller. As a graphics editor at The New York Times he focuses on developing data-driven climate stories.    Karthik Patanjali  is a designer, researcher, technologist and award- winning storyteller. He is a part of The New York Times’ Immersive Team, where he explores storytelling through immersive platforms, including AR, VR and 3D Web.    Anjali Singhvi  is a reporter and graphics editor at The New York Times. She is a trained architect and holds a masters degree in Urban Planning and Urban Analytics from Columbia University. She joined The Times in 2016.    Jon Huang  is a graphics editor at The New York Times with a background in computer science and photojournalism. He designs and builds interactive presentations. ",,,
Myanmar,The Myanmar Times,Small,Participant,Best data-driven reporting (small and large newsrooms),Myanmar's economy - a kitten among tigers,11/08/19,"Explainer,Open data,Infographics,Chart,Business,Economy","Animation,Scraping,Google Sheets,CSV","  This project is a feature story about Myanmar Economy that was compared by ASEAN countries. Myanmar is one of the lowest developing countries among the regional. This story has focused on the weak points of Myanmar macroeconomy like export, deficit, tax revenue, expenditure, foreign reserve, growth rate, inflation and some others. Almost in all of the indicators, Myanmar is the lowest. Therefore, I wrote the story which lets the decision makers in Myanmar know about the country's current economic situation among the regional. And then what is our weakness and how to prepare for it.  ","  This story has been published in print and online version in both English and Myanmar language. It has been shared 9.1 k on the web for English version and 3.6 k for Myanmar version. The Myanmar Times newspaper is one of the best media in Myanmar and it has been distributed all through the country and also has a lot of audiences including government policy makers, members of parliament, civil society, NGO, INGO and think tank organizations. Currently, the Myanmar Times has 1.7 million readers on the web and 4.6 million on Facebook page. This is the most-viewed stories in the Myanmar Times in 2019. And policy makers, members of Parliament, civil society and other organizations are aware of what the economic situation is.     I just thought this is the work of a business journalist. On the other hand, I have been looking forward to how to prepare for economic development and how to decide their policy and how to do the implementation process.  ","  I have used Google sheet, flourish and data wrapper tools for the story. I have learned how to use these tools from the training of Phandeeyar organization supported by World bank.     Firstly, I thought about what story I should write and how to focus my story, what data I should show and then how can I search for data I need and where can I get.     After thinking about it, I started my project. I searched for the data from a lot of organizations relatives with my project like World bank, Asia development bank, IMF and government ministries websites.     I got the raw data from those websites but I did not get some of the update data that I want.. And then, I put those raw data on the Google drive and I did the data cleaning process on the Google sheet. Finally, I got the data I need but I was not satisfied because I didn't get the update data for some facts.     When my data was finalized, I chose some charts for presentation. I have used the flourish tool for the interactive charts. I put those charts in my story.  ","  The hardest part of this project is the data searching process and data cleaning process. For data searching process, when I need cleaned data, I can not get that although I try to get from a lot of reliable website databases. Sometimes, they are not perfect because I can only get some of the countries data.. Even I can get those countries data, they are not updated. So it was a bit difficult for me to use those data in the charts because I need a standardized data..     The other hardest part was the data cleaning process. It was very difficult for me because most of the data are raw files.  "," Since data journalim is the initiative in Myanmar, most of the journalists don't know how to search the data, clean the data, analyse the data and visualize the data. From this project, they can see how the simple data in my story can be transformed into those interactive charts. Users can also see the insights easily through the visualizations. ",https://www.mmtimes.com/news/myanmars-economy-kitten-among-tigers.html,,,,,,,"Thit Nay Moe & Aung Kyaw Tun, The Myanmar Times, Phandeeyar"," I am working at the Myanmar Times Journal as a business editor. I have been working in journalism field for 11 years. I have written a lot of news and feature stories especially in business and economy. When I have worked four years as general reporter,  I became a business journalist, and then I became a business editor.  ",,,
Spain,Maldita.es,Small,Participant,Best data-driven reporting (small and large newsrooms),Maldito Dato,30/05/19,"Explainer,Database,Open data,Fact-checking,Elections,Politics,Corruption","Canvas,Adobe,Microsoft Excel,Google Sheets,CSV,R,PostgreSQL,Python","  Maldito Dato is a project of the non-profit Spanish media outlet Maldita.es. In Maldito Dato we belittle the statements of politicians with research through requests for public information and data journalism methodology. During the last year Maldito Dato had published more than 400 pieces and all of them used public information, data systematization and analysis and traditional reporting techniques. This seven pieces show how our work method can be used in different topics with simple and clear language for all kinds of publics.    ","  The content created by Maldito Dato had 1.065.000 unique visitors and 2.152.600 views in our web page in 2019. During the year our material had been republised or quoted in almost 100 media outlets in TV, radio and digital. Some of the media outlets that did it are Telemadrid, El Confidencial, El Español, El Mundo, Cadena Ser, eldiario.es, ABC and Onda Madrid, among others.          We also had impact in the application of transparency laws. For example, one of the pieces we are sending titled “Televisión Española paga 480.000 euros a Shine Iberia por cada programa de MasterChef Celebrity y 490.000 por los Junior” (Spanish Televisión pay 480.000 euros to Shine Iberia for every show of MasterChef Celebrity and 490.00 for the Junior), was a long legal process to get the information that allow us to discover, for the first time in Spain, how many money the public television spend in external tv show. Or in the case of the publication “El 44% de los bares y restaurantes madrileños inspeccionados en 2019 presentó problemas de higiene: consulta qué incumplieron y sus inspecciones” (The 44% of the pubs and restaurants inspected in Madrid in 2019 had hygiene problems) was the first time that the local administration of Madrid delivered this data to public, and now is a tool for social audit.   ","  We have our working methodology open and public in our website. This methodology defines how we choose the statements of the politicians that we verify based on the viralization and the affirmations that contain facts or statistics. We use fact-checking tools to verify it at first, then we consult public sources through the information we get with FOIA requests. We usually use R, Python, SQL and spreadsheets for databases and analysis, Flourish for data visualizations and we also have a system, developed internally, to organize and follow the process of FOIA requests.   ",  The most difficult part of creating and maintaining Maldito Dato is the permanent monitoring of all political sources in the country and the systematization and creation and maintenance of updated databases to refute politicians and provide valuable information to citizens.  ,"  Others can learn from this project that data journalism, fact-checking and transparency can be use to dismantle the politicians lies. Maldito Dato is the proof  that with little resources and a little team you can do journalism using the best methodology and techniques and become a public service.      Other projects also can learn that data journalism should not be all the time about big projects and complicated visualizations, but rather be a public service with simple languages to reach even more public.    ",https://maldita.es/malditodato/,https://maldita.es/malditodato/psoe-y-podemos-ganan-en-las-zonas-mas-pobres-de-espana-mientras-pp-y-vox-lo-hacen-en-las-mas-ricas/,https://maldita.es/malditodato/34-984-bienes-inmatriculados-por-la-iglesia-desde-1998-18-500-templos-y-mas-de-15-000-fincas/,https://maldita.es/malditodato/el-44-de-los-bares-y-restaurantes-madrilenos-inspeccionados-en-2019-tienen-problemas-de-higiene/,https://maldita.es/malditodato/el-documento-de-las-peticiones-de-unidas-podemos-y-el-de-las-propuestas-del-psoe-salieron-de-vicepresidencia-del-gobierno-segun-los-metadatos/,https://maldita.es/malditodato/por-primera-vez-conocemos-realmente-cuantos-votos-de-espanoles-en-el-extranjero-no-fueron-contabilizados-en-en-las-elecciones-generales-de-2019-mas-de-16-000-votos/,https://maldita.es/malditodato/television-espanola-paga-480-000-euros-a-shine-iberia-por-cada-programa-de-masterchef-celebrity-y-490-000-por-los-junior/,"Ignacio Calle, Sergio Sangiao, José Molina, Adela Vived, Rubén Díaz, Julio Montes, Clara Jiménez Cruz","  Ignacio Calle (1986) is specialized in investigative and data journalism and the coordinator of Maldito Dato. He used to work at the data team of LaSexta. He was member of the ICIJ teams in Falciani Leaks, Bahama Leaks and Panama Papers, that received the Pulitzer Prize in 2017. He is also founder of the Investigative Journalists Association in Spain.      Jose Molina (1993) is a data journalist. Graduated at computer engineering and specialized investigative journalism,  he knows programming for data science, data acquisition, analysis and visualization. He also loves investigative work and try to apply computational solutions to it.      Adela Vived (1994) is specialized in investigative and data journalism and visualization. She used to work in La Vanguardia newspaper and now she work in Maldito Dato as a reporter in charge of tell stories with gender perspective.      Sergio Sangiao (1995) is specialized in investigative and data journalism and visualization. He used to work in Servimedia and now in Maldito Dato he is in charge of the political topics, speech fact-checking and transparency.      Rubén Díaz (1990) used to work in corporate communication now he is part of Maldito Dato Team specialized in election and political analysis.      Clara Jiménez Díaz (1989)  is the cofounder and project manager of Maldita.es. She used to work in Intereconomía and LaSexta. She is member of the board of the International Fact-Checking Network.     Julio Montes (1983) is the cofounder and editor in chief of Maldita.es. He used to work in LaSexta and now he manage the Maldita.es team.      ",,,
Kyrgyzstan,Kloop Media - Kloop.kg,Small,Participant,Best data-driven reporting (small and large newsrooms),Be patient. How domestic violence kills children and cripples fate,26/12/19,"Long-form,Open data,Illustration,Crime,Human rights",Animation,"     We have compiled a database of two hundred articles from the Kyrgyz media over the past five years about rape, beatings and murders of children. And they tried to understand what unites these cases and why there is violence against children. ","<pre> The project did not resonate, but I think that it can change the views of readers and move them to action.</pre>","<pre> statistical analysis, visualization of analytical data</pre>","      <pre> We have collected 200 articles about beatings, murders, rape of children from news sites for 5 years. And they analyzed who the violence is coming from. They showed on the basis of statistics and studies that child abuse is increasing and that children suffer mainly from domestic violence. When violence occurs against young children, rapists in most cases are not responsible.</pre>",<pre> You can collect data yourself when they are not available in open sources. And comparing with open data to prove a hypothesis.</pre>,https://kloop.kg/blog/2019/12/26/terpi-kak-domashnee-nasilie-ubivaet-detej-i-kalechit-sudby/,https://docs.google.com/spreadsheets/d/1jrVQwCXBkuWzS46N51EN_lkJXOweSC2zzGqFrqz8MWs/edit#gid=1549156074,,,,,,"Aidai Bedelbek Kyzy, Saadat Tologonova, Anastasia Valeeva, Cholpon Uzakbaeva, Myrzayym Zhanybek Kyzy, Alexandra Titova"," Saadat Tologonova - coordinator of journalism school Kloop.kg, freelancer, Aidai Bedelbek Kyzy - freelancer on site Kloop.kg,  Anastasia Valeeva - editor, employee of Kyrgyzstan Data School, Cholpon Uzakbaeva - editor, employee of Kyrgyzstan Data School, Myrzayym Zhanybek Kyzy - editor, Alexandra Titova - illustrator, video team producer ",,,
United Kingdom,"The Bureau of Investigative Journalism, members of the Bureau Local Network",Small,Shortlist,Innovation (small and large newsrooms),Locked Out,10/04/19,"Investigation,Multiple-newsroom collaboration,Database,Open data","Scraping,Microsoft Excel,Google Sheets,Python"," Our #LockedOut investigation revealed just how hard it was to find an affordable property to rent while on housing benefits. We dug into data from across Great Britain to show, not only the state of affairs nationally, but also the local, granular picture. This collaborative project drew in journalists from across the country to tell an important story about the housing crisis and a leading cause of our spiralling homelessness epidemic. "," The work was welcomed by campaigners and lawyers across the country, with some officials saying it would help them create local policy. We heard of at least one legal case where lawyers used our data to prove their client and his family were unable to access an affordable home.       The work also prompted the UN Special Rapporteur on Housing to speak out on the issue and, inspired by our work she said she planned to look into affordability as an issue elsewhere.        Three months after publication the government announced an increase in the rate of Local Housing Allowance (housing benefit), though we have since shown this increase does little to improve things. "," We captured the details of 62,695 two-bed properties advertised on 15/09/2019, using the Nestoria API (which aggregates property data from online property listing sites.)       We used the latitude and longitude coordinates of each property found through the API to localise them within the right Broad Rental Market Area (BRMA) using shapefiles obtained from the Valuation Office Agency. We then analysed the data to ascertain the total number of affordable properties in each BRMA, as well as the increase in LHA that would be necessary to make the 30th percentile affordable.       We also knew that refusal to let to those on benefits makes the shortage even worse. Reporters contacted the landlords of 180 two-bed properties posing as a single mother and asked whether the landlord accepted people on benefits.       We also went further to ensure wide accessibility to our findings, building an interactive online tool which allowed anyone to find out their local situation.      As the housing market is ever evolving, it was clear that we needed to ensure others could reproduce our investigation later, so we published the code we used to collect and analyse the data in <a href=""https://github.com/bureau-local/lha-investigation"">a Github repository</a>. "," We knew this was an investigation with regional and local nuance, and so it was imperative that the findings were shared as far and wide as possible.       We worked on this story with our Bureau Local network. We wrote a public-facing Reporting Recipe and opened all relevant data at BRMA level (which is not publicly available) as well as a list of all the affordable properties. As well as publishing the story with HuffPost UK, our local reporter network produced more than 20 stories detailing the situation in their area.       We also held StoryCircles across the country - presenting our data and findings to people with real-life experience in physical settings. "," This project combined innovative data gathering and mapping techniques, paired with a real focus on making the findings accessible and useful to as many people as possible. This collaborative approach to data-journalism is breaking ground in the UK. The Bureau Local project (part of the Bureau of Investigative Journalism) has developed a clear framework for how these investigations can be done: we publish Reporting Recipes to help even someone with no journalistic experience dig into the story, we open up our data with clear guidance on how to read it, we have published the code we used so others can re-produce the investigation at a later date and we have even produced a guide to help people should they want to replicate our model for themselves. ",https://www.thebureauinvestigates.com/stories/2019-10-04/locked-out-how-britain-keeps-people-homeless,https://www.thebureauinvestigates.com/blog/2019-11-25/locked-out-stories-from-across-the-uk-expose-holes-in-the-housing-safety-net,https://www.thebureauinvestigates.com/blog/2020-01-10/taking-a-story-full-circle,https://github.com/bureau-local/lha-investigation,https://docs.google.com/document/d/1ydf6xjYjCQtHPCefNGRZyQEmAG23UOJpCoyIk0UL6HA/edit?usp=sharing,,,"Maeve McClenaghan, Charles Boutaud, Tom Blount, Charlotte Maher"," Maeve works on our Bureau Local team where she has led nationwide, collaborative investigations on issues including cuts to domestic violence refuges, politicians use of Facebook ""dark ads"" and homeless deaths. Maeve has previously produced investigations for BBC radio, the Guardian, Greenpeace and Buzzfeed UK. She has won numerous awards for her journalism and has been nominated three times for the Orwell Prize. In June 2019 she was named a Bertha fellow and currently writes about housing issues.   Charles is a developer-journalist who has experience investigating data for stories using computational method. He won a Canadian Online Publishing Award for his work on public transport data in Montréal with the Huffington Post, Québec.   Tom Blount is a research fellow at the University of Southampton, involved in research projects at a national and international level. He has a PhD in Computer Science, and has written papers on human-data interaction, game design, and (anti-)social argumentation.    Charlotte Maher joined the Bureau as a fellow in 2019. She has previously written for the London Evening Standard. Charlotte has a bachelor's degree in English from the University of Nottingham and an MA in Investigative Journalism from City, University of London. ",,,
United Kingdom,"The Bureau of Investigative Journalism, members of the Bureau Local network",Small,Shortlist,Best data-driven reporting (small and large newsrooms),Dying Homeless,03/11/19,"Investigation,Solutions journalism,Long-form,Cross-border,Multiple-newsroom collaboration,Database,Open data,Fact-checking,OSINT,Crowdsourcing,Human rights","Scraping,Microsoft Excel,Google Sheets,Python"," Dying Homeless project - a year long, collaborative investigation - revealed for the first time, the scale of homeless deaths across the UK.   After discovering that no official body held data on how and when people were dying homeless we started counting, aware that this deficit of information meant no-one knew how wide-scale this issue was or whether lessons could be learnt to prevent deaths in the future.   We revealed that at least 800 people died (October 2017 - April 2019). We logged the names and stories of hundreds of people who had lost their lives. "," The findings set the news agenda, provoked debates in parliament, and prompted Ministers to promise to do more to address the issue. The Secretary of State for Housing called our findings “utterly shocking” and promised case reviews into deaths.        Following our revelations about the lack of reviews into homeless deaths, several councils, including Brighton & Hove and Leeds have said they will undertake their own reviews into deaths in their area. The government’s Rough Sleeper Strategy recommended, for the first time, that deaths be reviewed.        We started our series using #makethemcount: a call for an official body to log these deaths. In December, prompted by our work, the Office for National Statistics produced the first ever official data on homeless deaths. This came after we shared our data with the ONS to help them develop their methodology. The Scottish National Records Office has now followed suit and will soon publish its first ever data on homeless deaths. The  Northern Irish equivalent has said it is exploring producing its own data now too.        We presented the findings to the APPG on Homelessness and the figures have been cited in parliament several times. Prompted by our network of reporters, at least 17 MPs and councillors have spoken out on the issue.       The work was also received positively by those working with and for homeless people. The Big Issue said: “To its huge credit, the Bureau’s research extensively looked into the stories of every one of those deaths… Putting faces and stories to numbers humanised the figure”.       Across the country, members of the public came together to remember those that passed away. There were protests about homeless deaths in Belfast, Birmingham, London and Manchester, memorial services in Brighton, Oxford, Luton and London, and physical markers erected in Long Eaton and Northampton "," For a year, we attended funerals and inquests, interviewed family members, collected coroner’s reports, shadowed homeless outreach teams and compiled Freedom of Information requests - all in an attempt to get to the true picture.       We worked out a careful methodology, after consulting various expert groups, and then created a first-of-its-kind database where we logged the details of each death.       This database was used to build a powerful visualisation, which allowed readers to read more about each person, as well as understanding the scale of the tragedy.       Using scraping, deep-web searches and FOI we were then able to reveal that, of 83 of the recent deaths in our database, not a single official review had been launched to explore the circumstances of the deaths, despite experts arguing that a review would be best practice       All our findings were opened up to those in our Bureau Local network, a group of local journalists across the country. More than 100 local news stories and 80 national stories were produced. "," Each death had to be fact-checked and verified, which was no easy task. We were also very aware of the need to report sensitively and respectfully. Indeed, it took many months to gain trust within the homelessness sector, but by the end of the project we had key sources across the country who were willing and able to share information as they got it.       As we drew our project to a close in April 2019, we were aware that we did not want the work to simply stop overnight. The Office for National Statistics was now producing official data but these deaths were about more than numbers, they were people. So we worked with the Museum of Homelessness and handed over the visualisation and commemorative project to them, helping them develop their methodology and website so that they could continue to tell the stories of those that pass away while homeless. "," This project was the perfect example of journalists not being thwarted by the fact there was no official data to go on. Rather than give up when we found out no-one was counting homeless deaths, we were spurred on address this evidentiary void. By working collaboratively with a network of journalists and bloggers across the country, we were able to crowd-source much of the information.       And, by working with official statisticians, and opening up our database to them, we were able to achieve real impact. ",https://www.thebureauinvestigates.com/stories/2019-03-11/homelessness-kills,https://www.thebureauinvestigates.com/stories/2018-04-23/dying-homeless,https://www.thebureauinvestigates.com/blog/2019-05-21/moving-on-with-respect-dying-homeless,https://docs.google.com/document/d/12UHuPn9-UfVEPd2i4qkGsTr7jNtqxTIyUEuQdtiQh30/edit?usp=sharing,,,,"Maeve McClenaghan, Charles Boutaud"," Maeve works on our Bureau Local team where she has led nationwide, collaborative investigations on issues including cuts to domestic violence refuges, politicians use of Facebook ""dark ads"" and homeless deaths. Maeve has previously produced investigations for BBC radio, the Guardian, Greenpeace and Buzzfeed UK. She has won numerous awards for her journalism and has been nominated three times for the Orwell Prize. In June 2019 she was named a Bertha fellow and currently writes about housing issues.   Charles is a developer-journalist who has experience investigating data for stories using computational method. He won a Canadian Online Publishing Award for his work on public transport data in Montréal with the Huffington Post, Québec. ",,,
United States,MIT Technology Review,Small,Shortlist,Best visualization (small and large newsrooms),Can you make AI fairer than a judge? Play our courtroom algorithm game,17/10/19,"Investigation,Explainer,Long-form,Quiz/game,Open data,Infographics,Chart,Crime","D3.js,JQuery,Json,CSV,Python,Node.js"," This interactive narrative shows why decision-making algorithms can’t be completely fair when operating on data from an unfair world. It visualizes real data used in an algorithmic system to predict whether an individual will be re-arrested. It then challenges readers to adjust the algorithm to make those predictions fairer. But after every adjustment, a new notion of fairness is revealed requiring more adjustment. This build up leads to the punchline: these notions, in practice, can never be satisfied all at once. While the story uses criminal justice data, it’s lesson holds true across industries, including in hiring, healthcare, and lending. "," Our exploration of algorithmic bias, while grounded in an example from the criminal justice system, is widely applicable across industries and fields that are increasingly adopting automated decision-making systems. We received feedback from readers across many fields—criminal justice, law, AI, social sciences, and healthcare—saying that our piece has become an important reference and/or teaching tool for understanding the impact and consequences of this phenomenon. Overall our piece has received over 80,000 views and been publicly commended by leading experts and institutions like <a href=""https://twitter.com/AINowInstitute/status/1184887683472154624"">AI Now</a>, <a href=""https://twitter.com/azeem/status/1184809241187618817"">Azeem Azhar</a>, <a href=""https://twitter.com/JanelleCShane/status/1184877632027951104"">Janelle Shane</a>, and <a href=""https://twitter.com/marylgray/status/1184803065926275073"">Mary Gray</a>. It has also been featured on <a href=""https://radio.wosu.org/post/tech-tuesday-ai-criminal-justice#stream/0"">radio</a> and <a href=""https://www.experian.com/blogs/news/datatalk/ai-judges-court/"">podcasts</a> and become assigned reading in university courses, including at <a href=""https://web.stanford.edu/class/cs182/"">Stanford University</a>.   While it’s difficult to track impact beyond these metrics, its wide reach shows how relevant and salient this topic is. As we’re seeing now, governments around the world have begun to actively draft legislation to address such bias. The work of researchers and journalists like us plays a critical role in helping policymakers understand how to regulate this issue. In the meantime, it’s also important to have a robust public conversation that unveils and elevates the weaknesses of algorithmic systems to the people affected by them so they can push back. ","  Data processing:  The data we were using was already publicly available <a href=""https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm"">through ProPublica</a> in .cvs form. We downloaded it and filtered it down to the relevant dimensions in Python, then reduced it by randomly sampling 500 rows.    Prototyping:  We began by sketching different visualization ideas on a whiteboard, then using Observable notebooks for rapid prototyping in Javascript and d3. With each prototype, we then conducted in-person user tests, where we would ask readers to read the story, then talk out loud about their thought process during the interactive sections. Based on their explicit and implicit feedback (stumbles, confusions, and otherwise), we’d develop new whiteboard sketches and improve our prototype. This cycle happened three times.    Production:  Once we finished prototyping, we transitioned to our own local development environments, and used Github to collaborate on code and Github Pages to deploy. We first got things working on desktop, then quickly optimized for mobile. Then again, we conducted user tests in the real article-reading environment on and refined the interaction details until it was a completely seamless experience. Finally we did more comprehensive quality-assurance tests by testing across different browsers and devices. "," The core of this project was translating obscure yet consequential statistical concepts into an approachable and engaging visualization—something that people would easily understand and that would compel them to care. All of this was hard, but the toughest part was making our data and information-dense visual as clean and simple as possible. We wanted to eliminate any cognitive burden, so people wouldn’t even realize the underlying complexity.   This required us to first deeply understand the material in order to distill it. We began by interviewing many experts in statistics, algorithm design, law, and criminal justice, to ensure that we understood the full pipeline from the technical statistical concepts to their impacts on the ground.   It also required many iterations of both visual and interaction design, as also detailed in the answer about tools and techniques. We conducted many rounds of user testing with people who were unfamiliar with the topic. We also had to overcome the design challenge of making the graphics and interactions work seamlessly on both desktop and mobile. The result is a unique interactive narrative that has quickly become a standard reference on the interplay between different conceptions of fairness, and why no algorithm can satisfy all of them.    "," This project offers lessons on both the content and process. From a content perspective, decision-making algorithms have completely permeated our lives. As many are struggling to understand and communicate what this means, including citizens, activists, and policymakers, our interactive narrative offers an authoritative, comprehensive, and clear answer.   From a process perspective, this project was a highly collaborative and iterative. Jonathan and Karen worked on everything together: we conducted our interviews together, pair-programmed together, and wrote the narrative in a tight feedback loop of communication. We also approached the story like a product, using prototyping, user testing, and quality assurance techniques to refine every element of the story. Both these facts undeniably strengthened the story and can offer a model for future collaborators.    ",https://www.technologyreview.com/s/613508/ai-fairer-than-judge-criminal-risk-assessment-algorithm/,,,,,,,Karen Hao and Jonathan Stray,"  Karen Hao  is the senior artificial intelligence reporter at MIT Technology Review. She covers the field’s cutting edge research and the technology’s ethics and social impacts. She also writes a weekly newsletter called <a href=""https://forms.technologyreview.com/the-algorithm/"">The Algorithm</a>, which was honored as one of the best newsletters on the internet by the 2018 Webby Awards. Prior to TR, she worked as a tech reporter and data scientist at Quartz, and an application engineer at the first startup to spin out of Google[x].    Jonathan Stray  is a research fellow at the Partnership on AI. He previously taught the dual masters degree in computer science and journalism Columbia university and built software for investigative journalism. He has worked as an editor at the Associated Press, a reporter in Hong Kong, and a research scientist at Adobe Systems. ",,,
Sri Lanka,Verité Research,Big,Participant,Open data,Budget Promises - Beyond Parliament,06/07/19,"Investigation,Explainer,Database,Open data,Infographics,Chart,Video,Agriculture,Economy","Animation,Microsoft Excel"," BudgetPromises.Org is an online platform launched in 2017 that tracks implementation of selected budget proposals in Sri Lanka. The platform addresses two questions on government's national budget:     Is the government doing what it is saying?   Is the government saying what it is doing?      As such, the platform tracks the level of progress in implementation of budget proposals as well as the openness of the government in sharing information relating to the implementation of budget proposals.   The platform is run by <a href=""https://www.veriteresearch.org/"">Verité Research</a>, an independent think tank in Colombo, Sri Lanka.     "," The key impact of BudgetPromises.Org has been changing the conversation about the government budget from ‘promise-making’ to ‘promise-keeping’. Since its inception, there was a notable shift in government discourse from making promises to keeping promises. Usually, discussions in parliament as well as the media during the budget speech focus exclusively on costs and benefits of new proposals. However, because of the influence of our platform, these discussions shifted to also focus on the importance of having systems in place to monitor the implementation of promises. These discussions also started to shift to the need for budgets to be formulated better.   In the mid-year assessment of the 2018 budget, we succeeded in changing the discourse from promise-making to promise-keeping. We focused on making government institutions accountable. In the most recent phase, the end-year assessment, we took the next step to highlight the importance of transparency—that is the importance of government being willing to proactively share information about the status of budget implementation with the public. The platform’s major impact lies in its ability to sustain the conversation about accountability and governance around budget issues in an environment where the media only talks about the budget when it is discussed by a politician.     We also observed a positive change in how the government monitors its own spending. In March 2019, <a href=""http://www.ft.lk/top-story/Six-committees-for-Budget-implementation--Mangala/26-674665"">the Minister of Finance announced that six committees would be appointed</a> to monitor the implementation of budget proposals. Like many statements made by Sri Lankan politicians, the minister’s promise might not actually amount to a change in practice any time soon. However, in the Sri Lankan context, even the fact that the Minister feels obliged to reassure the public that the government will be engaging in better monitoring of the budget is a win for democracy. "," The tools and techniques we used for this initiative are two fold:    Research tools    The research tools used to obtain the necessary data to run the platform are 1) government websites, 2) Over-the-counter (OTC) checks, and 3) Right to Information (RTI) requests. Web-checks were done at regular intervals to ensure no information was missed. OTC involved calling/visiting government agencies and directly requesting the data that we want. When the first two methods did not work, the team used RTI requests to collect data. In some cases, when the request did not give any results, we had to submit an appeal. Government ministries were classified on our platform according to their <a href=""http://www.budgetpromises.org/en/promise-tracker-2018-2/2018/2"">willingness to provide information.</a>     Communication/dissemination tools:     Once the platform was updated with the relevant information, we used both mainstream media and social media to disseminate our findings. To target the mainstream media, we held press conferences, did TV interviews, and submitted articles to newspapers. For social media, we utilised audio visual tools such as <a href=""https://www.youtube.com/watch?v=XBt6hL7nBg4"">videos</a> and <a href=""https://twitter.com/AzzamAmeen/status/989500603138113536"">infographics</a>. The team ensured that all findings were accessible to the public in short, bite-size formats, to reach maximum impact.      "," The government of Sri Lanka does not have open data policies. As a result, it is very difficult to obtain information about how different ministries spend their money. Hence the hardest part of a project like this is data collection. Some key challenges encountered by our team are:   1.     Difficulty in accessing information from ministries: We utilize Right to Information (RTI) requests to obtain relevant information about spending from different ministries. This process is not always smooth, and our team encountered difficulties such as 1) timelines and processes for RTI filing were not followed by officials at government agencies; and 2) it was often difficult to identify a single point of contact within governmental institutions with whom to file RTIs.   2.     Lack of compatible information: Even when relevant information is available, it is not always in a useful format. We found that there was a lack of compatibility between action plans and progress reports. This gap meant that it was very challenging for our team to assess progress on budget promises.   3.     Web checks – The team scans through government websites to look for budget information. However, most websites are not updated, and sometimes information can go missing. Further, there isn’t a standard format in how they report progress, and sometimes it is found in unrelated pages.   Due to these reasons, obtaining information was and continues to be one our biggest challenges ","The main lessons others can learn from this project are: Public engagement using data is a powerful form of accountability - While we sometimes communicate with policy makers directly, most of the impact we have achieved through BudgetPromises.Org is through the public communication of our findings. We use the media and social media to take our findings to the public, who in turn become more knowledgeable about what the government is doing with their money. When they have this information, they can hold the government accountable to the promises they make each year. Further, media pressure imposes reputation costs on politicians, who are forced to respond when they are accused of being inactive. Push for access to information as a right - The Right to Information Act (RTI) was passed in Sri Lanka in 2016. Under the act, a citizen can obtain public information by a) filing requests from public authorities; and b) obtaining information through proactive disclosure of information by public authorities. Sometimes public authorities aren't willing to share information th RTI requests, or the information received is inadequate. Further, the government remains weak in proactively providing information to the public. However, BudgetPromises.Org has continued to champion RTI as a research tool, and even tells the public when information is not accessible, so that they can also hold the government accountable for providing information that they as tax payers have a right to. When you can't find what you need, that is also an important finding - While we push for the government's implementation of budget promises, we also expose the lack of information, that should be available by law. An integral part of the platform is scoring the openness of ministries. We encourage the media to talk about this aspect as much as it focuses on implementation of",http://www.budgetpromises.org/en,https://economynext.com/only-8-pct-of-sri-lanka-govt-budget-promises-on-track-study-finds-11866/,http://www.ft.lk/top-story/Budget-2018-mired-on-implementation/26-664906,https://www.youtube.com/watch?v=PkuP2Kp93mo,https://www.youtube.com/watch?v=h3DgZhSMpqM&feature=emb_title,https://economynext.com/sri-lanka-parliament-committee-slams-failures-in-public-finance-management-13588/,https://www.veriteresearch.org/wp-content/uploads/2018/05/VR-Insight_The-new-minsters-of-finance-will-their-budget-keep-its-promises_.pdf,"Nishan de Mel, Subhashini Abeysinghe, Hasna Munas, Faiza Chathoor, Nilangika Fernando, Vidhya Nathaniel, Lahiri Jayasinghe, Chalani Ranwala","  Nishan de Mel  is the Executive Director of Verité Research. He is an economist with extensive academic, policy and private sector experience. In the 1990s, he sat on multiple Presidential Task Forces in Sri Lanka, playing an instrumental role in designing national policies on Health Sector Reform, Social Security and Tobacco & Alcohol. Nishan provides overall supervision and guidence for BudgetPromises.Org.    Subhashini Abeysinghe  is a Research Director at Verité Research. She is an economist specialising in trade. With a decade of experience at the Ceylon Chamber of Commerce’s Economic Intelligence Unit, she has an in-depth understanding of Sri Lanka’s economy, especially its international trade. Subhashini functioned as the research lead for BudgetPromises.Org.    Hasna Munas  is a Research Analyst who has worked on a range of assignments covering trade facilitation, Sri Lanka’s political and macro economy and youth employment and education. She was one of the key members in the team that developed the methodology for the platform.    Faiza Chathoor  is a Research Manager and has workd extensively on developing and maintaining BudgetPromises.Org and has also contributed to the dissemination of key findings. She is one of they key people involved in cordinateing with the platform's web partner and ensuring the website is upto date at all times.     Nilangika Fernando  was a key member of the team that developed the methodolocy of the platform. She also lead the design and visualization of the platform, including the creation of infographics. Nilangika was also invoved in presenting findings to the the media.     Vidhya Nathaniel  was the team leader of the Economics Team when BudgetPromises.Org was launched. She provided leadership and guidance for the development of the platform. She was also involved in guiding the development of the methodology used.    Lahiri Jayasinghe  is an Assistant Analyst who played an important role in the collection and analysis of data for BudgetPromises.Org. She was a member of the core team that developed the methodology.    Chalani Ranwala  heads the Communications Team at Verité Research and assisted in all dissemination related to BudgetPromises.Org since its inception. She assists the team to reach out to the media, general public and other stakeholders and helps them frame the key messages for dissemination.  ",,,
Germany,Deutsche Welle,Big,Participant,Best data-driven reporting (small and large newsrooms),"EU breaks promise of safe passage for 50,000 refugees",14/10/19,"Investigation,Podcast/radio,Chart,Politics,Immigration,Human rights","Creative Suite,Google Sheets","When European Union countries promised to take 50,000 refugees in a new resettlement program, it made headlines around the world. Two years later, as the deadline approached, everyone had forgotten about it. Meanwhile, refugee agencies trying to get people out of barbaric detention camps in Libya were complaining that the promised resettlement opportunities were slow in forthcoming - prolonging the suffering of desperate people. Enquiries to the European Commission revealed that the program was way behind. We collected data from 20 countries to produce an accountability piece revealing who had failed to deliver otheir pledges to help people in desperate"," The story was one of the most-read articles of the week on the DW English website.   The story was also told in TV form, in a segment on DW's English-language news bulletins including The Day, the evening program which is rebroadcast by PBS and other partners around the world.   On radio, the story was reported in DW's AfricaLink programme, which is broadcast on FM partner stations in Cameroon, Ghana, Kenya, Liberia, Malawi, Namibia, Nigeria, Sierra Leone, South Sudan, Uganda and Zambia, as well as online via Facebook and podcast platforms.   The story was followed up by international media outlets including the Jerusalem Post, as well as being syndicated by DW's partner media.   The story was widely shared on Twitter and Facebook, including by experts in the field. "," The story was mainly an exercise in data gathering, and then reporting and investigating that data to reveal an important finding. Therefore, the data analysis was performed entirely in Google Spreadsheets. We also used Adobe Illustrator to produce charts for publication. "," Using freedom of information laws and normal journalistic requests is a great way to obtain data - as long as you know what to ask for. This project started as a result of criticism of the EU reported by migration researchers, who said that slow progress on resettlement was holding up the evacuation of people trapped in detention camps in Libya.   The data collection exercise was more complicated as it involved interacting with government agencies in over 20 different jurisdictions and languages. Perseverance yielded a dataset that covered all of the main countries involved. "," Politicians rarely put numbers on their commitments. So when they do, it is important that data journalists hold them to account for their promises. ",https://www.dw.com/en/eu-breaks-promise-of-safe-passage-for-50000-refugees/a-50803664,,,,,,,Tom Wills," Tom Wills is a freelance journalist based in Berlin, Germany. He has been working part-time on the DW Data team since January 2019.   Before going freelance, Tom worked for national newspapers and magazines in the UK for five years.   He uses languages and tools like Python, PostgreSQL, PostGIS, bash and Git to find or create data, analyse it, and turn the results into news stories. Some people call this approach ‘computer-assisted reporting’ or ‘data journalism’. ",,,
United Kingdom,The Telegraph,Big,Participant,Best visualization (small and large newsrooms),Telegraph General Election 2019 results,12/12/19,"Breaking news,Infographics,Elections,Politics","Personalisation,Json,Google Sheets,R,Node.js"," The Telegraph's top-performing coverage during the 2019 UK General Election all had data journalism at the heart of it. Our interactive results forecaster was more accurate than the official exit poll, and our tactical voting tool, the only one by a major publisher to feature a guide for both Remain and Leave supporters, was our second most prolific subscription-driving piece of content ever. The data team's decision to make a live Brexit-focused graphic one of the centrepieces of our results page paid off with record audience and engagement figures. "," The Telegraph’s live 2019 General Election results page had the reader at the heart of it. We were the only UK publication to have a Brexit-focused graphic live-updating on our results page, with our histogram of Brexit vote versus party gains proving essential in telling anyone the story. Brexit proved the driver of the swings seen in the election, and our gamble with making a live Brexit-focused graphic one of our centrepieces paid off. It meant that we were able to provide visual-led updates of the stories of the election (e.g. <a href=""https://twitter.com/Ashley_J_Kirk/status/1205325253800529927"">https://twitter.com/Ashley_J_Kirk/status/1205325253800529927</a>) without having to due any heavy-lifting in ggplot or Illustrator. It also helped with a high reader engagement time, which in turn meant that the page led Google more than any other news publisher and attracted record page view figures from our core subscriber audience   Our data-driven stories for the election were among the most-subscribed pieces for The Telegraph this year. When it came to the results analysis, our Data Journalism team of three was aided due to the fact that we had an innovative toolkit of live-updating, diverse and story-driven results graphics. These graphics went beyond the standard live bar chart or map which show seat totals; we took a risk on understanding what the stories of the night were likely to be, as well as audience’s needs, and created the graphics based on this.   Our results page contained nine live-updating graphics, and each of these was a component that could be embedded in any other article. These individual embeds - called Particles - were then embedded in lots of pieces of online content, which helped lift the bar of our overall visual storytelling. We had millions of pageviews and our data journalism included some of the Telegraph's highest-ever subscribed content to date. "," There are vast arrays of technologies used to produce the live election data. Starting at the beginning with PA’s FTP server we ping it at regular intervals to keep track of the XML data files there are to process. If there are new files, we download, convert to JSON and then process in one of our 6 serverless AWS lambda functions we have for the general elections data. Keeping track of all these data files is achieved by storing the file name and modified date into a cache, we use Redis. The processed JSON files are also stored on an AWS S3 bucket for future reference.   All the data we process from PA automatically populates a programmatically generated Google Spreadsheet that allows our Data Journalism team to analyze and pluck out data that can drive live analysis.   We then download the data from the same Google Spreadsheet using another serverless function, which this time will have the Telegraph’s picks and comments added. Using this data we generate JSON data files, upload them to our public facing server and then invalidate our CDN cache to make sure the public see that latest results as soon as we upload them. The results page was created using our internally developed “Atoms” editor (a google doc’s style collaborative web page creator) in conjunction with technologies such as React to build each component on the page individually.   The whole tech stack was created using, AWS lambda function built in Node JS, AWS elasticache (redis), AWS Simple Storage Solution, AWS CloudFront, Adobe AEM, React js, Google Spreadsheets, Slack (for internal notifications of result and service status). The process from PA uploading new data to us then processing and displaying the information never took longer than 15 seconds. "," The hardest part of this project was balancing the needs of numeric automation and human-generated analysis. Our overnight team had a lot of other work to be getting on with, such as a forecaster and a score of analysis pieces, and so it would ideally be the case that the results page powered itself. At the same time, we needed this page to read as a story in its own right, as our readers don’t just want to see a list of graphics without any explanation. The answer here came from old-fashioned hard work and collaboration with design and development teams, to ensure journalists could report on and analyse the results in a bespoke CMS that updated graphics in real time with zero production delay. Although a large organisation, the Data Journalism team at The Telegraph consisted of two full time journalists and one fixed-term contractor. It’s hard to staff up during a 24-hour results cycle and meet such extremely ambitious targets, and so our accurate and innovative coverage relied upon results graphics automation that still had stories at the heart of them. "," In our opinion, election results pages shouldn’t be a data dashboard. While our data journalists, and the data journalism community as a whole, are very excited about all the numbers of election day, there is a difference between this interest and the interest of our readership. They want to know who won, why and what happens next - all when usually reading on a mobile phone while sleep-deprived.   A results page should be built to answer these questions in the most simple, comprehensible way, and our performance metrics prove that focusing on these more narrative-drive visualisations will lead to success. Graphics such as our Brexit histogram, line charts of key constituencies and a cartogram to show geographic distribution helped show this narrative. Each were used in multiple other stories aside from the results page as well, improving the impact of each individual component on the page. ",https://www.telegraph.co.uk/news/general-election-results-2019-maps-breakdown-constituency/,https://www.telegraph.co.uk/politics/2019/12/13/general-election-analysis-maps-charts-show-boris-johnson-won/,https://www.telegraph.co.uk/politics/2019/12/13/general-election-results-2019-live-boris-johnson-conservatives/,https://twitter.com/Ashley_J_Kirk/status/1205307471331516418,https://twitter.com/Ashley_J_Kirk/status/1205325253800529927,,,"Ashley Kirk, Richard Moynihan, Oliver Edgington, Bruno Riddy, George Ioakeimidis, Abby Rose, Ed Goldman"," The 2019 General election page was designed, edited and produced by Ashley Kirk (senior data journalist), Richard Moynihan (head of digital journalism), Oliver Edgington (head of editorial development), Bruno Riddy (senior developer), George Ioakeimidis (senior developer), Abby Rose (designer) and Ed Goldman (designer). ",,,
United Kingdom,The Telegraph,Big,Participant,Innovation (small and large newsrooms),Telegraph live General Election forecaster,12/12/19,"Breaking news,Elections,Politics","AI/Machine learning,D3.js,R"," Our machine-learning General Election predictor - unmatched by any other publisher - was borne out of collaboration with our data science and graphics teams. To coincide with the release of the official 10pm exit, we generated our own accurate seat-by-seat forecast of the results. Our forecasting model was closer to the final outcome than the exit poll, and that Labour’s vote share would hold up slightly better in certain areas. This was because of the wealth of data it took in: the last five national elections; demographic data and the latest polling data. ​ "," Our forecaster’s success lies not in the fact that complex methods were used, but that they were communicated in an accessible fashion - we feel the pursuit of this clarity is just as important as the pursuit of technological development. This helped drive the impact of this project. On social media, animated gifs were used to increase the impact of what some would see as quite a dull, numbers-heavy topic (<a href=""https://twitter.com/Telegraph/status/1205276976396873728"">https://twitter.com/Telegraph/status/1205276976396873728</a>).   Our first forecast had the Conservatives on 363 and Labour on 201 (which you can see from 10.23PM on the night here https://twitter.com/Ashley_J_Kirk/status/1205251919452786688). This was just two seats short for both major parties. Our very first forecast correctly called some of the biggest results of the night, such as that Jo Swinson would lose her seat, Luciana Berger and Chuka Umunna would fail to take Remain-London seats, and Dominic Raab would cling on despite a Lib Dem surge. The BBC's exit poll originally put the Conservatives on 368 and Labour on 191, so we were closer to the final result (BBC +3 for Con, we were -2; BBC -12 for Lab, we were -2).    The impact of this was that Telegraph readers were able to see who their own area was likely to be represented by even before the results had come in. Our metrics showed that this piece of data journalism is something that they were interested in, with the piece being one of the most subscribed and viewed pieces on results night. ​ "," Our general election predictor model was the most complex piece we’ve produced. The data used to power the model was a mixture of demographic data - obtained through various governmental departments and the 2011 Census - and historic polling figures compiled from various polling companies. We used this information to power a model - built in R - which predicted the votes for each party in each constituency.   This model would run machine learning on the last five national elections and their corresponding demographic data to find out what demographic data is the most important in determining how a constituency voted - with factors such as car ownership being surprising determinants which we wouldn’t have otherwise realised. This data was then combined with the latest regional polling and the exit poll to produce our own prediction of the final result.    We had great success with the model, with our forecast correctly calling over nine in 10 seats and getting closer to the final result than our competitors. "," The hardest part of forecasting elections is knowing whether or not your model works, and then persuading stakeholders within the newsroom to take the risk on new technology. The only real test of it comes once people have cast their vote - which is a highly unpredictable thing. At this point, it’s too late to change your model without incurring flaws in your methodology. So we had to persuade editors, many of whom did not fully understand the technologies involved, to take a risk.   The key with persuading editors and stakeholders at The Telegraph to take the risk with machine learning was not by showing them the exciting technologies involved, but was to show them results. The fact that we are producing exclusive news stories and visual projects on a weekly basis helps here. We have built a solid reputation within the newsroom, and so we are trusted when we pitch an out-of-the-box idea.    This meant that, when we said that we had a model that could correctly predict the outcome of the last four general elections, and would likely do the same in 2019 with engaging, subscriber-driving visualisations, they jumped at the idea. This meant that we were able to take our first steps in machine learning, an area that we hope to do a lot more in in the coming years. ​ "," From this project we have learned that, even in a small data journalism team where resources are tight, you can still be rewarded by taking risks and doing something out of your comfort zone. The Telegraph’s Data Journalism desk is consistently centred on either delivering exclusive news stories for the publication or producing visually-compelling pieces of journalism, but machine learning was time-intensive risk for us. The collaboration was weeks in the making, pulling together designs and data scientists together. The differing workflows involved with these different teams, who work in different parts of the company, meant that flexibility was required from all parts. But the risk and invested time was worth it, with our highly-accurate forecaster being one of The Telegraph’s best-performing pieces of online content on the 2019 General Election. ",https://www.telegraph.co.uk/politics/2019/12/12/follow-telegraphs-live-2019-general-election-seat-seat-forecast/,https://twitter.com/Ashley_J_Kirk/status/1205251919452786688,https://twitter.com/Telegraph/status/1205276976396873728,,,,,"Ashley Kirk, Patrick Scott, Davoud Ardali, Yves Jacquot, Sam Moppett, Josh Wilson"," This piece was produced by Telegraph data journalists Patrick Scott, Ashley Kirk and Josh Wilson. They produced the piece and built the interactive visualisations. They also  in collaboration with data scientists Davoud Ardali and Yves Jacquot, in order to build the forecasting model and use machine learning to work out the most important factors in deciding how a constituency votes. Designer Sam Moppett also helped with the illustrations used in the graphics, in order to make the article more engaging.  ​ ",,,
Bosnia and Herzegovina,Balkan Investigative Reporting Network in Bosnia and Herzegovina (BIRN BiH),Small,Shortlist,Open data,Database of Governments Officials Limos,18/01/19,"Database,Open data,Corruption","JQuery,Node.js"," “ Bosnian institutions and public companies last year launched tenders to procure 1,666 official vehicles, worth about 46 million euros, BIRN’s new database shows.”    This lede, which was published in January 2019 on BIRN’s flagship website, announced the publication of data on all funds spent on vehicles for government officials, institutions and public companies in one place.    The investigation and database offer a live, one-of-a-kind tool which ensures transparency of public spending. They have already proved to be a valuable anti-corruption tool, revealing that in 80 per cent of all tenders for vehicles, only one company applies for the procurement. "," The impact of BIRN’s database has been multifaceted. During the course of its existence, various posts, tenders and articles published on the database were republished a total of more than 250 times by domestic and regional media – showing an immense interest in the content.    Along with direct users of the database, which numbered tens of thousands, the database also garnered significant amounts of posts on social media. In some instances, our posts on social media were reworked into articles by other media. One of our posts was about how we used the keyword “ISOFIX” in the database to reveal all the car purchases in which a child car seat was requested - in order to show that these cars will definitely be used for private purposes - and posted the link to the results on twitter. Our post simply provided the link to the database, in which 11 such cars with child seat requests were listed. Several media outlets then took this post and wrote long-form articles about the phenomenon.    Along with republications and our direct engagement with the general readership, a specific part of our audience was the Public Procurement Agency, which is in charge of the tenders, in order to stop illegal work.    As previously mentioned, we marked certain tenders with a RED FLAG. In dozens of cases, these tenders were amended after BIRN Bosnia wrote about them, or flagged them, or asked institutions questions about them. This is a very direct way in which BIRN's work stopped potentially corrupt tenders, and saved public funds through ensuring the transparency of spending.   It becase for media in Bosnia and Herzegovine ""only relevant source of information for Governments Officials Limos in BiH"", as quoted. ", Both the Administration and the Presentation applications are hosted on Windows Server 2012. The applications are written in the C Sharp (C#) programming language and rely on Microsoft SQL Server 2008 R2 for the structured data content.   For the visual design and the front-end of both applications BIRN used:   - Bootstrap 4   - Material icons   - gulp.js   - PostCSS   - Node.js   - npm   - jQuery   - IBM Plex Sans Condensed ," Ordinary citizens do not have full access to public procurement portal. In order to see all data you need to register. BIRN registered and downloaded all tenders throughout the year for vehicle purchases and updated them daily on the database, providing daily transparency on all public spending on vehicles.    In order to show the amount of money spent in general, and not just for one cost type, the database had two other subsections. First, the register of vehicles, which showed what vehicles a certain company or public institution already owns. The second subsection is the expenses part, which has information about other expenses related to cars – such as petrol, parking, repairs and insurance.    When a tender for purchasing a car is published, BIRN journalists first put it in the database. Then they search through the public procurement website for the latest tender for insurance – which each institution or public company signs at the start of the year, and in which this institution or company lists all the assets it wants to insure. In this document, BIRN journalists find data about all vehicles already owned and the estimated cost. All these cars are then put in the register section.   Finally, BIRN journalists go through the data for the entire year for all tenders for expenses related to cars for each institution and put them in the third subsection. When this is completed, a certain company or public institution which is buying new cars also has two other subsections in which citizens can see all other information which might interest them. They will see if the car is a luxury one or not, and how much money is being spent on repairs, washing, parking, petrol and insurance for each institution’s fleet of cars. "," BIRN Bosnia and Herzegovina’s database is the first live database published online in the country, one that is updated daily and in which data changes in real time. BIRN journalists each day insert the new tenders and decisions, which provides a high level of transparency about public spending.    Furthermore the database is the first completely connected database in Bosnia and Herzegovina in which all entities are linked and searchable. Each entity in the database has its own subpage and can be compared and linked to others. It allows a much deeper search option then other journalistic and official databases.    During our research, we did not find any similar database worldwide on official cars or vehicles. For the first time in Bosnia and Herzegovina, data from public procurement has been presented in a user-friendly way, creating a tool that can be used by other journalists and researchers for further research.    One other aspect of this database is its goal to make public procurements more transparent by publishing documents otherwise only available to private companies that are registered with the official public procurement portal.    Finally, the database also seeks to maximise the impact of BIRN’s investigative work, by having a dedicated space for articles and investigations about the public procurement of vehicles. A video section is also included in which BIRN journalists used mobile journalism techniques to create shorter videos for social media to further boost impact. ",auta.detektor.ba,http://detektor.ba/en/bosnia-blows-millions-of-euros-on-official-limos/,https://bird.tools/how_to/how-to-make-database-of-governments-officials-limos/,,,,,"Semir Mujkić, Denis Džidić","<h1>Denis Dzidic</h1>  BIRN BiH Executive Director and Editor   Denis Dzidic is the Executive Director and Editor for BIRN Bosnia and Herzegovina. He joined BIRN in August 2008, and after working as a journalist, deputy editor and chief editor for BIRN Bosnia and Herzegovina’s flagship website Detektor.ba he has been named Executive Director as of October 1, 2019.   Before BIRN, Denis worked as a journalist for Oslobodjenje daily newspaper and the Institute for War and Peace Reporting both in Sarajevo and in Hague where he reported on transitional justice issues and war crimes.   Denis has worked as a journalist since 2006. He graduated from the Journalism Department of the Faculty of Political Science in Sarajevo, and he holds a master in International law.  <h1>Semir Mujkic</h1>  Detektor Managing Editor   Semir Mujkic is managing editor for BIRN Bosnia and Herzegovina. He joined BIRN in April 2017 and has worked as a journalist and deputy editor for BIRN BiH website Detektor.ba where he is managing editor as of October 2019.   He was awarded Second prize at EU Investigative Awards in 2019 for investigative series Corruption in Public Procurements; the Best multimedia investigation into corruption by Anticorruption network in Bosnia and Herzegovina in 2018; and a UNICEF prize for journalistic contribution for the protection and promotion of child rights in print media in 2016.   Semir has attended advanced training courses in journalism, Reuters’ training of trainers and a one-month training course on economic and political reporting in London and Berlin, as well as the BIRN Summer School of Investigative Reporting.   He graduated in journalism from the Faculty of Political Sciences in Sarajevo and has worked as a journalist since 2006, for Magazine Start and Online magazine Zurnal. ",,,
"Hong Kong S.A.R., China",Bloomberg,Big,Participant,Best data-driven reporting (small and large newsrooms),Companies Around the World Hit by Hong Kong Protests,11/06/19,"Multiple-newsroom collaboration,Database,Fact-checking,Business,Economy",AI/Machine learning," The project aimed to track the impact of Hong Kong's protests on multinational corporations through analysis of corporate earnings call transcripts. Through applying a natural language processing model, we were able to find many companies beyond those that had an obvious connection with Hong Kong or a brand presence in the city, such as payments processors, recruitment consultants, chicken farms and Crocs, the shoemaker. "," The project allowed us to cut through the euphemisms and obfuscatory language used by analysts and corporate executives to find words and phrases that were loosely associated with the protests but still relevant - for example, ``the political thing'' was how one described the growing unrest. By broadening the scope of companies covered with a minimum of time spent, it allowed a few reporters to quickly grasp a large swathe of the local economy in one fell swoop - and make the case to our readers that consumption patterns in Hong Kong are shifting, brands are increasingly looking to mainland Chinese markets and the former British colony is losing its allure as a shopping destination and a jumping-off point for executives travelling to the rest of the country. "," We used our proprietary Bloomberg Streams toolkit, which uses an AI-powered natural language processing model to search through news, Twitter accounts, and corporate filings. "," Generating a meaningful AI model through our Streams tools required ingesting and tagging a lot of data - we tagged almost 3,000 entries as either relevant or not. To get the project ready in time for companies' quarterly earnings season we had to divide up the labor, and towards this end we came up with an event we dubbed Streams Factory. This required around two dozen staff to train an AI on past examples of warnings by companies at investor days and other corporate access events for investment analysts. To encourage people to show up, we bribed them with cookies.   Natural language processing is a largely unexplored field within journalism. Bloomberg has vast amounts of data, but making sense of the deluge is nearly impossible for a single reporter. By finding a strategy that works, we now have a replicable model for future stories that we can deploy covering everything from self-driving car crashes to virus outbreaks in China. ","   Sometimes, successful application of technology is a labour-intensive exercise. It can help to break work down into batches; it can also help to have assistance from colleagues in other countries to advance the project when colleagues in another location are asleep. ",https://www.bloomberg.com/news/articles/2019-11-04/companies-around-the-world-get-hit-by-hong-kong-protests,,,,,,,"Gregory Turk, Gregor Stuart Hunter"," The project was lead by reporter Gregor Stuart Hunter and data-driven news automation editor Gregory Turk, and also involved around a dozen other reporters and editors who helped train the AI.    Gregor Stuart Hunter is a graduate of the University of St. Andrews, UCLA and City University London, who formerly worked at the Wall Street Journal in Hong Kong and The National newspaper in Abu Dhabi. He is a CFA Charterholder   Gregory Turk has worked as a reporter and editor for more than 14 years, four of those as Bloomberg's bureau chief in Shanghai. For the past two years, he's spearheaded Bloomberg's news automation efforts in Asia. He has a master's degree in journalism from Northwestern University's Medill School of Journalism. He holds a CFA charter. ",,,
Australia,"The Age, Sydney Morning Herald, Brisbane Times, WA Today",Big,Shortlist,Best data-driven reporting (small and large newsrooms),The Invisible Crime,13/09/19,"Investigation,Long-form,Documentary,Illustration,Infographics,Chart,Video,Audio,Culture,Women,Health,Crime,Human rights","Animation,D3.js,Canvas,Adobe,Creative Suite,Google Sheets,CSV","Sexual violence in Australia is pervasive but rarely prosecuted, much less punished. Invisible Crime journalism series aims to give texture to the experiences of those who survive these crimes and a data-driven analysis of the systems that too often fail to deliver justice. With a campaign of stories that included a 13-minute documentary and 10 articles that featured in The Age, The Sydney Morning Herald, The Brisbane Times and WA Today, which included o a mix of data visualisation, multimedia and months of shoe-leather reporting; our storytelling is grounded in hard numbers, while never losing sight of the people they","The Invisible Crime has reignited a discussion about the complexities of prosecuting sexual assault cases in the Australian court system, and whether there are better ways to approach this complex issue. It revealed failures in data kept on sexual assault You can read some of the opinion pieces it spurred here and here. We worked extremely hard to ensured all survivors victims felt empowered through the re-telling of their stories. Because of our approach, survivors were proud of the result and their involvement in it. One survivor said, ""Your interest in my story along with the fact you a) believed me and b) thought it to be important enough to include has been monumental in my recovery. No one ever asks us about it. So this is just a breath of fresh air"". ""The weirdest part was reading through it all and not remembering what my pseudonym was because multiple times I thought I was reading my own story only to realise it was someone else's. Our feelings and reactions are identical in some cases. I found this sad (for them) but so, so validating."" Feedback from the legal profession has also been positive. Australian Lawyers Alliance spokesperson and lawyer Greg Barns said, ""It's the best coverage I have seen in sexual assault and the legal system. It's such a difficult issue and striking the balance is fraught but you did it."" We've received countless emails and messages from people who were sexually assaulted and felt a connection to the stories and documentary. Many of those were victims who have never disclosed to anyone before. Our project is being used as evidence in a submission to the Queensland Law Reform Commission inquiry into the mistake of fact defence, and we've been asked to attend legal services to speak about our","     A mix of data visualisation, multimedia and months of shoe-leather reporting; our storytelling is grounded in hard numbers, while never losing sight of the people they represent.    We were analytical, methodical and transparent with our approach to the data. <a href=""https://www.smh.com.au/national/invisible-crime-about-the-data-20190807-p52eo8.html"">Our data editor's comprehensive disclaime</a>r means our methodology is public and accounted for.   On a technical level this was an ambitious project. Our <a href=""https://www.smh.com.au/interactive/2019/are-we-failing-victims-of-sexual-violence-v-2/"">key data story</a> includes multiple interactive elements, built painstakingly by designer Soren Frederiksen using WebGL - a cutting-edge technology that taps into a devices' graphics processor to make modern browsers capable of more than ever before -the team was able to create visualisations that both drove the narrative and broke new ground for the newsroom. The other multimedia <a href=""https://www.smh.com.au/interactive/2019/invisible-crime-index/live/index.html?resizable=true"">'In their own words' </a>was built with the aim of giving readers one snippet of a story, with broader context as they scroll. <a href=""http://m2.smh.com.au/video/video-news/video-national-news/they-say-as-the-victim-you-get-raped-twice-once-in-the-incident-and-then-in-the-court-20190821-5fpfu"">A feature documentary</a> is also a first for our newsrooms.  ","This was an ambitious project from the outset: our aim was to collate all available data from every state and legal jurisdiction in Australia, and for the first time illustrate the journey of a survivor of sexual assault through the entire criminal justice system - from reporting all the way through to conviction. No such body of work existed in Australia before this undertaking. Reporters spent six months developing this project, which included 20 Freedom of Information and data requests. This was an extraordinarily painstaking process. Some agencies were extremely reluctant to release data, so one of the hardest parts of this project was relentlessly advocating for the release of public information on the grounds that it was in the public interest. Once we had collected as many data sets as we could, the task was then to sort and present the data in an accurate way. Disparities in collection methods in each state made this task a very challenging one. We were meticulous with our approach, on such an important subject we chose not to make any assumptions in our collating o As well as being technically difficult, the subject matter itself made this a tough assignment. Our team conducted more than 30 hours of in-depth interviews with survivors, police, prosecutors, defence lawyers and judges. All story subjects must be treated with respect and fairness, but for survivors of sexual assault, our duty of care was of a much higher standard. It took a great deal of time to build up trust with each of our case studies - meeting them on multiple occasions and spending time with them completely off-the-record before a microphone or a camera ever entered the room. Each person was at a different stage of their recovery, and this required on-going emotional support and follow"," Our groundbreaking project, the first and biggest of its kind in Australia on such a sensitive and important issue, revealed inadequacies in our justice system. We aimed to reveal how you can connect data with real people's stories in an engaging and powerful way.    We believe other newsrooms can and should adopt a data driven approach to issues that are sensitive and often become the fodder for the culture wars. By grounding the reporting at each stage to hard facts and figures, it gave us a platform of legitimacy and authority to highight issues in the system in a practical way.    Through  The Invisible Crime  we revealed failures in policing when it comes to reporting crime, high rates of reports being withdrawn, and a lack of training for officers. We revealed the complexities of the judicial system, unable to cope with a crime so private, often with such little evidence. It also revealed the huge cultural shift needed in regards to consent education  new messages.    We would hope that in the future, governments and agencies put more effort and funding into high quality data collection. Through our Freedom of Information and Right to Information requests process, we realised just how archaic most data collection is for courts. Data can be patchy, and non-existent in some cases. Some files are sitting in warehouses in hard copy, unable to be accessed by the public or journalists on their behalf. This is an unacceptable status quo - for without data on the basic performance of our public institutions, there cannot be public trust in their efficacy and there is also no data on which to advocate for policy changes. ",https://www.smh.com.au/invisible-crime,http://m2.smh.com.au/video/video-news/video-national-news/they-say-as-the-victim-you-get-raped-twice-once-in-the-incident-and-then-in-the-court-20190821-5fpfu,https://www.smh.com.au/interactive/2019/are-we-failing-victims-of-sexual-violence-v-2/,https://www.smh.com.au/national/invisible-crime-about-the-data-20190807-p52eo8.html,https://www.smh.com.au/national/victoria/why-there-are-more-men-than-women-on-juries-20190821-p52jer.html,https://www.smh.com.au/interactive/2019/invisible-crime-index/live/index.html?resizable=true,https://www.smh.com.au/national/sexual-consent-why-no-is-not-enough-in-some-states-20190805-p52e1p.html,"Nicole Precel, Eleanor Marsh, Rachael Dexter, Soren Frederiksen, Craig Butt","  The Invisible Crime  team was an enormous cross platform project that was contributed to by dozens of people including designers, photographers, animators, editors, product development staff who are all listed below.   At it's core, the project was imagined, created and driven by a core team of five journalists: <a href=""https://www.smh.com.au/by/nicole-precel-gxzx81"">Nicole Precel (Journalist & Video Audio Producer)</a>, <a href=""https://www.smh.com.au/by/rachael-dexter-h01q8v"">Rachael Dexter (Journalist & Video Audio Producer)</a>, <a href=""https://www.smh.com.au/by/eleanor-marsh-h1gj2t"">Eleanor Marsh (Journalist & Video Audio Producer)</a>, <a href=""https://www.smh.com.au/by/craig-butt-hvf8q"">Craig Butt (Data journalist)</a> and<a href=""https://www.smh.com.au/by/soren-frederiksen-h0fwvz""> Soren Frederiksen (Full stack developer working on news coverage).</a>   This talented young team has in various forms worked together on award-winning data driven projects in past. Utilsing everyone's unique skills, the team will scrape and organise data sets, interrogate them and then creatively present and concisely report it's implications through text, infographics and video.     FULL CREDIT LIST FOR  THE INVISIBLE CRIME     Reporters:  Nicole Precel, Rachael Dexter and Eleanor Marsh   Data editor:  Craig Butt   Developers:  Soren Frederiksen and Reginal Sengkey   Design direction:  Mark Stehle   Video animator:  Kelly Bergsma   Video shooters & editors : Eleanor Marsh, Margaret Gordon, Kelly Bergsma and Cormac Lally   Photographers:  Simon Schluter, Justin McManus and Elle Marsh   Illustrations:  Jo Gay   Editors:  Margaret Easterbook, Tom McKendrick, Richard Hughes, Felicity Lewis, Maher Mugrabi   Product Design:  Adam Stone   Product Development:  Caleb Wong and Rebekah Chow ",,,
Australia,"The Age, The Sydney Morning Herald, Brisbane Times, WAToday",Big,Participant,Innovation (small and large newsrooms),The Invisible Crime,13/09/19,"Investigation,Explainer,Solutions journalism,Long-form,Documentary,Open data,Illustration,Infographics,Chart,Video,Audio,Culture,Women,Health,Crime,Human rights","D3.js,Canvas,Adobe,Creative Suite,Google Sheets,CSV"," Sexual violence in Australia is pervasive, rarely prosecuted and much less punished. The Invisible Crime journalism series aims to do two things: to give texture to the experiences of those who survive these crimes, and give a data-driven analysis of the criminal justice systems, revealing systemic failures.   Our campaign of stories included a 13-minute documentary and 10 articles that featured in The Age, The Sydney Morning Herald, The Brisbane Times and WA Today.<a href=""http://www.smh.com.au/interactive/2019/are-we-failing-victims-of-sexual-violence-v-2/""> Our pinnacle online  piece included our main </a><a href=""https://www.smh.com.au/interactive/2019/are-we-failing-victims-of-sexual-violence-v-2/"">interactive digital data visualisation</a>.  ","  The Invisible Crime  has pinpointed the complexities of prosecuting sexual assault cases in the Australian court systems and reignited calls for change. It revealed failures in recording and retention of data by police and the courts. It prompted pubic commentary about how victims have been failed, and how the justice system is ill-equipped to deal with such private crimes. Read topinion pieces<a href=""https://www.smh.com.au/national/path-through-criminal-justice-system-is-an-unnecessary-test-of-endurance-20190819-p52ijy.html""> here</a> and<a href=""https://www.smh.com.au/national/everyone-should-have-right-to-trial-by-judge-not-jury-20190821-p52j9i.html""> here.</a>   It's the first time in Australia anyone has mapped survivors journey through the justice system. We worked hard to ensured all survivors felt empowered through the re-telling of their stories. Because of our approach, survivors were proud of their involvement.   One survivor said, "" Your interest in my story along with the fact you a) believed me and b) thought it to be important enough to include has been monumental in my recovery. No one ever asks us about it. So this is just a breath of fresh air"".     ""The weirdest part was reading through it all and not remembering what my pseudonym was because multiple times I thought I was reading my own story. Our feelings and reactions are identical in some cases. I found this sad (for them) but so, so validating.""    Legal professionals have echoed their support. Australian Lawyers Alliance spokesperson Greg Barns said, ""It's the best coverage I have seen in sexual assault and the legal system. It's such a difficult issue and striking the balance is fraught but you did it.""   We've received countless messages from people who were sexually assaulted and thanked us. Many were victims who have never disclosed to anyone before.   It is being used as evidence in a submission to the Queensland Law Reform Commission inquiry into the mistake of fact defence, and we've been asked to attend legal services to speak about our project. ","  A mix of data visualisation, multimedia and months of shoe-leather reporting; our storytelling is grounded in hard numbers, while never losing sight of the people they represent.   We were analytical, methodical and transparent with our approach to the data.<a href=""https://www.smh.com.au/national/invisible-crime-about-the-data-20190807-p52eo8.html""> Our data editor’s comprehensive disclaime</a>r means our methodology is public and accounted for.   On a technical level this was an ambitious project. Our<a href=""https://www.smh.com.au/interactive/2019/are-we-failing-victims-of-sexual-violence-v-2/""> key data story</a> includes multiple interactive elements, built painstakingly by designer Soren Frederiksen using WebGL - a cutting-edge technology that taps into a devices' graphics processor to make modern browsers capable of more than ever before -the team was able to create visualisations that both drove the narrative and broke new ground for the newsroom. The other multimedia<a href=""https://www.smh.com.au/interactive/2019/invisible-crime-index/live/index.html?resizable=true""> 'In their own words'</a> was built with the aim of giving readers one snippet of a story, with broader context as they scroll.<a href=""http://m2.smh.com.au/video/video-news/video-national-news/they-say-as-the-victim-you-get-raped-twice-once-in-the-incident-and-then-in-the-court-20190821-5fpfu""> A feature documentary</a> is also a first for our newsrooms. "," This was an ambitious project from the outset: our aim was to collate all available data from every state and legal jurisdiction in Australia, and for the first time illustrate the journey of a survivor of sexual assault through the entire criminal justice system - from reporting all the way through to conviction. No such body of work existed in Australia before this undertaking.   Reporters spent more than six months developing this project, which included 20 Freedom of Information and data requests. This was an extraordinarily painstaking process. Some agencies were extremely reluctant to release data, so one of the hardest parts of this project was relentlessly advocating for the release of public information on the grounds that it was in the public interest. Once we had collected as many data sets as we could, the task was then to sort and present the data in an accurate way. Disparities in collection methods in each state made this task a very challenging one. We were meticulous with our approach, on such an important subject we chose not to make any assumptions in collating d   As well as being technically difficult, it was also emotionally taxing. Our team conducted more than 30 hours of in-depth interviews with survivors, police, prosecutors, defence lawyers and judges. All story subjects must be treated with respect and fairness, but for survivors of sexual assault, our duty of care was of a much higher standard. It took a great deal of time to build up trust with each of our case studies - meeting them on multiple occasions and spending time with them completely off-the-record before a microphone or a camera ever entered the room. Each person was at a different stage of their recovery, and this required on-going emotional support and follow up. "," Our groundbreaking project, the first and biggest of its kind in Australia on such a sensitive and important issue, revealed inadequacies in our justice system. We aimed to reveal how you can connect data with real people's stories in an engaging and powerful way.   We believe other newsrooms can and should adopt a data driven approach to issues that are sensitive and often become the fodder for the culture wars. By grounding the reporting at each stage to hard facts and figures, it gave us a platform of legitimacy and authority to highight issues in the system in a practical way.   Through The Invisible Crime we revealed failures in policing when it comes to reporting crime, high rates of reports being withdrawn, and a lack of training for officers. We revealed the complexities of the judicial system, unable to cope with a crime so private, often with such little evidence. It also revealed the huge cultural shift needed in regards to consent education messages.   We would hope that in the future, governments and agencies put more effort and funding into high quality data collection. Through our Freedom of Information and Right to Information requests process, we realised just how archaic most data collection is for courts. Data can be patchy, and non-existent in some cases. Some files are sitting in warehouses in hard copy, unable to be accessed by the public or journalists on their behalf. This is an unacceptable status quo - for without data on the basic performance of our public institutions, there cannot be public trust in their efficacy and there is also no data on which to advocate for policy changes. ",https://www.smh.com.au/invisible-crime,m2.smh.com.au/video/video-news/video-national-news/they-say-as-the-victim-you-get-raped-twice-once-in-the-incident-and-then-in-the-court-20190821-5fpfu,https://www.smh.com.au/interactive/2019/are-we-failing-victims-of-sexual-violence-v-2/,https://www.smh.com.au/interactive/2019/invisible-crime-index/live/index.html?resizable=true,https://www.smh.com.au/national/invisible-crime-about-the-data-20190807-p52eo8.html,https://www.smh.com.au/national/victoria/why-there-are-more-men-than-women-on-juries-20190821-p52jer.html,https://www.smh.com.au/national/sexual-consent-why-no-is-not-enough-in-some-states-20190805-p52e1p.html,"Nicole Precel, Rachael Dexter, Eleanor Marsh, Soren Frederiksen, Craig Butt"," The Invisible Crime team was an enormous cross platform project that was contributed to by dozens of people including designers, photographers, animators, editors, product development staff who are all listed below.   At it's core, the project was imagined, created and driven by a core team of five journalists:<a href=""https://www.smh.com.au/by/nicole-precel-gxzx81""> Nicole Precel (Journalist & Video Audio Producer)</a>,<a href=""https://www.smh.com.au/by/rachael-dexter-h01q8v""> Rachael Dexter (Journalist & Video Audio Producer)</a>,<a href=""https://www.smh.com.au/by/eleanor-marsh-h1gj2t""> Eleanor Marsh (Journalist & Video Audio Producer)</a>, <a href=""https://www.smh.com.au/by/craig-butt-hvf8q"">Craig Butt (Data journalist)</a> and<a href=""https://www.smh.com.au/by/soren-frederiksen-h0fwvz""> Soren Frederiksen (Full stack developer working on news coverage).</a>   This talented young team has in various forms worked together on award-winning data driven projects in the past. Utilising everyone's unique skills, the team will scrape and organise data sets, interrogate them and then creatively present and concisely report it's implications through text, infographics and video.    FULL CREDIT LIST FOR THE INVISIBLE CRIME     Reporters:  Nicole Precel, Rachael Dexter and Eleanor Marsh    Data editor:  Craig Butt    Developers:  Soren Frederiksen and Reginal Sengkey    Design direction:  Mark Stehle    Video animator:  Kelly Bergsma    Video shooters & editors:  Eleanor Marsh, Margaret Gordon, Kelly Bergsma and Cormac Lally    Photographers:  Simon Schluter, Justin McManus and Elle Marsh    Illustrations:  Jo Gay    Editors:  Margaret Easterbook, Tom McKendrick, Richard Hughes, Felicity Lewis, Maher Mugrabi    Product Design:  Adam Stone    Product Development:  Caleb Wong and Rebekah Chow ",,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,